
@Article{rs12071125,
AUTHOR = {Farhood, Helia and Perry, Stuart and Cheng, Eva and Kim, Juno},
TITLE = {Enhanced 3D Point Cloud from a Light Field Image},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1125},
URL = {https://www.mdpi.com/2072-4292/12/7/1125},
ISSN = {2072-4292},
ABSTRACT = {The importance of three-dimensional (3D) point cloud technologies in the field of agriculture environmental research has increased in recent years. Obtaining dense and accurate 3D reconstructions of plants and urban areas provide useful information for remote sensing. In this paper, we propose a novel strategy for the enhancement of 3D point clouds from a single 4D light field (LF) image. Using a light field camera in this way creates an easy way for obtaining 3D point clouds from one snapshot and enabling diversity in monitoring and modelling applications for remote sensing. Considering an LF image and associated depth map as an input, we first apply histogram equalization and histogram stretching to enhance the separation between depth planes. We then apply multi-modal edge detection by using feature matching and fuzzy logic from the central sub-aperture LF image and the depth map. These two steps of depth map enhancement are significant parts of our novelty for this work. After combing the two previous steps and transforming the point&ndash;plane correspondence, we can obtain the 3D point cloud. We tested our method with synthetic and real world image databases. To verify the accuracy of our method, we compared our results with two different state-of-the-art algorithms. The results showed that our method can reliably mitigate noise and had the highest level of detail compared to other existing methods.},
DOI = {10.3390/rs12071125}
}



@Article{rs12071145,
AUTHOR = {Kislov, Dmitry E. and Korznikov, Kirill A.},
TITLE = {Automatic Windthrow Detection Using Very-High-Resolution Satellite Imagery and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1145},
URL = {https://www.mdpi.com/2072-4292/12/7/1145},
ISSN = {2072-4292},
ABSTRACT = {Wind disturbances are significant phenomena in forest spatial structure and succession dynamics. They cause changes in biodiversity, impact on forest ecosystems at different spatial scales, and have a strong influence on economics and human beings. The reliable recognition and mapping of windthrow areas are of high importance from the perspective of forest management and nature conservation. Recent research in artificial intelligence and computer vision has demonstrated the incredible potential of neural networks in addressing image classification problems. The most efficient algorithms are based on artificial neural networks of nested and complex architecture (e.g., convolutional neural networks (CNNs)), which are usually referred to by a common term&mdash;deep learning. Deep learning provides powerful algorithms for the precise segmentation of remote sensing data. We developed an algorithm based on a U-Net-like CNN, which was trained to recognize windthrow areas in Kunashir Island, Russia. We used satellite imagery of very-high spatial resolution (0.5 m/pixel) as source data. We performed a grid search among 216 parameter combinations defining different U-Net-like architectures. The best parameter combination allowed us to achieve an overall accuracy for recognition of windthrow sites of up to 94% for forested landscapes by coniferous and mixed coniferous forests. We found that the false-positive decisions of our algorithm correspond to either seashore logs, which may look similar to fallen tree trunks, or leafless forest stands. While the former can be rectified by applying a forest mask, the latter requires the usage of additional information, which is not always provided by satellite imagery.},
DOI = {10.3390/rs12071145}
}



@Article{agriculture10040112,
AUTHOR = {Przybylak, Andrzej and Kozłowski, Radosław and Osuch, Ewa and Osuch, Andrzej and Rybacki, Piotr and Przygodziński, Przemysław},
TITLE = {Quality Evaluation of Potato Tubers Using Neural Image Analysis Method},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {112},
URL = {https://www.mdpi.com/2077-0472/10/4/112},
ISSN = {2077-0472},
ABSTRACT = {This paper describes the research aimed at developing an effective quality assessment method for potato tubers using neural image analysis techniques. Nowadays, the methods used to identify damage and diseases are time-consuming, require specialized knowledge, and often rely on subjective judgment. This study showed the use of the developed neural model as a tool supporting the evaluation of potato tubers during the sorting process in the storage room.},
DOI = {10.3390/agriculture10040112}
}



@Article{rs12071176,
AUTHOR = {Lin, Yukun and Zhu, Zhe and Guo, Wenxuan and Sun, Yazhou and Yang, Xiaoyuan and Kovalskyy, Valeriy},
TITLE = {Continuous Monitoring of Cotton Stem Water Potential using Sentinel-2 Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1176},
URL = {https://www.mdpi.com/2072-4292/12/7/1176},
ISSN = {2072-4292},
ABSTRACT = {Monitoring cotton status during the growing season is critical in increasing production efficiency. The water status in cotton is a key factor for yield and cotton quality. Stem water potential (SWP) is a precise indicator for assessing cotton water status. Satellite remote sensing is an effective approach for monitoring cotton growth at a large scale. The aim of this study is to estimate cotton water stress at a high temporal frequency and at a large scale. In this study, we measured midday SWP samples according to the acquisition dates of Sentinel-2 images and used them to build linear-regression-based and machine-learning-based models to estimate cotton water stress during the growing season (June to August, 2018). For the linear-regression-based method, we estimated SWP based on different Sentinel-2 spectral bands and vegetation indices, where the normalized difference index 45 (NDI45) achieved the best performance (R2 = 0.6269; RMSE = 3.6802 (-1*swp (bars))). For the machine-learning-based method, we used random forest regression to estimate SWP and received even better results (R2 = 0.6709; RMSE = 3.3742 (-1*swp (bars))). To find the best selection of input variables for the machine-learning-based approach, we tried three different data input datasets, including (1) 9 original spectral bands (e.g., blue, green, red, red edge, near infrared (NIR), and shortwave infrared (SWIR)), (2) 21 vegetation indices, and (3) a combination of original Sentinel-2 spectral bands and vegetation indices. The highest accuracy was achieved when only the original spectral bands were used. We also found the SWIR and red edge band were the most important spectral bands, and the vegetation indices based on red edge and NIR bands were particularly helpful. Finally, we applied the best approach for the linear-regression-based and the machine-learning-based methods to generate cotton water potential maps at a large scale and high temporal frequency. Results suggests that the methods developed here has the potential for continuous monitoring of SWP at large scales and the machine-learning-based method is preferred.},
DOI = {10.3390/rs12071176}
}



@Article{app10072528,
AUTHOR = {Deng, Lu and Chu, Hong-Hu and Shi, Peng and Wang, Wei and Kong, Xuan},
TITLE = {Region-Based CNN Method with Deformable Modules for Visually Classifying Concrete Cracks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2528},
URL = {https://www.mdpi.com/2076-3417/10/7/2528},
ISSN = {2076-3417},
ABSTRACT = {Cracks are often the most intuitive indicators for assessing the condition of in-service structures. Intelligent detection methods based on regular convolutional neural networks (CNNs) have been widely applied to the field of crack detection in recently years; however, these methods exhibit unsatisfying performance on the detection of out-of-plane cracks. To overcome this drawback, a new type of region-based CNN (R-CNN) crack detector with deformable modules is proposed in the present study. The core idea of the method is to replace the traditional regular convolution and pooling operation with a deformable convolution operation and a deformable pooling operation. The idea is implemented on three different regular detectors, namely the Faster R-CNN, region-based fully convolutional networks (R-FCN), and feature pyramid network (FPN)-based Faster R-CNN. To examine the advantages of the proposed method, the results obtained from the proposed detector and corresponding regular detectors are compared. The results show that the addition of deformable modules improves the mean average precisions (mAPs) achieved by the Faster R-CNN, R-FCN, and FPN-based Faster R-CNN for crack detection. More importantly, adding deformable modules enables these detectors to detect the out-of-plane cracks that are difficult for regular detectors to detect.},
DOI = {10.3390/app10072528}
}



@Article{s20072069,
AUTHOR = {Feng, Chuncheng and Zhang, Hua and Wang, Haoran and Wang, Shuang and Li, Yonglong},
TITLE = {Automatic Pixel-Level Crack Detection on Dam Surface Using Deep Convolutional Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2069},
URL = {https://www.mdpi.com/1424-8220/20/7/2069},
ISSN = {1424-8220},
ABSTRACT = {Crack detection on dam surfaces is an important task for safe inspection of hydropower stations. More and more object detection methods based on deep learning are being applied to crack detection. However, most of the methods can only achieve the classification and rough location of cracks. Pixel-level crack detection can provide more intuitive and accurate detection results for dam health assessment. To realize pixel-level crack detection, a method of crack detection on dam surface (CDDS) using deep convolution network is proposed. First, we use an unmanned aerial vehicle (UAV) to collect dam surface images along a predetermined trajectory. Second, raw images are cropped. Then crack regions are manually labelled on cropped images to create the crack dataset, and the architecture of CDDS network is designed. Finally, the CDDS network is trained, validated and tested using the crack dataset. To validate the performance of the CDDS network, the predicted results are compared with ResNet152-based, SegNet, UNet and fully convolutional network (FCN). In terms of crack segmentation, the recall, precision, F-measure and IoU are 80.45%, 80.31%, 79.16%, and 66.76%. The results on test dataset show that the CDDS network has better performance for crack detection of dam surfaces.},
DOI = {10.3390/s20072069}
}



@Article{rs12071210,
AUTHOR = {Mallinis, Giorgos and Chrysafis, Irene and Korakis, Georgios and Pana, Eleanna and Kyriazopoulos, Apostolos P.},
TITLE = {A Random Forest Modelling Procedure for a Multi-Sensor Assessment of Tree Species Diversity},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1210},
URL = {https://www.mdpi.com/2072-4292/12/7/1210},
ISSN = {2072-4292},
ABSTRACT = {Earth observation data can provide important information for tree species diversity mapping and monitoring. The relatively recent advances in remote sensing data characteristics and processing systems elevate the potential of satellite imagery for providing accurate, timely, consistent, and robust spatially explicit estimates of tree species diversity over forest ecosystems. This study was conducted in Northern Pindos National Park, the largest terrestrial park in Greece and aimed to assess the potential of four satellite sensors with different instrumental characteristics, for the estimation of tree diversity. Through field measurements, we originally quantified two diversity indices, namely the Shannon diversity index (H&rsquo;) and Simpson&rsquo;s diversity (D1). Random forest regression models were developed for associating remotely sensed spectral signal with tree species diversity within the area. The models generated from the use of the WorldView-2 image were the most accurate with a coefficient of determination of up to 0.44 for H&rsquo; and 0.37 for D1. The Sentinel-2 -based models of tree species diversity performed slightly worse, but were better than the Landsat-8 and RapidEye models. The coefficient of variation quantifying internal variability of spectral values within each plot provided little or no usage for improving the modelling accuracy. Our results suggest that very-high-spatial-resolution imagery provides the most important information for the assessment of tree species diversity in heterogeneous Mediterranean ecosystems.},
DOI = {10.3390/rs12071210}
}



@Article{rs12071213,
AUTHOR = {Raza, Muhammad M. and Harding, Chris and Liebman, Matt and Leandro, Leonor F.},
TITLE = {Exploring the Potential of High-Resolution Satellite Imagery for the Detection of Soybean Sudden Death Syndrome},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1213},
URL = {https://www.mdpi.com/2072-4292/12/7/1213},
ISSN = {2072-4292},
ABSTRACT = {Sudden death syndrome (SDS) is one of the major yield-limiting soybean diseases in the Midwestern United States. Effective management for SDS requires accurate detection in soybean fields. Since traditional scouting methods are time-consuming, labor-intensive, and often destructive, alternative methods to monitor SDS in large soybean fields are needed. This study explores the potential of using high-resolution (3 m) PlanetScope satellite imagery for detection of SDS using the random forest classification algorithm. Image data from blue, green, red, and near-infrared (NIR) spectral bands, the calculated normalized difference vegetation index (NDVI), and crop rotation information were used to detect healthy and SDS-infected quadrats in a soybean field experiment with different rotation treatments, located in Boone County, Iowa. Datasets collected during the 2016, 2017, and 2018 soybean growing seasons were analyzed. The results indicate that spectral features, when combined with ground-based information, can detect areas in soybean plots that are at risk for disease, even before foliar symptoms develop. The classification of healthy and diseased soybean quadrats was &gt;75% accurate and the area under the receiver operating characteristic curve (AUROC) was &gt;70%. Our results indicate that high-resolution satellite imagery and random forest analyses have the potential to detect SDS in soybean fields, and that this approach may facilitate large-scale monitoring of SDS (and possibly other economically important soybean diseases). It may also be useful for guiding recommendations for site-specific management in current and future seasons.},
DOI = {10.3390/rs12071213}
}



@Article{s20072125,
AUTHOR = {Silveira Kupssinskü, Lucas and Thomassim Guimarães, Tainá and Menezes de Souza, Eniuce and C. Zanotta, Daniel and Roberto Veronez, Mauricio and Gonzaga, Luiz and Mauad, Frederico Fábio},
TITLE = {A Method for Chlorophyll-a and Suspended Solids Prediction through Remote Sensing and Machine Learning},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2125},
URL = {https://www.mdpi.com/1424-8220/20/7/2125},
ISSN = {1424-8220},
ABSTRACT = {Total Suspended Solids (TSS) and chlorophyll-a concentration are two critical parameters to monitor water quality. Since directly collecting samples for laboratory analysis can be expensive, this paper presents a methodology to estimate this information through remote sensing and Machine Learning (ML) techniques. TSS and chlorophyll-a are optically active components, therefore enabling measurement by remote sensing. Two study cases in distinct water bodies are performed, and those cases use different spatial resolution data from Sentinel-2 spectral images and unmanned aerial vehicles together with laboratory analysis data. In consonance with the methodology, supervised ML algorithms are trained to predict the concentration of TSS and chlorophyll-a. The predictions are evaluated separately in both study areas, where both TSS and chlorophyll-a models achieved R-squared values above 0.8.},
DOI = {10.3390/s20072125}
}



@Article{s20072126,
AUTHOR = {Barbedo, Jayme Garcia Arnal and Koenigkan, Luciano Vieira and Santos, Patrícia Menezes and Ribeiro, Andrea Roberto Bueno},
TITLE = {Counting Cattle in UAV Images—Dealing with Clustered Animals and Animal/Background Contrast Changes},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {2126},
URL = {https://www.mdpi.com/1424-8220/20/7/2126},
ISSN = {1424-8220},
ABSTRACT = {The management of livestock in extensive production systems may be challenging, especially in large areas. Using Unmanned Aerial Vehicles (UAVs) to collect images from the area of interest is quickly becoming a viable alternative, but suitable algorithms for extraction of relevant information from the images are still rare. This article proposes a method for counting cattle which combines a deep learning model for rough animal location, color space manipulation to increase contrast between animals and background, mathematical morphology to isolate the animals and infer the number of individuals in clustered groups, and image matching to take into account image overlap. Using Nelore and Canchim breeds as a case study, the proposed approach yields accuracies over 90% under a wide variety of conditions and backgrounds.},
DOI = {10.3390/s20072126}
}



@Article{ijgi9040238,
AUTHOR = {Xu, Zhiqiang and Chen, Yumin and Yang, Fan and Chu, Tianyou and Zhou, Hongyan},
TITLE = {A Postearthquake Multiple Scene Recognition Model Based on Classical SSD Method and Transfer Learning},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {238},
URL = {https://www.mdpi.com/2220-9964/9/4/238},
ISSN = {2220-9964},
ABSTRACT = {The recognition of postearthquake scenes plays an important role in postearthquake rescue and reconstruction. To overcome the over-reliance on expert visual interpretation and the poor recognition performance of traditional machine learning in postearthquake scene recognition, this paper proposes a postearthquake multiple scene recognition (PEMSR) model based on the classical deep learning Single Shot MultiBox Detector (SSD) method. In this paper, a labeled postearthquake scenes dataset is constructed by segmenting acquired remote sensing images, which are classified into six categories: landslide, houses, ruins, trees, clogged and ponding. Due to the insufficiency and imbalance of the original dataset, transfer learning and a data augmentation and balancing strategy are utilized in the PEMSR model. To evaluate the PEMSR model, the evaluation metrics of precision, recall and F1 score are used in the experiment. Multiple experimental test results demonstrate that the PEMSR model shows a stronger performance in postearthquake scene recognition. The PEMSR model improves the detection accuracy of each scene compared with SSD by transfer learning and data augmentation strategy. In addition, the average detection time of the PEMSR model only needs 0.4565s, which is far less than the 8.3472s of the traditional Histogram of Oriented Gradient + Support Vector Machine (HOG+SVM) method.},
DOI = {10.3390/ijgi9040238}
}



@Article{jmse8040274,
AUTHOR = {Pan, Xinliang and Jiang, Tao and Zhang, Zhen and Sui, Baikai and Liu, Chenxi and Zhang, Linjing},
TITLE = {A New Method for Extracting Laver Culture Carriers Based on Inaccurate Supervised Classification with FCN-CRF},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {8},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {274},
URL = {https://www.mdpi.com/2077-1312/8/4/274},
ISSN = {2077-1312},
ABSTRACT = {Timely monitoring of marine aquaculture has considerable significance for marine ecological protection and maritime safety and security. Considering that supervised learning needs to rely on a large number of training samples and the characteristics of intensive and regular distribution of the laver aquaculture zone, in this paper, an inaccurate supervised classification model based on fully convolutional neural network and conditional random filed (FCN-CRF) is designed for the study of a laver aquaculture zone in Lianyungang, Jiangsu Province. The proposed model can extract the aquaculture zone and calculate the area and quantity of laver aquaculture net simultaneously. The FCN is used to extract the laver aquaculture zone by roughly making the training label. Then, the CRF is used to extract the isolated laver aquaculture net with high precision. The results show that the     k a p p a     coefficient of the proposed model is 0.984, the      F 1      is 0.99, and the recognition effect is outstanding. For label production, the fault tolerance rate is high and does not affect the final classification accuracy, thereby saving more label production time. The findings provide a data basis for future aquaculture yield estimation and offshore resource planning as well as technical support for marine ecological supervision and marine traffic management.},
DOI = {10.3390/jmse8040274}
}



@Article{s20082223,
AUTHOR = {El-Kadi, Omar and El-Shazly, Adel and Nassar, Khaled},
TITLE = {Robust In-Plane Structures Oscillation Monitoring by Terrestrial Photogrammetry},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2223},
URL = {https://www.mdpi.com/1424-8220/20/8/2223},
ISSN = {1424-8220},
ABSTRACT = {Oscillation monitoring commonly requires complex setups integrating various types of sensors associated with intensive computations to achieve an adequate rate of observations and accuracy. This research presents a simple, cost-effective approach that allows two-dimensional oscillation monitoring by terrestrial photogrammetry using non-metric cameras. Tedious camera calibration procedures are eliminated by using a grid target that allows geometric correction to be performed to the frame&rsquo;s region of interest at which oscillations are monitored. Region-based convolutional neural networks (Faster R-CNN) techniques are adopted to minimize the light exposure limitations, commonly constraining applications of terrestrial photogrammetry. The proposed monitoring procedure is tested at outdoor conditions to check its reliability and accuracy and examining the effect of using Faster R-CNN on monitoring results. The proposed artificial intelligence (AI) aided oscillation monitoring allowed sub-millimeter accuracy monitoring with observation rates up to 60 frames per second and gained the benefit of high optical zoom offered by market available bridge cameras to monitor oscillation of targets 100 m apart with high accuracy.},
DOI = {10.3390/s20082223}
}



@Article{rs12081294,
AUTHOR = {Miyoshi, Gabriela Takahashi and Arruda, Mauro dos Santos and Osco, Lucas Prado and Marcato Junior, José and Gonçalves, Diogo Nunes and Imai, Nilton Nobuhiro and Tommaselli, Antonio Maria Garcia and Honkavaara, Eija and Gonçalves, Wesley Nunes},
TITLE = {A Novel Deep Learning Method to Identify Single Tree Species in UAV-Based Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {1294},
URL = {https://www.mdpi.com/2072-4292/12/8/1294},
ISSN = {2072-4292},
ABSTRACT = {Deep neural networks are currently the focus of many remote sensing approaches related to forest management. Although they return satisfactory results in most tasks, some challenges related to hyperspectral data remain, like the curse of data dimensionality. In forested areas, another common problem is the highly-dense distribution of trees. In this paper, we propose a novel deep learning approach for hyperspectral imagery to identify single-tree species in highly-dense areas. We evaluated images with 25 spectral bands ranging from 506 to 820 nm taken over a semideciduous forest of the Brazilian Atlantic biome. We included in our network&rsquo;s architecture a band combination selection phase. This phase learns from multiple combinations between bands which contributed the most for the tree identification task. This is followed by a feature map extraction and a multi-stage model refinement of the confidence map to produce accurate results of a highly-dense target. Our method returned an f-measure, precision and recall values of 0.959, 0.973, and 0.945, respectively. The results were superior when compared with a principal component analysis (PCA) approach. Compared to other learning methods, ours estimate a combination of hyperspectral bands that most contribute to the mentioned task within the network&rsquo;s architecture. With this, the proposed method achieved state-of-the-art performance for detecting and geolocating individual tree-species in UAV-based hyperspectral images in a complex forest.},
DOI = {10.3390/rs12081294}
}



@Article{s20082355,
AUTHOR = {Redweik, Paula and de Sanjosé Blasco, José Juan and Sánchez-Fernández, Manuel and Atkinson, Alan D. and Martínez Corrales, Luís Francisco},
TITLE = {Tower of Belém (Lisbon)–Status Quo 3D Documentation and Material Origin Determination},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2355},
URL = {https://www.mdpi.com/1424-8220/20/8/2355},
ISSN = {1424-8220},
ABSTRACT = {The Tower of Bel&eacute;m, an early 16th century defense tower located at the mouth of the Tagus river, is the iconic symbol of Lisbon. It belongs to the Bel&eacute;m complex, classified since 1983 as a World Heritage Site by the UNESCO, and it is the second most visited monument in Portugal. On November 1st, 1755, there was a heavy earthquake in Lisbon followed by a tsunami, causing between 60,000 and 100,000 deaths. There is a possibility of a repetition of such a catastrophe, which could bring about the collapse of the structure. This was the reasoning behind the decision to evaluate the Tower of Bel&eacute;m by means of surveys using Terrestrial Laser Scanning and photogrammetry. Until now, there was no high-resolution 3D model of the interior and exterior of the tower. A complete 3D documentation of the state of the Tower was achieved with a cloud of more than 6,200 million 3D points in the ETRS89 PT-TM06 coordinate system. Additionally, measurements were made using a hyperspectral camera and a spectroradiometer to characterize the stone material used in the Tower. The result is a digital 3D representation of the Tower of Bel&eacute;m, and the identification of the quarries that may have been used to extract its stone. The work carried out combines geometrical and material analysis. The methods used may constitute a guide when documenting and intervening in similar heritage elements. Finally, the information contained therein will allow an eventual reconstruction of the Tower in the case of another catastrophe.},
DOI = {10.3390/s20082355}
}



@Article{electronics9040689,
AUTHOR = {Kouhdaragh, Vahid and Verde, Francesco and Gelli, Giacinto and Abouei, Jamshid},
TITLE = {On the Application of Machine Learning to the Design of UAV-Based 5G Radio Access Networks},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {689},
URL = {https://www.mdpi.com/2079-9292/9/4/689},
ISSN = {2079-9292},
ABSTRACT = {A groundbreaking design of radio access networks (RANs) is needed to fulfill 5G traffic requirements. To this aim, a cost-effective and flexible strategy consists of complementing terrestrial RANs with unmanned aerial vehicles (UAVs). However, several problems must be solved in order to effectively deploy such UAV-based RANs (U-RANs). Indeed, due to the high complexity and heterogeneity of these networks, model-based design approaches, often relying on restrictive assumptions and constraints, exhibit severe limitation in real-world scenarios. Moreover, design of a set of appropriate protocols for such U-RANs is a highly sophisticated task. In this context, machine learning (ML) emerges as a useful tool to obtain practical and effective solutions. In this paper, we discuss why, how, and which types of ML methods are useful for designing U-RANs, by focusing in particular on supervised and reinforcement learning strategies.},
DOI = {10.3390/electronics9040689}
}



@Article{sym12040676,
AUTHOR = {Alsharif, Mohammed H. and Kelechi, Anabi Hilary and Albreem, Mahmoud A. and Chaudhry, Shehzad Ashraf and Zia, M. Sultan and Kim, Sunghwan},
TITLE = {Sixth Generation (6G) Wireless Networks: Vision, Research Activities, Challenges and Potential Solutions},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {676},
URL = {https://www.mdpi.com/2073-8994/12/4/676},
ISSN = {2073-8994},
ABSTRACT = {The standardization activities of the fifth generation communications are clearly over and deployment has commenced globally. To sustain the competitive edge of wireless networks, industrial and academia synergy have begun to conceptualize the next generation of wireless communication systems (namely, sixth generation, (6G)) aimed at laying the foundation for the stratification of the communication needs of the 2030s. In support of this vision, this study highlights the most promising lines of research from the recent literature in common directions for the 6G project. Its core contribution involves exploring the critical issues and key potential features of 6G communications, including: (i) vision and key features; (ii) challenges and potential solutions; and (iii) research activities. These controversial research topics were profoundly examined in relation to the motivation of their various sub-domains to achieve a precise, concrete, and concise conclusion. Thus, this article will contribute significantly to opening new horizons for future research directions.},
DOI = {10.3390/sym12040676}
}



@Article{su12093501,
AUTHOR = {Lin, Mengyi and Li, Fu-Yuan and Zhou, Haibin},
TITLE = {A Research on the Combination of Oblique Photography and Mobile Applications Based on the Sustainable Development of Tourism},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {3501},
URL = {https://www.mdpi.com/2071-1050/12/9/3501},
ISSN = {2071-1050},
ABSTRACT = {Tourism is one of the world&rsquo;s fastest driving forces of economic development, playing an important role in achieving sustainable development goals. In modern society, mobile social media is a communication and decision-making platform for users and a source of big data information about travel. Obtaining and analyzing travel data can provide customer-oriented information about travel destinations and comprehensive services for both tourism operators and tourists. It has a positive impact on the sustainable development of society, economy, environment, and humanities. Starting with theoretical analysis and empirical research, this study combines social media and oblique photography, conducts a case study of the Pingtan comprehensive experimental area in China, and develops an app about online travelling to provide corresponding information for consumers&rsquo; decisions. This study also discusses the potential value of the app, i.e., assisting the development of smart travel in city, achieving sustainable development of tourism, and contributing to tourism globally.},
DOI = {10.3390/su12093501}
}



@Article{ai1020010,
AUTHOR = {Tang, Ziyang and Liu, Xiang and Chen, Hanlin and Hupy, Joseph and Yang, Baijian},
TITLE = {Deep Learning Based Wildfire Event Object Detection from 4K Aerial Images Acquired by UAS},
JOURNAL = {AI},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {166--179},
URL = {https://www.mdpi.com/2673-2688/1/2/10},
ISSN = {2673-2688},
ABSTRACT = {Unmanned Aerial Systems, hereafter referred to as UAS, are of great use in hazard events such as wildfire due to their ability to provide high-resolution video imagery over areas deemed too dangerous for manned aircraft and ground crews. This aerial perspective allows for identification of ground-based hazards such as spot fires and fire lines, and to communicate this information with fire fighting crews. Current technology relies on visual interpretation of UAS imagery, with little to no computer-assisted automatic detection. With the help of big labeled data and the significant increase of computing power, deep learning has seen great successes on object detection with fixed patterns, such as people and vehicles. However, little has been done for objects, such as spot fires, with amorphous and irregular shapes. Additional challenges arise when data are collected via UAS as high-resolution aerial images or videos; an ample solution must provide reasonable accuracy with low delays. In this paper, we examined 4K (    3840 × 2160    ) videos collected by UAS from a controlled burn and created a set of labeled video sets to be shared for public use. We introduce a coarse-to-fine framework to auto-detect wildfires that are sparse, small, and irregularly-shaped. The coarse detector adaptively selects the sub-regions that are likely to contain the objects of interest while the fine detector passes only the details of the sub-regions, rather than the entire 4K region, for further scrutiny. The proposed two-phase learning therefore greatly reduced time overhead and is capable of maintaining high accuracy. Compared against the real-time one-stage object backbone of YoloV3, the proposed methods improved the mean average precision(mAP) from     0 . 29     to     0 . 67    , with an average inference speed of 7.44 frames per second. Limitations and future work are discussed with regard to the design and the experiment results.},
DOI = {10.3390/ai1020010}
}



@Article{rs12091438,
AUTHOR = {Silva, Vanessa Sousa da and Silva, Carlos Alberto and Mohan, Midhun and Cardil, Adrián and Rex, Franciel Eduardo and Loureiro, Gabrielle Hambrecht and Almeida, Danilo Roberti Alves de and Broadbent, Eben North and Gorgens, Eric Bastos and Dalla Corte, Ana Paula and Silva, Emanuel Araújo and Valbuena, Rubén and Klauberg, Carine},
TITLE = {Combined Impact of Sample Size and Modeling Approaches for Predicting Stem Volume in Eucalyptus spp. Forest Plantations Using Field and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1438},
URL = {https://www.mdpi.com/2072-4292/12/9/1438},
ISSN = {2072-4292},
ABSTRACT = {Light Detection and Ranging (LiDAR) remote sensing has been established as one of the most promising tools for large-scale forest monitoring and mapping. Continuous advances in computational techniques, such as machine learning algorithms, have been increasingly improving our capability to model forest attributes accurately and at high spatial and temporal resolution. While there have been previous studies exploring the use of LiDAR and machine learning algorithms for forest inventory modeling, as yet, no studies have demonstrated the combined impact of sample size and different modeling techniques for predicting and mapping stem total volume in industrial Eucalyptus spp. tree plantations. This study aimed to compare the combined effects of parametric and nonparametric modeling methods for estimating volume in Eucalyptus spp. tree plantation using airborne LiDAR data while varying the reference data (sample size). The modeling techniques were compared in terms of root mean square error (RMSE), bias, and R2 with 500 simulations. The best performance was verified for the ordinary least-squares (OLS) method, which was able to provide comparable results to the traditional forest inventory approaches using only 40% (n = 63; ~0.04 plots/ha) of the total field plots, followed by the random forest (RF) algorithm with identical sample size values. This study provides solutions for increasing the industry efficiency in monitoring and managing forest plantation stem volume for the paper and pulp supply chain.},
DOI = {10.3390/rs12091438}
}



@Article{rs12091444,
AUTHOR = {Abdollahi, Abolfazl and Pradhan, Biswajeet and Shukla, Nagesh and Chakraborty, Subrata and Alamri, Abdullah},
TITLE = {Deep Learning Approaches Applied to Remote Sensing Datasets for Road Extraction: A State-Of-The-Art Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1444},
URL = {https://www.mdpi.com/2072-4292/12/9/1444},
ISSN = {2072-4292},
ABSTRACT = {One of the most challenging research subjects in remote sensing is feature extraction, such as road features, from remote sensing images. Such an extraction influences multiple scenes, including map updating, traffic management, emergency tasks, road monitoring, and others. Therefore, a systematic review of deep learning techniques applied to common remote sensing benchmarks for road extraction is conducted in this study. The research is conducted based on four main types of deep learning methods, namely, the GANs model, deconvolutional networks, FCNs, and patch-based CNNs models. We also compare these various deep learning models applied to remote sensing datasets to show which method performs well in extracting road parts from high-resolution remote sensing images. Moreover, we describe future research directions and research gaps. Results indicate that the largest reported performance record is related to the deconvolutional nets applied to remote sensing images, and the F1 score metric of the generative adversarial network model, DenseNet method, and FCN-32 applied to UAV and Google Earth images are high: 96.08%, 95.72%, and 94.59%, respectively.},
DOI = {10.3390/rs12091444}
}



@Article{drones4020018,
AUTHOR = {Gorkin, Robert and Adams, Kye and Berryman, Matthew J and Aubin, Sam and Li, Wanqing and Davis, Andrew R and Barthelemy, Johan},
TITLE = {Sharkeye: Real-Time Autonomous Personal Shark Alerting via Aerial Surveillance},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2504-446X/4/2/18},
ISSN = {2504-446X},
ABSTRACT = {While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens.},
DOI = {10.3390/drones4020018}
}



@Article{s20092721,
AUTHOR = {Khaki, Saeed and Pham, Hieu and Han, Ye and Kuhl, Andy and Kent, Wade and Wang, Lizhi},
TITLE = {Convolutional Neural Networks for Image-Based Corn Kernel Detection and Counting},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {2721},
URL = {https://www.mdpi.com/1424-8220/20/9/2721},
ISSN = {1424-8220},
ABSTRACT = {Precise in-season corn grain yield estimates enable farmers to make real-time accurate harvest and grain marketing decisions minimizing possible losses of profitability. A well developed corn ear can have up to 800 kernels, but manually counting the kernels on an ear of corn is labor-intensive, time consuming and prone to human error. From an algorithmic perspective, the detection of the kernels from a single corn ear image is challenging due to the large number of kernels at different angles and very small distance among the kernels. In this paper, we propose a kernel detection and counting method based on a sliding window approach. The proposed method detects and counts all corn kernels in a single corn ear image taken in uncontrolled lighting conditions. The sliding window approach uses a convolutional neural network (CNN) for kernel detection. Then, a non-maximum suppression (NMS) is applied to remove overlapping detections. Finally, windows that are classified as kernel are passed to another CNN regression model for finding the     ( x , y )     coordinates of the center of kernel image patches. Our experiments indicate that the proposed method can successfully detect the corn kernels with a low detection error and is also able to detect kernels on a batch of corn ears positioned at different angles.},
DOI = {10.3390/s20092721}
}



@Article{w12051369,
AUTHOR = {Jiang, Ling and Hu, Yang and Xia, Xilin and Liang, Qiuhua and Soltoggio, Andrea and Kabir, Syed Rezwan},
TITLE = {A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for Reconstructing High-Resolution Urban DEMs},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1369},
URL = {https://www.mdpi.com/2073-4441/12/5/1369},
ISSN = {2073-4441},
ABSTRACT = {The scarcity of high-resolution urban digital elevation model (DEM) datasets, particularly in certain developing countries, has posed a challenge for many water-related applications such as flood risk management. A solution to address this is to develop effective approaches to reconstruct high-resolution DEMs from their low-resolution equivalents that are more widely available. However, the current high-resolution DEM reconstruction approaches mainly focus on natural topography. Few attempts have been made for urban topography, which is typically an integration of complex artificial and natural features. This study proposed a novel multi-scale mapping approach based on convolutional neural network (CNN) to deal with the complex features of urban topography and to reconstruct high-resolution urban DEMs. The proposed multi-scale CNN model was firstly trained using urban DEMs that contained topographic features at different resolutions, and then used to reconstruct the urban DEM at a specified (high) resolution from a low-resolution equivalent. A two-level accuracy assessment approach was also designed to evaluate the performance of the proposed urban DEM reconstruction method, in terms of numerical accuracy and morphological accuracy. The proposed DEM reconstruction approach was applied to a 121 km2 urbanized area in London, United Kingdom. Compared with other commonly used methods, the current CNN-based approach produced superior results, providing a cost-effective innovative method to acquire high-resolution DEMs in other data-scarce regions.},
DOI = {10.3390/w12051369}
}



@Article{s20102778,
AUTHOR = {Azimi, Mohsen and Eslamlou, Armin Dadras and Pekcan, Gokhan},
TITLE = {Data-Driven Structural Health Monitoring and Damage Detection through Deep Learning: State-of-the-Art Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2778},
URL = {https://www.mdpi.com/1424-8220/20/10/2778},
ISSN = {1424-8220},
ABSTRACT = {Data-driven methods in structural health monitoring (SHM) is gaining popularity due to recent technological advancements in sensors, as well as high-speed internet and cloud-based computation. Since the introduction of deep learning (DL) in civil engineering, particularly in SHM, this emerging and promising tool has attracted significant attention among researchers. The main goal of this paper is to review the latest publications in SHM using emerging DL-based methods and provide readers with an overall understanding of various SHM applications. After a brief introduction, an overview of various DL methods (e.g., deep neural networks, transfer learning, etc.) is presented. The procedure and application of vibration-based, vision-based monitoring, along with some of the recent technologies used for SHM, such as sensors, unmanned aerial vehicles (UAVs), etc. are discussed. The review concludes with prospects and potential limitations of DL-based methods in SHM applications.},
DOI = {10.3390/s20102778}
}



@Article{s20102780,
AUTHOR = {Chun, Pang-jo and Yamane, Tatsuro and Izumi, Shota and Kuramoto, Naoya},
TITLE = {Development of a Machine Learning-Based Damage Identification Method Using Multi-Point Simultaneous Acceleration Measurement Results},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2780},
URL = {https://www.mdpi.com/1424-8220/20/10/2780},
ISSN = {1424-8220},
ABSTRACT = {It is necessary to assess damage properly for the safe use of a structure and for the development of an appropriate maintenance strategy. Although many efforts have been made to measure the vibration of a structure to determine the degree of damage, the accuracy of evaluation is not high enough, so it is difficult to say that a damage evaluation based on vibrations in a structure has not been put to practical use. In this study, we propose a method to evaluate damage by measuring the acceleration of a structure at multiple points and interpreting the results with a Random Forest, which is a kind of supervised machine learning. The proposed method uses the maximum response acceleration, standard deviation, logarithmic decay rate, and natural frequency to improve the accuracy of damage assessment. We propose a three-step Random Forest method to evaluate various damage types based on the results of these many measurements. Then, the accuracy of the proposed method is verified based on the results of a cross-validation and a vibration test of an actual damaged specimen.},
DOI = {10.3390/s20102780}
}



@Article{agriculture10050170,
AUTHOR = {Hong, Suk-Ju and Kim, Sang-Yeon and Kim, Eungchan and Lee, Chang-Hyup and Lee, Jung-Sup and Lee, Dong-Soo and Bang, Jiwoong and Kim, Ghiseok},
TITLE = {Moth Detection from Pheromone Trap Images Using Deep Learning Object Detectors},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {170},
URL = {https://www.mdpi.com/2077-0472/10/5/170},
ISSN = {2077-0472},
ABSTRACT = {Diverse pheromones and pheromone-based traps, as well as images acquired from insects captured by pheromone-based traps, have been studied and developed to monitor the presence and abundance of pests and to protect plants. The purpose of this study is to construct models that detect three species of pest moths in pheromone trap images using deep learning object detection methods and compare their speed and accuracy. Moth images in pheromone traps were collected for training and evaluation of deep learning detectors. Collected images were then subjected to a labeling process that defines the ground truths of target objects for their box locations and classes. Because there were a few negative objects in the dataset, non-target insects were labeled as unknown class and images of non-target insects were added to the dataset. Moreover, data augmentation methods were applied to the training process, and parameters of detectors that were pre-trained with the COCO dataset were used as initial parameter values. Seven detectors&mdash;Faster R-CNN ResNet 101, Faster R-CNN ResNet 50, Faster R-CNN Inception v.2, R-FCN ResNet 101, Retinanet ResNet 50, Retinanet Mobile v.2, and SSD Inception v.2 were trained and evaluated. Faster R-CNN ResNet 101 detector exhibited the highest accuracy (mAP as 90.25), and seven different detector types showed different accuracy and speed. Furthermore, when unexpected insects were included in the collected images, a four-class detector with an unknown class (non-target insect) showed lower detection error than a three-class detector.},
DOI = {10.3390/agriculture10050170}
}



@Article{rs12101567,
AUTHOR = {Zhang, Yishan and Wu, Lun and Ren, Huazhong and Deng, Licui and Zhang, Pengcheng},
TITLE = {Retrieval of Water Quality Parameters from Hyperspectral Images Using Hybrid Bayesian Probabilistic Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1567},
URL = {https://www.mdpi.com/2072-4292/12/10/1567},
ISSN = {2072-4292},
ABSTRACT = {The protection of water resources is of paramount importance to human beings&rsquo; practical lives. Monitoring and improving water quality nowadays has become an important topic. In this study, a novel Bayesian probabilistic neural network (BPNN) improved from ordinary Bayesian probability methods has been developed to quantitatively predict water quality parameters including phosphorus, nitrogen, chemical oxygen demand (COD), biochemical oxygen demand (BOD), and chlorophyll a. The proposed method, based on conventional Bayesian probability methods, involves feature engineering and deep neural networks. Additionally, it extracts significant information for each endmember from combinations of spectra by feature extraction, with spectral unmixing based on mathematical and statistical analysis, and calculates each of the water quality parameters. The experimental results show the great performance of the proposed model with all coefficient of determination      R 2      over 0.9 greater than the values (0.6&ndash;0.8) from conventional methods, which are greater than ordinary Bayesian probability analysis. The mean percent of absolute error (MPAE) is taken into account as an important statistical criterion to evaluate model performance, and our results show that MPAE ranges from 4% (nitrogen) to 10% (COD). The root mean squared errors (RMSEs) of phosphorus, nitrogen, COD, BOD, and chlorophyll-a (Chla) are 0.03 mg/L, 0.28 mg/L, 3.28 mg/L, 0.49 mg/L, and 0.75 &mu;g/L, respectively. In comparison with other deep learning methods, this study takes a relatively small amount of data as training data to train the proposed model and the proposed model is then tested on the same amount of testing data, achieving a greater performance. Thus, the proposed method is time-saving and more effective. This study proposes a more compatible and effective method to assist with decomposing combinations of hyperspectral signatures in order to calculate the content level of each water quality parameter. Moreover, the proposed method is practically applied to hyperspectral image data on board an unmanned aerial vehicle in order to monitor the water quality on a large scale and trace the location of pollution sources in the Maozhou River, Guangdong Province of China, obtaining well-explained and significant results.},
DOI = {10.3390/rs12101567}
}



@Article{rs12101574,
AUTHOR = {Pan, Zhuokun and Xu, Jiashu and Guo, Yubin and Hu, Yueming and Wang, Guangxing},
TITLE = {Deep Learning Segmentation and Classification for Urban Village Using a Worldview Satellite Image Based on U-Net},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1574},
URL = {https://www.mdpi.com/2072-4292/12/10/1574},
ISSN = {2072-4292},
ABSTRACT = {Unplanned urban settlements exist worldwide. The geospatial information of these areas is critical for urban management and reconstruction planning but usually unavailable. Automatically characterizing individual buildings in the unplanned urban village using remote sensing imagery is very challenging due to complex landscapes and high-density settlements. The newly emerging deep learning method provides the potential to characterize individual buildings in a complex urban village. This study proposed an urban village mapping paradigm based on U-net deep learning architecture. The study area is located in Guangzhou City, China. The Worldview satellite image with eight pan-sharpened bands at a 0.5-m spatial resolution and building boundary vector file were used as research purposes. There are ten sites of the urban villages included in this scene of the Worldview image. The deep neural network model was trained and tested based on the selected six and four sites of the urban village, respectively. Models for building segmentation and classification were both trained and tested. The results indicated that the U-net model reached overall accuracy over 86% for building segmentation and over 83% for the classification. The F1-score ranged from 0.9 to 0.98 for the segmentation, and from 0.63 to 0.88 for the classification. The Interaction over Union reached over 90% for the segmentation and 86% for the classification. The superiority of the deep learning method has been demonstrated through comparison with Random Forest and object-based image analysis. This study fully showed the feasibility, efficiency, and potential of the deep learning in delineating individual buildings in the high-density urban village. More importantly, this study implied that through deep learning methods, mapping unplanned urban settlements could further characterize individual buildings with considerable accuracy.},
DOI = {10.3390/rs12101574}
}



@Article{agronomy10050718,
AUTHOR = {Cholula, Uriel and da Silva, Jorge A. and Marconi, Thiago and Thomasson, J. Alex and Solorzano, Jorge and Enciso, Juan},
TITLE = {Forecasting Yield and Lignocellulosic Composition of Energy Cane Using Unmanned Aerial Systems},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {718},
URL = {https://www.mdpi.com/2073-4395/10/5/718},
ISSN = {2073-4395},
ABSTRACT = {Crop monitoring and appropriate agricultural management practices of elite germplasm will enhance bioenergy&rsquo;s efficiency. Unmanned aerial systems (UAS) may be a useful tool for this purpose. The objective of this study was to assess the use of UAS with true color and multispectral imagery to predict the yield and total cellulosic content (TCC) of newly created energy cane germplasm. A trial was established in the growing season of 2016 at the Texas A&amp;M AgriLife Research Center in Weslaco, Texas, where 15 energy cane elite lines and three checks were grown on experimental plots, arranged in a complete block design and replicated four times. Four flights were executed at different growth stages in 2018, at the first ratoon crop, using two multi-rotor UAS: the DJI Phantom 4 Pro equipped with RGB camera and the DJI Matrice 100, equipped with multispectral sensor (SlantRange 3p). Canopy cover, canopy height, NDVI (Normalized Difference Vegetation Index), and ExG (Excess Green Index) were extracted from the images and used to perform a stepwise regression to obtain the yield and TCC models. The results showed a good agreement between the predicted and the measured yields (R2 = 0.88); however, a low coefficient of determination was found between the predicted and the observed TCC (R2 = 0.30). This study demonstrated the potential application of UAS to estimate energy cane yield with high accuracy, enabling plant breeders to phenotype larger populations and make selections with higher confidence.},
DOI = {10.3390/agronomy10050718}
}



@Article{electronics9050832,
AUTHOR = {Li, Shuai and Sun, Kuangyuan and Luo, Yukui and Yadav, Nandakishor and Choi, Ken},
TITLE = {Novel CNN-Based AP2D-Net Accelerator: An Area and Power Efficient Solution for Real-Time Applications on Mobile FPGA},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {832},
URL = {https://www.mdpi.com/2079-9292/9/5/832},
ISSN = {2079-9292},
ABSTRACT = {Standard convolutional neural networks (CNNs) have large amounts of data redundancy, and the same accuracy can be obtained even in lower bit weights instead of floating-point representation. Most CNNs have to be developed and executed on high-end GPU-based workstations, for which it is hard to transplant the existing implementations onto portable edge FPGAs because of the limitation of on-chip block memory storage size and battery capacity. In this paper, we present adaptive pointwise convolution and 2D convolution joint network (AP2D-Net), an ultra-low power and relatively high throughput system combined with dynamic precision weights and activation. Our system has high performance, and we make a trade-off between accuracy and power efficiency by adopting unmanned aerial vehicle (UAV) object detection scenarios. We evaluate our system on the Zynq UltraScale+ MPSoC Ultra96 mobile FPGA platform. The target board can get the real-time speed of 30 fps under 5.6 W, and the FPGA on-chip power is only 0.6 W. The power efficiency of our system is 2.8&times; better than the best system design on a Jetson TX2 GPU and 1.9&times; better than the design on a PYNQ-Z1 SoC FPGA.},
DOI = {10.3390/electronics9050832}
}



@Article{s20102906,
AUTHOR = {Qi, Bing and Wen, Fuzhong and Liu, Fanming and Cheng, Jianhua},
TITLE = {Modification of MTEA-Based Temperature Drift Error Compensation Model for MEMS-Gyros},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {2906},
URL = {https://www.mdpi.com/1424-8220/20/10/2906},
ISSN = {1424-8220},
ABSTRACT = {The conventional temperature drift error (TDE) compensation model cannot decouple temperature dependence of Si-based materials because temperature correlated quantities (TCQ) have not been obtained comprehensively, and Micro-Electro-Mechanical System gyros&rsquo; (MEMS-gyros&rsquo;) environmental adaptability is reduced in diverse, complicated conditions. The study presents modification of TDE compensation model of MEMS-gyros based on microstructure thermal effect analysis (MTEA). First, Si-based materials&rsquo; temperature dependence was studied in microstructure with thermal expansion effect and TCQ that determines the structural deformation were extracted to modify the conventional model, including temperature variation and its square. Second, a precise TDE test method was formed by analyzing heat conduction process between MEMS-gyros and thermal chamber, and temperature experiments were designed and conducted. Third, the modified model&rsquo;s parameters were identified based on radical basis function artificial neural network (RBF ANN) and its performance was evaluated. Last, the conventional and modified models were compared in performance. The experimental results show MEMS-gyros&rsquo; bias stability was up to 10% of the conventional model, the temperature dependence of Si-based materials was decoupled better by the modified one and the environmental adaptability of MEMS-gyros was improved to expand their application in diverse complicated conditions.},
DOI = {10.3390/s20102906}
}



@Article{agriengineering2020019,
AUTHOR = {Deng, Xiaoling and Tong, Zejing and Lan, Yubin and Huang, Zixiao},
TITLE = {Detection and Location of Dead Trees with Pine Wilt Disease Based on Deep Learning and UAV Remote Sensing},
JOURNAL = {AgriEngineering},
VOLUME = {2},
YEAR = {2020},
NUMBER = {2},
PAGES = {294--307},
URL = {https://www.mdpi.com/2624-7402/2/2/19},
ISSN = {2624-7402},
ABSTRACT = {Pine wilt disease causes huge economic losses to pine wood forestry because of its destructiveness and rapid spread. This paper proposes a detection and location method of pine wood nematode disease at a large scale adopting UAV (Unmanned Aerial Vehicle) remote sensing and artificial intelligence technology. The UAV remote sensing images were enhanced by computer vision tools. A Faster-RCNN (Faster Region Convolutional Neural Networks) deep learning framework based on a RPN (Region Proposal Network) network and the ResNet residual neural network were used to train the pine wilt diseased dead tree detection model. The loss function and the anchors in the RPN of the convolutional neural network were optimized. Finally, the location of pine wood nematode dead tree was conducted, which generated the geographic information on the detection results. The results show that ResNet101 performed better than VGG16 (Visual Geometry Group 16) convolutional neural network. The detection accuracy was improved and reached to about 90% after a series of optimizations to the network, meaning that the optimization methods proposed in this paper are feasible to pine wood nematode dead tree detection.},
DOI = {10.3390/agriengineering2020019}
}



@Article{rs12101680,
AUTHOR = {Dai, Chenguang and Zhang, Zhenchao and Lin, Dong},
TITLE = {An Object-Based Bidirectional Method for Integrated Building Extraction and Change Detection between Multimodal Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1680},
URL = {https://www.mdpi.com/2072-4292/12/10/1680},
ISSN = {2072-4292},
ABSTRACT = {Building extraction and change detection are two important tasks in the remote sensing domain. Change detection between airborne laser scanning data and photogrammetric data is vulnerable to dense matching errors, mis-alignment errors and data gaps. This paper proposes an unsupervised object-based method for integrated building extraction and change detection. Firstly, terrain, roofs and vegetation are extracted from the precise laser point cloud, based on &ldquo;bottom-up&rdquo; segmentation and clustering. Secondly, change detection is performed in an object-based bidirectional manner: Heightened buildings and demolished buildings are detected by taking the laser scanning data as reference, while newly-built buildings are detected by taking the dense matching data as reference. Experiments on two urban data sets demonstrate its effectiveness and robustness. The object-based change detection achieves a recall rate of 92.31% and a precision rate of 88.89% for the Rotterdam dataset; it achieves a recall rate of 85.71% and a precision rate of 100% for the Enschede dataset. It can not only extract unchanged building footprints, but also assign heightened or demolished labels to the changed buildings.},
DOI = {10.3390/rs12101680}
}



@Article{en13112860,
AUTHOR = {Piccinini, Fabio and Pierdicca, Roberto and Malinverni, Eva Savina},
TITLE = {A Relational Conceptual Model in GIS for the Management of Photovoltaic Systems},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {2860},
URL = {https://www.mdpi.com/1996-1073/13/11/2860},
ISSN = {1996-1073},
ABSTRACT = {The aim of this manuscript is to define an operational pipeline of work, from data acquisition to the report creation, for the smart management of PV plants. To achieve such an ambitious result, we exploit the implementation of a conceptual model, deployed through a relational database to retrieve any kind of information related to the PV plant. The motivation that drove this research is due to the increasing construction of PV plants. In fact, following European and international investments that heavily stimulated the use of clean energy, the need to maintain PV plants in their maximum efficiency for their whole lifecycle emerged, to bring about benefits from both the ecological and the economic points of view. While the research community focuses on finding new and automatic ways to detect faults automatically, few efforts have been made considering the so-called Operation and Maintenance (O&amp;M). A relational conceptual model may facilitate the management of heterogeneous sources of information, which are common in complex PV plants. The purpose of the present study is to provide companies and insiders with a GIS-based tool to maintain the energy efficiency of a PV plant. Indeed, it is a common practice used by companies dealing with O&amp;M of PV plants to create technical reports about the health status of the plants. This operation, made manually, is very time consuming and error prone. To overcome this latter drawback, this work attempts to encourage the use of GIS in the PV plants O&amp;M, which proves to be efficient to deal with fault management and to assure a good level of energy production. The developed conceptual model, tested on two real case studies, proved to be complete, cost-effective and efficient to be replicated in other existing plants.},
DOI = {10.3390/en13112860}
}



@Article{rs12111838,
AUTHOR = {Zhang, Zhao and Flores, Paulo and Igathinathane, C. and L. Naik, Dayakar and Kiran, Ravi and Ransom, Joel K.},
TITLE = {Wheat Lodging Detection from UAS Imagery Using Machine Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1838},
URL = {https://www.mdpi.com/2072-4292/12/11/1838},
ISSN = {2072-4292},
ABSTRACT = {The current mainstream approach of using manual measurements and visual inspections for crop lodging detection is inefficient, time-consuming, and subjective. An innovative method for wheat lodging detection that can overcome or alleviate these shortcomings would be welcomed. This study proposed a systematic approach for wheat lodging detection in research plots (372 experimental plots), which consisted of using unmanned aerial systems (UAS) for aerial imagery acquisition, manual field evaluation, and machine learning algorithms to detect the occurrence or not of lodging. UAS imagery was collected on three different dates (23 and 30 July 2019, and 8 August 2019) after lodging occurred. Traditional machine learning and deep learning were evaluated and compared in this study in terms of classification accuracy and standard deviation. For traditional machine learning, five types of features (i.e. gray level co-occurrence matrix, local binary pattern, Gabor, intensity, and Hu-moment) were extracted and fed into three traditional machine learning algorithms (i.e., random forest (RF), neural network, and support vector machine) for detecting lodged plots. For the datasets on each imagery collection date, the accuracies of the three algorithms were not significantly different from each other. For any of the three algorithms, accuracies on the first and last date datasets had the lowest and highest values, respectively. Incorporating standard deviation as a measurement of performance robustness, RF was determined as the most satisfactory. Regarding deep learning, three different convolutional neural networks (simple convolutional neural network, VGG-16, and GoogLeNet) were tested. For any of the single date datasets, GoogLeNet consistently had superior performance over the other two methods. Further comparisons between RF and GoogLeNet demonstrated that the detection accuracies of the two methods were not significantly different from each other (p &gt; 0.05); hence, the choice of any of the two would not affect the final detection accuracies. However, considering the fact that the average accuracy of GoogLeNet (93%) was larger than RF (91%), it was recommended to use GoogLeNet for wheat lodging detection. This research demonstrated that UAS RGB imagery, coupled with the GoogLeNet machine learning algorithm, can be a novel, reliable, objective, simple, low-cost, and effective (accuracy &gt; 90%) tool for wheat lodging detection.},
DOI = {10.3390/rs12111838}
}



@Article{app10113953,
AUTHOR = {de la Fuente Castillo, Víctor and Díaz-Álvarez, Alberto and Manso-Callejo, Miguel-Ángel and Serradilla García, Francisco},
TITLE = {Grammar Guided Genetic Programming for Network Architecture Search and Road Detection on Aerial Orthophotography},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3953},
URL = {https://www.mdpi.com/2076-3417/10/11/3953},
ISSN = {2076-3417},
ABSTRACT = {Photogrammetry involves aerial photography of the Earth&rsquo;s surface and subsequently processing the images to provide a more accurate depiction of the area (Orthophotography). It is used by the Spanish Instituto Geogr&aacute;fico Nacional to update road cartography but requires a significant amount of manual labor due to the need to perform visual inspection of all tiled images. Deep learning techniques (artificial neural networks with more than one hidden layer) can perform road detection but it is still unclear how to find the optimal network architecture. Our main goal is the automatic design of deep neural network architectures with grammar-guided genetic programming. In this kind of evolutive algorithm, all the population individuals (here candidate network architectures) are constrained to rules specified by a grammar that defines valid and useful structural patterns to guide the search process. Grammar used includes well-known complex structures (e.g., Inception-like modules) combined with a custom designed mutation operator (dynamically links the mutation probability to structural diversity). Pilot results show that the system is able to design models for road detection that obtain test accuracies similar to that reached by state-of-the-art models when evaluated over a dataset from the Spanish National Aerial Orthophotography Plan.},
DOI = {10.3390/app10113953}
}



@Article{s20113245,
AUTHOR = {Zhang, Tianyao and Hu, Xiaoguang and Xiao, Jin and Zhang, Guofeng},
TITLE = {A Machine Learning Method for Vision-Based Unmanned Aerial Vehicle Systems to Understand Unknown Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3245},
URL = {https://www.mdpi.com/1424-8220/20/11/3245},
ISSN = {1424-8220},
ABSTRACT = {What makes unmanned aerial vehicles (UAVs) intelligent is their capability of sensing and understanding new unknown environments. Some studies utilize computer vision algorithms like Visual Simultaneous Localization and Mapping (VSLAM) and Visual Odometry (VO) to sense the environment for pose estimation, obstacles avoidance and visual servoing. However, understanding the new environment (i.e., make the UAV recognize generic objects) is still an essential scientific problem that lacks a solution. Therefore, this paper takes a step to understand the items in an unknown environment. The aim of this research is to enable the UAV with basic understanding capability for a high-level UAV flock application in the future. Specially, firstly, the proposed understanding method combines machine learning and traditional algorithm to understand the unknown environment through RGB images; secondly, the You Only Look Once (YOLO) object detection system is integrated (based on TensorFlow) in a smartphone to perceive the position and category of 80 classes of objects in the images; thirdly, the method makes the UAV more intelligent and liberates the operator from labor; fourthly, detection accuracy and latency in working condition are quantitatively evaluated, and properties of generality (can be used in various platforms), transportability (easily deployed from one platform to another) and scalability (easily updated and maintained) for UAV flocks are qualitatively discussed. The experiments suggest that the method has enough accuracy to recognize various objects with high computational speed, and excellent properties of generality, transportability and scalability.},
DOI = {10.3390/s20113245}
}



@Article{s20123336,
AUTHOR = {Tang, Ta-Wei and Kuo, Wei-Han and Lan, Jauh-Hsiang and Ding, Chien-Fang and Hsu, Hakiem and Young, Hong-Tsu},
TITLE = {Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3336},
URL = {https://www.mdpi.com/1424-8220/20/12/3336},
ISSN = {1424-8220},
ABSTRACT = {Recently, researchers have been studying methods to introduce deep learning into automated optical inspection (AOI) systems to reduce labor costs. However, the integration of deep learning in the industry may encounter major challenges such as sample imbalance (defective products that only account for a small proportion). Therefore, in this study, an anomaly detection neural network, dual auto-encoder generative adversarial network (DAGAN), was developed to solve the problem of sample imbalance. With skip-connection and dual auto-encoder architecture, the proposed method exhibited excellent image reconstruction ability and training stability. Three datasets, namely public industrial detection training set, MVTec AD, with mobile phone screen glass and wood defect detection datasets, were used to verify the inspection ability of DAGAN. In addition, training with a limited amount of data was proposed to verify its detection ability. The results demonstrated that the areas under the curve (AUCs) of DAGAN were better than previous generative adversarial network-based anomaly detection models in 13 out of 17 categories in these datasets, especially in categories with high variability or noise. The maximum AUC improvement was 0.250 (toothbrush). Moreover, the proposed method exhibited better detection ability than the U-Net auto-encoder, which indicates the function of discriminator in this application. Furthermore, the proposed method had a high level of AUCs when using only a small amount of training data. DAGAN can significantly reduce the time and cost of collecting and labeling data when it is applied to industrial detection.},
DOI = {10.3390/s20123336}
}



@Article{s20123405,
AUTHOR = {Bilal, Diyar Khalis and Unel, Mustafa and Yildiz, Mehmet and Koc, Bahattin},
TITLE = {Realtime Localization and Estimation of Loads on Aircraft Wings from Depth Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3405},
URL = {https://www.mdpi.com/1424-8220/20/12/3405},
ISSN = {1424-8220},
ABSTRACT = {This paper deals with the development of a realtime structural health monitoring system for airframe structures to localize and estimate the magnitude of the loads causing deflections to the critical components, such as wings. To this end, a framework that is based on artificial neural networks is developed where features that are extracted from a depth camera are utilized. The localization of the load is treated as a multinomial logistic classification problem and the load magnitude estimation as a logistic regression problem. The neural networks trained for classification and regression are preceded with an autoencoder, through which maximum informative data at a much smaller scale are extracted from the depth features. The effectiveness of the proposed method is validated by an experimental study performed on a composite unmanned aerial vehicle (UAV) wing subject to concentrated and distributed loads, and the results obtained by the proposed method are superior when compared with a method based on Castigliano&rsquo;s theorem.},
DOI = {10.3390/s20123405}
}



@Article{s20123460,
AUTHOR = {Shokravi, Hoofar and Shokravi, Hooman and Bakhary, Norhisham and Heidarrezaei, Mahshid and Rahimian Koloor, Seyed Saeid and Petrů, Michal},
TITLE = {Vehicle-Assisted Techniques for Health Monitoring of Bridges},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3460},
URL = {https://www.mdpi.com/1424-8220/20/12/3460},
ISSN = {1424-8220},
ABSTRACT = {Bridges are designed to withstand different types of loads, including dead, live, environmental, and occasional loads during their service period. Moving vehicles are the main source of the applied live load on bridges. The applied load to highway bridges depends on several traffic parameters such as weight of vehicles, axle load, configuration of axles, position of vehicles on the bridge, number of vehicles, direction, and vehicle&rsquo;s speed. The estimation of traffic loadings on bridges are generally notional and, consequently, can be excessively conservative. Hence, accurate prediction of the in-service performance of a bridge structure is very desirable and great savings can be achieved through the accurate assessment of the applied traffic load in existing bridges. In this paper, a review is conducted on conventional vehicle-based health monitoring methods used for bridges. Vision-based, weigh in motion (WIM), bridge weigh in motion (BWIM), drive-by and vehicle bridge interaction (VBI)-based models are the methods that are generally used in the structural health monitoring (SHM) of bridges. The performance of vehicle-assisted methods is studied and suggestions for future work in this area are addressed, including alleviating the downsides of each approach to disentangle the complexities, and adopting intelligent and autonomous vehicle-assisted methods for health monitoring of bridges.},
DOI = {10.3390/s20123460}
}



@Article{jmse8060449,
AUTHOR = {Abaspur Kazerouni, Iman and Dooly, Gerard and Toal, Daniel},
TITLE = {Underwater Image Enhancement and Mosaicking System Based on A-KAZE Feature Matching},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {8},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {449},
URL = {https://www.mdpi.com/2077-1312/8/6/449},
ISSN = {2077-1312},
ABSTRACT = {Feature extraction and matching is a key component in image stitching and a critical step in advancing image reconstructions, machine vision and robotic perception algorithms. This paper presents a fast and robust underwater image mosaicking system based on (2D)2PCA and A-KAZE key-points extraction and optimal seam-line methods. The system utilizes image enhancement as a preprocessing step to improve quality and allow for greater keyframe extraction and matching performance, leading to better quality mosaicking. The application focus of this paper is underwater imaging and it demonstrates the suitability of the developed system in advanced underwater reconstructions. The results show that the proposed method can address the problems of noise, mismatching and quality issues which are typically found in underwater image datasets. The results demonstrate the proposed method as scale-invariant and show improvements in terms of processing speed and system robustness over other methods found in the literature.},
DOI = {10.3390/jmse8060449}
}



@Article{ijgi9060403,
AUTHOR = {Zhang, Xueyan},
TITLE = {Village-Level Homestead and Building Floor Area Estimates Based on UAV Imagery and U-Net Algorithm},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {403},
URL = {https://www.mdpi.com/2220-9964/9/6/403},
ISSN = {2220-9964},
ABSTRACT = {China&rsquo;s rural population has declined markedly with the acceleration of urbanization and industrialization, but the area under rural homesteads has continued to expand. Proper rural land use and management require large-scale, efficient, and low-cost rural residential surveys; however, such surveys are time-consuming and difficult to accomplish. Unmanned aerial vehicle (UAV) technology coupled with a deep learning architecture and 3D modelling can provide a potential alternative to traditional surveys for gathering rural homestead information. In this study, a method to estimate the village-level homestead area, a 3D-based building height model (BHM), and the number of building floors based on UAV imagery and the U-net algorithm was developed, and the respective estimation accuracies were found to be 0.92, 0.99, and 0.89. This method is rapid and inexpensive compared to the traditional time-consuming and costly household surveys, and, thus, it is of great significance to the ongoing use and management of rural homestead information, especially with regards to the confirmation of homestead property rights in China. Further, the proposed combination of UAV imagery and U-net technology may have a broader application in rural household surveys, as it can provide more information for decision-makers to grasp the current state of the rural socio-economic environment.},
DOI = {10.3390/ijgi9060403}
}



@Article{rs12121984,
AUTHOR = {Hegarty-Craver, Meghan and Polly, Jason and O’Neil, Margaret and Ujeneza, Noel and Rineer, James and Beach, Robert H. and Lapidus, Daniel and Temple, Dorota S.},
TITLE = {Remote Crop Mapping at Scale: Using Satellite Imagery and UAV-Acquired Data as Ground Truth},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1984},
URL = {https://www.mdpi.com/2072-4292/12/12/1984},
ISSN = {2072-4292},
ABSTRACT = {Timely and accurate agricultural information is needed to inform resource allocation and sustainable practices to improve food security in the developing world. Obtaining this information through traditional surveys is time consuming and labor intensive, making it difficult to collect data at the frequency and resolution needed to accurately estimate the planted areas of key crops and their distribution during the growing season. Remote sensing technologies can be leveraged to provide consistent, cost-effective, and spatially disaggregated data at high temporal frequency. In this study, we used imagery acquired from unmanned aerial vehicles to create a high-fidelity ground-truth dataset that included examples of large mono-cropped fields, small intercropped fields, and natural vegetation. The imagery was acquired in three rounds of flights at six sites in different agro-ecological zones to capture growing conditions. This dataset was used to train and test a random forest model that was implemented in Google Earth Engine for classifying cropped land using freely available Sentinel-1 and -2 data. This model achieved an overall accuracy of 83%, and a 91% accuracy for maize specifically. The model results were compared with Rwanda&rsquo;s Seasonal Agricultural Survey, which highlighted biases in the dataset including a lack of examples of mixed land cover.},
DOI = {10.3390/rs12121984}
}



@Article{app10124247,
AUTHOR = {Zhong, Kefeng and Teng, Shuai and Liu, Gen and Chen, Gongfa and Cui, Fangsen},
TITLE = {Structural Damage Features Extracted by Convolutional Neural Networks from Mode Shapes},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {4247},
URL = {https://www.mdpi.com/2076-3417/10/12/4247},
ISSN = {2076-3417},
ABSTRACT = {This paper aims to locate damaged rods in a three-dimensional (3D) steel truss and reveals some internal working mechanisms of the convolutional neural network (CNN), which is based on the first-order modal parameters and CNN. The CNN training samples (including a large number of damage scenarios) are created by ABAQUS and PYTHON scripts. The mode shapes and mode curvature differences are taken as the inputs of the CNN training samples, respectively, and the damage locating accuracy of the CNN is investigated. Finally, the features extracted from each convolutional layer of the CNN are checked to reveal some internal working mechanisms of the CNN and explain the specific meanings of some features. The results show that the CNN-based damage detection method using mode shapes as the inputs has a higher locating accuracy for all damage degrees, while the method using mode curvature differences as the inputs has a lower accuracy for the targets with a low damage degree; however, with the increase of the target damage degree, it gradually achieves the same good locating accuracy as mode shapes. The features extracted from each convolutional layer show that the CNN can obtain the difference between the sample to be classified and the average of training samples in shallow layers, and then amplify the difference in the subsequent convolutional layer, which is similar to a power function, finally it produces a distinguishable peak signal at the damage location. Then a damage locating method is derived from the feature extraction of the CNN. All of these results indicate that the CNN using first-order modal parameters not only has a powerful damage location ability, but also opens up a new way to extract damage features from the measurement data.},
DOI = {10.3390/app10124247}
}



@Article{rs12121990,
AUTHOR = {Wagner, Matthias P. and Oppelt, Natascha},
TITLE = {Deep Learning and Adaptive Graph-Based Growing Contours for Agricultural Field Extraction},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1990},
URL = {https://www.mdpi.com/2072-4292/12/12/1990},
ISSN = {2072-4292},
ABSTRACT = {Field mapping and information on agricultural landscapes is of increasing importance for many applications. Monitoring schemes and national cadasters provide a rich source of information but their maintenance and regular updating is costly and labor-intensive. Automatized mapping of fields based on remote sensing imagery may aid in this task and allow for a faster and more regular observation. Although remote sensing has seen extensive use in agricultural research topics, such as plant health monitoring, crop type classification, yield prediction, and irrigation, field delineation and extraction has seen comparatively little research interest. In this study, we present a field boundary detection technique based on deep learning and a variety of image features, and combine it with the graph-based growing contours (GGC) method to extract agricultural fields in a study area in northern Germany. The boundary detection step only requires red, green, and blue (RGB) data and is therefore largely independent of the sensor used. We compare different image features based on color and luminosity information and evaluate their usefulness for the task of field boundary detection. A model based on texture metrics, gradient information, Hessian matrix eigenvalues, and local statistics showed good results with accuracies up to 88.2%, an area under the ROC curve (AUC) of up to 0.94, and F1 score of up to 0.88. The exclusive use of these universal image features may also facilitate transferability to other regions. We further present modifications to the GGC method intended to aid in upscaling of the method through process acceleration with a minimal effect on results. We combined the boundary detection results with the GGC method for field polygon extraction. Results were promising, with the new GGC version performing similarly or better than the original version while experiencing an acceleration of 1.3&times; to 2.3&times; on different subsets and input complexities. Further research may explore other applications of the GGC method outside agricultural remote sensing and field extraction.},
DOI = {10.3390/rs12121990}
}



@Article{rs12121994,
AUTHOR = {Luo, Xin and Tian, Xiaoyue and Zhang, Huijie and Hou, Weimin and Leng, Geng and Xu, Wenbo and Jia, Haitao and He, Xixu and Wang, Meng and Zhang, Jian},
TITLE = {Fast Automatic Vehicle Detection in UAV Images Using Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1994},
URL = {https://www.mdpi.com/2072-4292/12/12/1994},
ISSN = {2072-4292},
ABSTRACT = {Vehicle targets in unmanned aerial vehicle (UAV) images are generally small, so a significant amount of detailed information on targets may be lost after neural computing, which leads to the poor performances of the existing recognition algorithms. Based on convolutional neural networks that utilize the YOLOv3 algorithm, this article focuses on the development of a quick automatic vehicle detection method for UAV images. First, a vehicle dataset for target recognition is constructed. Then, a novel YOLOv3 vehicle detection framework is proposed according to the following characteristics: The vehicle targets in the UAV image are relatively small and dense. The average precision (AP) increased by 5.48%, from 92.01% to 97.49%, which still remains the rather high processing speed of the YOLO network. Finally, the proposed framework is tested using three datasets: COWC, VEDAI, and CAR. The experimental results demonstrate that our method had a better detection capability.},
DOI = {10.3390/rs12121994}
}



@Article{en13123223,
AUTHOR = {Foudeh, Husam A. and Luk, Patrick and Whidborne, James},
TITLE = {Application of Norm Optimal Iterative Learning Control to Quadrotor Unmanned Aerial Vehicle for Monitoring Overhead Power System},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3223},
URL = {https://www.mdpi.com/1996-1073/13/12/3223},
ISSN = {1996-1073},
ABSTRACT = {Wind disturbances and noise severely affect Unmanned Aerial Vehicles (UAV) when monitoring and finding faults in overhead power lines. Accordingly, we propose repetitive learning as a new solution for the problem. In particular, the performance of Iterative Learning Control (ILC) that are based on optimal approaches are examined, namely (i) Gradient-based ILC and (ii) Norm Optimal ILC. When considering the repetitive nature of fault-finding tasks for electrical overhead power lines, this study develops, implements and evaluates optimal ILC algorithms for a UAV model. Moreover, we suggest attempting a learning gain variation on the standard optimal algorithms instead of heuristically selecting from the previous range. The results of both simulations and experiments of gradient-based norm optimal control reveal that the proposed ILC algorithm has not only contributed to good trajectory tracking, but also good convergence speed and the ability to cope with exogenous disturbances such as wind gusts.},
DOI = {10.3390/en13123223}
}



@Article{rs12122002,
AUTHOR = {Panagiotou, Emmanouil and Chochlakis, Georgios and Grammatikopoulos, Lazaros and Charou, Eleni},
TITLE = {Generating Elevation Surface from a Single RGB Remotely Sensed Image Using Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2002},
URL = {https://www.mdpi.com/2072-4292/12/12/2002},
ISSN = {2072-4292},
ABSTRACT = {Generating Digital Elevation Models (DEM) from satellite imagery or other data sources constitutes an essential tool for a plethora of applications and disciplines, ranging from 3D flight planning and simulation, autonomous driving and satellite navigation, such as GPS, to modeling water flow, precision farming and forestry. The task of extracting this 3D geometry from a given surface hitherto requires a combination of appropriately collected corresponding samples and/or specialized equipment, as inferring the elevation from single image data is out of reach for contemporary approaches. On the other hand, Artificial Intelligence (AI) and Machine Learning (ML) algorithms have experienced unprecedented growth in recent years as they can extrapolate rules in a data-driven manner and retrieve convoluted, nonlinear one-to-one mappings, such as an approximate mapping from satellite imagery to DEMs. Therefore, we propose an end-to-end Deep Learning (DL) approach to construct this mapping and to generate an absolute or relative point cloud estimation of a DEM given a single RGB satellite (Sentinel-2 imagery in this work) or drone image. The model has been readily extended to incorporate available information from the non-visible electromagnetic spectrum. Unlike existing methods, we only exploit one image for the production of the elevation data, rendering our approach less restrictive and constrained, but suboptimal compared to them at the same time. Moreover, recent advances in software and hardware allow us to make the inference and the generation extremely fast, even on moderate hardware. We deploy Conditional Generative Adversarial networks (CGAN), which are the state-of-the-art approach to image-to-image translation. We expect our work to serve as a springboard for further development in this field and to foster the integration of such methods in the process of generating, updating and analyzing DEMs.},
DOI = {10.3390/rs12122002}
}



@Article{rs12122010,
AUTHOR = {Seydi, Seyd Teymoor and Hasanlou, Mahdi and Amani, Meisam},
TITLE = {A New End-to-End Multi-Dimensional CNN Framework for Land Cover/Land Use Change Detection in Multi-Source Remote Sensing Datasets},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2010},
URL = {https://www.mdpi.com/2072-4292/12/12/2010},
ISSN = {2072-4292},
ABSTRACT = {The diversity of change detection (CD) methods and the limitations in generalizing these techniques using different types of remote sensing datasets over various study areas have been a challenge for CD applications. Additionally, most CD methods have been implemented in two intensive and time-consuming steps: (a) predicting change areas, and (b) decision on predicted areas. In this study, a novel CD framework based on the convolutional neural network (CNN) is proposed to not only address the aforementioned problems but also to considerably improve the level of accuracy. The proposed CNN-based CD network contains three parallel channels: the first and second channels, respectively, extract deep features on the original first- and second-time imagery and the third channel focuses on the extraction of change deep features based on differencing and staking deep features. Additionally, each channel includes three types of convolution kernels: 1D-, 2D-, and 3D-dilated-convolution. The effectiveness and reliability of the proposed CD method are evaluated using three different types of remote sensing benchmark datasets (i.e., multispectral, hyperspectral, and Polarimetric Synthetic Aperture RADAR (PolSAR)). The results of the CD maps are also evaluated both visually and statistically by calculating nine different accuracy indices. Moreover, the results of the CD using the proposed method are compared to those of several state-of-the-art CD algorithms. All the results prove that the proposed method outperforms the other remote sensing CD techniques. For instance, considering different scenarios, the Overall Accuracies (OAs) and Kappa Coefficients (KCs) of the proposed CD method are better than 95.89% and 0.805, respectively, and the Miss Detection (MD) and the False Alarm (FA) rates are lower than 12% and 3%, respectively.},
DOI = {10.3390/rs12122010}
}



@Article{rs12122012,
AUTHOR = {Kucharczyk, Maja and Hay, Geoffrey J. and Ghaffarian, Salar and Hugenholtz, Chris H.},
TITLE = {Geographic Object-Based Image Analysis: A Primer and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2012},
URL = {https://www.mdpi.com/2072-4292/12/12/2012},
ISSN = {2072-4292},
ABSTRACT = {Geographic object-based image analysis (GEOBIA) is a remote sensing image analysis paradigm that defines and examines image-objects: groups of neighboring pixels that represent real-world geographic objects. Recent reviews have examined methodological considerations and highlighted how GEOBIA improves upon the 30+ year pixel-based approach, particularly for H-resolution imagery. However, the literature also exposes an opportunity to improve guidance on the application of GEOBIA for novice practitioners. In this paper, we describe the theoretical foundations of GEOBIA and provide a comprehensive overview of the methodological workflow, including: (i) software-specific approaches (open-source and commercial); (ii) best practices informed by research; and (iii) the current status of methodological research. Building on this foundation, we then review recent research on the convergence of GEOBIA with deep convolutional neural networks, which we suggest is a new form of GEOBIA. Specifically, we discuss general integrative approaches and offer recommendations for future research. Overall, this paper describes the past, present, and anticipated future of GEOBIA in a novice-accessible format, while providing innovation and depth to experienced practitioners.},
DOI = {10.3390/rs12122012}
}



@Article{rs12122019,
AUTHOR = {Zhao, Yibo and Lei, Shaogang and Yang, Xingchen and Gong, Chuangang and Wang, Cangjiao and Cheng, Wei and Li, Heng and She, Changchao},
TITLE = {Study on Spectral Response and Estimation of Grassland Plants Dust Retention Based on Hyperspectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2019},
URL = {https://www.mdpi.com/2072-4292/12/12/2019},
ISSN = {2072-4292},
ABSTRACT = {Accurate monitoring of plant dust retention can provide a basis for dust pollution control and environmental protection. The aims of this study were to analyze the spectral response features of grassland plants to mining dust and to predict the spatial distribution of dust retention using hyperspectral data. The dust retention content was determined by an electronic analytical balance and a leaf area meter. The leaf reflectance spectrum was measured by a handheld hyperspectral camera, and the airborne hyperspectral data were obtained using an imaging spectrometer. We analyzed the difference between the leaf spectral before and after dust removal. The sensitive spectra of dust retention on the leaf- and the canopy-scale were determined through two-dimensional correlation spectroscopy (2DCOS). The competitive adaptive reweighted sampling (CARS) algorithm was applied to select the feature bands of canopy dust retention. The estimation model of canopy dust retention was built through random forest regression (RFR), and the dust distribution map was obtained based on the airborne hyperspectral image. The results showed that dust retention enhanced the spectral reflectance of leaves in the visible wavelength but weakened the reflectance in the near-infrared wavelength. Caused by the canopy structure and multiple scattering, a slight difference in the sensitive spectra on dust retention existed between the canopy and leaves. Similarly, the sensitive spectra of leaves and the canopy were closely related to dust and plant physiological parameters. The estimation model constructed through 2DCOS-CARS-RFR showed higher precision, compared with genetic algorithm-random forest regression (GA-RFR) and simulated annealing algorithm-random forest regression (SAA-RFR). Spatially, the amount of canopy dust increased and then decreased with increasing distance from the mining area, reaching a maximum within 300&ndash;500 m. This study not only demonstrated the importance of extracting feature bands based on the response of plant physical and chemical parameters to dust, but also laid a foundation for the rapid and non-destructive monitoring of grassland plant dust retention.},
DOI = {10.3390/rs12122019}
}



@Article{s20133664,
AUTHOR = {Zhang, Qichen and Zhu, Meiqiang and Zou, Liang and Li, Ming and Zhang, Yong},
TITLE = {Learning Reward Function with Matching Network for Mapless Navigation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3664},
URL = {https://www.mdpi.com/1424-8220/20/13/3664},
ISSN = {1424-8220},
ABSTRACT = {Deep reinforcement learning (DRL) has been successfully applied in mapless navigation. An important issue in DRL is to design a reward function for evaluating actions of agents. However, designing a robust and suitable reward function greatly depends on the designer&rsquo;s experience and intuition. To address this concern, we consider employing reward shaping from trajectories on similar navigation tasks without human supervision, and propose a general reward function based on matching network (MN). The MN-based reward function is able to gain the experience by pre-training through trajectories on different navigation tasks and accelerate the training speed of DRL in new tasks. The proposed reward function keeps the optimal strategy of DRL unchanged. The simulation results on two static maps show that the DRL converge with less iterations via the learned reward function than the state-of-the-art mapless navigation methods. The proposed method performs well in dynamic maps with partially moving obstacles. Even when test maps are different from training maps, the proposed strategy is able to complete the navigation tasks without additional training.},
DOI = {10.3390/s20133664}
}



@Article{app10134574,
AUTHOR = {Ghaffarian, Saman and Rezaie Farhadabad, Ali and Kerle, Norman},
TITLE = {Post-Disaster Recovery Monitoring with Google Earth Engine},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {4574},
URL = {https://www.mdpi.com/2076-3417/10/13/4574},
ISSN = {2076-3417},
ABSTRACT = {Post-disaster recovery is a complex process in terms of measuring its progress after a disaster and understanding its components and influencing factors. During this process, disaster planners and governments need reliable information to make decisions towards building the affected region back to normal (pre-disaster), or even improved, conditions. Hence, it is essential to use methods to understand the dynamics/variables of the post-disaster recovery process, and rapid and cost-effective data and tools to monitor the process. Google Earth Engine (GEE) provides free access to vast amounts of remote sensing (RS) data and a powerful computing environment in a cloud platform, making it an attractive tool to analyze earth surface data. In this study we assessed the suitability of GEE to analyze and track recovery. To do so, we employed GEE to assess the recovery process over a three-year period after Typhoon Haiyan, which struck Leyte island, in the Philippines, in 2013. We developed an approach to (i) generate cloud and shadow-free image composites from Landsat 7 and 8 satellite imagery and produce land cover classification data using the Random Forest method, and (ii) generate damage and recovery maps based on post-classification change analysis. The method produced land cover maps with accuracies &gt;88%. We used the model to produce damage and three time-step recovery maps for 62 municipalities on Leyte island. The results showed that most of the municipalities had recovered after three years in terms of returning to the pre-disaster situation based on the selected land cover change analysis. However, more analysis (e.g., functional assessment) based on detailed data (e.g., land use maps) is needed to evaluate the more complex and subtle socio-economic aspects of the recovery. The study showed that GEE has good potential for monitoring the recovery process for extensive regions. However, the most important limitation is the lack of very-high-resolution RS data that are critical to assess the process in detail, in particular in complex urban environments.},
DOI = {10.3390/app10134574}
}



@Article{s20133750,
AUTHOR = {Tantiparimongkol, Lalida and Phasukkit, Pattarapong},
TITLE = {IR-UWB Pulse Generation Using FPGA Scheme for through Obstacle Human Detection},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3750},
URL = {https://www.mdpi.com/1424-8220/20/13/3750},
ISSN = {1424-8220},
ABSTRACT = {This research proposes a scheme of field programmable gate array (FPGA) to generate an impulse-radio ultra-wideband (IR-UWB) pulse. The FPGA scheme consists of three parts: digital clock manager, four-delay-paths stratagem, and edge combiner. The IR-UWB radar system is designed to detect human subjects from their respiration underneath the rubble in the aftermath of an earthquake and to locate the human subjects based on range estimation. The proposed IR-UWB radar system is experimented with human subjects lying underneath layers of stacked clay bricks in supine and prone position. The results reveal that the IR-UWB radar system achieves a pulse duration of 540 ps with a bandwidth of 2.073 GHz (fractional bandwidth of 1.797). In addition, the IR-UWB technology can detect human subjects underneath the rubble from respiration and identify the location of human subjects by range estimation. The novelty of this research lies in the use of the FPGA scheme to achieve an IR-UWB pulse with a 2.073 GHz (117 MHz&ndash;2.19 GHz) bandwidth, thereby rendering the technology suitable for a wide range of applications, in addition to through-obstacle detection.},
DOI = {10.3390/s20133750}
}



@Article{app10134668,
AUTHOR = {Cao, Dalu and Bai, Guangchen},
TITLE = {A Study on Aeroengine Conceptual Design Considering Multi-Mission Performance Reliability},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {4668},
URL = {https://www.mdpi.com/2076-3417/10/13/4668},
ISSN = {2076-3417},
ABSTRACT = {Owing to the realization of multi-mission adaptability requires more complex mechanical structure, the candidates of future aviation propulsion are confronted with more overall reliability problems than that of the conventional gas turbine engine. This situation is challenging to a traditional aeroengine deterministic design method. To overcome this challenge, the Reliability-based Multi-Design Point Methodology is proposed for aeroengine conceptual design. The presented methodology adopted an unconventional approach of engaging the reliability prediction by artificial neural network (ANN) surrogate models rather than the time-consuming Monte Carlo (MC) simulation. Based on the Adaptive Particle swarm optimization, the utilization of the pre-training technique optimizes the initial network parameters to acquire better-conditioned initial network, which is sited closer to designated optimum so that contributes to the convergence property. Moreover, a new hybrid algorithm is presented to integrate the pre-training technique into neural network training procedure in order to enhance the ANN performance. The proposed methodology is applied to the cycle design of a turbofan engine with uncertainty component performance. The testing results certify that the prediction accuracy of pre-trained ANN is improved with negligible computational cost, which only spent nearly one-millionth as much time as the MC-based probabilistic analysis (0.1267 s vs. 95,262 s, for 20 testing samples). The MC simulation results substantiate that optimal cycle parameters precisely improve the engine overall performance to simultaneously reach expected reliability (&ge;98.9%) in multiple operating conditions without unnecessary performance redundancy, which verifies the efficiency of the presented methodology. The presented efforts provide a novel approach for aeroengine cycle design, and enrich reliability design theory as well.},
DOI = {10.3390/app10134668}
}



@Article{agriculture10070277,
AUTHOR = {García-Martínez, Héctor and Flores-Magdaleno, Héctor and Ascencio-Hernández, Roberto and Khalil-Gardezi, Abdul and Tijerina-Chávez, Leonardo and Mancilla-Villa, Oscar R. and Vázquez-Peña, Mario A.},
TITLE = {Corn Grain Yield Estimation from Vegetation Indices, Canopy Cover, Plant Density, and a Neural Network Using Multispectral and RGB Images Acquired with Unmanned Aerial Vehicles},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {277},
URL = {https://www.mdpi.com/2077-0472/10/7/277},
ISSN = {2077-0472},
ABSTRACT = {Corn yields vary spatially and temporally in the plots as a result of weather, altitude, variety, plant density, available water, nutrients, and planting date; these are the main factors that influence crop yield. In this study, different multispectral and red-green-blue (RGB) vegetation indices were analyzed, as well as the digitally estimated canopy cover and plant density, in order to estimate corn grain yield using a neural network model. The relative importance of the predictor variables was also analyzed. An experiment was established with five levels of nitrogen fertilization (140, 200, 260, 320, and 380 kg/ha) and four replicates, in a completely randomized block design, resulting in 20 experimental polygons. Crop information was captured using two sensors (Parrot Sequoia_4.9, and DJI FC6310_8.8) mounted on an unmanned aerial vehicle (UAV) for two flight dates at 47 and 79 days after sowing (DAS). The correlation coefficient between the plant density, obtained through the digital count of corn plants, and the corn grain yield was 0.94; this variable was the one with the highest relative importance in the yield estimation according to Garson&rsquo;s algorithm. The canopy cover, digitally estimated, showed a correlation coefficient of 0.77 with respect to the corn grain yield, while the relative importance of this variable in the yield estimation was 0.080 and 0.093 for 47 and 79 DAS, respectively. The wide dynamic range vegetation index (WDRVI), plant density, and canopy cover showed the highest correlation coefficient and the smallest errors (R = 0.99, mean absolute error (MAE) = 0.028 t ha&minus;1, root mean square error (RMSE) = 0.125 t ha&minus;1) in the corn grain yield estimation at 47 DAS, with the WDRVI index and the density being the variables with the highest relative importance for this crop development date. For the 79 DAS flight, the combination of the normalized difference vegetation index (NDVI), normalized difference red edge (NDRE), WDRVI, excess green (EXG), triangular greenness index (TGI), and visible atmospherically resistant index (VARI), as well as plant density and canopy cover, generated the highest correlation coefficient and the smallest errors (R = 0.97, MAE = 0.249 t ha&minus;1, RMSE = 0.425 t ha&minus;1) in the corn grain yield estimation, where the density and the NDVI were the variables with the highest relative importance, with values of 0.295 and 0.184, respectively. However, the WDRVI, plant density, and canopy cover estimated the corn grain yield with acceptable precision (R = 0.96, MAE = 0.209 t ha&minus;1, RMSE = 0.449 t ha&minus;1). The generated neural network models provided a high correlation coefficient between the estimated and the observed corn grain yield, and also showed acceptable errors in the yield estimation. The spectral information registered through remote sensors mounted on unmanned aerial vehicles and its processing in vegetation indices, canopy cover, and plant density allowed the characterization and estimation of corn grain yield. Such information is very useful for decision-making and agricultural activities planning.},
DOI = {10.3390/agriculture10070277}
}



@Article{electronics9071121,
AUTHOR = {Kong, Weiren and Zhou, Deyun and Yang, Zhen and Zhao, Yiyang and Zhang, Kai},
TITLE = {UAV Autonomous Aerial Combat Maneuver Strategy Generation with Observation Error Based on State-Adversarial Deep Deterministic Policy Gradient and Inverse Reinforcement Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1121},
URL = {https://www.mdpi.com/2079-9292/9/7/1121},
ISSN = {2079-9292},
ABSTRACT = {With the development of unmanned aerial vehicle (UAV) and artificial intelligence (AI) technology, Intelligent UAV will be widely used in future autonomous aerial combat. Previous researches on autonomous aerial combat within visual range (WVR) have limitations due to simplifying assumptions, limited robustness, and ignoring sensor errors. In this paper, in order to consider the error of the aircraft sensors, we model the aerial combat WVR as a state-adversarial Markov decision process (SA-MDP), which introduce the small adversarial perturbations on state observations and these perturbations do not alter the environment directly, but can mislead the agent into making suboptimal decisions. Meanwhile, we propose a novel autonomous aerial combat maneuver strategy generation algorithm with high-performance and high-robustness based on state-adversarial deep deterministic policy gradient algorithm (SA-DDPG), which add a robustness regularizers related to an upper bound on performance loss at the actor-network. At the same time, a reward shaping method based on maximum entropy (MaxEnt) inverse reinforcement learning algorithm (IRL) is proposed to improve the aerial combat strategy generation algorithm&rsquo;s efficiency. Finally, the efficiency of the aerial combat strategy generation algorithm and the performance and robustness of the resulting aerial combat strategy is verified by simulation experiments. Our main contributions are three-fold. First, to introduce the observation errors of UAV, we are modeling air combat as SA-MDP. Second, to make the strategy network of air combat maneuver more robust in the presence of observation errors, we introduce regularizers into the policy gradient. Third, to solve the problem that air combat&rsquo;s reward function is too sparse, we use MaxEnt IRL to design a shaping reward to accelerate the convergence of SA-DDPG.},
DOI = {10.3390/electronics9071121}
}



@Article{ani10071207,
AUTHOR = {Akçay, Hüseyin Gökhan and Kabasakal, Bekir and Aksu, Duygugül and Demir, Nusret and Öz, Melih and Erdoğan, Ali},
TITLE = {Automated Bird Counting with Deep Learning for Regional Bird Distribution Mapping},
JOURNAL = {Animals},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1207},
URL = {https://www.mdpi.com/2076-2615/10/7/1207},
PubMedID = {32708550},
ISSN = {2076-2615},
ABSTRACT = {A challenging problem in the field of avian ecology is deriving information on bird population movement trends. This necessitates the regular counting of birds which is usually not an easily-achievable task. A promising attempt towards solving the bird counting problem in a more consistent and fast way is to predict the number of birds in different regions from their photos. For this purpose, we exploit the ability of computers to learn from past data through deep learning which has been a leading sub-field of AI for image understanding. Our data source is a collection of on-ground photos taken during our long run of birding activity. We employ several state-of-the-art generic object-detection algorithms to learn to detect birds, each being a member of one of the 38 identified species, in natural scenes. The experiments revealed that computer-aided counting outperformed the manual counting with respect to both accuracy and time. As a real-world application of image-based bird counting, we prepared the spatial bird order distribution and species diversity maps of Turkey by utilizing the geographic information system (GIS) technology. Our results suggested that deep learning can assist humans in bird monitoring activities and increase citizen scientists&rsquo; participation in large-scale bird surveys.},
DOI = {10.3390/ani10071207}
}



@Article{rs12142283,
AUTHOR = {Battulwar, Rushikesh and Winkelmaier, Garrett and Valencia, Jorge and Naghadehi, Masoud Zare and Peik, Bijan and Abbasi, Behrooz and Parvin, Bahram and Sattarvand, Javad},
TITLE = {A Practical Methodology for Generating High-Resolution 3D Models of Open-Pit Slopes Using UAVs: Flight Path Planning and Optimization},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2283},
URL = {https://www.mdpi.com/2072-4292/12/14/2283},
ISSN = {2072-4292},
ABSTRACT = {High-resolution terrain models of open-pit mine highwalls and benches are essential in developing new automated slope monitoring systems for operational optimization. This paper presents several contributions to the field of remote sensing in surface mines providing a practical framework for generating high-resolution images using low-trim Unmanned Aerial Vehicles (UAVs). First, a novel mobile application was developed for autonomous drone flights to follow mine terrain and capture high-resolution images of the mine surface. In this article, case study is presented showcasing the ability of developed software to import area terrain, plan the flight accordingly, and finally execute the area mapping mission autonomously. Next, to model the drone&rsquo;s battery performance, empirical studies were conducted considering various flight scenarios. A multivariate linear regression model for drone power consumption was derived from experimental data. The model has also been validated using data from a test flight. Finally, a genetic algorithm for solving the problem of flight planning and optimization has been employed. The developed power consumption model was used as the fitness function in the genetic algorithm. The designed algorithm was then validated using simulation studies. It is shown that the offered path optimization can reduce the time and energy of high-resolution imagery missions by over 50%. The current work provides a practical framework for stability monitoring of open-pit highwalls while achieving required energy optimization and imagery performance.},
DOI = {10.3390/rs12142283}
}



@Article{f11070763,
AUTHOR = {Klemmt, Hans-Joachim and Seitz, Rudolf and Straub, Christoph},
TITLE = {Application of Haralick’s Texture Features for Rapid Detection of Windthrow Hotspots in Orthophotos},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {763},
URL = {https://www.mdpi.com/1999-4907/11/7/763},
ISSN = {1999-4907},
ABSTRACT = {Windthrow and storm damage are crucial issues in practical forestry. We propose a method for rapid detection of windthrow hotspots in airborne digital orthophotos. Therefore, we apply Haralick&rsquo;s texture features on 50 &times; 50 m cells of the orthophotos and classify the cells with a random forest algorithm. We apply the classification results from a training data set on a validation set. The overall classification accuracy of the proposed method varies between 76% for fine distinction of the cells and 96% for a distinction level that tried to detect only severe damaged cells. The proposed method enables the rapid detection of windthrow hotspots in forests immediately after their occurrence in single-date data. It is not adequate for the determination of areas with only single fallen trees. Future research will investigate the possibilities and limitations when applying the method on other data sources (e.g., optical satellite data).},
DOI = {10.3390/f11070763}
}



@Article{s20143954,
AUTHOR = {Ahmed, Habib and La, Hung Manh and Gucunski, Nenad},
TITLE = {Review of Non-Destructive Civil Infrastructure Evaluation for Bridges: State-of-the-Art Robotic Platforms, Sensors and Algorithms},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3954},
URL = {https://www.mdpi.com/1424-8220/20/14/3954},
ISSN = {1424-8220},
ABSTRACT = {The non-destructive evaluation (NDE) of civil infrastructure has been an active area of research in recent decades. The traditional inspection of civil infrastructure mostly relies on visual inspection using human inspectors. To facilitate this process, different sensors for data collection and techniques for data analyses have been used to effectively carry out this task in an automated fashion. This review-based study will examine some of the recent developments in the field of autonomous robotic platforms for NDE and the structural health monitoring (SHM) of bridges. Some of the salient features of this review-based study will be discussed in the light of the existing surveys and reviews that have been published in the recent past, which will enable the clarification regarding the novelty of the present review-based study. The review methodology will be discussed in sufficient depth, which will provide insights regarding some of the primary aspects of the review methodology followed by this review-based study. In order to provide an in-depth examination of the state-of-the-art, the current research will examine the three major research streams. The first stream relates to technological robotic platforms developed for NDE of bridges. The second stream of literature examines myriad sensors used for the development of robotic platforms for the NDE of bridges. The third stream of literature highlights different algorithms for the surface- and sub-surface-level analysis of bridges that have been developed by studies in the past. A number of challenges towards the development of robotic platforms have also been discussed.},
DOI = {10.3390/s20143954}
}



@Article{rs12142313,
AUTHOR = {El Mahrad, Badr and Newton, Alice and Icely, John D. and Kacimi, Ilias and Abalansa, Samuel and Snoussi, Maria},
TITLE = {Contribution of Remote Sensing Technologies to a Holistic Coastal and Marine Environmental Management Framework: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2313},
URL = {https://www.mdpi.com/2072-4292/12/14/2313},
ISSN = {2072-4292},
ABSTRACT = {Coastal and marine management require the evaluation of multiple environmental threats and issues. However, there are gaps in the necessary data and poor access or dissemination of existing data in many countries around the world. This research identifies how remote sensing can contribute to filling these gaps so that environmental agencies, such as the United Nations Environmental Programme, European Environmental Agency, and International Union for Conservation of Nature, can better implement environmental directives in a cost-effective manner. Remote sensing (RS) techniques generally allow for uniform data collection, with common acquisition and reporting methods, across large areas. Furthermore, these datasets are sometimes open-source, mainly when governments finance satellite missions. Some of these data can be used in holistic, coastal and marine environmental management frameworks, such as the DAPSI(W)R(M) framework (Drivers–Activities–Pressures–State changes–Impacts (on Welfare)–Responses (as Measures), an updated version of Drivers–Pressures–State–Impact–Responses. The framework is a useful and holistic problem-structuring framework that can be used to assess the causes, consequences, and responses to change in the marine environment. Six broad classifications of remote data collection technologies are reviewed for their potential contribution to integrated marine management, including Satellite-based Remote Sensing, Aerial Remote Sensing, Unmanned Aerial Vehicles, Unmanned Surface Vehicles, Unmanned Underwater Vehicles, and Static Sensors. A significant outcome of this study is practical inputs into each component of the DAPSI(W)R(M) framework. The RS applications are not expected to be all-inclusive; rather, they provide insight into the current use of the framework as a foundation for developing further holistic resource technologies for management strategies in the future. A significant outcome of this research will deliver practical insights for integrated coastal and marine management and demonstrate the usefulness of RS to support the implementation of environmental goals, descriptors, targets, and policies, such as the Water Framework Directive, Marine Strategy Framework Directive, Ocean Health Index, and United Nations Sustainable Development Goals. Additionally, the opportunities and challenges of these technologies are discussed.},
DOI = {10.3390/rs12142313}
}



@Article{s20144011,
AUTHOR = {Yu, Ziyang and Ustin, Susan L. and Zhang, Zhongchen and Liu, Huanjun and Zhang, Xinle and Meng, Xiangtian and Cui, Yang and Guan, Haixiang},
TITLE = {Estimation of a New Canopy Structure Parameter for Rice Using Smartphone Photography},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4011},
URL = {https://www.mdpi.com/1424-8220/20/14/4011},
ISSN = {1424-8220},
ABSTRACT = {The objective of this study was to develop a low-cost method for rice growth information obtained quickly using digital images taken with smartphone. A new canopy parameter, namely, the canopy volume parameter (CVP), was proposed and developed for rice using the leaf area index (LAI) and plant height (PH). Among these parameters, the CVP was selected as an optimal parameter to characterize rice yields during the growth period. Rice canopy images were acquired with a smartphone. Image feature parameters were extracted, including the canopy cover (CC) and numerous vegetation indices (VIs), before and after image segmentation. A rice CVP prediction model in which the CC and VIs served as independent variables was established using a random forest (RF) regression algorithm. The results revealed the following. The CVP was better than the LAI and PH for predicting the final yield. And a CVP prediction model constructed according to a local modelling method for distinguishing different types of rice varieties was the most accurate (coefficient of determination (R2) = 0.92; root mean square error (RMSE) = 0.44). These findings indicate that digital images can be used to track the growth of crops over time and provide technical support for estimating rice yields.},
DOI = {10.3390/s20144011}
}



@Article{vehicles2030026,
AUTHOR = {Yañez-Badillo, Hugo and Tapia-Olvera, Ruben and Beltran-Carbajal, Francisco},
TITLE = {Adaptive Neural Motion Control of a Quadrotor UAV},
JOURNAL = {Vehicles},
VOLUME = {2},
YEAR = {2020},
NUMBER = {3},
PAGES = {468--490},
URL = {https://www.mdpi.com/2624-8921/2/3/26},
ISSN = {2624-8921},
ABSTRACT = {Unmanned Aerial Vehicles have generated considerable interest in different research fields. The motion control problem is among the most important issues to be solved since system dynamic stability depends on the robustness of the main controller against endogenous and exogenous disturbances. In spite of different controllers have been introduced in the literature for motion control of fixed and rotary wing vehicles, there are some challenges for improving controller features such as simplicity, robustness, efficiency, adaptability, and stability. This paper outlines a novel approach to deal with the induced effects of external disturbances affecting the flight of a quadrotor unmanned aerial vehicle. The aim of our study is to further extend the current knowledge of quadrotor motion control by using both adaptive and robust control strategies. A new adaptive neural trajectory tracking control strategy based on B-spline artificial neural networks and on-line disturbance estimation for a quadrotor is proposed. A linear extended state observer is used for estimating time-varying disturbances affecting the controlled nonlinear system dynamics. B-spline artificial neural networks are properly synthesized for on-line calculating control gains of an adaptive Proportional Integral Derivative (PID) scheme. Simulation results highlight the implementation of such a controller is able to reject disturbances meanwhile perform proper motion control by exploiting the robustness, disturbance rejection, adaptability, and self-learning capabilities.},
DOI = {10.3390/vehicles2030026}
}



@Article{app10144991,
AUTHOR = {Villaseñor, Carlos and Gallegos, Alberto A. and Gomez-Avila, Javier and López-González, Gehová and Rios, Jorge D. and Arana-Daniel, Nancy},
TITLE = {Environment Classification for Unmanned Aerial Vehicle Using Convolutional Neural Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4991},
URL = {https://www.mdpi.com/2076-3417/10/14/4991},
ISSN = {2076-3417},
ABSTRACT = {Environment classification is one of the most critical tasks for Unmanned Aerial Vehicles (UAV). Since water accumulation may destabilize UAV, clouds must be detected and avoided. In a previous work presented by the authors, Superpixel Segmentation (SPS) descriptors with low computational cost are used to classify ground, sky, and clouds. In this paper, an enhanced approach to classify the environment in those three classes is presented. The proposed scheme consists of a Convolutional Neural Network (CNN) trained with a dataset generated by both, an human expert and a Support Vector Machine (SVM) to capture context and precise localization. The advantage of using this approach is that the CNN classifies each pixel, instead of a cluster like in SPS, which improves the resolution of the classification, also, is less tedious for the human expert to generate a few training samples instead of the normal amount that it is required. This proposal is implemented for images obtained from video and photographic cameras mounted on a UAV facing in the same direction of the vehicle flight. Experimental results and comparison with other approaches are shown to demonstrate the effectiveness of the algorithm.},
DOI = {10.3390/app10144991}
}



@Article{rs12152359,
AUTHOR = {Blanco, Víctor and Blaya-Ros, Pedro José and Castillo, Cristina and Soto-Vallés, Fulgencio and Torres-Sánchez, Roque and Domingo, Rafael},
TITLE = {Potential of UAS-Based Remote Sensing for Estimating Tree Water Status and Yield in Sweet Cherry Trees},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2359},
URL = {https://www.mdpi.com/2072-4292/12/15/2359},
ISSN = {2072-4292},
ABSTRACT = {The present work aims to assess the usefulness of five vegetation indices (VI) derived from multispectral UAS imagery to capture the effects of deficit irrigation on the canopy structure of sweet cherry trees (Prunus avium L.) in southeastern Spain. Three irrigation treatments were assayed, a control treatment and two regulated deficit irrigation treatments. Four airborne flights were carried out during two consecutive seasons; to compare the results of the remote sensing VI, the conventional and continuous water status indicators commonly used to manage sweet cherry tree irrigation were measured, including midday stem water potential (&Psi;s) and maximum daily shrinkage (MDS). Simple regression between individual VIs and &Psi;s or MDS found stronger relationships in postharvest than in preharvest. Thus, the normalized difference vegetation index (NDVI), resulted in the strongest relationship with &Psi;s (r2 = 0.67) and MDS (r2 = 0.45), followed by the normalized difference red edge (NDRE). The sensitivity analysis identified the optimal soil adjusted vegetation index (OSAVI) as the VI with the highest coefficient of variation in postharvest and the difference vegetation index (DVI) in preharvest. A new index is proposed, the transformed red range vegetation index (TRRVI), which was the only VI able to statistically identify a slight water deficit applied in preharvest. The combination of the VIs studied was used in two machine learning models, decision tree and artificial neural networks, to estimate the extra labor needed for harvesting and the sweet cherry yield.},
DOI = {10.3390/rs12152359}
}



@Article{app10155075,
AUTHOR = {Fang, Peng and Zhang, Xiwang and Wei, Panpan and Wang, Yuanzheng and Zhang, Huiyi and Liu, Feng and Zhao, Jun},
TITLE = {The Classification Performance and Mechanism of Machine Learning Algorithms in Winter Wheat Mapping Using Sentinel-2 10 m Resolution Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {5075},
URL = {https://www.mdpi.com/2076-3417/10/15/5075},
ISSN = {2076-3417},
ABSTRACT = {Machine learning algorithms are crucial for crop identification and mapping. However, many works only focus on the identification results of these algorithms, but pay less attention to their classification performance and mechanism. In this paper, based on Google Earth Engine (GEE), Sentinel-2 10 m resolution images during a specific phenological period of winter wheat were obtained. Then, support vector machine (SVM), random forest (RF), and classification and regression tree (CART) machine learning algorithms were employed to identify and map winter wheat in a large-scale area. The hyperparameters of the three machine learning algorithms were tuned by grid search and the 5-fold cross-validation method. The classification performance of the three machine learning algorithms were compared, the results of which demonstrate that SVM achieves best performance in identifying winter wheat, and its overall accuracy (OA), user&rsquo;s accuracy (UA), producer&rsquo;s accuracy (PA), and kappa coefficient (Kappa) are 0.94, 0.95, 0.95, and 0.92, respectively. Moreover, 50 various combinations of training and validation sets were used to analyze the generalization ability of the algorithms, and the results show that the average OA of SVM, RF, and CART are 0.93, 0.92, and 0.88, respectively, thus indicating that SVM and RF are more robust than CART. To further explore the sensitivity of SVM, RF, and CART to variations of the algorithm parameters&mdash;namely, (C and gamma), (tree and split), and (maxD and minSP)&mdash;we employed the grid search method to iterate these parameters, respectively, and to analyze the effect of these parameters on the accuracy scores and classification residuals. It was found that with the change of (C and gamma) in (0.01~1000), SVM&rsquo;s maximum variation of accuracy score is up to 0.63, and the maximum variation of residuals is 76,215 km2. We concluded that SVM is sensitive to the parameters (C and gamma) and presents a positive correlation. When the parameters (tree and split) change between (100~600) and (1~6), respectively, the RF&rsquo;s maximum variation of accuracy score is 0.08, and the maximum variation of residuals is 1157 km2, indicating that RF is low in sensitivity toward the parameters (tree and split). When the parameters (maxD and minSP) are between (10~60), the maximum accuracy change value is 0.06, and the maximum variation of residuals is 6943 km2. Therefore, compared to RF, CART is sensitive to the parameters (maxD and minSP) and has poor robustness. In general, under the conditions of the hyperparameters, SVM and RF exhibit optimal classification performance, while CART has relatively inferior performance. Meanwhile, SVM, RF, and CART have different sensitivities toward the algorithm parameters; that is, SVM and CART are more sensitive to the algorithm parameters, while RF has low sensitivity toward changes in the algorithm parameters. The different parameters cause great changes in the accuracy scores and residuals, so it is necessary to determine the algorithm hyperparameters. Generally, default parameters can be used to achieve crop classification, but we recommend the enumeration method, similar to grid search, as a practical way to improve the classification performance of the algorithm if the best classification effect is expected.},
DOI = {10.3390/app10155075}
}



@Article{rs12152379,
AUTHOR = {Pulido, Dagoberto and Salas, Joaquín and Rös, Matthias and Puettmann, Klaus and Karaman, Sertac},
TITLE = {Assessment of Tree Detection Methods in Multispectral Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2379},
URL = {https://www.mdpi.com/2072-4292/12/15/2379},
ISSN = {2072-4292},
ABSTRACT = {Detecting individual trees and quantifying their biomass is crucial for carbon accounting procedures at the stand, landscape, and national levels. A significant challenge for many organizations is the amount of effort necessary to document carbon storage levels, especially in terms of human labor. To advance towards the goal of efficiently assessing the carbon content of forest, we evaluate methods to detect trees from high-resolution images taken from unoccupied aerial systems (UAS). In the process, we introduce the Digital Elevated Vegetation Model (DEVM), a representation that combines multispectral images, digital surface models, and digital terrain models. We show that the DEVM facilitates the development of refined synthetic data to detect individual trees using deep learning-based approaches. We carried out experiments in two tree fields located in different countries. Simultaneously, we perform comparisons among an array of classical and deep learning-based methods highlighting the precision and reliability of the DEVM.},
DOI = {10.3390/rs12152379}
}



@Article{rs12152394,
AUTHOR = {Zhou, Hongmin and Wang, Changjing and Zhang, Guodong and Xue, Huazhu and Wang, Jingdi and Wan, Huawei},
TITLE = {Generating a Spatio-Temporal Complete 30 m Leaf Area Index from Field and Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2394},
URL = {https://www.mdpi.com/2072-4292/12/15/2394},
ISSN = {2072-4292},
ABSTRACT = {The leaf area index (LAI) is an important parameter for vegetation monitoring and land surface ecosystem research. Although a variety of LAI products have been generated, the moderate to coarse spatial resolution and low temporal resolution of these products are insufficient for regional-scale analysis. In this study, a modified ensemble Kalman filter model (MEnKF) was proposed to generate spatio-temporal complete 30 m LAI data. High-quality, filtered historical Moderate-resolution Imaging Spectroradiometer (MODIS) LAI data were used to obtain the LAI background, and an LAI temporal dynamic model was constructed based on it. An improved back-propagation (BP) neural network based on a simulated annealing algorithm (SA-BP) was constructed with paired Landsat surface reflectance data and field LAI data to generate a 30 m LAI. The MEnKF was used to estimate the spatio-temporal complete LAI beginning from the LAI peak value position where Landsat observations were available. The spatio-temporal 30 m LAI was estimated in farmland (Pshenichne), grassland (Zhangbei), and woodland (Genhe) sites. The results indicate that the MEnKF-estimated LAI is consistent with the field measurements for all sites (the coefficient of determination (     R 2     ) = 0.70; root mean squared error (RMSE) = 0.40) and is better than that of the conventional sequence data assimilation algorithm (     R 2      = 0.40; RMSE = 0.78). The regional LAI captures the vegetation growth pattern and is consistent with the Landsat LAI, with an      R 2      larger than 0.65 and an RMSE less than 0.51. The proposed MEnKF algorithm, which effectively avoids error accumulation in the data assimilation scheme, is an efficient method for spatio-temporal complete 30 m LAI estimation.},
DOI = {10.3390/rs12152394}
}



@Article{f11080808,
AUTHOR = {Prosekov, Alexander and Kuznetsov, Alexander and Rada, Artem and Ivanova, Svetlana},
TITLE = {Methods for Monitoring Large Terrestrial Animals in the Wild},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {808},
URL = {https://www.mdpi.com/1999-4907/11/8/808},
ISSN = {1999-4907},
ABSTRACT = {Reliable information about wildlife is absolutely important for making informed management decisions. The issues with the effectiveness of the control and monitoring of both large and small wild animals are relevant to assess and protect the world&rsquo;s biodiversity. Monitoring becomes part of the methods in wildlife ecology for observation, assessment, and forecasting of the human environment. World practice reveals the potential of the joint application of both proven traditional and modern technologies using specialized equipment to organize environmental control and management processes. Monitoring large terrestrial animals require an individual approach due to their low density and larger habitat. Elk/moose are such animals. This work aims to evaluate the methods for monitoring large wild animals, suitable for controlling the number of elk/moose in the framework of nature conservation activities. Using different models allows determining the population size without affecting the animals and without significant financial costs. Although, the accuracy of each model is determined by its postulates implementation and initial conditions that need statistical data. Depending on the geographical, climatic, and economic conditions in each territory, it is possible to use different tools and equipment (e.g., cameras, GPS sensors, and unmanned aerial vehicles), a flexible variation of which will allow reaching the golden mean between the desires and capabilities of researchers.},
DOI = {10.3390/f11080808}
}



@Article{rs12152397,
AUTHOR = {Schlosser, Aletta Dóra and Szabó, Gergely and Bertalan, László and Varga, Zsolt and Enyedi, Péter and Szabó, Szilárd},
TITLE = {Building Extraction Using Orthophotos and Dense Point Cloud Derived from Visual Band Aerial Imagery Based on Machine Learning and Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2397},
URL = {https://www.mdpi.com/2072-4292/12/15/2397},
ISSN = {2072-4292},
ABSTRACT = {Urban sprawl related increase of built-in areas requires reliable monitoring methods and remote sensing can be an efficient technique. Aerial surveys, with high spatial resolution, provide detailed data for building monitoring, but archive images usually have only visible bands. We aimed to reveal the efficiency of visible orthophotographs and photogrammetric dense point clouds in building detection with segmentation-based machine learning (with five algorithms) using visible bands, texture information, and spectral and morphometric indices in different variable sets. Usually random forest (RF) had the best (99.8%) and partial least squares the worst overall accuracy (~60%). We found that &gt;95% accuracy can be gained even in class level. Recursive feature elimination (RFE) was an efficient variable selection tool, its result with six variables was like when we applied all the available 31 variables. Morphometric indices had 82% producer&rsquo;s and 85% user&rsquo;s Accuracy (PA and UA, respectively) and combining them with spectral and texture indices, it had the largest contribution in the improvement. However, morphometric indices are not always available but by adding texture and spectral indices to red-green-blue (RGB) bands the PA improved with 12% and the UA with 6%. Building extraction from visual aerial surveys can be accurate, and archive images can be involved in the time series of a monitoring.},
DOI = {10.3390/rs12152397}
}



@Article{rs12152426,
AUTHOR = {Pleșoianu, Alin-Ionuț and Stupariu, Mihai-Sorin and Șandric, Ionuț and Pătru-Stupariu, Ileana and Drăguț, Lucian},
TITLE = {Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2426},
URL = {https://www.mdpi.com/2072-4292/12/15/2426},
ISSN = {2072-4292},
ABSTRACT = {Traditional methods for individual tree-crown (ITC) detection (image classification, segmentation, template matching, etc.) applied to very high-resolution remote sensing imagery have been shown to struggle in disparate landscape types or image resolutions due to scale problems and information complexity. Deep learning promised to overcome these shortcomings due to its superior performance and versatility, proven with reported detection rates of ~90%. However, such models still find their limits in transferability across study areas, because of different tree conditions (e.g., isolated trees vs. compact forests) and/or resolutions of the input data. This study introduces a highly replicable deep learning ensemble design for ITC detection and species classification based on the established single shot detector (SSD) model. The ensemble model design is based on varying the input data for the SSD models, coupled with a voting strategy for the output predictions. Very high-resolution unmanned aerial vehicles (UAV), aerial remote sensing imagery and elevation data are used in different combinations to test the performance of the ensemble models in three study sites with highly contrasting spatial patterns. The results show that ensemble models perform better than any single SSD model, regardless of the local tree conditions or image resolution. The detection performance and the accuracy rates improved by 3&ndash;18% with only as few as two participant single models, regardless of the study site. However, when more than two models were included, the performance of the ensemble models only improved slightly and even dropped.},
DOI = {10.3390/rs12152426}
}



@Article{rs12152427,
AUTHOR = {Cai, Yiming and Ding, Yalin and Zhang, Hongwen and Xiu, Jihong and Liu, Zhiming},
TITLE = {Geo-Location Algorithm for Building Targets in Oblique Remote Sensing Images Based on Deep Learning and Height Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2427},
URL = {https://www.mdpi.com/2072-4292/12/15/2427},
ISSN = {2072-4292},
ABSTRACT = {To improve the accuracy of the geographic positioning of a single aerial remote sensing image, the height information of a building in the image must be considered. Oblique remote sensing images are essentially two-dimensional images and produce a large positioning error if a traditional positioning algorithm is used to locate the building directly. To address this problem, this study uses a convolutional neural network to automatically detect the location of buildings in remote sensing images. Moreover, it optimizes an automatic building recognition algorithm for oblique aerial remote sensing images based on You Only Look Once V4 (YOLO V4). This study also proposes a positioning algorithm for the building target, which uses the imaging angle to estimate the height of a building, and combines the spatial coordinate transformation matrix to calculate high-accuracy geo-location of target buildings. Simulation analysis shows that the traditional positioning algorithm inevitably leads to large errors in the positioning of building targets. When the target height is 50 m and the imaging angle is 70&deg;, the positioning error is 114.89 m. Flight tests show that the algorithm established in this study can improve the positioning accuracy of building targets by approximately 20%&ndash;50% depending on the difference in target height.},
DOI = {10.3390/rs12152427}
}



@Article{robotics9030059,
AUTHOR = {Chaoui, Hicham and Yadav, Sumit and Ahmadi, Rosita Sharif and Bouzid, Allal El Moubarek},
TITLE = {Adaptive Interval Type-2 Fuzzy Logic Control of a Three Degree-of-Freedom Helicopter},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2218-6581/9/3/59},
ISSN = {2218-6581},
ABSTRACT = {This paper combines interval type-2 fuzzy logic with adaptive control theory for the control of a three degree-of-freedom (DOF) helicopter. This strategy yields robustness to various kinds of uncertainties and guaranteed stability of the closed-loop control system. Thus, precise trajectory tracking is maintained under various operational conditions with the presence of various types of uncertainties. Unlike other controllers, the proposed controller approximates the helicopter&rsquo;s inverse dynamic model and assumes no a priori knowledge of the helicopter&rsquo;s dynamics or parameters. The proposed controller is applied to a 3-DOF helicopter model and compared against three other controllers, i.e., PID control, adaptive control, and adaptive sliding-mode control. Numerical results show its high performance and robustness under the presence of uncertainties. To better assess the performance of the control system, two quantitative tracking performance metrics are introduced, i.e., the integral of the tracking errors and the integral of the control signals. Comparative numerical results reveal the superiority of the proposed method by achieving the highest tracking accuracy with the lowest control effort.},
DOI = {10.3390/robotics9030059}
}



@Article{smartcities3030039,
AUTHOR = {Su, Wen-Hao},
TITLE = {Advanced Machine Learning in Point Spectroscopy, RGB- and Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review},
JOURNAL = {Smart Cities},
VOLUME = {3},
YEAR = {2020},
NUMBER = {3},
PAGES = {767--792},
URL = {https://www.mdpi.com/2624-6511/3/3/39},
ISSN = {2624-6511},
ABSTRACT = {Crop productivity is readily reduced by competition from weeds. It is particularly important to control weeds early to prevent yield losses. Limited herbicide choices and increasing costs of weed management are threatening the profitability of crops. Smart agriculture can use intelligent technology to accurately measure the distribution of weeds in the field and perform weed control tasks in selected areas, which cannot only improve the effectiveness of pesticides, but also increase the economic benefits of agricultural products. The most important thing for an automatic system to remove weeds within crop rows is to utilize reliable sensing technology to achieve accurate differentiation of weeds and crops at specific locations in the field. In recent years, there have been many significant achievements involving the differentiation of crops and weeds. These studies are related to the development of rapid and non-destructive sensors, as well as the analysis methods for the data obtained. This paper presents a review of the use of three sensing methods including spectroscopy, color imaging, and hyperspectral imaging in the discrimination of crops and weeds. Several algorithms of machine learning have been employed for data analysis such as convolutional neural network (CNN), artificial neural network (ANN), and support vector machine (SVM). Successful applications include the weed detection in grain crops (such as maize, wheat, and soybean), vegetable crops (such as tomato, lettuce, and radish), and fiber crops (such as cotton) with unsupervised or supervised learning. This review gives a brief introduction into proposed sensing and machine learning methods, then provides an overview of instructive examples of these techniques for weed/crop discrimination. The discussion describes the recent progress made in the development of automated technology for accurate plant identification as well as the challenges and future prospects. It is believed that this review is of great significance to those who study automatic plant care in crops using intelligent technology.},
DOI = {10.3390/smartcities3030039}
}



@Article{jimaging6080076,
AUTHOR = {Daffara, Claudia and Muradore, Riccardo and Piccinelli, Nicola and Gaburro, Nicola and de Rubeis, Tullio and Ambrosini, Dario},
TITLE = {A Cost-Effective System for Aerial 3D Thermography of Buildings},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {76},
URL = {https://www.mdpi.com/2313-433X/6/8/76},
ISSN = {2313-433X},
ABSTRACT = {Three-dimensional (3D) imaging and infrared (IR) thermography are powerful tools in many areas in engineering and sciences. Their joint use is of great interest in the buildings sector, allowing inspection and non-destructive testing of elements as well as an evaluation of the energy efficiency. When dealing with large and complex structures, as buildings (particularly historical) generally are, 3D thermography inspection is enhanced by Unmanned Aerial Vehicles (UAV&mdash;also known as drones). The aim of this paper is to propose a simple and cost-effective system for aerial 3D thermography of buildings. Special attention is thus payed to instrument and reconstruction software choice. After a very brief introduction to IR thermography for buildings and 3D thermography, the system is described. Some experimental results are given to validate the proposal.},
DOI = {10.3390/jimaging6080076}
}



@Article{rs12152490,
AUTHOR = {Ichim, Loretta and Popescu, Dan},
TITLE = {Segmentation of Vegetation and Flood from Aerial Images Based on Decision Fusion of Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {15},
ARTICLE-NUMBER = {2490},
URL = {https://www.mdpi.com/2072-4292/12/15/2490},
ISSN = {2072-4292},
ABSTRACT = {The detection and evaluation of flood damage in rural zones are of great importance for farmers, local authorities, and insurance companies. To this end, the paper proposes an efficient system based on five neural networks to assess the degree of flooding and the remaining vegetation. After a previous analysis the following neural networks were selected as primary classifiers: you only look once network (YOLO), generative adversarial network (GAN), AlexNet, LeNet, and residual network (ResNet). Their outputs were connected in a decision fusion scheme, as a new convolutional layer, considering two sets of components: (a) the weights, corresponding to the proven accuracy of the primary neural networks in the validation phase, and (b) the probabilities generated by the neural networks as primary classification results in the operational (testing) phase. Thus, a subjective behavior (individual interpretation of single neural networks) was transformed into a more objective behavior (interpretation based on fusion of information). The images, difficult to be segmented, were obtained from an unmanned aerial vehicle photogrammetry flight after a moderate flood in a rural region of Romania and make up our database. For segmentation and evaluation of the flooded zones and vegetation, the images were first decomposed in patches and, after classification the resulting marked patches were re-composed in segmented images. From the performance analysis point of view, better results were obtained with the proposed system than the neural networks taken separately and with respect to some works from the references.},
DOI = {10.3390/rs12152490}
}



@Article{app10165436,
AUTHOR = {Kim, Dong-Hyun and Go, Yong-Guk and Choi, Soo-Mi},
TITLE = {An Aerial Mixed-Reality Environment for First-Person-View Drone Flying},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/2076-3417/10/16/5436},
ISSN = {2076-3417},
ABSTRACT = {A drone be able to fly without colliding to preserve the surroundings and its own safety. In addition, it must also incorporate numerous features of interest for drone users. In this paper, an aerial mixed-reality environment for first-person-view drone flying is proposed to provide an immersive experience and a safe environment for drone users by creating additional virtual obstacles when flying a drone in an open area. The proposed system is effective in perceiving the depth of obstacles, and enables bidirectional interaction between real and virtual worlds using a drone equipped with a stereo camera based on human binocular vision. In addition, it synchronizes the parameters of the real and virtual cameras to effectively and naturally create virtual objects in a real space. Based on user studies that included both general and expert users, we confirm that the proposed system successfully creates a mixed-reality environment using a flying drone by quickly recognizing real objects and stably combining them with virtual objects.},
DOI = {10.3390/app10165436}
}



@Article{s20164455,
AUTHOR = {Mayor, Vicente and Estepa, Rafael and Estepa, Antonio and Madinabeitia, Germán},
TITLE = {Energy-Efficient UAVs Deployment for QoS-Guaranteed VoWiFi Service},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4455},
URL = {https://www.mdpi.com/1424-8220/20/16/4455},
ISSN = {1424-8220},
ABSTRACT = {This paper formulates a new problem for the optimal placement of Unmanned Aerial Vehicles (UAVs) geared towards wireless coverage provision for Voice over WiFi (VoWiFi) service to a set of ground users confined in an open area. Our objective function is constrained by coverage and by VoIP speech quality and minimizes the ratio between the number of UAVs deployed and energy efficiency in UAVs, hence providing the layout that requires fewer UAVs per hour of service. Solutions provide the number and position of UAVs to be deployed, and are found using well-known heuristic search methods such as genetic algorithms (used for the initial deployment of UAVs), or particle swarm optimization (used for the periodical update of the positions). We examine two communication services: (a) one bidirectional VoWiFi channel per user; (b) single broadcast VoWiFi channel for announcements. For these services, we study the results obtained for an increasing number of users confined in a small area of 100 m2 as well as in a large area of 10,000 m2. Results show that the drone turnover rate is related to both users&rsquo; sparsity and the number of users served by each UAV. For the unicast service, the ratio of UAVs per hour of service tends to increase with user sparsity and the power of radio communication represents 14&ndash;16% of the total UAV energy consumption depending on ground user density. In large areas, solutions tend to locate UAVs at higher altitudes seeking increased coverage, which increases energy consumption due to hovering. However, in the VoWiFi broadcast communication service, the traffic is scarce, and solutions are mostly constrained only by coverage. This results in fewer UAVs deployed, less total power consumption (between 20% and 75%), and less sensitivity to the number of served users.},
DOI = {10.3390/s20164455}
}



@Article{rs12162576,
AUTHOR = {de Bem, Pablo Pozzobon and de Carvalho Júnior, Osmar Abílio and de Carvalho, Osmar Luiz Ferreira and Gomes, Roberto Arnaldo Trancoso and Fontes Guimarães, Renato},
TITLE = {Performance Analysis of Deep Convolutional Autoencoders with Different Patch Sizes for Change Detection from Burnt Areas},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2576},
URL = {https://www.mdpi.com/2072-4292/12/16/2576},
ISSN = {2072-4292},
ABSTRACT = {Fire is one of the primary sources of damages to natural environments globally. Estimates show that approximately 4 million km2 of land burns yearly. Studies have shown that such estimates often underestimate the real extent of burnt land, which highlights the need to find better, state-of-the-art methods to detect and classify these areas. This study aimed to analyze the use of deep convolutional Autoencoders in the classification of burnt areas, considering different sample patch sizes. A simple Autoencoder and the U-Net and ResUnet architectures were evaluated. We collected Landsat 8 OLI+ data from three scenes in four consecutive dates to detect the changes specifically in the form of burnt land. The data were sampled according to four different sampling strategies to evaluate possible performance changes related to sampling window sizes. The training stage used two scenes, while the validation stage used the remaining scene. The ground truth change mask was created using the Normalized Burn Ratio (NBR) spectral index through a thresholding approach. The classifications were evaluated according to the F1 index, Kappa index, and mean Intersection over Union (mIoU) value. Results have shown that the U-Net and ResUnet architectures offered the best classifications with average F1, Kappa, and mIoU values of approximately 0.96, representing excellent classification results. We have also verified that a sampling window size of 256 by 256 pixels offered the best results.},
DOI = {10.3390/rs12162576}
}



@Article{agriculture10080348,
AUTHOR = {Wei, Marcelo Chan Fu and Molin, José Paulo},
TITLE = {Soybean Yield Estimation and Its Components: A Linear Regression Approach},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {348},
URL = {https://www.mdpi.com/2077-0472/10/8/348},
ISSN = {2077-0472},
ABSTRACT = {Soybean yield estimation is either based on yield monitors or agro-meteorological and satellite imagery data, but they present several limiting factors regarding on-farm decision level. Aware that machine learning approaches have been largely applied to estimate soybean yield and the availability of data regarding soybean yield and its components (number of grains (NG) and thousand grains weight (TGW)), there is an opportunity to study their relationships. The objective was to explore the relationships between soybean yield and its components, generate equations to estimate yield and evaluate its prediction accuracy. The training dataset was composed of soybean yield and its components&rsquo; data from 2010 to 2019. Linear regression models based on NG, TGW and yield were fitted on the training dataset and applied to a validation dataset composed of 58 on-field collected samples. It was found that globally TGW and NG presented weak (r = 0.50) and strong (r = 0.92) linear relationships with yield, respectively. In addition to that, applying the fitted models to the validation dataset, model based on NG presented the highest accuracy, coefficient of determination (R2) of 0.70, mean absolute error (MAE) of 639.99 kg ha&minus;1 and root mean squared error (RMSE) of 726.67 kg ha&minus;1.},
DOI = {10.3390/agriculture10080348}
}



@Article{rs12162578,
AUTHOR = {Li, Daoliang and Zhang, Pan and Chen, Tao and Qin, Wei},
TITLE = {Recent Development and Challenges in Spectroscopy and Machine Vision Technologies for Crop Nitrogen Diagnosis: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2578},
URL = {https://www.mdpi.com/2072-4292/12/16/2578},
ISSN = {2072-4292},
ABSTRACT = {Recent development of non-destructive optical techniques, such as spectroscopy and machine vision technologies, have laid a good foundation for real-time monitoring and precise management of crop N status. However, their advantages and disadvantages have not been systematically summarized and evaluated. Here, we reviewed the state-of-the-art of non-destructive optical methods for monitoring the N status of crops, and summarized their advantages and disadvantages. We mainly focused on the contribution of spectral and machine vision technology to the accurate diagnosis of crop N status from three aspects: system selection, data processing, and estimation methods. Finally, we discussed the opportunities and challenges of the application of these technologies, followed by recommendations for future work to address the challenges.},
DOI = {10.3390/rs12162578}
}



@Article{ijgi9080485,
AUTHOR = {Ding, Kaimeng and Liu, Yueming and Xu, Qin and Lu, Fuqiang},
TITLE = {A Subject-Sensitive Perceptual Hash Based on MUM-Net for the Integrity Authentication of High Resolution Remote Sensing Images},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {485},
URL = {https://www.mdpi.com/2220-9964/9/8/485},
ISSN = {2220-9964},
ABSTRACT = {Data security technology is of great significance to the application of high resolution remote sensing image (HRRS) images. As an important data security technology, perceptual hash overcomes the shortcomings of cryptographic hashing that is not robust and can achieve integrity authentication of HRRS images based on perceptual content. However, the existing perceptual hash does not take into account whether the user focuses on certain types of information of the HRRS image. In this paper, we introduce the concept of subject-sensitive perceptual hash, which can be seen as a special case of conventional perceptual hash, for the integrity authentication of HRRS image. To achieve subject-sensitive perceptual hash, we propose a new deep convolutional neural network architecture, named MUM-Net, for extracting robust features of HRRS images. MUM-Net is the core of perceptual hash algorithm, and it uses focal loss as the loss function to overcome the imbalance between the positive and negative samples in the training samples. The robust features extracted by MUM-Net are further compressed and encoded to obtain the perceptual hash sequence of HRRS image. Experiments show that our algorithm has higher tamper sensitivity to subject-related malicious tampering, and the robustness is improved by about 10% compared to the existing U-net-based algorithm; compared to other deep learning-based algorithms, this algorithm achieves a better balance between robustness and tampering sensitivity, and has better overall performance.},
DOI = {10.3390/ijgi9080485}
}



@Article{rs12162640,
AUTHOR = {Vlachopoulos, Odysseas and Leblon, Brigitte and Wang, Jinfei and Haddadi, Ataollah and LaRocque, Armand and Patterson, Greg},
TITLE = {Delineation of Crop Field Areas and Boundaries from UAS Imagery Using PBIA and GEOBIA with Random Forest Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2640},
URL = {https://www.mdpi.com/2072-4292/12/16/2640},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aircraft systems (UAS) have been proven cost- and time-effective remote-sensing platforms for precision agriculture applications. This study presents a method for automatic delineation of field areas and boundaries that uses UAS multispectral orthomosaics acquired over 7 vegetated fields having a variety of crops in Prince Edward Island (PEI). This information is needed by crop insurance agencies and growers for an accurate determination of crop insurance premiums. The field areas and boundaries were delineated by applying both a pixel-based and an object-based supervised random forest (RF) classifier applied to reflectance and vegetation index images, followed by a vectorization pipeline. Both methodologies performed exceptionally well, resulting in a mean area goodness of fit (AGoF) for the field areas greater than 98% and a mean boundary mean positional error (BMPE) lower than 0.8 m for the seven surveyed fields.},
DOI = {10.3390/rs12162640}
}



@Article{rs12162646,
AUTHOR = {Zhang, Shiyu and Zhuo, Li and Zhang, Hui and Li, Jiafeng},
TITLE = {Object Tracking in Unmanned Aerial Vehicle Videos via Multifeature Discrimination and Instance-Aware Attention Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2646},
URL = {https://www.mdpi.com/2072-4292/12/16/2646},
ISSN = {2072-4292},
ABSTRACT = {Visual object tracking in unmanned aerial vehicle (UAV) videos plays an important role in a variety of fields, such as traffic data collection, traffic monitoring, as well as film and television shooting. However, it is still challenging to track the target robustly in UAV vision task due to several factors such as appearance variation, background clutter, and severe occlusion. In this paper, we propose a novel two-stage UAV tracking framework, which includes a target detection stage based on multifeature discrimination and a bounding-box estimation stage based on the instance-aware attention network. In the target detection stage, we explore a feature representation scheme for a small target that integrates handcrafted features, low-level deep features, and high-level deep features. Then, the correlation filter is used to roughly predict target location. In the bounding-box estimation stage, an instance-aware intersection over union (IoU)-Net is integrated together with an instance-aware attention network to estimate the target size based on the bounding-box proposals generated in the target detection stage. Extensive experimental results on the UAV123 and UAVDT datasets show that our tracker, running at over 25 frames per second (FPS), has superior performance as compared with state-of-the-art UAV visual tracking approaches.},
DOI = {10.3390/rs12162646}
}



@Article{rs12162657,
AUTHOR = {Stereńczak, Krzysztof and Zapłata, Rafał and Wójcik, Jarosław and Kraszewski, Bartłomiej and Mielcarek, Miłosz and Mitelsztedt, Krzysztof and Białczak, Małgorzata and Krok, Grzegorz and Kuberski, Łukasz and Markiewicz, Anna and Modzelewska, Aneta and Parkitna, Karolina and Piasecka, Żaneta and Pilch, Kamil and Rzeczycki, Karol and Sadkowski, Rafał and Wietecha, Martyna and Rysiak, Piotr and von Gadow, Klaus and Cieszewski, Chris J.},
TITLE = {ALS-Based Detection of Past Human Activities in the Białowieża Forest—New Evidence of Unknown Remains of Past Agricultural Systems},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {2657},
URL = {https://www.mdpi.com/2072-4292/12/16/2657},
ISSN = {2072-4292},
ABSTRACT = {The Białowieża Forest (BF), a unique ecosystem of historical significance in central Europe, has a long history of assumed human settlement, with at least 200 known archaeological sites (until 2016). This study uncovers new evidence of the cultural heritage of this unique forest area using Airborne Laser Scanning (ALS) technology combined with traditional archaeological field assessment methods to verify the ALS data interpretations and to provide additional evidence about the function and origin of the newly detected archaeological sites. The results of this study include (1) a scientific approach for an improved identification of archaeological resources in forest areas; (2) new evidence about the history of the human use of the BF based on ALS data, covering the entire Polish part of the BF; and (3) an improved remote sensing infrastructure, supporting existing GIS (Geographic Information System) systems for the BF, a famous UNESCO Heritage site. Our study identified numerous locations with evidence of past human agricultural activities known in the literature as &ldquo;field systems&rdquo;, &ldquo;lynchets&rdquo; and &ldquo;Celtic fields&rdquo;. The initial identification included more than 300 km of possible field boundaries and plough headlands, many of which we have verified on the ground. Various past human activities creating those boundaries have existed since the (pre-) Roman Period up to the 13th century AD. The results of this study demonstrate that past human activities in the Polish part of the Białowieża Forest had been more prevalent than previously believed. As a practical result of the described activities, a geodatabase was created; this has practical applications for the system of monument protection in Poland, as well as for local communities and the BF&rsquo;s management and conservation. The more widely achieved results are in line with the implementation of the concept of a cultural heritage inventory in forested and protected areas&mdash;the actions taken specify (built globally) the forms of protection and management of cultural and environmental goods.},
DOI = {10.3390/rs12162657}
}



@Article{s20174709,
AUTHOR = {Wang, Bin and Gu, Yinjuan},
TITLE = {An Improved FBPN-Based Detection Network for Vehicles in Aerial Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {4709},
URL = {https://www.mdpi.com/1424-8220/20/17/4709},
ISSN = {1424-8220},
ABSTRACT = {With the development of artificial intelligence and big data analytics, an increasing number of researchers have tried to use deep-learning technology to train neural networks and achieved great success in the field of vehicle detection. However, as a special domain of object detection, vehicle detection in aerial images still has made limited progress because of low resolution, complex backgrounds and rotating objects. In this paper, an improved feature-balanced pyramid network (FBPN) has been proposed to enhance the network&rsquo;s ability to detect small objects. By combining FBPN with modified faster region convolutional neural network (faster-RCNN), a vehicle detection framework for aerial images is proposed. The focal loss function is adopted in the proposed framework to reduce the imbalance between easy and hard samples. The experimental results based on the VEDIA, USCAS-AOD, and DOTA datasets show that the proposed framework outperforms other state-of-the-art vehicle detection algorithms for aerial images.},
DOI = {10.3390/s20174709}
}



@Article{rs12172729,
AUTHOR = {Yang, Jianxiu and Xie, Xuemei and Shi, Guangming and Yang, Wenzhe},
TITLE = {A Feature-Enhanced Anchor-Free Network for UAV Vehicle Detection},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2729},
URL = {https://www.mdpi.com/2072-4292/12/17/2729},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection based on unmanned aerial vehicle (UAV) images is a challenging task. One reason is that the objects are small size, low-resolution, and large scale variations, resulting in weak feature representation. Another reason is the imbalance between positive and negative examples. In this paper, we propose a novel architecture for UAV vehicle detection to solve above problems. In detail, we use anchor-free mechanism to eliminate predefined anchors, which can reduce complicated computation and relieve the imbalance between positive and negative samples. Meanwhile, to enhance the features for vehicles, we design a multi-scale semantic enhancement block (MSEB) and an effective 49-layer backbone which is based on the DetNet59. The proposed network offers appropriate receptive fields that match the small-sized vehicles, and involves precise localization information provided by the contexts with high resolution. The MSEB strengthens discriminative feature representation at various scales, without reducing the spatial resolution of prediction layers. Experiments show that the proposed method achieves the state-of-the-art performance. Particularly, the main part of vehicles, much smaller ones, the accuracy is about 2% higher than other existing methods.},
DOI = {10.3390/rs12172729}
}



@Article{rs12172732,
AUTHOR = {Abdulridha, Jaafar and Ampatzidis, Yiannis and Qureshi, Jawwad and Roberts, Pamela},
TITLE = {Laboratory and UAV-Based Identification and Classification of Tomato Yellow Leaf Curl, Bacterial Spot, and Target Spot Diseases in Tomato Utilizing Hyperspectral Imaging and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2732},
URL = {https://www.mdpi.com/2072-4292/12/17/2732},
ISSN = {2072-4292},
ABSTRACT = {Tomato crops are susceptible to multiple diseases, several of which may be present during the same season. Therefore, rapid disease identification could enhance crop management consequently increasing the yield. In this study, nondestructive methods were developed to detect diseases that affect tomato crops, such as bacterial spot (BS), target spot (TS), and tomato yellow leaf curl (TYLC) for two varieties of tomato (susceptible and tolerant to TYLC only) by using hyperspectral sensing in two conditions: a) laboratory (benchtop scanning), and b) in field using an unmanned aerial vehicle (UAV-based). The stepwise discriminant analysis (STDA) and the radial basis function were applied to classify the infected plants and distinguish them from noninfected or healthy (H) plants. Multiple vegetation indices (VIs) and the M statistic method were utilized to distinguish and classify the diseased plants. In general, the classification results between healthy and diseased plants were highly accurate for all diseases; for instance, when comparing H vs. BS, TS, and TYLC in the asymptomatic stage and laboratory conditions, the classification rates were 94%, 95%, and 100%, respectively. Similarly, in the symptomatic stage, the classification rates between healthy and infected plants were 98% for BS, and 99&ndash;100% for TS and TYLC diseases. The classification results in the field conditions also showed high values of 98%, 96%, and 100%, for BS, TS, and TYLC, respectively. The VIs that could best identify these diseases were the renormalized difference vegetation index (RDVI), and the modified triangular vegetation index 1 (MTVI 1) in both laboratory and field. The results were promising and suggest the possibility to identify these diseases using remote sensing.},
DOI = {10.3390/rs12172732}
}



@Article{math8091415,
AUTHOR = {Li, Juan and Lei, Hong and Alavi, Amir H. and Wang, Gai-Ge},
TITLE = {Elephant Herding Optimization: Variants, Hybrids, and Applications},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1415},
URL = {https://www.mdpi.com/2227-7390/8/9/1415},
ISSN = {2227-7390},
ABSTRACT = {Elephant herding optimization (EHO) is a nature-inspired metaheuristic optimization algorithm based on the herding behavior of elephants. EHO uses a clan operator to update the distance of the elephants in each clan with respect to the position of a matriarch elephant. The superiority of the EHO method to several state-of-the-art metaheuristic algorithms has been demonstrated for many benchmark problems and in various application areas. A comprehensive review for the EHO-based algorithms and their applications are presented in this paper. Various aspects of the EHO variants for continuous optimization, combinatorial optimization, constrained optimization, and multi-objective optimization are reviewed. Future directions for research in the area of EHO are further discussed.},
DOI = {10.3390/math8091415}
}



@Article{rs12172817,
AUTHOR = {Mao, Wanliu and Lu, Debin and Hou, Li and Liu, Xue and Yue, Wenze},
TITLE = {Comparison of Machine-Learning Methods for Urban Land-Use Mapping in Hangzhou City, China},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2817},
URL = {https://www.mdpi.com/2072-4292/12/17/2817},
ISSN = {2072-4292},
ABSTRACT = {Urban land-use information is important for urban land-resource planning and management. However, current methods using traditional surveys cannot meet the demand for the rapid development of urban land management. There is an urgent need to develop new methods to overcome the shortcomings of conventional methods. To address the issue, this study used the random forest (RF), support vector machine (SVM), and artificial neural network (ANN) models to build machine-leaning methods for urban land-use classification. Taking Hangzhou as an example, these machine-leaning methods could all successfully classify the essential urban land use into 6 Level I classes and 13 Level II classes based on the semantic features extracted from Sentinel-2A images, multi-source features of types of points of interest (POIs), land surface temperature, night lights, and building height. The validation accuracy of the RF model for the Level I and Level II land use was 79.88% and 71.89%, respectively, performing better compared to SVM (78.40% and 68.64%) and ANN models (71.30% and 63.02%). However, the variations of the user accuracy among the methods depended on the urban land-use level. For the Level I land-use classification, the user accuracy was high, except for the transportation land by all methods. In general, the RF and SVM models performed better than the ANN model. For the Level II land-use classification, the user accuracy of different models was quite distinct. With the RF model, the user accuracy of educational and medical land was above 80%. Moreover, with the SVM model, the user accuracy of the business office and educational land classification was above 75%. However, the user accuracy of the ANN model on the Level II land-use classification was poor. Our results showed that the RF model performs best, followed by SVM model, and ANN model was relatively poor in the essential urban land-use classification. The results proved that the use of machine-learning methods can quickly extract land-use types with high accuracy, and provided a better method choice for urban land-use information acquisition.},
DOI = {10.3390/rs12172817}
}



@Article{app10176053,
AUTHOR = {Yu, Hang and Gong, Jiulu and Chen, Derong},
TITLE = {Object Detection Using Multi-Scale Balanced Sampling},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {6053},
URL = {https://www.mdpi.com/2076-3417/10/17/6053},
ISSN = {2076-3417},
ABSTRACT = {Detecting small objects and objects with large scale variants are always challenging for deep learning based object detection approaches. Many efforts have been made to solve these problems such as adopting more effective network structures, image features, loss functions, etc. However, for both small objects detection and detecting objects with various scale in single image, the first thing should be solve is the matching mechanism between anchor boxes and ground-truths. In this paper, an approach based on multi-scale balanced sampling(MB-RPN) is proposed for the difficult matching of small objects and detecting multi-scale objects. According to the scale of the anchor boxes, different positive and negative sample IOU discriminate thresholds are adopted to improve the probability of matching the small object area with the anchor boxes so that more small object samples are included in the training process. Moreover, the balanced sampling method is proposed for the collected samples, the samples are further divided and uniform sampling to ensure the diversity of samples in training process. Several datasets are adopted to evaluate the MB-RPN, the experimental results show that compare with the similar approach, MB-RPN improves detection performances effectively.},
DOI = {10.3390/app10176053}
}



@Article{rs12172833,
AUTHOR = {Arabameri, Alireza and Asadi Nalivan, Omid and Chandra Pal, Subodh and Chakrabortty, Rabin and Saha, Asish and Lee, Saro and Pradhan, Biswajeet and Tien Bui, Dieu},
TITLE = {Novel Machine Learning Approaches for Modelling the Gully Erosion Susceptibility},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {17},
ARTICLE-NUMBER = {2833},
URL = {https://www.mdpi.com/2072-4292/12/17/2833},
ISSN = {2072-4292},
ABSTRACT = {The extreme form of land degradation caused by the formation of gullies is a major challenge for the sustainability of land resources. This problem is more vulnerable in the arid and semi-arid environment and associated damage to agriculture and allied economic activities. Appropriate modeling of such erosion is therefore needed with optimum accuracy for estimating vulnerable regions and taking appropriate initiatives. The Golestan Dam has faced an acute problem of gully erosion over the last decade and has adversely affected society. Here, the artificial neural network (ANN), general linear model (GLM), maximum entropy (MaxEnt), and support vector machine (SVM) machine learning algorithm with 90/10, 80/20, 70/30, 60/40, and 50/50 random partitioning of training and validation samples was selected purposively for estimating the gully erosion susceptibility. The main objective of this work was to predict the susceptible zone with the maximum possible accuracy. For this purpose, random partitioning approaches were implemented. For this purpose, 20 gully erosion conditioning factors were considered for predicting the susceptible areas by considering the multi-collinearity test. The variance inflation factor (VIF) and tolerance (TOL) limit were considered for multi-collinearity assessment for reducing the error of the models and increase the efficiency of the outcome. The ANN with 50/50 random partitioning of the sample is the most optimal model in this analysis. The area under curve (AUC) values of receiver operating characteristics (ROC) in ANN (50/50) for the training and validation data are 0.918 and 0.868, respectively. The importance of the causative factors was estimated with the help of the Jackknife test, which reveals that the most important factor is the topography position index (TPI). Apart from this, the prioritization of all predicted models was estimated taking into account the training and validation data set, which should help future researchers to select models from this perspective. This type of outcome should help planners and local stakeholders to implement appropriate land and water conservation measures.},
DOI = {10.3390/rs12172833}
}



@Article{ijgi9090527,
AUTHOR = {Liu, Jiantao and Feng, Quanlong and Wang, Ying and Batsaikhan, Bayartungalag and Gong, Jianhua and Li, Yi and Liu, Chunting and Ma, Yin},
TITLE = {Urban Green Plastic Cover Mapping Based on VHR Remote Sensing Images and a Deep Semi-Supervised Learning Framework},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {527},
URL = {https://www.mdpi.com/2220-9964/9/9/527},
ISSN = {2220-9964},
ABSTRACT = {With the rapid process of both urban sprawl and urban renewal, large numbers of old buildings have been demolished in China, leading to wide spread construction sites, which could cause severe dust contamination. To alleviate the accompanied dust pollution, green plastic mulch has been widely used by local governments of China. Therefore, timely and accurate mapping of urban green plastic covered regions is of great significance to both urban environmental management and the understanding of urban growth status. However, the complex spatial patterns of the urban landscape make it challenging to accurately identify these areas of green plastic cover. To tackle this issue, we propose a deep semi-supervised learning framework for green plastic cover mapping using very high resolution (VHR) remote sensing imagery. Specifically, a multi-scale deformable convolution neural network (CNN) was exploited to learn representative and discriminative features under complex urban landscapes. Afterwards, a semi-supervised learning strategy was proposed to integrate the limited labeled data and massive unlabeled data for model co-training. Experimental results indicate that the proposed method could accurately identify green plastic-covered regions in Jinan with an overall accuracy (OA) of 91.63%. An ablation study indicated that, compared with supervised learning, the semi-supervised learning strategy in this study could increase the OA by 6.38%. Moreover, the multi-scale deformable CNN outperforms several classic CNN models in the computer vision field. The proposed method is the first attempt to map urban green plastic-covered regions based on deep learning, which could serve as a baseline and useful reference for future research.},
DOI = {10.3390/ijgi9090527}
}



@Article{rs12182866,
AUTHOR = {Ren, Yongfeng and Yu, Yongtao and Guan, Haiyan},
TITLE = {DA-CapsUNet: A Dual-Attention Capsule U-Net for Road Extraction from Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2866},
URL = {https://www.mdpi.com/2072-4292/12/18/2866},
ISSN = {2072-4292},
ABSTRACT = {The up-to-date and information-accurate road database plays a significant role in many applications. Recently, with the improvement in image resolutions and quality, remote sensing images have provided an important data source for road extraction tasks. However, due to the topology variations, spectral diversities, and complex scenarios, it is still challenging to realize fully automated and highly accurate road extractions from remote sensing images. This paper proposes a novel dual-attention capsule U-Net (DA-CapsUNet) for road region extraction by combining the advantageous properties of capsule representations and the powerful features of attention mechanisms. By constructing a capsule U-Net architecture, the DA-CapsUNet can extract and fuse multiscale capsule features to recover a high-resolution and semantically strong feature representation. By designing the multiscale context-augmentation and two types of feature attention modules, the DA-CapsUNet can exploit multiscale contextual properties at a high-resolution perspective and generate an informative and class-specific feature encoding. Quantitative evaluations on a large dataset showed that the DA-CapsUNet provides a competitive road extraction performance with a precision of 0.9523, a recall of 0.9486, and an F-score of 0.9504, respectively. Comparative studies with eight recently developed deep learning methods also confirmed the applicability and superiority or compatibility of the DA-CapsUNet in road extraction tasks.},
DOI = {10.3390/rs12182866}
}



@Article{app10186147,
AUTHOR = {Li, Jin and Yan, Daifu and Luan, Kuan and Li, Zeyu and Liang, Hong},
TITLE = {Deep Learning-Based Bird’s Nest Detection on Transmission Lines Using UAV Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6147},
URL = {https://www.mdpi.com/2076-3417/10/18/6147},
ISSN = {2076-3417},
ABSTRACT = {In order to ensure the safety of transmission lines, the use of unmanned aerial vehicle (UAV) images for automatic object detection has important application prospects, such as the detection of birds&rsquo; nests. The traditional bird&rsquo;s nest detection methods mainly include the study of morphological characteristics of the bird&rsquo;s nest. These methods have poor applicability and low accuracy. In this work, we propose a deep learning-based birds&rsquo; nests automatic detection framework&mdash;region of interest (ROI) mining faster region-based convolutional neural networks (RCNN). First, the prior dimensions of anchors are obtained by using k-means clustering to improve the accuracy of coordinate boxes generation. Second, in order to balance the number of foreground and background samples in the training process, the focal loss function is introduced in the region proposal network (RPN) classification stage. Finally, the ROI mining module is added to solve the class imbalance problem in the classification stage, combined with the characteristics of difficult-to-classify bird&rsquo;s nest samples in the UAV images. After parameter optimization and experimental verification, the deep learning-based bird&rsquo;s nest automatic detection framework proposed in this work achieves high detection accuracy. In addition, the mean average precision (mAP) and formula 1 (F1) score of the proposed method are higher than the original faster RCNN and cascade RCNN. Our comparative analysis verifies the effectiveness of the proposed method.},
DOI = {10.3390/app10186147}
}



@Article{math8091499,
AUTHOR = {Tran, Huu Khoa and Chiou, Juing-Shian and Dang, Viet-Hung},
TITLE = {New Fusion Algorithm-Reinforced Pilot Control for an Agricultural Tricopter UAV},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1499},
URL = {https://www.mdpi.com/2227-7390/8/9/1499},
ISSN = {2227-7390},
ABSTRACT = {Currently, fuzzy proportional integral derivative (PID) controller schemes, which include simplified fuzzy reasoning decision methodologies and PID parameters, are broadly and efficaciously practiced in various fields from industrial applications, military service, to rescue operations, civilian information and also horticultural observation and agricultural surveillance. A fusion particle swarm optimization (PSO)&ndash;evolutionary programming (EP) algorithm, which is an improved version of the stochastic optimization strategy PSO, was presented for designing and optimizing controller gains in this study. The mathematical calculations of this study include the reproduction of EP with PSO. By minimizing the integral of the absolute error (IAE) criterion that is used for estimating the system response as a fitness function, the obtained integrated design of the fusion PSO&ndash;EP algorithm generated and updated the new elite parameters for proposed controller schemes. This progression was used for the complicated non-linear systems of the attitude-control pilot models of a tricopter unmanned aerial vehicle (UAV) to demonstrate an improvement on the performance in terms of rapid response, precision, reliability, and stability.},
DOI = {10.3390/math8091499}
}



@Article{s20185055,
AUTHOR = {Guo, Yahui and Wang, Hanxi and Wu, Zhaofei and Wang, Shuxin and Sun, Hongyong and Senthilnath, J. and Wang, Jingzhe and Robin Bryant, Christopher and Fu, Yongshuo},
TITLE = {Modified Red Blue Vegetation Index for Chlorophyll Estimation and Yield Prediction of Maize from Visible Images Captured by UAV},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5055},
URL = {https://www.mdpi.com/1424-8220/20/18/5055},
ISSN = {1424-8220},
ABSTRACT = {The vegetation index (VI) has been successfully used to monitor the growth and to predict the yield of agricultural crops. In this paper, a long-term observation was conducted for the yield prediction of maize using an unmanned aerial vehicle (UAV) and estimations of chlorophyll contents using SPAD-502. A new vegetation index termed as modified red blue VI (MRBVI) was developed to monitor the growth and to predict the yields of maize by establishing relationships between MRBVI- and SPAD-502-based chlorophyll contents. The coefficients of determination (R2s) were 0.462 and 0.570 in chlorophyll contents&rsquo; estimations and yield predictions using MRBVI, and the results were relatively better than the results from the seven other commonly used VI approaches. All VIs during the different growth stages of maize were calculated and compared with the measured values of chlorophyll contents directly, and the relative error (RE) of MRBVI is the lowest at 0.355. Further, machine learning (ML) methods such as the backpropagation neural network model (BP), support vector machine (SVM), random forest (RF), and extreme learning machine (ELM) were adopted for predicting the yields of maize. All VIs calculated for each image captured during important phenological stages of maize were set as independent variables and the corresponding yields of each plot were defined as dependent variables. The ML models used the leave one out method (LOO), where the root mean square errors (RMSEs) were 2.157, 1.099, 1.146, and 1.698 (g/hundred grain weight) for BP, SVM, RF, and ELM. The mean absolute errors (MAEs) were 1.739, 0.886, 0.925, and 1.356 (g/hundred grain weight) for BP, SVM, RF, and ELM, respectively. Thus, the SVM method performed better in predicting the yields of maize than the other ML methods. Therefore, it is strongly suggested that the MRBVI calculated from images acquired at different growth stages integrated with advanced ML methods should be used for agricultural- and ecological-related chlorophyll estimation and yield predictions.},
DOI = {10.3390/s20185055}
}



@Article{electronics9091459,
AUTHOR = {Kundid Vasić, Mirela and Papić, Vladan},
TITLE = {Multimodel Deep Learning for Person Detection in Aerial Images},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1459},
URL = {https://www.mdpi.com/2079-9292/9/9/1459},
ISSN = {2079-9292},
ABSTRACT = {In this paper, we propose a novel method for person detection in aerial images of nonurban terrain gathered by an Unmanned Aerial Vehicle (UAV), which plays an important role in Search And Rescue (SAR) missions. The UAV in SAR operations contributes significantly due to the ability to survey a larger geographical area from an aerial viewpoint. Because of the high altitude of recording, the object of interest (person) covers a small part of an image (around 0.1%), which makes this task quite challenging. To address this problem, a multimodel deep learning approach is proposed. The solution consists of two different convolutional neural networks in region proposal, as well as in the classification stage. Additionally, contextual information is used in the classification stage in order to improve the detection results. Experimental results tested on the HERIDAL dataset achieved precision of 68.89% and a recall of 94.65%, which is better than current state-of-the-art methods used for person detection in similar scenarios. Consequently, it may be concluded that this approach is suitable for usage as an auxiliary method in real SAR operations.},
DOI = {10.3390/electronics9091459}
}



@Article{s20185073,
AUTHOR = {Khan, Khalil and Albattah, Waleed and Khan, Rehan Ullah and Qamar, Ali Mustafa and Nayab, Durre},
TITLE = {Advances and Trends in Real Time Visual Crowd Analysis},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5073},
URL = {https://www.mdpi.com/1424-8220/20/18/5073},
ISSN = {1424-8220},
ABSTRACT = {Real time crowd analysis represents an active area of research within the computer vision community in general and scene analysis in particular. Over the last 10 years, various methods for crowd management in real time scenario have received immense attention due to large scale applications in people counting, public events management, disaster management, safety monitoring an so on. Although many sophisticated algorithms have been developed to address the task; crowd management in real time conditions is still a challenging problem being completely solved, particularly in wild and unconstrained conditions. In the proposed paper, we present a detailed review of crowd analysis and management, focusing on state-of-the-art methods for both controlled and unconstrained conditions. The paper illustrates both the advantages and disadvantages of state-of-the-art methods. The methods presented comprise the seminal research works on crowd management, and monitoring and then culminating state-of-the-art methods of the newly introduced deep learning methods. Comparison of the previous methods is presented, with a detailed discussion of the direction for future research work. We believe this review article will contribute to various application domains and will also augment the knowledge of the crowd analysis within the research community.},
DOI = {10.3390/s20185073}
}



@Article{app10186210,
AUTHOR = {Zheng, Ruihao and Xiong, Chen and Deng, Xiangbin and Li, Qiangsheng and Li, Yi},
TITLE = {Assessment of Earthquake Destructive Power to Structures Based on Machine Learning Methods},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {6210},
URL = {https://www.mdpi.com/2076-3417/10/18/6210},
ISSN = {2076-3417},
ABSTRACT = {This study presents a machine learning-based method for the destructive power assessment of earthquake to structures. First, the analysis procedure of the method is presented, and the backpropagation neural network (BPNN) and convolutional neural network (CNN) are used as the machine learning algorithms. Second, the optimized BPNN architecture is obtained by discussing the influence of a different number of hidden layers and nodes. Third, the CNN architecture is proposed based on several classical deep learning networks. To build the machine learning models, 50,570 time-history analysis results of a structural system subjected to different ground motions are used as training, validation, and test samples. The results of the BPNN indicate that the features extraction method based on the short-time Fourier transform (STFT) can well reflect the frequency-/time-domain characteristics of ground motions. The results of the CNN indicate that the CNN exhibits better accuracy (R2 = 0.8737) compared with that of the BPNN (R2 = 0.6784). Furthermore, the CNN model exhibits remarkable computational efficiency, the prediction of 1000 structures based on the CNN model takes 0.762 s, while 507.81 s are required for the conventional time-history analysis (THA)-based simulation. Feature visualization of different layers of the CNN reveals that the shallow to deep layers of the CNN can extract the high to low-frequency features of ground motions. The proposed method can assist in the fast prediction of engineering demand parameters of large-number structures, which facilitates the damage or loss assessments of regional structures for timely emergency response and disaster relief after earthquake.},
DOI = {10.3390/app10186210}
}



@Article{ijgi9090539,
AUTHOR = {Huang, Faming and Yang, Jianbo and Zhang, Biao and Li, Yijing and Huang, Jinsong and Chen, Na},
TITLE = {Regional Terrain Complexity Assessment Based on Principal Component Analysis and Geographic Information System: A Case of Jiangxi Province, China},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {539},
URL = {https://www.mdpi.com/2220-9964/9/9/539},
ISSN = {2220-9964},
ABSTRACT = {Regional terrain complexity assessment (TCA) is an important theoretical foundation for geological feature identification, hydrological information extraction and land resources utilization. However, the previous TCA models have many disadvantages; for example, comprehensive consideration and redundancy information analysis of terrain factors is lacking, and the terrain complexity index is difficult to quantify. To overcome these drawbacks, a TCA model based on principal component analysis (PCA) and a geographic information system (GIS) is proposed. Taking Jiangxi province of China as an example, firstly, ten terrain factors are extracted using a digital elevation model (DEM) in GIS software. Secondly, PCA is used to analyze the information redundancy of these terrain factors and deal with data compression. Then, the comprehensive evaluation of the compressed terrain factors is conducted to obtain quantitative terrain complexity indexes and a terrain complexity map (TCM). Finally, the TCM produced by the PCA method is compared with those produced by the slope-only, the variation coefficient and K-means clustering models based on the topographic map drawn by the Bureau of Land and Resources of Jiangxi province. Meanwhile, the TCM is also verified by the actual three-dimensional aerial images. Results show that the correlation coefficients between the TCMs produced by the PCA, slope-only, variable coefficient and K-means clustering models and the local topographic map are 0.894, 0.763, 0.816 and 0.788, respectively. It is concluded that the TCM of the PCA method matches well with the actual field terrain features, and the PCA method can reflect the regional terrain complexity characteristics more comprehensively and accurately when compared to the other three methods.},
DOI = {10.3390/ijgi9090539}
}



@Article{s20185130,
AUTHOR = {Guo, Yahui and Yin, Guodong and Sun, Hongyong and Wang, Hanxi and Chen, Shouzhi and Senthilnath, J. and Wang, Jingzhe and Fu, Yongshuo},
TITLE = {Scaling Effects on Chlorophyll Content Estimations with RGB Camera Mounted on a UAV Platform Using Machine-Learning Methods},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5130},
URL = {https://www.mdpi.com/1424-8220/20/18/5130},
ISSN = {1424-8220},
ABSTRACT = {Timely monitoring and precise estimation of the leaf chlorophyll contents of maize are crucial for agricultural practices. The scale effects are very important as the calculated vegetation index (VI) were crucial for the quantitative remote sensing. In this study, the scale effects were investigated by analyzing the linear relationships between VI calculated from red&ndash;green&ndash;blue (RGB) images from unmanned aerial vehicles (UAV) and ground leaf chlorophyll contents of maize measured using SPAD-502. The scale impacts were assessed by applying different flight altitudes and the highest coefficient of determination (R2) can reach 0.85. We found that the VI from images acquired from flight altitude of 50 m was better to estimate the leaf chlorophyll contents using the DJI UAV platform with this specific camera (5472 &times; 3648 pixels). Moreover, three machine-learning (ML) methods including backpropagation neural network (BP), support vector machine (SVM), and random forest (RF) were applied for the grid-based chlorophyll content estimation based on the common VI. The average values of the root mean square error (RMSE) of chlorophyll content estimations using ML methods were 3.85, 3.11, and 2.90 for BP, SVM, and RF, respectively. Similarly, the mean absolute error (MAE) were 2.947, 2.460, and 2.389, for BP, SVM, and RF, respectively. Thus, the ML methods had relative high precision in chlorophyll content estimations using VI; in particular, the RF performed better than BP and SVM. Our findings suggest that the integrated ML methods with RGB images of this camera acquired at a flight altitude of 50 m (spatial resolution 0.018 m) can be perfectly applied for estimations of leaf chlorophyll content in agriculture.},
DOI = {10.3390/s20185130}
}



@Article{robotics9030069,
AUTHOR = {Noonan, John and Rivlin, Ehud and Rotstein, Hector},
TITLE = {FloorVLoc: A Modular Approach to Floorplan Monocular Localization},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/2218-6581/9/3/69},
ISSN = {2218-6581},
ABSTRACT = {Intelligent vehicles for search and rescue, whose mission is assisting emergency personnel by visually exploring an unfamiliar building, require accurate localization. With GPS not available, and approaches relying on new infrastructure installation, artificial landmarks, or pre-constructed dense 3D maps not feasible, the question is whether there is an approach which can combine ubiquitous prior map information with a monocular camera for accurate positioning. Enter FloorVLoc&mdash;Floorplan Vision Vehicle Localization. We provide a means to integrate a monocular camera with a floorplan in a unified and modular fashion so that any monocular visual Simultaneous Localization and Mapping (SLAM) system can be seamlessly incorporated for global positioning. Using a floorplan is especially beneficial since walls are geometrically stable, the memory footprint is low, and prior map information is kept at a minimum. Furthermore, our theoretical analysis of the visual features associated with the walls shows how drift is corrected. To see this approach in action, we developed two full global positioning systems based on the core methodology introduced, operating in both Monte Carlo Localization and linear optimization frameworks. Experimental evaluation of the systems in simulation and a challenging real-world environment demonstrates that FloorVLoc performs with an average error of 0.06 m across 80 m in real-time.},
DOI = {10.3390/robotics9030069}
}



@Article{rs12182934,
AUTHOR = {Xu, Jin and Quackenbush, Lindi J. and Volk, Timothy A. and Im, Jungho},
TITLE = {Forest and Crop Leaf Area Index Estimation Using Remote Sensing: Research Trends and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2934},
URL = {https://www.mdpi.com/2072-4292/12/18/2934},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is an important vegetation leaf structure parameter in forest and agricultural ecosystems. Remote sensing techniques can provide an effective alternative to field-based observation of LAI. Differences in canopy structure result in different sensor types (active or passive), platforms (terrestrial, airborne, or satellite), and models being appropriate for the LAI estimation of forest and agricultural systems. This study reviews the application of remote sensing-based approaches across different system configurations (passive, active, and multisource sensors on different collection platforms) that are used to estimate forest and crop LAI and explores uncertainty analysis in LAI estimation. A comparison of the difference in LAI estimation for forest and agricultural applications given the different structure of these ecosystems is presented, particularly as this relates to spatial scale. The ease of use of empirical models supports these as the preferred choice for forest and crop LAI estimation. However, performance variation among different empirical models for forest and crop LAI estimation limits the broad application of specific models. The development of models that facilitate the strategic incorporation of local physiology and biochemistry parameters for specific forests and crop growth stages from various temperature zones could improve the accuracy of LAI estimation models and help develop models that can be applied more broadly. In terms of scale issues, both spectral and spatial scales impact the estimation of LAI. Exploration of the quantitative relationship between scales of data from different sensors could help forest and crop managers more appropriately and effectively apply different data sources. Uncertainty coming from various sources results in reduced accuracy in estimating LAI. While Bayesian approaches have proven effective to quantify LAI estimation uncertainty based on the uncertainty of model inputs, there is still a need to quantify uncertainty from remote sensing data source, ground measurements and related environmental factors to mitigate the impacts of model uncertainty and improve LAI estimation.},
DOI = {10.3390/rs12182934}
}



@Article{s20185186,
AUTHOR = {Rutkowski, Adam and Kawalec, Adam},
TITLE = {Some of Problems of Direction Finding of Ground-Based Radars Using Monopulse Location System Installed on Unmanned Aerial Vehicle},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {5186},
URL = {https://www.mdpi.com/1424-8220/20/18/5186},
ISSN = {1424-8220},
ABSTRACT = {Locating active radars in real environmental conditions is a very important and complex task. The efficiency of the direction finding (DF) of ground-based radars and other microwave emitters using unmanned aerial vehicles (UAV) is dependent on the parameters of applied devices for angle location of microwave emitters, and on the construction and modes of operation of the observed transmitting antenna systems. An additional factor having the influence on DF of the radar, when are used systems installed on the UAV, is the rotation of the antenna of a radar. The accuracy of estimation of direction of any microwave transmitter is determined by the terrain properties that surround the transmitter and the objects reflecting microwave signals. The exemplary shapes of the radar antenna patterns and the associated relationships with the probability of remotely detecting the radar and determining its bearings are described. The simulated patterns of the signals received at an emitter-locating device mounted on a UAV and the expected results of a monopulse DF based on these signals are presented. The novelty of this work is the analysis of the DF efficiency of radars in conditions where intense multi-path phenomena appear, and for various amplitudes and phases of the direct signal and multi-path signals that reach the UAV when assuming that so-called simple signals and linear frequency modulation (LFM) signals are transmitted by the radar. The primary focus is on multi-path phenomenon, which can make it difficult, but not entirely impossible, to detect activity and location of radar with a low-flying small UAV and using only monopulse techniques, that is, when only a single pulse emitted by a radar must be sufficient to DF of this radar. Direction of arrival (DOA) algorithms of signals in dense signal environment were not presented in the work, but relevant suggestions were made for the design of such algorithms.},
DOI = {10.3390/s20185186}
}



@Article{rs12182977,
AUTHOR = {Sapkota, Bishwa and Singh, Vijay and Neely, Clark and Rajan, Nithya and Bagavathiannan, Muthukumar},
TITLE = {Detection of Italian Ryegrass in Wheat and Prediction of Competitive Interactions Using Remote-Sensing and Machine-Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2977},
URL = {https://www.mdpi.com/2072-4292/12/18/2977},
ISSN = {2072-4292},
ABSTRACT = {Italian ryegrass (Lolium perenne ssp. multiflorum (Lam) Husnot) is a troublesome weed species in wheat (Triticum aestivum) production in the United States, severely affecting grain yields. Spatial mapping of ryegrass infestation in wheat fields and early prediction of its impact on yield can assist management decision making. In this study, unmanned aerial systems (UAS)-based red, green and blue (RGB) imageries acquired at an early wheat growth stage in two different experimental sites were used for developing predictive models. Deep neural networks (DNNs) coupled with an extensive feature selection method were used to detect ryegrass in wheat and estimate ryegrass canopy coverage. Predictive models were developed by regressing early-season ryegrass canopy coverage (%) with end-of-season (at wheat maturity) biomass and seed yield of ryegrass, as well as biomass and grain yield reduction (%) of wheat. Italian ryegrass was detected with high accuracy (precision = 95.44 &plusmn; 4.27%, recall = 95.48 &plusmn; 5.05%, F-score = 95.56 &plusmn; 4.11%) using the best model which included four features: hue, saturation, excess green index, and visible atmospheric resistant index. End-of-season ryegrass biomass was predicted with high accuracy (R2 = 0.87), whereas the other variables had moderate to high accuracy levels (R2 values of 0.74 for ryegrass seed yield, 0.73 for wheat biomass reduction, and 0.69 for wheat grain yield reduction). The methodology demonstrated in the current study shows great potential for mapping and quantifying ryegrass infestation and predicting its competitive response in wheat, allowing for timely management decisions.},
DOI = {10.3390/rs12182977}
}



@Article{rs12182982,
AUTHOR = {Gée, Christelle and Denimal, Emmanuel},
TITLE = {RGB Image-Derived Indicators for Spatial Assessment of the Impact of Broadleaf Weeds on Wheat Biomass},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {2982},
URL = {https://www.mdpi.com/2072-4292/12/18/2982},
ISSN = {2072-4292},
ABSTRACT = {In precision agriculture, the development of proximal imaging systems embedded in autonomous vehicles allows to explore new weed management strategies for site-specific plant application. Accurate monitoring of weeds while controlling wheat growth requires indirect measurements of leaf area index (LAI) and above-ground dry matter biomass (BM) at early growth stages. This article explores the potential of RGB images to assess crop-weed competition in a wheat (Triticum aestivum L.) crop by generating two new indicators, the weed pressure (WP) and the local wheat biomass production (&delta;BMc). The fractional vegetation cover (FVC) of the crop and the weeds was automatically determined from the images with a SVM-RBF classifier, using bag of visual word vectors as inputs. It is based on a new vegetation index called MetaIndex, defined as a vote of six indices widely used in the literature. Beyond a simple map of weed infestation, the map of WP describes the crop-weed competition. The map of &delta;BMc, meanwhile, evaluates the local wheat above-ground biomass production and informs us about a potential stress. It is generated from the wheat FVC because it is highly correlated with LAI (r2 = 0.99) and BM (r2 = 0.93) obtained by destructive methods. By combining these two indicators, we aim at determining whether the origin of the wheat stress is due to weeds or not. This approach opens up new perspectives for the monitoring of weeds and the monitoring of their competition during crop growth with non-destructive and proximal sensing technologies in the early stages of development.},
DOI = {10.3390/rs12182982}
}



@Article{atmos11090987,
AUTHOR = {Kim, Hyun Il and Han, Kun Yeun},
TITLE = {Linking Hydraulic Modeling with a Machine Learning Approach for Extreme Flood Prediction and Response},
JOURNAL = {Atmosphere},
VOLUME = {11},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {987},
URL = {https://www.mdpi.com/2073-4433/11/9/987},
ISSN = {2073-4433},
ABSTRACT = {An emergency action plan (EAP) for reservoirs and urban areas downstream of dams can alleviate damage caused by extreme flooding. An EAP is a disaster action plan that can designate evacuation paths for vulnerable districts. Generally, calculation of dam-break discharge in accordance with dam inflow conditions, calculation of maximum water surface elevation as per hydraulic channel routing, and flood map generation using topographical data are prepared for the purposes of creating an EAP. However, rainfall and flood patterns exhibited in the context of climate change can be extremely diverse. In order to prepare an efficient flood response, techniques should be considered that are capable of generating flood maps promptly while taking dam inflow conditions into account. Therefore, this study aims to propose methodology that is capable of generating flood maps rapidly for any dam inflow conditions. The proposed methodology was performed by linking a dynamic numerical analysis model (DAMBRK) with a random forest regression technique. The previous standard method of drawing flood maps often requires a significant amount of time depending on accuracy and personnel availability; however, the technique proposed here is capable of generating a flood map within one minute. Through use of this methodology, the time taken to prepare flood maps in large-scale water-disaster situations can be reduced. Moreover, methodology for estimating flood risk via use of flood mapping has been proposed. This study would provide assistance in establishing disaster countermeasures that take various flood scenarios into account by promptly providing flood inundation information to disaster-related agencies.},
DOI = {10.3390/atmos11090987}
}



@Article{rs12183007,
AUTHOR = {Liu, Bo and Du, Shihong and Du, Shouji and Zhang, Xiuyuan},
TITLE = {Incorporating Deep Features into GEOBIA Paradigm for Remote Sensing Imagery Classification: A Patch-Based Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3007},
URL = {https://www.mdpi.com/2072-4292/12/18/3007},
ISSN = {2072-4292},
ABSTRACT = {The fast and accurate creation of land use/land cover maps from very-high-resolution (VHR) remote sensing imagery is crucial for urban planning and environmental monitoring. Geographic object-based image analysis methods (GEOBIA) provide an effective solution using image objects instead of individual pixels in VHR remote sensing imagery analysis. Simultaneously, convolutional neural networks (CNN) have been widely used in the image processing field because of their powerful feature extraction capabilities. This study presents a patch-based strategy for integrating deep features into GEOBIA for VHR remote sensing imagery classification. To extract deep features from irregular image objects through CNN, a patch-based approach is proposed for representing image objects and learning patch-based deep features, and a deep features aggregation method is proposed for aggregating patch-based deep features into object-based deep features. Finally, both object and deep features are integrated into a GEOBIA paradigm for classifying image objects. We explored the influences of segmentation scales and patch sizes in our method and explored the effectiveness of deep and object features in classification. Moreover, we performed 5-fold stratified cross validations 50 times to explore the uncertainty of our method. Additionally, we explored the importance of deep feature aggregation, and we evaluated our method by comparing it with three state-of-the-art methods in a Beijing dataset and Zurich dataset. The results indicate that smaller segmentation scales were more conducive to VHR remote sensing imagery classification, and it was not appropriate to select too large or too small patches as the patch size should be determined by imagery and its resolution. Moreover, we found that deep features are more effective than object features, while object features still matter for image classification, and deep feature aggregation is a critical step in our method. Finally, our method can achieve the highest overall accuracies compared with the state-of-the-art methods, and the overall accuracies are 91.21% for the Beijing dataset and 99.05% for the Zurich dataset.},
DOI = {10.3390/rs12183007}
}



@Article{rs12183015,
AUTHOR = {Machefer, Mélissande and Lemarchand, François and Bonnefond, Virginie and Hitchins, Alasdair and Sidiropoulos, Panagiotis},
TITLE = {Mask R-CNN Refitting Strategy for Plant Counting and Sizing in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3015},
URL = {https://www.mdpi.com/2072-4292/12/18/3015},
ISSN = {2072-4292},
ABSTRACT = {This work introduces a method that combines remote sensing and deep learning into a framework that is tailored for accurate, reliable and efficient counting and sizing of plants in aerial images. The investigated task focuses on two low-density crops, potato and lettuce. This double objective of counting and sizing is achieved through the detection and segmentation of individual plants by fine-tuning an existing deep learning architecture called Mask R-CNN. This paper includes a thorough discussion on the optimal parametrisation to adapt the Mask R-CNN architecture to this novel task. As we examine the correlation of the Mask R-CNN performance to the annotation volume and granularity (coarse or refined) of remotely sensed images of plants, we conclude that transfer learning can be effectively used to reduce the required amount of labelled data. Indeed, a previously trained Mask R-CNN on a low-density crop can improve performances after training on new crops. Once trained for a given crop, the Mask R-CNN solution is shown to outperform a manually-tuned computer vision algorithm. Model performances are assessed using intuitive metrics such as Mean Average Precision (mAP) from Intersection over Union (IoU) of the masks for individual plant segmentation and Multiple Object Tracking Accuracy (MOTA) for detection. The presented model reaches an mAP of 0.418 for potato plants and 0.660 for lettuces for the individual plant segmentation task. In detection, we obtain a MOTA of 0.781 for potato plants and 0.918 for lettuces.},
DOI = {10.3390/rs12183015}
}



@Article{rs12183035,
AUTHOR = {Lai, Ying-Chih and Huang, Zong-Ying},
TITLE = {Detection of a Moving UAV Based on Deep Learning-Based Distance Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3035},
URL = {https://www.mdpi.com/2072-4292/12/18/3035},
ISSN = {2072-4292},
ABSTRACT = {Distance information of an obstacle is important for obstacle avoidance in many applications, and could be used to determine the potential risk of object collision. In this study, the detection of a moving fixed-wing unmanned aerial vehicle (UAV) with deep learning-based distance estimation to conduct a feasibility study of sense and avoid (SAA) and mid-air collision avoidance of UAVs is proposed by using a monocular camera to detect and track an incoming UAV. A quadrotor is regarded as an owned UAV, and it is able to estimate the distance of an incoming fixed-wing intruder. The adopted object detection method is based on the you only look once (YOLO) object detector. Deep neural network (DNN) and convolutional neural network (CNN) methods are applied to exam their performance in the distance estimation of moving objects. The feature extraction of fixed-wing UAVs is based on the VGG-16 model, and then its result is applied to the distance network to estimate the object distance. The proposed model is trained by using synthetic images from animation software and validated by using both synthetic and real flight videos. The results show that the proposed active vision-based scheme is able to detect and track a moving UAV with high detection accuracy and low distance errors.},
DOI = {10.3390/rs12183035}
}



@Article{computers9030075,
AUTHOR = {Contreras, Ruben and Ayala, Angel and Cruz, Francisco},
TITLE = {Unmanned Aerial Vehicle Control through Domain-Based Automatic Speech Recognition},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {75},
URL = {https://www.mdpi.com/2073-431X/9/3/75},
ISSN = {2073-431X},
ABSTRACT = {Currently, unmanned aerial vehicles, such as drones, are becoming a part of our lives and extend to many areas of society, including the industrialized world. A common alternative for controlling the movements and actions of the drone is through unwired tactile interfaces, for which different remote control devices are used. However, control through such devices is not a natural, human-like communication interface, which sometimes is difficult to master for some users. In this research, we experimented with a domain-based speech recognition architecture to effectively control an unmanned aerial vehicle such as a drone. The drone control was performed in a more natural, human-like way to communicate the instructions. Moreover, we implemented an algorithm for command interpretation using both Spanish and English languages, as well as to control the movements of the drone in a simulated domestic environment. We conducted experiments involving participants giving voice commands to the drone in both languages in order to compare the effectiveness of each, considering the mother tongue of the participants in the experiment. Additionally, different levels of distortion were applied to the voice commands to test the proposed approach when it encountered noisy input signals. The results obtained showed that the unmanned aerial vehicle was capable of interpreting user voice instructions. Speech-to-action recognition improved for both languages with phoneme matching in comparison to only using the cloud-based algorithm without domain-based instructions. Using raw audio inputs, the cloud-based approach achieves 74.81% and 97.04% accuracy for English and Spanish instructions, respectively. However, with our phoneme matching approach the results are improved, yielding 93.33% accuracy for English and 100.00% accuracy for Spanish.},
DOI = {10.3390/computers9030075}
}



@Article{rs12183084,
AUTHOR = {Abdellatif, Mohamed and Peel, Harriet and Cohn, Anthony G. and Fuentes, Raul},
TITLE = {Pavement Crack Detection from Hyperspectral Images Using a Novel Asphalt Crack Index},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3084},
URL = {https://www.mdpi.com/2072-4292/12/18/3084},
ISSN = {2072-4292},
ABSTRACT = {Detection of road pavement cracks is important and needed at an early stage to repair the road and extend its lifetime for maintaining city roads. Cracks are hard to detect from images taken with visible spectrum cameras due to noise and ambiguity with background textures besides the lack of distinct features in cracks. Hyperspectral images are sensitive to surface material changes and their potential for road crack detection is explored here. The key observation is that road cracks reveal the interior material that is different from the worn surface material. A novel asphalt crack index is introduced here as an additional clue that is sensitive to the spectra in the range 450&ndash;550 nm. The crack index is computed and found to be strongly correlated with the appearance of fresh asphalt cracks. The new index is then used to differentiate cracks from road surfaces. Several experiments have been made, which confirmed that the proposed index is effective for crack detection. The recall-precision analysis showed an increase in the associated F1-score by an average of 21.37% compared to the VIS2 metric in the literature (a metric used to classify pavement condition from hyperspectral data).},
DOI = {10.3390/rs12183084}
}



@Article{agriculture10090416,
AUTHOR = {Chen, Pei-Chun and Chiang, Yen-Cheng and Weng, Pei-Yi},
TITLE = {Imaging Using Unmanned Aerial Vehicles for Agriculture Land Use Classification},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {416},
URL = {https://www.mdpi.com/2077-0472/10/9/416},
ISSN = {2077-0472},
ABSTRACT = {An unmanned aerial vehicle (UAV) was used to capture high-resolution aerial images of crop fields. Software-based image analysis was performed to classify land uses. The purpose was to help relevant agencies use aerial imaging in managing agricultural production. This study involves five townships in the Chianan Plain of Chiayi County, Taiwan. About 100 ha of farmland in each township was selected as a sample area, and a quadcopter and a handheld fixed-wing drone were used to capture visible-light images and multispectral images. The survey was carried out from August to October 2018 and aerial photographs were captured in clear and dry weather. This study used high-resolution images captured from a UAV to classify the uses of agricultural land, and then employed information from multispectral images and elevation data from a digital surface model. The results revealed that visible-light images led to low interpretation accuracy. However, multispectral images and elevation data increased the accuracy rate to nearly 90%. Accordingly, such images and data can effectively enhance the accuracy of land use classification. The technology can reduce costs that are associated with labor and time and can facilitate the establishment of a real-time mapping database.},
DOI = {10.3390/agriculture10090416}
}



@Article{rs12183104,
AUTHOR = {An, Gangqiang and Xing, Minfeng and He, Binbin and Liao, Chunhua and Huang, Xiaodong and Shang, Jiali and Kang, Haiqi},
TITLE = {Using Machine Learning for Estimating Rice Chlorophyll Content from In Situ Hyperspectral Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {3104},
URL = {https://www.mdpi.com/2072-4292/12/18/3104},
ISSN = {2072-4292},
ABSTRACT = {Chlorophyll is an essential pigment for photosynthesis in crops, and leaf chlorophyll content can be used as an indicator for crop growth status and help guide nitrogen fertilizer applications. Estimating crop chlorophyll content plays an important role in precision agriculture. In this study, a variable, rate of change in reflectance between wavelengths &lsquo;a&rsquo; and &lsquo;b&rsquo; (RCRWa-b), derived from in situ hyperspectral remote sensing data combined with four advanced machine learning techniques, Gaussian process regression (GPR), random forest regression (RFR), support vector regression (SVR), and gradient boosting regression tree (GBRT), were used to estimate the chlorophyll content (measured by a portable soil&ndash;plant analysis development meter) of rice. The performances of the four machine learning models were assessed and compared using root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination (R2). The results revealed that four features of RCRWa-b, RCRW551.0&ndash;565.6, RCRW739.5&ndash;743.5, RCRW684.4&ndash;687.1 and RCRW667.9&ndash;672.0, were effective in estimating the chlorophyll content of rice, and the RFR model generated the highest prediction accuracy (training set: RMSE = 1.54, MAE =1.23 and R2 = 0.95; validation set: RMSE = 2.64, MAE = 1.99 and R2 = 0.80). The GPR model was found to have the strongest generalization (training set: RMSE = 2.83, MAE = 2.16 and R2 = 0.77; validation set: RMSE = 2.97, MAE = 2.30 and R2 = 0.76). We conclude that RCRWa-b is a useful variable to estimate chlorophyll content of rice, and RFR and GPR are powerful machine learning algorithms for estimating the chlorophyll content of rice.},
DOI = {10.3390/rs12183104}
}



@Article{rs12193119,
AUTHOR = {Yang, Shuting and Gu, Lingjia and Li, Xiaofeng and Jiang, Tao and Ren, Ruizhi},
TITLE = {Crop Classification Method Based on Optimal Feature Selection and Hybrid CNN-RF Networks for Multi-Temporal Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3119},
URL = {https://www.mdpi.com/2072-4292/12/19/3119},
ISSN = {2072-4292},
ABSTRACT = {Although efforts and progress have been made in crop classification using optical remote sensing images, it is still necessary to make full use of the high spatial, temporal, and spectral resolutions of remote sensing images. However, with the increasing volume of remote sensing data, a key emerging issue in the field of crop classification is how to find useful information from massive data to balance classification accuracy and processing time. To address this challenge, we developed a novel crop classification method, combining optimal feature selection (OFSM) with hybrid convolutional neural network-random forest (CNN-RF) networks for multi-temporal optical remote sensing images. This research used 234 features including spectral, segmentation, color, and texture features from three scenes of Sentinel-2 images to identify crop types in the Jilin province of northeast China. To effectively extract the effective features of remote sensing data with lower time requirements, the use of OFSM was proposed with the results compared with two traditional feature selection methods (TFSM): random forest feature importance selection (RF-FI) and random forest recursive feature elimination (RF-RFE). Although the time required for OFSM was 26.05 s, which was between RF-FI with 1.97 s and RF-RFE with 132.54 s, OFSM outperformed RF-FI and RF-RFE in terms of the overall accuracy (OA) of crop classification by 4% and 0.3%, respectively. On the basis of obtaining effective feature information, to further improve the accuracy of crop classification we designed two hybrid CNN-RF networks to leverage the advantages of one-dimensional convolution (Conv1D) and Visual Geometry Group (VGG) with random forest (RF), respectively. Based on the selected optimal features using OFSM, four networks were tested for comparison: Conv1D-RF, VGG-RF, Conv1D, and VGG. Conv1D-RF achieved the highest OA at 94.27% as compared with VGG-RF (93.23%), Conv1D (92.59%), and VGG (91.89%), indicating that the Conv1D-RF method with optimal feature input provides an effective and efficient method of time series representation for multi-temporal crop-type classification.},
DOI = {10.3390/rs12193119}
}



@Article{drones4040064,
AUTHOR = {Raoult, Vincent and Colefax, Andrew P and Allan, Blake M. and Cagnazzi, Daniele and Castelblanco-Martínez, Nataly and Ierodiaconou, Daniel and Johnston, David W. and Landeo-Yauri, Sarah and Lyons, Mitchell and Pirotta, Vanessa and Schofield, Gail and Butcher, Paul A},
TITLE = {Operational Protocols for the Use of Drones in Marine Animal Research},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {64},
URL = {https://www.mdpi.com/2504-446X/4/4/64},
ISSN = {2504-446X},
ABSTRACT = {The use of drones to study marine animals shows promise for the examination of numerous aspects of their ecology, behaviour, health and movement patterns. However, the responses of some marine phyla to the presence of drones varies broadly, as do the general operational protocols used to study them. Inconsistent methodological approaches could lead to difficulties comparing studies and can call into question the repeatability of research. This review draws on current literature and researchers with a wealth of practical experience to outline the idiosyncrasies of studying various marine taxa with drones. We also outline current best practice for drone operation in marine environments based on the literature and our practical experience in the field. The protocols outlined herein will be of use to researchers interested in incorporating drones as a tool into their research on marine animals and will help form consistent approaches for drone-based studies in the future.},
DOI = {10.3390/drones4040064}
}



@Article{s20195495,
AUTHOR = {El Boudani, Brahim and Kanaris, Loizos and Kokkinis, Akis and Kyriacou, Michalis and Chrysoulas, Christos and Stavrou, Stavros and Dagiuklas, Tasos},
TITLE = {Implementing Deep Learning Techniques in 5G IoT Networks for 3D Indoor Positioning: DELTA (DeEp Learning-Based Co-operaTive Architecture)},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5495},
URL = {https://www.mdpi.com/1424-8220/20/19/5495},
ISSN = {1424-8220},
ABSTRACT = {In the near future, the fifth-generation wireless technology is expected to be rolled out, offering low latency, high bandwidth and multiple antennas deployed in a single access point. This ecosystem will help further enhance various location-based scenarios such as assets tracking in smart factories, precise smart management of hydroponic indoor vertical farms and indoor way-finding in smart hospitals. Such a system will also integrate existing technologies like the Internet of Things (IoT), WiFi and other network infrastructures. In this respect, 5G precise indoor localization using heterogeneous IoT technologies (Zigbee, Raspberry Pi, Arduino, BLE, etc.) is a challenging research area. In this work, an experimental 5G testbed has been designed integrating C-RAN and IoT networks. This testbed is used to improve both vertical and horizontal localization (3D Localization) in a 5G IoT environment. To achieve this, we propose the DEep Learning-based co-operaTive Architecture (DELTA) machine learning model implemented on a 3D multi-layered fingerprint radiomap. The DELTA begins by estimating the 2D location. Then, the output is recursively used to predict the 3D location of a mobile station. This approach is going to benefit use cases such as 3D indoor navigation in multi-floor smart factories or in large complex buildings. Finally, we have observed that the proposed model has outperformed traditional algorithms such as Support Vector Machine (SVM) and K-Nearest Neighbor (KNN).},
DOI = {10.3390/s20195495}
}



@Article{rs12193164,
AUTHOR = {Banerjee, Bikram Pratap and Spangenberg, German and Kant, Surya},
TITLE = {Fusion of Spectral and Structural Information from Aerial Images for Improved Biomass Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3164},
URL = {https://www.mdpi.com/2072-4292/12/19/3164},
ISSN = {2072-4292},
ABSTRACT = {Efficient, precise and timely measurement of plant traits is important in the assessment of a breeding population. Estimating crop biomass in breeding trials using high-throughput technologies is difficult, as reproductive and senescence stages do not relate to reflectance spectra, and multiple growth stages occur concurrently in diverse genotypes. Additionally, vegetation indices (VIs) saturate at high canopy coverage, and vertical growth profiles are difficult to capture using VIs. A novel approach was implemented involving a fusion of complementary spectral and structural information, to calculate intermediate metrics such as crop height model (CHM), crop coverage (CC) and crop volume (CV), which were finally used to calculate dry (DW) and fresh (FW) weight of above-ground biomass in wheat. The intermediate metrics, CHM (R2 = 0.81, SEE = 4.19 cm) and CC (OA = 99.2%, &Kappa; = 0.98) were found to be accurate against equivalent ground truth measurements. The metrics CV and CV&times;VIs were used to develop an effective and accurate linear regression model relationship with DW (R2 = 0.96 and SEE = 69.2 g/m2) and FW (R2 = 0.89 and SEE = 333.54 g/m2). The implemented approach outperformed commonly used VIs for estimation of biomass at all growth stages in wheat. The achieved results strongly support the applicability of the proposed approach for high-throughput phenotyping of germplasm in wheat and other crop species.},
DOI = {10.3390/rs12193164}
}



@Article{agriculture10100436,
AUTHOR = {Niazian, Mohsen and Niedbała, Gniewko},
TITLE = {Machine Learning for Plant Breeding and Biotechnology},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {436},
URL = {https://www.mdpi.com/2077-0472/10/10/436},
ISSN = {2077-0472},
ABSTRACT = {Classical univariate and multivariate statistics are the most common methods used for data analysis in plant breeding and biotechnology studies. Evaluation of genetic diversity, classification of plant genotypes, analysis of yield components, yield stability analysis, assessment of biotic and abiotic stresses, prediction of parental combinations in hybrid breeding programs, and analysis of in vitro-based biotechnological experiments are mainly performed by classical statistical methods. Despite successful applications, these classical statistical methods have low efficiency in analyzing data obtained from plant studies, as the genotype, environment, and their interaction (G &times; E) result in nondeterministic and nonlinear nature of plant characteristics. Large-scale data flow, including phenomics, metabolomics, genomics, and big data, must be analyzed for efficient interpretation of results affected by G &times; E. Nonlinear nonparametric machine learning techniques are more efficient than classical statistical models in handling large amounts of complex and nondeterministic information with &ldquo;multiple-independent variables versus multiple-dependent variables&rdquo; nature. Neural networks, partial least square regression, random forest, and support vector machines are some of the most fascinating machine learning models that have been widely applied to analyze nonlinear and complex data in both classical plant breeding and in vitro-based biotechnological studies. High interpretive power of machine learning algorithms has made them popular in the analysis of plant complex multifactorial characteristics. The classification of different plant genotypes with morphological and molecular markers, modeling and predicting important quantitative characteristics of plants, the interpretation of complex and nonlinear relationships of plant characteristics, and predicting and optimizing of in vitro breeding methods are the examples of applications of machine learning in conventional plant breeding and in vitro-based biotechnological studies. Precision agriculture is possible through accurate measurement of plant characteristics using imaging techniques and then efficient analysis of reliable extracted data using machine learning algorithms. Perfect interpretation of high-throughput phenotyping data is applicable through coupled machine learning-image processing. Some applied and potentially applicable capabilities of machine learning techniques in conventional and in vitro-based plant breeding studies have been discussed in this overview. Discussions are of great value for future studies and could inspire researchers to apply machine learning in new layers of plant breeding.},
DOI = {10.3390/agriculture10100436}
}



@Article{rs12193171,
AUTHOR = {Park, Jinseok and Jang, Seongju and Hong, Rokgi and Suh, Kyo and Song, Inhong},
TITLE = {Development of Land Cover Classification Model Using AI Based FusionNet Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3171},
URL = {https://www.mdpi.com/2072-4292/12/19/3171},
ISSN = {2072-4292},
ABSTRACT = {Prompt updates of land cover maps are important, as spatial information of land cover is widely used in many areas. However, current manual digitizing methods are time consuming and labor intensive, hindering rapid updates of land cover maps. The objective of this study was to develop an artificial intelligence (AI) based land cover classification model that allows for rapid land cover classification from high-resolution remote sensing (HRRS) images. The model comprises of three modules: pre-processing, land cover classification, and post-processing modules. The pre-processing module separates the HRRS image into multiple aspects by overlapping 75% using the sliding window algorithm. The land cover classification module was developed using the convolutional neural network (CNN) concept, based the FusionNet network and used to assign a land cover type to the separated HRRS images. Post-processing module determines ultimate land cover types by summing up the separated land cover result from the land cover classification module. Model training and validation were conducted to evaluate the performance of the developed model. The land cover maps and orthographic images of 547.29 km2 in area from the Jeonnam province in Korea were used to train the model. For model validation, two spatial and temporal different sites, one from Subuk-myeon of Jeonnam province in 2018 and the other from Daseo-myeon of Chungbuk province in 2016, were randomly chosen. The model performed reasonably well, demonstrating overall accuracies of 0.81 and 0.71, and kappa coefficients of 0.75 and 0.64, for the respective validation sites. The model performance was better when only considering the agricultural area by showing overall accuracy of 0.83 and kappa coefficients of 0.73. It was concluded that the developed model may assist rapid land cover update especially for agricultural areas and incorporation field boundary lineation is suggested as future study to further improve the model accuracy.},
DOI = {10.3390/rs12193171}
}



@Article{rs12193184,
AUTHOR = {Camarretta, Nicolò and A. Harrison, Peter and Lucieer, Arko and M. Potts, Brad and Davidson, Neil and Hunt, Mark},
TITLE = {From Drones to Phenotype: Using UAV-LiDAR to Detect Species and Provenance Variation in Tree Productivity and Structure},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3184},
URL = {https://www.mdpi.com/2072-4292/12/19/3184},
ISSN = {2072-4292},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) for remote sensing of natural environments has increased over the last decade. However, applications of this technology for high-throughput individual tree phenotyping in a quantitative genetic framework are rare. We here demonstrate a two-phased analytical pipeline that rapidly phenotypes and filters for genetic signals in traditional and novel tree productivity and architectural traits derived from ultra-dense light detection and ranging (LiDAR) point clouds. The goal of this study was rapidly phenotype individual trees to understand the genetic basis of ecologically and economically significant traits important for guiding the management of natural resources. Individual tree point clouds were acquired using UAV-LiDAR captured over a multi-provenance common-garden restoration field trial located in Tasmania, Australia, established using two eucalypt species (Eucalyptus pauciflora and Eucalyptus tenuiramis). Twenty-five tree productivity and architectural traits were calculated for each individual tree point cloud. The first phase of the analytical pipeline found significant species differences in 13 of the 25 derived traits, revealing key structural differences in productivity and crown architecture between species. The second phase investigated the within species variation in the same 25 structural traits. Significant provenance variation was detected for 20 structural traits in E. pauciflora and 10 in E. tenuiramis, with signals of divergent selection found for 11 and 7 traits, respectively, putatively driven by the home-site environment shaping the observed variation. Our results highlight the genetic-based diversity within and between species for traits important for forest structure, such as crown density and structural complexity. As species and provenances are being increasingly translocated across the landscape to mitigate the effects of rapid climate change, our results that were achieved through rapid phenotyping using UAV-LiDAR, raise the need to understand the functional value of productivity and architectural traits reflecting species and provenance differences in crown structure and the interplay they have on the dependent biotic communities.},
DOI = {10.3390/rs12193184}
}



@Article{agriculture10100451,
AUTHOR = {López-Calderón, Magali J. and Estrada-Ávalos, Juan and Rodríguez-Moreno, Víctor M. and Mauricio-Ruvalcaba, Jorge E. and Martínez-Sifuentes, Aldo R. and Delgado-Ramírez, Gerardo and Miguel-Valle, Enrique},
TITLE = {Estimation of Total Nitrogen Content in Forage Maize (Zea mays L.) Using Spectral Indices: Analysis by Random Forest},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {451},
URL = {https://www.mdpi.com/2077-0472/10/10/451},
ISSN = {2077-0472},
ABSTRACT = {Knowing the total Nitrogen content (Nt) of forage maize (Zea mays) is important so that decisions can be made quickly and efficiently to adjust the timing and amount of both irrigation and fertilizer. In 2017 and 2018 during three growing cycles in two study plots, leaf samples were collected and the Dumas method was used to estimate Nt. During the same growing seasons and on the same sampling plots, a Parrot Sequoia camera mounted on an unmanned aerial vehicle (UAV) was used to collect high resolution images of forage maize study plots. Thirteen multispectral indices were generated and, from these, a Random Forest (RF) algorithm was used to estimate Nt. RF is a machine-learning technique and is designed to work with extremely large datasets. Overall analysis showed five of the 13 indices as the most important. One of these five, the Transformed Chlorophyll Absorption in Reflectance Index/Optimized Soil-Adjusted Vegetation Index, was found to be the most important for estimation of Nt in forage maize (R2 = 0.76). RF handled the complex dataset in a time-efficient manner and Nt did not differ significantly when compared between traditional methods of evaluating Nt at the canopy level and using UAVs and RF to estimate Nt in forage maize. This result is an opportunity to explore many new research options in precision farming and digital agriculture.},
DOI = {10.3390/agriculture10100451}
}



@Article{rs12193216,
AUTHOR = {Maimaitiyiming, Matthew and Sagan, Vasit and Sidike, Paheding and Maimaitijiang, Maitiniyazi and Miller, Allison J. and Kwasniewski, Misha},
TITLE = {Leveraging Very-High Spatial Resolution Hyperspectral and Thermal UAV Imageries for Characterizing Diurnal Indicators of Grapevine Physiology},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3216},
URL = {https://www.mdpi.com/2072-4292/12/19/3216},
ISSN = {2072-4292},
ABSTRACT = {Efficient and accurate methods to monitor crop physiological responses help growers better understand crop physiology and improve crop productivity. In recent years, developments in unmanned aerial vehicles (UAV) and sensor technology have enabled image acquisition at very-high spectral, spatial, and temporal resolutions. However, potential applications and limitations of very-high-resolution (VHR) hyperspectral and thermal UAV imaging for characterization of plant diurnal physiology remain largely unknown, due to issues related to shadow and canopy heterogeneity. In this study, we propose a canopy zone-weighting (CZW) method to leverage the potential of VHR (&le;9 cm) hyperspectral and thermal UAV imageries in estimating physiological indicators, such as stomatal conductance (Gs) and steady-state fluorescence (Fs). Diurnal flights and concurrent in-situ measurements were conducted during grapevine growing seasons in 2017 and 2018 in a vineyard in Missouri, USA. We used neural net classifier and the Canny edge detection method to extract pure vine canopy from the hyperspectral and thermal images, respectively. Then, the vine canopy was segmented into three canopy zones (sunlit, nadir, and shaded) using K-means clustering based on the canopy shadow fraction and canopy temperature. Common reflectance-based spectral indices, sun-induced chlorophyll fluorescence (SIF), and simplified canopy water stress index (siCWSI) were computed as image retrievals. Using the coefficient of determination (R2) established between the image retrievals from three canopy zones and the in-situ measurements as a weight factor, weighted image retrievals were calculated and their correlation with in-situ measurements was explored. The results showed that the most frequent and the highest correlations were found for Gs and Fs, with CZW-based Photochemical reflectance index (PRI), SIF, and siCWSI (PRICZW, SIFCZW, and siCWSICZW), respectively. When all flights combined for the given field campaign date, PRICZW, SIFCZW, and siCWSICZW significantly improved the relationship with Gs and Fs. The proposed approach takes full advantage of VHR hyperspectral and thermal UAV imageries, and suggests that the CZW method is simple yet effective in estimating Gs and Fs.},
DOI = {10.3390/rs12193216}
}



@Article{rs12193228,
AUTHOR = {Qiu, Zhengchao and Xiang, Haitao and Ma, Fei and Du, Changwen},
TITLE = {Qualifications of Rice Growth Indicators Optimized at Different Growth Stages Using Unmanned Aerial Vehicle Digital Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3228},
URL = {https://www.mdpi.com/2072-4292/12/19/3228},
ISSN = {2072-4292},
ABSTRACT = {The accurate estimation of the key growth indicators of rice is conducive to rice production, and the rapid monitoring of these indicators can be achieved through remote sensing using the commercial RGB cameras of unmanned aerial vehicles (UAVs). However, the method of using UAV RGB images lacks an optimized model to achieve accurate qualifications of rice growth indicators. In this study, we established a correlation between the multi-stage vegetation indices (VIs) extracted from UAV imagery and the leaf dry biomass, leaf area index, and leaf total nitrogen for each growth stage of rice. Then, we used the optimal VI (OVI) method and object-oriented segmentation (OS) method to remove the noncanopy area of the image to improve the estimation accuracy. We selected the OVI and the models with the best correlation for each growth stage to establish a simple estimation model database. The results showed that the OVI and OS methods to remove the noncanopy area can improve the correlation between the key growth indicators and VI of rice. At the tillering stage and early jointing stage, the correlations between leaf dry biomass (LDB) and the Green Leaf Index (GLI) and Red Green Ratio Index (RGRI) were 0.829 and 0.881, respectively; at the early jointing stage and late jointing stage, the coefficient of determination (R2) between the Leaf Area Index (LAI) and Modified Green Red Vegetation Index (MGRVI) was 0.803 and 0.875, respectively; at the early stage and the filling stage, the correlations between the leaf total nitrogen (LTN) and UAV vegetation index and the Excess Red Vegetation Index (ExR) were 0.861 and 0.931, respectively. By using the simple estimation model database established using the UAV-based VI and the measured indicators at different growth stages, the rice growth indicators can be estimated for each stage. The proposed estimation model database for monitoring rice at the different growth stages is helpful for improving the estimation accuracy of the key rice growth indicators and accurately managing rice production.},
DOI = {10.3390/rs12193228}
}



@Article{rs12193237,
AUTHOR = {Osco, Lucas Prado and Junior, José Marcato and Ramos, Ana Paula Marques and Furuya, Danielle Elis Garcia and Santana, Dthenifer Cordeiro and Teodoro, Larissa Pereira Ribeiro and Gonçalves, Wesley Nunes and Baio, Fábio Henrique Rojo and Pistori, Hemerson and Junior, Carlos Antonio da Silva and Teodoro, Paulo Eduardo},
TITLE = {Leaf Nitrogen Concentration and Plant Height Prediction for Maize Using UAV-Based Multispectral Imagery and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3237},
URL = {https://www.mdpi.com/2072-4292/12/19/3237},
ISSN = {2072-4292},
ABSTRACT = {Under ideal conditions of nitrogen (N), maize (Zea mays L.) can grow to its full potential, reaching maximum plant height (PH). As a rapid and nondestructive approach, the analysis of unmanned aerial vehicles (UAV)-based imagery may be of assistance to estimate N and height. The main objective of this study is to present an approach to predict leaf nitrogen concentration (LNC, g kg&minus;1) and PH (m) with machine learning techniques and UAV-based multispectral imagery in maize plants. An experiment with 11 maize cultivars under two rates of N fertilization was carried during the 2017/2018 and 2018/2019 crop seasons. The spectral vegetation indices (VI) normalized difference vegetation index (NDVI), normalized difference red-edge index (NDRE), green normalized difference vegetation (GNDVI), and the soil adjusted vegetation index (SAVI) were extracted from the images and, in a computational system, used alongside the spectral bands as input parameters for different machine learning models. A randomized 10-fold cross-validation strategy, with a total of 100 replicates, was used to evaluate the performance of 9 supervised machine learning (ML) models using the Pearson&rsquo;s correlation coefficient (r), mean absolute error (MAE), coefficient of regression (R&sup2;), and root mean square error (RMSE) metrics. The results indicated that the random forest (RF) algorithm performed better, with r and RMSE, respectively, of 0.91 and 1.9 g.kg&minus;&sup1; for LNC, and 0.86 and 0.17 m for PH. It was also demonstrated that VIs contributed more to the algorithm&rsquo;s performances than individual spectral bands. This study concludes that the RF model is appropriate to predict both agronomic variables in maize and may help farmers to monitor their plants based upon their LNC and PH diagnosis and use this knowledge to improve their production rates in the subsequent seasons.},
DOI = {10.3390/rs12193237}
}



@Article{s20205762,
AUTHOR = {Santos, André A. and Rocha, Filipe A. S. and Reis, Agnaldo J. da R. and Guimarães, Frederico G.},
TITLE = {Automatic System for Visual Detection of Dirt Buildup on Conveyor Belts Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5762},
URL = {https://www.mdpi.com/1424-8220/20/20/5762},
ISSN = {1424-8220},
ABSTRACT = {Conveyor belts are the most widespread means of transportation for large quantities of materials in the mining sector. Therefore, autonomous methods that can help human beings to perform the inspection of the belt conveyor system is a major concern for companies. In this context, we present in this work a novel and automatic visual detector that recognizes dirt buildup on the structures of conveyor belts, which is one of the tasks of the maintenance inspectors. This visual detector can be embedded as sensors in autonomous robots for the inspection activity. The proposed system involves training a convolutional neural network from RGB images. The use of the transfer learning technique, i.e., retraining consolidated networks for image classification with our collected images has shown very effective. Two different approaches for transfer learning have been analyzed. The best one presented an average accuracy of 0.8975 with an F-1 Score of 0.8773 for the dirt recognition. A field validation experiment served to evaluate the performance of the proposed system in a real time classification task.},
DOI = {10.3390/s20205762}
}



@Article{ijgi9100601,
AUTHOR = {Song, Ahram and Kim, Yongil},
TITLE = {Semantic Segmentation of Remote-Sensing Imagery Using Heterogeneous Big Data: International Society for Photogrammetry and Remote Sensing Potsdam and Cityscape Datasets},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {601},
URL = {https://www.mdpi.com/2220-9964/9/10/601},
ISSN = {2220-9964},
ABSTRACT = {Although semantic segmentation of remote-sensing (RS) images using deep-learning networks has demonstrated its effectiveness recently, compared with natural-image datasets, obtaining RS images under the same conditions to construct data labels is difficult. Indeed, small datasets limit the effective learning of deep-learning networks. To address this problem, we propose a combined U-net model that is trained using a combined weighted loss function and can handle heterogeneous datasets. The network consists of encoder and decoder blocks. The convolutional layers that form the encoder blocks are shared with the heterogeneous datasets, and the decoder blocks are assigned separate training weights. Herein, the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Cityscape datasets are used as the RS and natural-image datasets, respectively. When the layers are shared, only visible bands of the ISPRS Potsdam data are used. Experimental results show that when same-sized heterogeneous datasets are used, the semantic segmentation accuracy of the Potsdam data obtained using our proposed method is lower than that obtained using only the Potsdam data (four bands) with other methods, such as SegNet, DeepLab-V3+, and the simplified version of U-net. However, the segmentation accuracy of the Potsdam images is improved when the larger Cityscape dataset is used. The combined U-net model can effectively train heterogeneous datasets and overcome the insufficient training data problem in the context of RS-image datasets. Furthermore, it is expected that the proposed method can not only be applied to segmentation tasks of aerial images but also to tasks with various purposes of using big heterogeneous datasets.},
DOI = {10.3390/ijgi9100601}
}



@Article{rs12203318,
AUTHOR = {Na, Jiaming and Xue, Kaikai and Xiong, Liyang and Tang, Guoan and Ding, Hu and Strobl, Josef and Pfeifer, Norbert},
TITLE = {UAV-Based Terrain Modeling under Vegetation in the Chinese Loess Plateau: A Deep Learning and Terrain Correction Ensemble Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3318},
URL = {https://www.mdpi.com/2072-4292/12/20/3318},
ISSN = {2072-4292},
ABSTRACT = {Accurate topographic mapping is a critical task for various environmental applications because elevation affects hydrodynamics and vegetation distributions. UAV photogrammetry is popular in terrain modelling because of its lower cost compared to laser scanning. However, this method is restricted in vegetation area with a complex terrain, due to reduced ground visibility and lack of robust and automatic filtering algorithms. To solve this problem, this work proposed an ensemble method of deep learning and terrain correction. First, image matching point cloud was generated by UAV photogrammetry. Second, vegetation points were identified based on U-net deep learning network. After that, ground elevation was corrected by estimating vegetation height to generate the digital terrain model (DTM). Two scenarios, namely, discrete and continuous vegetation areas were considered. The vegetation points in the discrete area were directly removed and then interpolated, and terrain correction was applied for the points in the continuous areas. Case studies were conducted in three different landforms in the loess plateau of China, and accuracy assessment indicated that the overall accuracy of vegetation detection was 95.0%, and the MSE (Mean Square Error) of final DTM (Digital Terrain Model) was 0.024 m.},
DOI = {10.3390/rs12203318}
}



@Article{rs12203325,
AUTHOR = {Riddell, Audrey P. and Fitzgerald, Stephen A. and Qi, Chu and Strimbu, Bogdan M.},
TITLE = {Classification Strategies for Unbalanced Binary Maps: Finding Ponderosa Pine (Pinus ponderosa) in the Willamette Valley},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3325},
URL = {https://www.mdpi.com/2072-4292/12/20/3325},
ISSN = {2072-4292},
ABSTRACT = {Forest species classifications are becoming increasingly automated as advances are made in machine learning. Complex algorithms can reach high accuracies, but are not always suitable for small-scale classifications, which may benefit from simpler conventional methods. The goal of this classification was to identify contiguous stands of ponderosa pine (Pinus ponderosa Douglas ex Lawson) against a mix of forest and non-forest background in the southern Willamette Valley, Oregon. The study area is approximately 816,600 ha, considerably larger than most study areas used for presenting techniques for tree species classification. To achieve the objective, we used two classification procedures, one parametric and one non-parametric. For the parametric method, we selected the maximum likelihood (ML) algorithm, whereas for the non-parametric method we chose the random forest (RF) algorithm. To identify ponderosa pine, we used 1 m spatial resolution red-green-blue-infrared (RGBI) aerial images supplied by the U.S. National Agriculture Imagery Program (NAIP) and 1 m spatial resolution canopy height models (CHMs) provided by the Oregon Department of Geology and Mineral Industries (DOGAMI). We tested four data variations for each method: Aerial imagery, CHM-masked aerial imagery, aerial imagery with an additional CHM band, and CHM-masked aerial imagery with a CHM band. The parametric classifications of aerial imagery alone reached an average kappa coefficient of 0.29, which increased to 0.51 when masked with CHM data. The incorporation of CHM data as a fifth band resulted in a similar improvement in kappa (0.47), but the most effective parametric method was the incorporation of CHM data as both a fifth band and a post-classification mask, resulting in a kappa coefficient of 0.89. The non-parametric classification of aerial imagery achieved a mean validation kappa coefficient of 0.85 collectively and 0.90 individually, which only increased by approximately 0.01 or less when the CHM masks were applied. The addition of the CHM band increased the kappa value to 0.91 for both individual and collective tile classifications. The highest kappa of all methods was achieved through five-band non-parametric classification with the addition of the CHM band (0.94) for both collective and individual classifications. Our results suggest that parametric methods, when enhanced with a CHM mask, could be suitable for large-area, small-scale classifications based on RGBI imagery, but a non-parametric classification of fused spectral and height data will generally achieve the highest accuracy for large, unbalanced datasets.},
DOI = {10.3390/rs12203325}
}



@Article{rs12203328,
AUTHOR = {Imangholiloo, Mohammad and Saarinen, Ninni and Holopainen, Markus and Yu, Xiaowei and Hyyppä, Juha and Vastaranta, Mikko},
TITLE = {Using Leaf-Off and Leaf-On Multispectral Airborne Laser Scanning Data to Characterize Seedling Stands},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3328},
URL = {https://www.mdpi.com/2072-4292/12/20/3328},
ISSN = {2072-4292},
ABSTRACT = {Information from seedling stands in time and space is essential for sustainable forest management. To fulfil these informational needs with limited resources, remote sensing is seen as an intriguing alternative for forest inventorying. The structure and tree species composition in seedling stands have created challenges for capturing this information using sensors providing sparse point densities that do not have the ability to penetrate canopy gaps or provide spectral information. Therefore, multispectral airborne laser scanning (mALS) systems providing dense point clouds coupled with multispectral intensity data theoretically offer advantages for the characterization of seedling stands. The aim of this study was to investigate the capability of Optech Titan mALS data to characterize seedling stands in leaf-off and leaf-on conditions, as well as to retrieve the most important forest inventory attributes, such as distinguishing deciduous from coniferous trees, and estimating tree density and height. First, single-tree detection approaches were used to derive crown boundaries and tree heights from which forest structural attributes were aggregated for sample plots. To predict tree species, a random forests classifier was trained using features from two single-channel intensities (SCIs) with wavelengths of 1550 (SCI-Ch1) and 1064 nm (SCI-Ch2), and multichannel intensity (MCI) data composed of three mALS channels. The most important and uncorrelated features were analyzed and selected from 208 features. The highest overall accuracies in classification of Norway spruce, birch, and nontree class in leaf-off and leaf-on conditions obtained using SCI-Ch1 and SCI-Ch2 were 87.36% and 69.47%, respectively. The use of MCI data improved classification by up to 96.55% and 92.54% in leaf-off and leaf-on conditions, respectively. Overall, leaf-off data were favorable for distinguishing deciduous from coniferous trees and tree density estimation with a relative root mean square error (RMSE) of 37.9%, whereas leaf-on data provided more accurate height estimations, with a relative RMSE of 10.76%. Determining the canopy threshold for separating ground returns from vegetation returns was found to be critical, as mapped trees might have a height below one meter. The results showed that mALS data provided benefits for characterizing seedling stands compared to single-channel ALS systems.},
DOI = {10.3390/rs12203328}
}



@Article{app10207120,
AUTHOR = {Mohammed, Thaha and Albeshri, Aiiad and Katib, Iyad and Mehmood, Rashid},
TITLE = {UbiPriSEQ—Deep Reinforcement Learning to Manage Privacy, Security, Energy, and QoS in 5G IoT HetNets},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7120},
URL = {https://www.mdpi.com/2076-3417/10/20/7120},
ISSN = {2076-3417},
ABSTRACT = {5G networks and Internet of Things (IoT) offer a powerful platform for ubiquitous environments with their ubiquitous sensing, high speeds and other benefits. The data, analytics, and other computations need to be optimally moved and placed in these environments, dynamically, such that energy-efficiency and QoS demands are best satisfied. A particular challenge in this context is to preserve privacy and security while delivering quality of service (QoS) and energy-efficiency. Many works have tried to address these challenges but without a focus on optimizing all of them and assuming fixed models of environments and security threats. This paper proposes the UbiPriSEQ framework that uses Deep Reinforcement Learning (DRL) to adaptively, dynamically, and holistically optimize QoS, energy-efficiency, security, and privacy. UbiPriSEQ is built on a three-layered model and comprises two modules. UbiPriSEQ devises policies and makes decisions related to important parameters including local processing and offloading rates for data and computations, radio channel states, transmit power, task priority, and selection of fog nodes for offloading, data migration, and so forth. UbiPriSEQ is implemented in Python over the TensorFlow platform and is evaluated using a real-life application in terms of SINR, privacy metric, latency, and utility function, manifesting great promise.},
DOI = {10.3390/app10207120}
}



@Article{s20205805,
AUTHOR = {Ai, Tianfu and Xu, Bin and Xiang, Changle and Fan, Wei and Zhang, Yibo},
TITLE = {Modeling of a Novel Coaxial Ducted Fan Aerial Robot Combined with Corner Environment by Using Artificial Neural Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5805},
URL = {https://www.mdpi.com/1424-8220/20/20/5805},
ISSN = {1424-8220},
ABSTRACT = {A novel coaxial ducted fan aerial robot with a manipulator is proposed which can achieve some hover operation tasks in a corner environment, such as switching on and off a wall-attached button on the corner. In order to study the aerodynamic interference between the prototype and the environment when the aerial robot is hovering in the corner environment, a method for the comprehensive modeling of the prototype and corner environment based on the artificial neural network is presented. By using the CFD simulation software, the flow field of the prototype at different positions with the corner effect is analyzed. After determining the input, output and structure of the neural network model, the Adam and gradient descent algorithms are selected as the neural network training algorithms, respectively. In addition, to optimize the initial weights and biases of the neural network model, the genetic algorithm is precisely used. The three-dimensional prediction surfaces generated by the three methods of the neural network, kriging surface and the polynomial fitting are compared. The results show that the neural network has high prediction accuracy, and can be applied to the comprehensive modeling of the prototype and the corner environment.},
DOI = {10.3390/s20205805}
}



@Article{app10207147,
AUTHOR = {Yoon, Hyung-Koo},
TITLE = {Relationship between Aspect Ratio and Crack Density in Porous-Cracked Rocks Using Experimental and Optimization Methods},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7147},
URL = {https://www.mdpi.com/2076-3417/10/20/7147},
ISSN = {2076-3417},
ABSTRACT = {Aspect ratio and crack density are essential parameters to understand the physical properties of porous-cracked rocks, although it is difficult to independently determine each parameter, as both are closely linked. The objective of this study is to propose a relationship between aspect ratio and crack density that can be used to solve for each through experimental and optimization methods. Two different constitutive equations are solved to create expressions explicitly defining aspect ratio and crack density, with all remaining variables arranged as functions of elastic wave velocity. Ten core specimens extracted from construction sites, with diameters of 46 mm, are subjected to artificial weathering to identify how their crack density and aspect ratio evolved with time. The artificial weathering process consisted of chemical and physical weathering cycles using saline solution and slake durability tests, respectively. Compressional and shear wave velocities are measured at every weathering step, and both aspect ratio and crack density are calculated. The random forest as an optimization method is selected to define the important score among input variables. The calculated aspect ratios and crack densities are converted into a crack porosity, the reliability of which is verified through percentage of crack porosity (~6%) in total porosity. This study demonstrates that the relationship between aspect ratio and crack density is robust and has wide-ranging applications in determining individual aspect ratio and crack density parameters in porous-cracked rock.},
DOI = {10.3390/app10207147}
}



@Article{agriculture10100475,
AUTHOR = {Abraham, Emerson Rodolfo and Mendes dos Reis, João Gilberto and Vendrametto, Oduvaldo and Oliveira Costa Neto, Pedro Luiz de and Carlo Toloi, Rodrigo and Souza, Aguinaldo Eduardo de and Oliveira Morais, Marcos de},
TITLE = {Time Series Prediction with Artificial Neural Networks: An Analysis Using Brazilian Soybean Production},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {475},
URL = {https://www.mdpi.com/2077-0472/10/10/475},
ISSN = {2077-0472},
ABSTRACT = {Food production to meet human demand has been a challenge to society. Nowadays, one of the main sources of feeding is soybean. Considering agriculture food crops, soybean is sixth by production volume and the fourth by both production area and economic value. The grain can be used directly to human consumption, but it is highly used as a source of protein for animal production that corresponds 75% of the total, or as oil and derived food products. Brazil and the US are the most important players responsible for more than 70% of world production. Therefore, a reliable forecasting is essential for decision-makers to plan adequate policies to this important commodity and to establish the necessary logistical resources. In this sense, this study aims to predict soybean harvest area, yield, and production using Artificial Neural Networks (ANN) and compare with classical methods of Time Series Analysis. To this end, we collected data from a time series (1961&ndash;2016) regarding soybean production in Brazil. The results reveal that ANN is the best approach to predict soybean harvest area and production while classical linear function remains more effective to predict soybean yield. Moreover, ANN presents as a reliable model to predict time series and can help the stakeholders to anticipate the world soybean offer.},
DOI = {10.3390/agriculture10100475}
}



@Article{s20205845,
AUTHOR = {Abreu Maranhão, João Paulo and Carvalho Lustosa da Costa, João Paulo and Pignaton de Freitas, Edison and Javidi, Elnaz and Timóteo de Sousa Júnior, Rafael},
TITLE = {Error-Robust Distributed Denial of Service Attack Detection Based on an Average Common Feature Extraction Technique},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5845},
URL = {https://www.mdpi.com/1424-8220/20/20/5845},
ISSN = {1424-8220},
ABSTRACT = {In recent years, advanced threats against Cyber&ndash;Physical Systems (CPSs), such as Distributed Denial of Service (DDoS) attacks, are increasing. Furthermore, traditional machine learning-based intrusion detection systems (IDSs) often fail to efficiently detect such attacks when corrupted datasets are used for IDS training. To face these challenges, this paper proposes a novel error-robust multidimensional technique for DDoS attack detection. By applying the well-known Higher Order Singular Value Decomposition (HOSVD), initially, the average value of the common features among instances is filtered out from the dataset. Next, the filtered data are forwarded to machine learning classification algorithms in which traffic information is classified as a legitimate or a DDoS attack. In terms of results, the proposed scheme outperforms traditional low-rank approximation techniques, presenting an accuracy of 98.94%, detection rate of 97.70% and false alarm rate of 4.35% for a dataset corruption level of 30% with a random forest algorithm applied for classification. In addition, for error-free conditions, it is found that the proposed approach outperforms other related works, showing accuracy, detection rate and false alarm rate of 99.87%, 99.86% and 0.16%, respectively, for the gradient boosting classifier.},
DOI = {10.3390/s20205845}
}



@Article{rs12203392,
AUTHOR = {Dong, Xiancong and Li, Xiaojie and Zheng, Xingming and Jiang, Tao and Li, Xiaofeng},
TITLE = {Effect of Saline Soil Cracks on Satellite Spectral Inversion Electrical Conductivity},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3392},
URL = {https://www.mdpi.com/2072-4292/12/20/3392},
ISSN = {2072-4292},
ABSTRACT = {The dehydration cracking of saline soil is a kind of common natural phenomenon, and the cracks of saline soil will affect the satellite spectrum, and then affect the accuracy of satellite spectral inversion of electrical conductivity (EC). This study introduces the concept of crack rate (CR) to describe the crack information of saline soil, and quantifies the influence of saline soil crack on the EC of satellite spectral inversion. In 2014 and 2020, the satellite-ground synchronous observation experiments of soda-type inland saline soil and coastal chlorinated-type saline soil were carried out, and the CR of surface cracked saline soil was extracted by an image processing algorithm. For the saline soil spectrum data, the correlation analysis method is used to establish the best band combination that characterizes the relationship between the different saline soil spectrum data and salinity, and the EC inversion model is established using the BP neural network method. The results show that: after the CR is introduced, the determination coefficient (R2) for the EC of soda-type saline soil satellite spectral inversion increased from 0.59 to 0.67, with an increase of 14.42%, and the mean square error (MSE) reduced from 0.20 to 0.16, with a decrease of 19.49%. The R2 for the EC of coastal chlorinated-type saline soil satellite spectral inversion increased from 0.64 to 0.75, an increase of 17.73%, and the MSE decreased from 0.16 to 0.12, a decrease of 25.15%. The study proved the influence of the cracks in the saline soil on the satellite spectrum and provided a new way to improve the accuracy of the satellite spectrum inversion of the EC of the cracked saline soil.},
DOI = {10.3390/rs12203392}
}



@Article{s20205857,
AUTHOR = {Johnson, Brandy J. and Malanoski, Anthony P. and Erickson, Jeffrey S.},
TITLE = {Development of a Colorimetric Sensor for Autonomous, Networked, Real-Time Application},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5857},
URL = {https://www.mdpi.com/1424-8220/20/20/5857},
ISSN = {1424-8220},
ABSTRACT = {This review describes an ongoing effort intended to develop wireless sensor networks for real-time monitoring of airborne targets across a broad area. The goal is to apply the spectrophotometric characteristics of porphyrins and metalloporphyrins in a colorimetric array for detection and discrimination of changes in the chemical composition of environmental air samples. The work includes hardware, software, and firmware design as well as development of algorithms for identification of event occurrence and discrimination of targets. Here, we describe the prototype devices and algorithms related to this effort as well as work directed at selection of indicator arrays for use with the system. Finally, we review the field trials completed with the prototype devices and discuss the outlook for further development.},
DOI = {10.3390/s20205857}
}



@Article{rs12203396,
AUTHOR = {Colorado, Julian D. and Cera-Bornacelli, Natalia and Caldas, Juan S. and Petro, Eliel and Rebolledo, Maria C. and Cuellar, David and Calderon, Francisco and Mondragon, Ivan F. and Jaramillo-Botero, Andres},
TITLE = {Estimation of Nitrogen in Rice Crops from UAV-Captured Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3396},
URL = {https://www.mdpi.com/2072-4292/12/20/3396},
ISSN = {2072-4292},
ABSTRACT = {Leaf nitrogen (N) directly correlates to chlorophyll production, affecting crop growth and yield. Farmers use soil plant analysis development (SPAD) devices to calculate the amount of chlorophyll present in plants. However, monitoring large-scale crops using SPAD is prohibitively time-consuming and demanding. This paper presents an unmanned aerial vehicle (UAV) solution for estimating leaf N content in rice crops, from multispectral imagery. Our contribution is twofold: (i) a novel trajectory control strategy to reduce the angular wind-induced perturbations that affect image sampling accuracy during UAV flight, and (ii) machine learning models to estimate the canopy N via vegetation indices (VIs) obtained from the aerial imagery. This approach integrates an image processing algorithm using the GrabCut segmentation method with a guided filtering refinement process, to calculate the VIs according to the plots of interest. Three machine learning methods based on multivariable linear regressions (MLR), support vector machines (SVM), and neural networks (NN), were applied and compared through the entire phonological cycle of the crop: vegetative (V), reproductive (R), and ripening (Ri). Correlations were obtained by comparing our methods against an assembled ground-truth of SPAD measurements. The higher N correlations were achieved with NN: 0.98 (V), 0.94 (R), and 0.89 (Ri). We claim that the proposed UAV stabilization control algorithm significantly improves on the N-to-SPAD correlations by minimizing wind perturbations in real-time and reducing the need for offline image corrections.},
DOI = {10.3390/rs12203396}
}



@Article{app10207272,
AUTHOR = {Cira, Calimanut-Ionut and Alcarria, Ramón and Manso-Callejo, Miguel-Ángel and Serradilla, Francisco},
TITLE = {A Deep Learning-Based Solution for Large-Scale Extraction of the Secondary Road Network from High-Resolution Aerial Orthoimagery},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7272},
URL = {https://www.mdpi.com/2076-3417/10/20/7272},
ISSN = {2076-3417},
ABSTRACT = {Secondary roads represent the largest part of the road network. However, due to the absence of clearly defined edges, presence of occlusions, and differences in widths, monitoring and mapping them represents a great effort for public administration. We believe that recent advancements in machine vision allow the extraction of these types of roads from high-resolution remotely sensed imagery and can enable the automation of the mapping operation. In this work, we leverage these advances and propose a deep learning-based solution capable of efficiently extracting the surface area of secondary roads at a large scale. The solution is based on hybrid segmentation models trained with high-resolution remote sensing imagery divided in tiles of 256 &times; 256 pixels and their correspondent segmentation masks, resulting in increases in performance metrics of 2.7&ndash;3.5% when compared to the original architectures. The best performing model achieved Intersection over Union and F1 scores of maximum 0.5790 and 0.7120, respectively, with a minimum loss of 0.4985 and was integrated on a web platform which handles the evaluation of large areas, the association of the semantic predictions with geographical coordinates, the conversion of the tiles&rsquo; format and the generation of geotiff results compatible with geospatial databases.},
DOI = {10.3390/app10207272}
}



@Article{rs12203416,
AUTHOR = {Temitope Yekeen, Shamsudeen and Balogun, Abdul-Lateef},
TITLE = {Advances in Remote Sensing Technology, Machine Learning and Deep Learning for Marine Oil Spill Detection, Prediction and Vulnerability Assessment},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3416},
URL = {https://www.mdpi.com/2072-4292/12/20/3416},
ISSN = {2072-4292},
ABSTRACT = {Although advancements in remote sensing technology have facilitated quick capture and identification of the source and location of oil spills in water bodies, the presence of other biogenic elements (lookalikes) with similar visual attributes hinder rapid detection and prompt decision making for emergency response. To date, different methods have been applied to distinguish oil spills from lookalikes with limited success. In addition, accurately modeling the trajectory of oil spills remains a challenge. Thus, we aim to provide further insights on the multi-faceted problem by undertaking a holistic review of past and current approaches to marine oil spill disaster reduction as well as explore the potentials of emerging digital trends in minimizing oil spill hazards. The scope of previous reviews is extended by covering the inter-related dimensions of detection, discrimination, and trajectory prediction of oil spills for vulnerability assessment. Findings show that both optical and microwave airborne and satellite remote sensors are used for oil spill monitoring with microwave sensors being more widely used due to their ability to operate under any weather condition. However, the accuracy of both sensors is affected by the presence of biogenic elements, leading to false positive depiction of oil spills. Statistical image segmentation has been widely used to discriminate lookalikes from oil spills with varying levels of accuracy but the emergence of digitalization technologies in the fourth industrial revolution (IR 4.0) is enabling the use of Machine learning (ML) and deep learning (DL) models, which are more promising than the statistical methods. The Support Vector Machine (SVM) and Artificial Neural Network (ANN) are the most used machine learning algorithms for oil spill detection, although the restriction of ML models to feed forward image classification without support for the end-to-end trainable framework limits its accuracy. On the other hand, deep learning models&rsquo; strong feature extraction and autonomous learning capability enhance their detection accuracy. Also, mathematical models based on lagrangian method have improved oil spill trajectory prediction with higher real time accuracy than the conventional worst case, average and survey-based approaches. However, these newer models are unable to quantify oil droplets and uncertainty in vulnerability prediction. Considering that there is yet no single best remote sensing technique for unambiguous detection and discrimination of oil spills and lookalikes, it is imperative to advance research in the field in order to improve existing technology and develop specialized sensors for accurate oil spill detection and enhanced classification, leveraging emerging geospatial computer vision initiatives.},
DOI = {10.3390/rs12203416}
}



@Article{s20205904,
AUTHOR = {Digulescu, Angela and Despina-Stoian, Cristina and Stănescu, Denis and Popescu, Florin and Enache, Florin and Ioana, Cornel and Rădoi, Emanuel and Rîncu, Iulian and Șerbănescu, Alexandru},
TITLE = {New Approach of UAV Movement Detection and Characterization Using Advanced Signal Processing Methods Based on UWB Sensing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5904},
URL = {https://www.mdpi.com/1424-8220/20/20/5904},
ISSN = {1424-8220},
ABSTRACT = {In the last years, the commercial drone/unmanned aerial vehicles market has grown due to their technological performances (provided by the multiple onboard available sensors), low price, and ease of use. Being very attractive for an increasing number of applications, their presence represents a major issue for public or classified areas with a special status, because of the rising number of incidents. Our paper proposes a new approach for the drone movement detection and characterization based on the ultra-wide band (UWB) sensing system and advanced signal processing methods. This approach characterizes the movement of the drone using classical methods such as correlation, envelope detection, time-scale analysis, but also a new method, the recurrence plot analysis. The obtained results are compared in terms of movement map accuracy and required computation time in order to offer a future starting point for the drone intrusion detection.},
DOI = {10.3390/s20205904}
}



@Article{electronics9101735,
AUTHOR = {Rodríguez-Abreo, Omar and Garcia-Guendulain, Juan Manuel and Hernández-Alvarado, Rodrigo and Flores Rangel, Alejandro and Fuentes-Silva, Carlos},
TITLE = {Genetic Algorithm-Based Tuning of Backstepping Controller for a Quadrotor-Type Unmanned Aerial Vehicle},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1735},
URL = {https://www.mdpi.com/2079-9292/9/10/1735},
ISSN = {2079-9292},
ABSTRACT = {Backstepping is a control technique based on Lyapunov&rsquo;s theory that has been successfully implemented in the control of motors and robots by several nonlinear methods. However, there are no standardized methods for tuning control gains (unlike the PIDs). This paper shows the tuning gains of the backstepping controller, using Genetic Algorithms (GA), for an Unmanned Aerial Vehicle (UAV), quadrotor type, designed for autonomous trajectory tracking. First, a dynamic model of the vehicle is obtained through the Newton‒Euler methodology. Then, the control law is obtained, and self-tuning is performed, through which we can obtain suitable values of the gains in order to achieve the design requirements. In this work, the establishment time and maximum impulse are considered as such. The tuning and simulations of the system response were performed using the MATLAB-Simulink environment, obtaining as a result the compliance of the design parameters and the correct tracking of different trajectories. The results show that self-tuning by means of genetic algorithms satisfactorily adjusts for the gains of a backstepping controller applied to a quadrotor and allows for the implementation of a control system that responds appropriately to errors of different magnitude.},
DOI = {10.3390/electronics9101735}
}



@Article{rs12203460,
AUTHOR = {Liu, Jia and Chen, Jianjun and Qin, Qiaoting and You, Haotian and Han, Xiaowen and Zhou, Guoqing},
TITLE = {Patch Pattern and Ecological Risk Assessment of Alpine Grassland in the Source Region of the Yellow River},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3460},
URL = {https://www.mdpi.com/2072-4292/12/20/3460},
ISSN = {2072-4292},
ABSTRACT = {The source region of the Yellow River (SRYR) is an important water conservation and animal husbandry resource in China. It is of great significance to understand the patch pattern and ecological risk of alpine grassland in the SRYR for ecological environment management. This study first used 12 unmanned aerial vehicle (UAV) aerial images and eight moderate resolution imaging spectroradiometer (MODIS) vegetation index product MOD13Q1 images from July to August in 2019 to extract alpine grassland patch patterns in the SRYR, then constructed an ecological risk model based on the landscape vulnerability index and landscape disturbance index, and finally combined spatial self-reliance correlation and semi-variance analysis methods to explore the spatial distribution of ecological risks. The results showed that the patch fragmentation degree (Pi), area weighted shape index (AWMSI), and separation degree (Si) of the four grassland types in the SRYR are ordered as follows: alpine steppe &gt; degraded meadow &gt; alpine meadow &gt; swamp meadow. Moreover, the greater the fractional vegetation cover (FVC), the greater the landscape dominance index (DOi), and the better the ecosystem stability. The spatial difference of ecological risk in the SRYR shows a situation of low risk in the east (ERImin=1.5355) and high risk in the west (ERImax = 70.6429). High FVC was found in low and mild low risk areas where the vegetation types are mainly swamp meadow and shrub, while low FVC was found in high and mild high-risk areas where the vegetation types are mainly alpine steppe and degraded meadow. The spatial distribution of ecological risk of the SRYR has obvious positive spatial correlation (Moran's I = 0.863), the spatial aggregation distribution is distinct, and the local space has significant high-high aggregation and low&ndash;low aggregation phenomena. The results of this study reveal that patch characteristics have good indicative significance for alpine grassland ecological protection and should be considered in future studies. In addition, the ecological risk in the SRYR is relatively high, especially in the western region, which should be taken seriously in future ecological management and governance.},
DOI = {10.3390/rs12203460}
}



@Article{app10217482,
AUTHOR = {Urrea, Claudio and Kern, John and Alvarado, Johanna},
TITLE = {Design and Evaluation of a New Fuzzy Control Algorithm Applied to a Manipulator Robot},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7482},
URL = {https://www.mdpi.com/2076-3417/10/21/7482},
ISSN = {2076-3417},
ABSTRACT = {In this article, we propose a new scheme for a fuzzy logic controller, which includes acceleration as one of its linguistic variables, as opposed to other techniques and approaches that have been developed and reported in the literature. This method is used for controlling the tracking of the trajectory followed by the joints of a 2-DoF manipulator robot. To this end, a complete simulation environment is developed through the MatLab/Simulink&reg; software. The dynamic model of the manipulator robot includes a vector that consists of the estimate of the friction forces present in the joints. Then, a controller based on fuzzy logic is designed and implemented for each joint. Finally, the performance of the developed system is assessed and then compared to the performance of a classic PID controller. The incorporation of the fuzzy variable acceleration significantly improved the system&rsquo;s response.},
DOI = {10.3390/app10217482}
}



@Article{app10217519,
AUTHOR = {Xie, Chunli and Wang, Xia and Qian, Cheng and Wang, Mengqi},
TITLE = {A Source Code Similarity Based on Siamese Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7519},
URL = {https://www.mdpi.com/2076-3417/10/21/7519},
ISSN = {2076-3417},
ABSTRACT = {Finding similar code snippets is a fundamental task in the field of software engineering. Several approaches have been proposed for this task by using statistical language model which focuses on syntax and structure of codes rather than deep semantic information underlying codes. In this paper, a Siamese Neural Network is proposed that maps codes into continuous space vectors and try to capture their semantic meaning. Firstly, an unsupervised pre-trained method that models code snippets as a weighted series of word vectors. The weights of the series are fitted by the Term Frequency-Inverse Document Frequency (TF-IDF). Then, a Siamese Neural Network trained model is constructed to learn semantic vector representation of code snippets. Finally, the cosine similarity is provided to measure the similarity score between pairs of code snippets. Moreover, we have implemented our approach on a dataset of functionally similar code. The experimental results show that our method improves some performance over single word embedding method.},
DOI = {10.3390/app10217519}
}



@Article{rs12213511,
AUTHOR = {Eskandari, Roghieh and Mahdianpari, Masoud and Mohammadimanesh, Fariba and Salehi, Bahram and Brisco, Brian and Homayouni, Saeid},
TITLE = {Meta-analysis of Unmanned Aerial Vehicle (UAV) Imagery for Agro-environmental Monitoring Using Machine Learning and Statistical Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3511},
URL = {https://www.mdpi.com/2072-4292/12/21/3511},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) imaging systems have recently gained significant attention from researchers and practitioners as a cost-effective means for agro-environmental applications. In particular, machine learning algorithms have been applied to UAV-based remote sensing data for enhancing the UAV capabilities of various applications. This systematic review was performed on studies through a statistical meta-analysis of UAV applications along with machine learning algorithms in agro-environmental monitoring. For this purpose, a total number of 163 peer-reviewed articles published in 13 high-impact remote sensing journals over the past 20 years were reviewed focusing on several features, including study area, application, sensor type, platform type, and spatial resolution. The meta-analysis revealed that 62% and 38% of the studies applied regression and classification models, respectively. Visible sensor technology was the most frequently used sensor with the highest overall accuracy among classification articles. Regarding regression models, linear regression and random forest were the most frequently applied models in UAV remote sensing imagery processing. Finally, the results of this study confirm that applying machine learning approaches on UAV imagery produces fast and reliable results. Agriculture, forestry, and grassland mapping were found as the top three UAV applications in this review, in 42%, 22%, and 8% of the studies, respectively.},
DOI = {10.3390/rs12213511}
}



@Article{w12113010,
AUTHOR = {Wang, Ruimeng and Xia, Haoming and Qin, Yaochen and Niu, Wenhui and Pan, Li and Li, Rumeng and Zhao, Xiaoyang and Bian, Xiqing and Fu, Pinde},
TITLE = {Dynamic Monitoring of Surface Water Area during 1989–2019 in the Hetao Plain Using Landsat Data in Google Earth Engine},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3010},
URL = {https://www.mdpi.com/2073-4441/12/11/3010},
ISSN = {2073-4441},
ABSTRACT = {The spatio-temporal change of the surface water is very important to agricultural, economic, and social development in the Hetao Plain, as well as the structure and function of the ecosystem. To understand the long-term changes of the surface water area in the Hetao Plain, we used all available Landsat images (7534 scenes) and adopted the modified Normalized Difference Water Index (mNDWI), Enhanced Vegetation Index (EVI), and Normalized Difference Vegetation Index (NDVI) to map the open-surface water from 1989 to 2019 in the Google Earth Engine (GEE) cloud platform. We further analyzed precipitation, temperature, and irrigated area, revealing the impact of climate change and human activities on long-term surface water changes. The results show the following. (1) In the last 31 years, the maximum, seasonal, and annual average water body area values in the Hetao Plain have exhibited a downward trend. Meanwhile, the number of maximum, seasonal, and permanent water bodies displayed a significant upward trend. (2) The variation of the surface water area in the Hetao Plain is mainly affected by the maximum water body area, while the variation of the water body number is mainly affected by the number of minimum water bodies. (3) Precipitation has statistically significant positive effects on the water body area and water body number, which has statistically significant negative effects with temperature and irrigation. The findings of this study can be used to help the policy-makers and farmers understand changing water resources and its driving mechanism and provide a reference for water resources management, agricultural irrigation, and ecological protection.},
DOI = {10.3390/w12113010}
}



@Article{rs12213533,
AUTHOR = {Pedro, Dário and Matos-Carvalho, João P. and Azevedo, Fábio and Sacoto-Martins, Ricardo and Bernardo, Luís and Campos, Luís and Fonseca, José M. and Mora, André},
TITLE = {FFAU—Framework for Fully Autonomous UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3533},
URL = {https://www.mdpi.com/2072-4292/12/21/3533},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), although hardly a new technology, have recently gained a prominent role in many industries being widely used not only among enthusiastic consumers, but also in high demanding professional situations, and will have a massive societal impact over the coming years. However, the operation of UAVs is fraught with serious safety risks, such as collisions with dynamic obstacles (birds, other UAVs, or randomly thrown objects). These collision scenarios are complex to analyze in real-time, sometimes being computationally impossible to solve with existing State of the Art (SoA) algorithms, making the use of UAVs an operational hazard and therefore significantly reducing their commercial applicability in urban environments. In this work, a conceptual framework for both stand-alone and swarm (networked) UAVs is introduced, with a focus on the architectural requirements of the collision avoidance subsystem to achieve acceptable levels of safety and reliability. The SoA principles for collision avoidance against stationary objects are reviewed and a novel approach is described, using deep learning techniques to solve the computational intensive problem of real-time collision avoidance with dynamic objects. The proposed framework includes a web-interface allowing the full control of UAVs as remote clients with a supervisor cloud-based platform. The feasibility of the proposed approach was demonstrated through experimental tests using a UAV, developed from scratch using the proposed framework. Test flight results are presented for an autonomous UAV monitored from multiple countries across the world.},
DOI = {10.3390/rs12213533}
}



@Article{app10217622,
AUTHOR = {Vujasinović, Stéphane and Becker, Stefan and Breuer, Timo and Bullinger, Sebastian and Scherer-Negenborn, Norbert and Arens, Michael},
TITLE = {Integration of the 3D Environment for UAV Onboard Visual Object Tracking},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7622},
URL = {https://www.mdpi.com/2076-3417/10/21/7622},
ISSN = {2076-3417},
ABSTRACT = {Single visual object tracking from an unmanned aerial vehicle (UAV) poses fundamental challenges such as object occlusion, small-scale objects, background clutter, and abrupt camera motion. To tackle these difficulties, we propose to integrate the 3D structure of the observed scene into a detection-by-tracking algorithm. We introduce a pipeline that combines a model-free visual object tracker, a sparse 3D reconstruction, and a state estimator. The 3D reconstruction of the scene is computed with an image-based Structure-from-Motion (SfM) component that enables us to leverage a state estimator in the corresponding 3D scene during tracking. By representing the position of the target in 3D space rather than in image space, we stabilize the tracking during ego-motion and improve the handling of occlusions, background clutter, and small-scale objects. We evaluated our approach on prototypical image sequences, captured from a UAV with low-altitude oblique views. For this purpose, we adapted an existing dataset for visual object tracking and reconstructed the observed scene in 3D. The experimental results demonstrate that the proposed approach outperforms methods using plain visual cues as well as approaches leveraging image-space-based state estimations. We believe that our approach can be beneficial for trafficmonitoring, video surveillance, and navigation.},
DOI = {10.3390/app10217622}
}



@Article{electronics9111795,
AUTHOR = {Wei, Wei and Zhang, Chen and Deng, Dexiang},
TITLE = {Content Estimation of Foreign Fibers in Cotton Based on Deep Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1795},
URL = {https://www.mdpi.com/2079-9292/9/11/1795},
ISSN = {2079-9292},
ABSTRACT = {Cotton foreign fibers directly affect the quality of a textile product; the less foreign fibers in raw cotton, the higher the quality grade of the textile product. Based on the foreign fiber clean machine, this paper proposed an evaluation method of foreign fiber content using deep learning. First of all, a large number of images of foreign fibers were collected from different production lines and annotated to obtain the mask image dataset of foreign fibers. Secondly, by comparing the image segmentation algorithm based on deep learning, tests showed that U-Net has a better performance on different segment metrics evaluations, and U-Net is improved to realize the real-time segmentation of foreign fiber images. The actual size of the foreign fiber could be calculated through the combination of the segment result and the mechanical parameters of the machine. Finally, the test results showed that the relative error between the estimated size and the actual size was less than 4%. After the prototype test, the algorithm was deployed on the actual production line and, by comparing the algorithm data in a random time with the actual foreign fiber statistical data, the overall error was less than 2%. The test showed that the new evaluation method can fully reflect the content of foreign fiber in raw cotton.},
DOI = {10.3390/electronics9111795}
}



@Article{drones4040069,
AUTHOR = {Garzon-Lopez, Carol X. and Lasso, Eloisa},
TITLE = {Species Classification in a Tropical Alpine Ecosystem Using UAV-Borne RGB and Hyperspectral Imagery},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/2504-446X/4/4/69},
ISSN = {2504-446X},
ABSTRACT = {P&aacute;ramos host more than 3500 vascular plant species and are crucial water providers for millions of people in the northern Andes. Monitoring species distribution at large scales is an urgent conservation priority in the face of ongoing climatic changes and increasing anthropogenic pressure on this ecosystem. For the first time in this ecosystem, we explored the potential of unoccupied aerial vehicles (UAV)-borne red, green, and blue wavelengths (RGB) and hyperspectral imagery for p&aacute;ramo species classification by collecting both types of images in a 10-ha area, and ground vegetation cover data from 10 plots within this area. Five plots were used for calibration and the other five for validation. With the hyperspectral data, we tested our capacity to detect five representative p&aacute;ramo species with different growth forms using support vector machine (SVM) and random forest (RF) classifiers in combination with three feature selection methods and two class groups. Using RGB images, we could classify 21 species with an accuracy greater than 97%. From hyperspectral imaging, the highest accuracy (89%) was found using models built with RF or SVM classifiers combined with a binary grouping method and the sequential floating forward selection feature. Our results demonstrate that p&aacute;ramo species can be accurately mapped using both RGB and hyperspectral imagery.},
DOI = {10.3390/drones4040069}
}



@Article{rs12213587,
AUTHOR = {Masjedi, Ali and Crawford, Melba M. and Carpenter, Neal R. and Tuinstra, Mitchell R.},
TITLE = {Multi-Temporal Predictive Modelling of Sorghum Biomass Using UAV-Based Hyperspectral and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3587},
URL = {https://www.mdpi.com/2072-4292/12/21/3587},
ISSN = {2072-4292},
ABSTRACT = {High-throughput phenotyping using high spatial, spectral, and temporal resolution remote sensing (RS) data has become a critical part of the plant breeding chain focused on reducing the time and cost of the selection process for the &ldquo;best&rdquo; genotypes with respect to the trait(s) of interest. In this paper, the potential of accurate and reliable sorghum biomass prediction using visible and near infrared (VNIR) and short-wave infrared (SWIR) hyperspectral data as well as light detection and ranging (LiDAR) data acquired by sensors mounted on UAV platforms is investigated. Predictive models are developed using classical regression-based machine learning methods for nine experiments conducted during the 2017 and 2018 growing seasons at the Agronomy Center for Research and Education (ACRE) at Purdue University, Indiana, USA. The impact of the regression method, data source, timing of RS and field-based biomass reference data acquisition, and the number of samples on the prediction results are investigated. R2 values for end-of-season biomass ranged from 0.64 to 0.89 for different experiments when features from all the data sources were included. Geometry-based features derived from the LiDAR point cloud to characterize plant structure and chemistry-based features extracted from hyperspectral data provided the most accurate predictions. Evaluation of the impact of the time of data acquisition during the growing season on the prediction results indicated that although the most accurate and reliable predictions of final biomass were achieved using remotely sensed data from mid-season to end-of-season, predictions in mid-season provided adequate results to differentiate between promising varieties for selection. The analysis of variance (ANOVA) of the accuracies of the predictive models showed that both the data source and regression method are important factors for a reliable prediction; however, the data source was more important with 69% significance, versus 28% significance for the regression method.},
DOI = {10.3390/rs12213587}
}



@Article{s20216247,
AUTHOR = {Calvario, Gabriela and Alarcón, Teresa E. and Dalmau, Oscar and Sierra, Basilio and Hernandez, Carmen},
TITLE = {An Agave Counting Methodology Based on Mathematical Morphology and Images Acquired through Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6247},
URL = {https://www.mdpi.com/1424-8220/20/21/6247},
ISSN = {1424-8220},
ABSTRACT = {Blue agave is an important commercial crop in Mexico, and it is the main source of the traditional mexican beverage known as tequila. The variety of blue agave crop known as Tequilana Weber is a crucial element for tequila agribusiness and the agricultural economy in Mexico. The number of agave plants in the field is one of the main parameters for estimating production of tequila. In this manuscript, we describe a mathematical morphology-based algorithm that addresses the agave automatic counting task. The proposed methodology was applied to a set of real images collected using an Unmanned Aerial Vehicle equipped with a digital Red-Green-Blue (RGB) camera. The number of plants automatically identified in the collected images was compared to the number of plants counted by hand. Accuracy of the proposed algorithm depended on the size heterogeneity of plants in the field and illumination. Accuracy ranged from 0.8309 to 0.9806, and performance of the proposed algorithm was satisfactory.},
DOI = {10.3390/s20216247}
}



@Article{computers9040087,
AUTHOR = {Correia, Sérgio D. and Fé, João and Tomic, Slavisa and Beko, Marko},
TITLE = {Development of a Test-Bench for Evaluating the Embedded Implementation of the Improved Elephant Herding Optimization Algorithm Applied to Energy-Based Acoustic Localization},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {87},
URL = {https://www.mdpi.com/2073-431X/9/4/87},
ISSN = {2073-431X},
ABSTRACT = {The present work addresses the development of a test-bench for the embedded implementation, validity, and testing of the recently proposed Improved Elephant Herding Optimization (iEHO) algorithm, applied to the acoustic localization problem. The implemented methodology aims to corroborate the feasibility of applying iEHO in real-time applications on low complexity and low power devices, where three different electronic modules are used and tested. Swarm-based metaheuristic methods are usually examined by employing high-level languages on centralized computers, demonstrating their capability in finding global or good local solutions. This work considers iEHO implementation in C-language running on an embedded processor. Several random scenarios are generated, uploaded, and processed by the embedded processor to demonstrate the algorithm’s effectiveness and the test-bench usability, low complexity, and high reliability. On the one hand, the results obtained in our test-bench are concordant with the high-level implementations using MatLab® in terms of accuracy. On the other hand, concerning the processing time and as a breakthrough, the results obtained over the test-bench allow to demonstrate a high suitability of the embedded iEHO implementation for real-time applications due to its low latency.},
DOI = {10.3390/computers9040087}
}



@Article{rs12213617,
AUTHOR = {Trevisan, Rodrigo and Pérez, Osvaldo and Schmitz, Nathan and Diers, Brian and Martin, Nicolas},
TITLE = {High-Throughput Phenotyping of Soybean Maturity Using Time Series UAV Imagery and Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3617},
URL = {https://www.mdpi.com/2072-4292/12/21/3617},
ISSN = {2072-4292},
ABSTRACT = {Soybean maturity is a trait of critical importance for the development of new soybean cultivars, nevertheless, its characterization based on visual ratings has many challenges. Unmanned aerial vehicles (UAVs) imagery-based high-throughput phenotyping methodologies have been proposed as an alternative to the traditional visual ratings of pod senescence. However, the lack of scalable and accurate methods to extract the desired information from the images remains a significant bottleneck in breeding programs. The objective of this study was to develop an image-based high-throughput phenotyping system for evaluating soybean maturity in breeding programs. Images were acquired twice a week, starting when the earlier lines began maturation until the latest ones were mature. Two complementary convolutional neural networks (CNN) were developed to predict the maturity date. The first using a single date and the second using the five best image dates identified by the first model. The proposed CNN architecture was validated using more than 15,000 ground truth observations from five trials, including data from three growing seasons and two countries. The trained model showed good generalization capability with a root mean squared error lower than two days in four out of five trials. Four methods of estimating prediction uncertainty showed potential at identifying different sources of errors in the maturity date predictions. The architecture developed solves limitations of previous research and can be used at scale in commercial breeding programs.},
DOI = {10.3390/rs12213617}
}



@Article{agriculture10110523,
AUTHOR = {Ngo, Ha Quang Thinh and Huynh, Van Ngoc Son and Nguyen, Thanh Phuong and Nguyen, Hung},
TITLE = {Sustainable Agriculture: Stable Robust Control in Presence of Uncertainties for Multi-Functional Indoor Transportation of Farm Products},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {523},
URL = {https://www.mdpi.com/2077-0472/10/11/523},
ISSN = {2077-0472},
ABSTRACT = {Currently, integrated trends play a key role in every aspect of automation applications. In particular, if the mechanization of agriculture becomes a competitive factor among farmers or nations, then the multi-functional transportation of agricultural products is inevitable in global trade. In sustainable transportation, the challenge of overcoming stable control in harsh environments, such as through imprecise parameters or varying loads, should be addressed. In this paper, a novel controller for a nonholonomic mechanical system able to adapt to uncertainties is proposed. Based on the multi-functional autonomous carrier (MAC), the system configuration of the kinematic and dynamic model is launched in order to identify the unstable problems that arise when tracking the trajectory. To solve these troubles, the decoupled formation of a MAC system has been investigated by considering two second-order components, namely a linear speed-based sub-system and angular speed-based sub-system. To stabilize the whole system using the Lyapunov theory, the advanced control techniques are studied. To validate the proposed approach, a series of test scenarios have been carried out. From the superior performance of numerous trials, it is clear that our approach is effective, feasible, and reasonable for the advanced control of agricultural applications.},
DOI = {10.3390/agriculture10110523}
}



@Article{su12219177,
AUTHOR = {Mandal, Vishal and Mussah, Abdul Rashid and Jin, Peng and Adu-Gyamfi, Yaw},
TITLE = {Artificial Intelligence-Enabled Traffic Monitoring System},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {9177},
URL = {https://www.mdpi.com/2071-1050/12/21/9177},
ISSN = {2071-1050},
ABSTRACT = {Manual traffic surveillance can be a daunting task as Traffic Management Centers operate a myriad of cameras installed over a network. Injecting some level of automation could help lighten the workload of human operators performing manual surveillance and facilitate making proactive decisions which would reduce the impact of incidents and recurring congestion on roadways. This article presents a novel approach to automatically monitor real time traffic footage using deep convolutional neural networks and a stand-alone graphical user interface. The authors describe the results of research received in the process of developing models that serve as an integrated framework for an artificial intelligence enabled traffic monitoring system. The proposed system deploys several state-of-the-art deep learning algorithms to automate different traffic monitoring needs. Taking advantage of a large database of annotated video surveillance data, deep learning-based models are trained to detect queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level segmentation approach is applied to detect traffic queues and predict severity. Real-time object detection algorithms coupled with different tracking systems are deployed to automatically detect stranded vehicles as well as perform vehicular counts. At each stage of development, interesting experimental results are presented to demonstrate the effectiveness of the proposed system. Overall, the results demonstrate that the proposed framework performs satisfactorily under varied conditions without being immensely impacted by environmental hazards such as blurry camera views, low illumination, rain, or snow.},
DOI = {10.3390/su12219177}
}



@Article{s20216299,
AUTHOR = {Bhowmick, Sutanu and Nagarajaiah, Satish and Veeraraghavan, Ashok},
TITLE = {Vision and Deep Learning-Based Algorithms to Detect and Quantify Cracks on Concrete Surfaces from UAV Videos},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6299},
URL = {https://www.mdpi.com/1424-8220/20/21/6299},
ISSN = {1424-8220},
ABSTRACT = {Immediate assessment of structural integrity of important civil infrastructures, like bridges, hospitals, or dams, is of utmost importance after natural disasters. Currently, inspection is performed manually by engineers who look for local damages and their extent on significant locations of the structure to understand its implication on its global stability. However, the whole process is time-consuming and prone to human errors. Due to their size and extent, some regions of civil structures are hard to gain access for manual inspection. In such situations, a vision-based system of Unmanned Aerial Vehicles (UAVs) programmed with Artificial Intelligence algorithms may be an effective alternative to carry out a health assessment of civil infrastructures in a timely manner. This paper proposes a framework of achieving the above-mentioned goal using computer vision and deep learning algorithms for detection of cracks on the concrete surface from its image by carrying out image segmentation of pixels, i.e., classification of pixels in an image of the concrete surface and whether it belongs to cracks or not. The image segmentation or dense pixel level classification is carried out using a deep neural network architecture named U-Net. Further, morphological operations on the segmented images result in dense measurements of crack geometry, like length, width, area, and crack orientation for individual cracks present in the image. The efficacy and robustness of the proposed method as a viable real-life application was validated by carrying out a laboratory experiment of a four-point bending test on an 8-foot-long concrete beam of which the video is recorded using a camera mounted on a UAV-based, as well as a still ground-based, video camera. Detection, quantification, and localization of damage on a civil infrastructure using the proposed framework can directly be used in the prognosis of the structure&rsquo;s ability to withstand service loads.},
DOI = {10.3390/s20216299}
}



@Article{s20216348,
AUTHOR = {Ghazal, Mohammed and Basmaji, Tasnim and Yaghi, Maha and Alkhedher, Mohammad and Mahmoud, Mohamed and El-Baz, Ayman S.},
TITLE = {Cloud-Based Monitoring of Thermal Anomalies in Industrial Environments Using AI and the Internet of Robotic Things},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6348},
URL = {https://www.mdpi.com/1424-8220/20/21/6348},
ISSN = {1424-8220},
ABSTRACT = {Recent advancements in cloud computing, artificial intelligence, and the internet of things (IoT) create new opportunities for autonomous industrial environments monitoring. Nevertheless, detecting anomalies in harsh industrial settings remains challenging. This paper proposes an edge-fog-cloud architecture with mobile IoT edge nodes carried on autonomous robots for thermal anomalies detection in aluminum factories. We use companion drones as fog nodes to deliver first response services and a cloud back-end for thermal anomalies analysis. We also propose a self-driving deep learning architecture and a thermal anomalies detection and visualization algorithm. Our results show our robot surveyors are low-cost, deliver reduced response time, and more accurately detect anomalies compared to human surveyors or fixed IoT nodes monitoring the same industrial area. Our self-driving architecture has a root mean square error of 0.19 comparable to VGG-19 with a significantly reduced complexity and three times the frame rate at 60 frames per second. Our thermal to visual registration algorithm maximizes mutual information in the image-gradient domain while adapting to different resolutions and camera frame rates.},
DOI = {10.3390/s20216348}
}



@Article{rs12223698,
AUTHOR = {Pastucha, Elżbieta and Puniach, Edyta and Ścisłowicz, Agnieszka and Ćwiąkała, Paweł and Niewiem, Witold and Wiącek, Paweł},
TITLE = {3D Reconstruction of Power Lines Using UAV Images to Monitor Corridor Clearance},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3698},
URL = {https://www.mdpi.com/2072-4292/12/22/3698},
ISSN = {2072-4292},
ABSTRACT = {Regular power line inspections are essential to ensure the reliability of electricity supply. The inspections of overground power submission lines include corridor clearance monitoring and fault identification. The power lines corridor is a three-dimensional space around power cables defined by a set distance. Any obstacles breaching this space should be detected, as they potentially threaten the safety of the infrastructure. Corridor clearance monitoring is usually performed either by a labor-intensive total station survey (TS), terrestrial laser scanning (TLS), or expensive airborne laser scanning (ALS) from a plane or a helicopter. This paper proposes a method that uses unmanned aerial vehicle (UAV) images to monitor corridor clearance. To maintain the adequate accuracy of the relative position of wires in regard to surrounding obstacles, the same data were used both to reconstruct a point cloud representation of a digital surface model (DSM) and a 3D power line. The proposed algorithm detects power lines in a series of images using decorrelation stretch for initial image processing, the modified Prewitt filter for edge enhancement, random sample consensus (RANSAC) with additional parameters for line fitting, and epipolar geometry for 3D reconstruction. DSM points intruding into the corridor are then detected by calculating the spatial distance between a reconstructed power line and the DSM point cloud representation. Problematic objects are localized by segmenting points into voxels and then subsequent clusterization. The processing results were compared to the results of two verification methods&mdash;TS and TLS. The comparison results show that the proposed method can be used to survey power lines with an accuracy consistent with that of classical measurements.},
DOI = {10.3390/rs12223698}
}



@Article{s20226442,
AUTHOR = {Barmpoutis, Panagiotis and Papaioannou, Periklis and Dimitropoulos, Kosmas and Grammalidis, Nikos},
TITLE = {A Review on Early Forest Fire Detection Systems Using Optical Remote Sensing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6442},
URL = {https://www.mdpi.com/1424-8220/20/22/6442},
ISSN = {1424-8220},
ABSTRACT = {The environmental challenges the world faces nowadays have never been greater or more complex. Global areas covered by forests and urban woodlands are threatened by natural disasters that have increased dramatically during the last decades, in terms of both frequency and magnitude. Large-scale forest fires are one of the most harmful natural hazards affecting climate change and life around the world. Thus, to minimize their impacts on people and nature, the adoption of well-planned and closely coordinated effective prevention, early warning, and response approaches are necessary. This paper presents an overview of the optical remote sensing technologies used in early fire warning systems and provides an extensive survey on both flame and smoke detection algorithms employed by each technology. Three types of systems are identified, namely terrestrial, airborne, and spaceborne-based systems, while various models aiming to detect fire occurrences with high accuracy in challenging environments are studied. Finally, the strengths and weaknesses of fire detection systems based on optical remote sensing are discussed aiming to contribute to future research projects for the development of early warning fire systems.},
DOI = {10.3390/s20226442}
}



@Article{f11111190,
AUTHOR = {Lebedev, Vadim G. and Lebedeva, Tatyana N. and Chernodubov, Aleksey I. and Shestibratov, Konstantin A.},
TITLE = {Genomic Selection for Forest Tree Improvement: Methods, Achievements and Perspectives},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1190},
URL = {https://www.mdpi.com/1999-4907/11/11/1190},
ISSN = {1999-4907},
ABSTRACT = {The breeding of forest trees is only a few decades old, and is a much more complicated, longer, and expensive endeavor than the breeding of agricultural crops. One breeding cycle for forest trees can take 20&ndash;30 years. Recent advances in genomics and molecular biology have revolutionized traditional plant breeding based on visual phenotype assessment: the development of different types of molecular markers has made genotype selection possible. Marker-assisted breeding can significantly accelerate the breeding process, but this method has not been shown to be effective for selection of complex traits on forest trees. This new method of genomic selection is based on the analysis of all effects of quantitative trait loci (QTLs) using a large number of molecular markers distributed throughout the genome, which makes it possible to assess the genomic estimated breeding value (GEBV) of an individual. This approach is expected to be much more efficient for forest tree improvement than traditional breeding. Here, we review the current state of the art in the application of genomic selection in forest tree breeding and discuss different methods of genotyping and phenotyping. We also compare the accuracies of genomic prediction models and highlight the importance of a prior cost-benefit analysis before implementing genomic selection. Perspectives for the further development of this approach in forest breeding are also discussed: expanding the range of species and the list of valuable traits, the application of high-throughput phenotyping methods, and the possibility of using epigenetic variance to improve of forest trees.},
DOI = {10.3390/f11111190}
}



@Article{app10228008,
AUTHOR = {Kim, Byunghyun and Cho, Soojin},
TITLE = {Automated Multiple Concrete Damage Detection Using Instance Segmentation Deep Learning Model},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8008},
URL = {https://www.mdpi.com/2076-3417/10/22/8008},
ISSN = {2076-3417},
ABSTRACT = {In many developed countries with a long history of urbanization, there is an increasing need for automated computer vision (CV)-based inspection to replace conventional labor-intensive visual inspection. This paper proposes a technique for the automated detection of multiple concrete damage based on a state-of-the-art deep learning framework, Mask R-CNN, developed for instance segmentation. The structure of Mask R-CNN, which consists of three stages (region proposal, classification, and segmentation) is optimized for multiple concrete damage detection. The optimized Mask R-CNN is trained with 765 concrete images including cracks, efflorescence, rebar exposure, and spalling. The performance of the trained Mask R-CNN is evaluated with 25 actual test images containing damage as well as environmental objects. Two types of metrics are proposed to measure localization and segmentation performance. On average, 90.41% precision and 90.81% recall are achieved for localization and 87.24% precision and 87.58% recall for segmentation, which indicates the excellent field applicability of the trained Mask R-CNN. This paper also qualitatively discusses the test results by explaining that the architecture of Mask R-CNN that is optimized for general object detection purposes, can be modified to detect long and slender shapes of cracks, rebar exposure, and efflorescence in further research.},
DOI = {10.3390/app10228008}
}



@Article{agronomy10111762,
AUTHOR = {Zhao, Biquan and Li, Jiating and Baenziger, P. Stephen and Belamkar, Vikas and Ge, Yufeng and Zhang, Jian and Shi, Yeyin},
TITLE = {Automatic Wheat Lodging Detection and Mapping in Aerial Imagery to Support High-Throughput Phenotyping and In-Season Crop Management},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1762},
URL = {https://www.mdpi.com/2073-4395/10/11/1762},
ISSN = {2073-4395},
ABSTRACT = {Latest advances in unmanned aerial vehicle (UAV) technology and convolutional neural networks (CNNs) allow us to detect crop lodging in a more precise and accurate way. However, the performance and generalization of a model capable of detecting lodging when the plants may show different spectral and morphological signatures have not been investigated much. This study investigated and compared the performance of models trained using aerial imagery collected at two growth stages of winter wheat with different canopy phenotypes. Specifically, three CNN-based models were trained with aerial imagery collected at early grain filling stage only, at physiological maturity only, and at both stages. Results show that the multi-stage model trained by images from both growth stages outperformed the models trained by images from individual growth stages on all testing data. The mean accuracy of the multi-stage model was 89.23% for both growth stages, while the mean of the other two models were 52.32% and 84.9%, respectively. This study demonstrates the importance of diversity of training data in big data analytics, and the feasibility of developing a universal decision support system for wheat lodging detection and mapping multi-growth stages with high-resolution remote sensing imagery.},
DOI = {10.3390/agronomy10111762}
}



@Article{app10228105,
AUTHOR = {Kim, Jung Jin and Kim, Ah-Ram and Lee, Seong-Won},
TITLE = {Artificial Neural Network-Based Automated Crack Detection and Analysis for the Inspection of Concrete Structures},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8105},
URL = {https://www.mdpi.com/2076-3417/10/22/8105},
ISSN = {2076-3417},
ABSTRACT = {The damage investigation and inspection methods for infrastructures performed in small-scale (type III) facilities usually involve a visual examination by an inspector using surveying tools (e.g., cracking, crack microscope, etc.) in the field. These methods can interfere with the subjectivity of the inspector, which may reduce the objectivity and reliability of the record. Therefore, a new image analysis technique is needed to automatically detect cracks and analyze the characteristics of the cracks objectively. In this study, an image analysis technique using deep learning is developed to detect cracks and analyze characteristics (e.g., length, and width) in images for small-scale facilities. Three stages of image processing pipeline are proposed to obtain crack detection and its characteristics. In the first and second stages, two-dimensional convolutional neural networks are used for crack image detection (e.g., classification and segmentation). Based on convolution neural network for the detection, hierarchical feature learning architecture is applied into our deep learning network. After deep learning-based detection, in the third stage, thinning and tracking algorithms are applied to analyze length and width of crack in the image. The performance of the proposed method was tested using various crack images with label and the results showed good performance of crack detection and its measurement.},
DOI = {10.3390/app10228105}
}



@Article{rs12223778,
AUTHOR = {Fu, Yuanyuan and Yang, Guijun and Li, Zhenhai and Song, Xiaoyu and Li, Zhenhong and Xu, Xingang and Wang, Pei and Zhao, Chunjiang},
TITLE = {Winter Wheat Nitrogen Status Estimation Using UAV-Based RGB Imagery and Gaussian Processes Regression},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3778},
URL = {https://www.mdpi.com/2072-4292/12/22/3778},
ISSN = {2072-4292},
ABSTRACT = {Predicting the crop nitrogen (N) nutrition status is critical for optimizing nitrogen fertilizer application. The present study examined the ability of multiple image features derived from unmanned aerial vehicle (UAV) RGB images for winter wheat N status estimation across multiple critical growth stages. The image features consisted of RGB-based vegetation indices (VIs), color parameters, and textures, which represented image features of different aspects and different types. To determine which N status indicators could be well-estimated, we considered two mass-based N status indicators (i.e., the leaf N concentration (LNC) and plant N concentration (PNC)) and two area-based N status indicators (i.e., the leaf N density (LND) and plant N density (PND)). Sixteen RGB-based VIs associated with crop growth were selected. Five color space models, including RGB, HSV, L*a*b*, L*c*h*, and L*u*v*, were used to quantify the winter wheat canopy color. The combination of Gaussian processes regression (GPR) and Gabor-based textures with four orientations and five scales was proposed to estimate the winter wheat N status. The gray level co-occurrence matrix (GLCM)-based textures with four orientations were extracted for comparison. The heterogeneity in the textures of different orientations was evaluated using the measures of mean and coefficient of variation (CV). The variable importance in projection (VIP) derived from partial least square regression (PLSR) and a band analysis tool based on Gaussian processes regression (GPR-BAT) were used to identify the best performing image features for the N status estimation. The results indicated that (1) the combination of RGB-based VIs or color parameters only could produce reliable estimates of PND and the GPR model based on the combination of color parameters yielded a higher accuracy for the estimation of PND (R2val = 0.571, RMSEval = 2.846 g/m2, and RPDval = 1.532), compared to that based on the combination of RGB-based VIs; (2) there was no significant heterogeneity in the textures of different orientations and the textures of 45 degrees were recommended in the winter wheat N status estimation; (3) compared with the RGB-based VIs and color parameters, the GPR model based on the Gabor-based textures produced a higher accuracy for the estimation of PND (R2val = 0.675, RMSEval = 2.493 g/m2, and RPDval = 1.748) and the PLSR model based on the GLCM-based textures produced a higher accuracy for the estimation of PNC (R2val = 0.612, RMSEval = 0.380%, and RPDval = 1.601); and (4) the combined use of RGB-based VIs, color parameters, and textures produced comparable estimation results to using textures alone. Both VIP-PLSR and GPR-BAT analyses confirmed that image textures contributed most to the estimation of winter wheat N status. The experimental results reveal the potential of image textures derived from high-definition UAV-based RGB images for the estimation of the winter wheat N status. They also suggest that a conventional low-cost digital camera mounted on a UAV could be well-suited for winter wheat N status monitoring in a fast and non-destructive way.},
DOI = {10.3390/rs12223778}
}



@Article{rs12223789,
AUTHOR = {Li, Bo and Gan, Zhigang and Chen, Daqing and Sergey Aleksandrovich, Dyachenko},
TITLE = {UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2072-4292/12/22/3789},
ISSN = {2072-4292},
ABSTRACT = {This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.},
DOI = {10.3390/rs12223789}
}



@Article{w12113231,
AUTHOR = {Lu, Yijie and Zhang, Zhen and Huang, Danni},
TITLE = {Glacier Mapping Based on Random Forest Algorithm: A Case Study over the Eastern Pamir},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3231},
URL = {https://www.mdpi.com/2073-4441/12/11/3231},
ISSN = {2073-4441},
ABSTRACT = {Debris-covered glaciers are common features on the eastern Pamir and serve as important indicators of climate change promptly. However, mapping of debris-covered glaciers in alpine regions is still challenging due to many factors including the spectral similarity between debris and the adjacent bedrock, shadows cast from mountains and clouds, and seasonal snow cover. Considering that few studies have added movement velocity features when extracting glacier boundaries, we innovatively developed an automatic algorithm consisting of rule-based image segmentation and Random Forest to extract information about debris-covered glaciers with Landsat-8 OLI/TIRS data for spectral, texture and temperature features, multi-digital elevation models (DEMs) for elevation and topographic features, and the Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) for movement velocity features, and accuracy evaluation was performed to determine the optimal feature combination extraction of debris-covered glaciers. The study found that the overall accuracy of extracting debris-covered glaciers using combined movement velocity features is 97.60%, and the Kappa coefficient is 0.9624, which is better than the extraction results using other schemes. The high classification accuracy obtained using our method overcomes most of the above-mentioned challenges and can detect debris-covered glaciers, illustrating that this method can be executed efficiently, which will further help water resources management.},
DOI = {10.3390/w12113231}
}



@Article{rs12223783,
AUTHOR = {Khanal, Sami and KC, Kushal and Fulton, John P. and Shearer, Scott and Ozkan, Erdal},
TITLE = {Remote Sensing in Agriculture—Accomplishments, Limitations, and Opportunities},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3783},
URL = {https://www.mdpi.com/2072-4292/12/22/3783},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34%), followed by the United States (20%) and China (11%). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability.},
DOI = {10.3390/rs12223783}
}



@Article{s20226623,
AUTHOR = {Kampczyk, Arkadiusz},
TITLE = {An Innovative Approach to Surveying the Geometry of Visibility Triangles at Railway Level Crossings},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6623},
URL = {https://www.mdpi.com/1424-8220/20/22/6623},
ISSN = {1424-8220},
ABSTRACT = {Railway level crossings (RLCs) in Poland are classified according to their protection systems. Category D, which is a form of passive RLC, aims to ensure safe and efficient operation. Surveying is essential to prepare and control the geometry of the visibility triangles used at RLCs. This article presents a new approach to monitoring the geometry of visibility triangles of RLCs using an electronic total station and a magnetic measuring square (MMS). Its main assumptions are presented together with the application of the innovative measuring instruments. Visibility is demonstrated taking into account the angles of intersection of the road axis with the track axis of the railway line and additional attributes related to the analysis and evaluation of general visibility conditions. The research highlights controversies that have received special attention against the background of the safety status of railway level crossings. As a case study, the RLC located on a single-track railway line in Poland is examined. The final section presents applications of the results obtained according to the proposed methodology. It is shown that the proposed approach is practical and effective. In addition to surveyors, the survey methodology can be used by road and rail traffic engineers and policy makers to further improve traffic safety at RLCs. This is an important global research task.},
DOI = {10.3390/s20226623}
}



@Article{s20226680,
AUTHOR = {Sayeed, Mohd Abuzar and Kumar, Rajesh and Sharma, Vishal and Sayeed, Mohd Asim},
TITLE = {Efficient Deployment with Throughput Maximization for UAVs Communication Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6680},
URL = {https://www.mdpi.com/1424-8220/20/22/6680},
ISSN = {1424-8220},
ABSTRACT = {The article presents a throughput maximization approach for UAV assisted ground networks. Throughput maximization involves minimizing delay and packet loss through UAV trajectory optimization, reinforcing the congested nodes and transmission channels. The aggressive reinforcement policy is achieved by characterizing nodes, links, and overall topology through delay, loss, throughput, and distance. A position-aware graph neural network (GNN) is used for characterization, prediction, and dynamic UAV trajectory enhancement. To establish correctness, the proposed approach is validated against optimized link state routing (OLSR) driven UAV assisted ground networks. The proposed approach considerably outperforms the classical approach by demonstrating significant gains in throughput and packet delivery ratio with notable decrements in delay and packet loss. The performance analysis of the proposed approach against software-defined UAVs (U-S) and UAVs as base stations (U-B) verifies the consistency and gains in average throughput while minimizing delay and packet loss. The scalability test of the proposed approach is performed by varying data rates and the number of UAVs.},
DOI = {10.3390/s20226680}
}



@Article{rs12223839,
AUTHOR = {Tian, Xiaomin and Chen, Long and Zhang, Xiaoli and Chen, Erxue},
TITLE = {Improved Prototypical Network Model for Forest Species Classification in Complex Stand},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3839},
URL = {https://www.mdpi.com/2072-4292/12/22/3839},
ISSN = {2072-4292},
ABSTRACT = {Deep learning has become an effective method for hyperspectral image classification. However, the high band correlation and data volume associated with airborne hyperspectral images, and the insufficiency of training samples, present challenges to the application of deep learning in airborne image classification. Prototypical networks are practical deep learning networks that have demonstrated effectiveness in handling small-sample classification. In this study, an improved prototypical network is proposed (by adding L2 regularization to the convolutional layer and dropout to the maximum pooling layer) to address the problem of overfitting in small-sample classification. The proposed network has an optimal sample window for classification, and the window size is related to the area and distribution of the study area. After performing dimensionality reduction using principal component analysis, the time required for training using hyperspectral images shortened significantly, and the test accuracy increased drastically. Furthermore, when the size of the sample window was 27 &times; 27 after dimensionality reduction, the overall accuracy of forest species classification was 98.53%, and the Kappa coefficient was 0.9838. Therefore, by using an improved prototypical network with a sample window of an appropriate size, the network yielded desirable classification results, thereby demonstrating its suitability for the fine classification and mapping of tree species.},
DOI = {10.3390/rs12223839}
}



@Article{f11121239,
AUTHOR = {Scharvogel, Daniel and Brandmeier, Melanie and Weis, Manuel},
TITLE = {A Deep Learning Approach for Calamity Assessment Using Sentinel-2 Data},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1239},
URL = {https://www.mdpi.com/1999-4907/11/12/1239},
ISSN = {1999-4907},
ABSTRACT = {The number of severe storm events has increased in recent decades due to climate change. These storms are one of the main causes for timber loss in European forests and damaged areas are prone to further degradation by, for example, bark beetle infestations. Usually, manual mapping of damaged areas based on aerial photographs is conducted by forest departments. This is very time-consuming and therefore automatic detection of windthrows based on active and passive remote sensing data is an ongoing research topic. In this study we evaluated state-of-the-art Convolutional Neural Networks (CNNs) in combination with Geographic Information Systems (GIS) for calamity assessment. The study area is in in the northern part of Hesse (Germany) and was covered by twelve Sentinel-2 scenes from 2018. Labels of damaged areas from the Friedericke storm (18 January 2018) were provided by HessenForst. We conducted several experiments based on a custom U-Net setup to derive the optimal architecture and input data as well as to assess the transferability of the model. Results highlight the possibility to detect damaged forest areas using Sentinel-2 data. Using a binary classification, accuracies of more than 92% were achieved with an Intersection over Union (IoU) score of 46.6%. The proposed workflow was integrated into ArcGIS and is suitable for fast detection of damaged areas directly after a storm and for disaster management but is limited by the deca-meter spatial resolution of the Sentinel-2 data.},
DOI = {10.3390/f11121239}
}



@Article{rs12233855,
AUTHOR = {Tseng, Chun-Wei and Song, Cheng-En and Wang, Su-Fen and Chen, Yi-Chin and Tu, Jien-Yi and Yang, Ci-Jian and Chuang, Chih-Wei},
TITLE = {Application of High-Resolution Radar Rain Data to the Predictive Analysis of Landslide Susceptibility under Climate Change in the Laonong Watershed, Taiwan},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3855},
URL = {https://www.mdpi.com/2072-4292/12/23/3855},
ISSN = {2072-4292},
ABSTRACT = {Extreme rainfall has caused severe road damage and landslide disasters in mountainous areas. Rainfall forecasting derived from remote sensing data has been widely adopted for disaster prevention and early warning as a trend in recent years. By integrating high-resolution radar rain data, for example, the QPESUMS (quantitative precipitation estimation and segregation using multiple sensors) system provides a great opportunity to establish the extreme climate-based landslide susceptibility model, which would be helpful in the prevention of hillslope disasters under climate change. QPESUMS was adopted to obtain spatio-temporal rainfall patterns, and further, multi-temporal landslide inventories (2003&ndash;2018) would integrate with other explanatory factors and therefore, we can establish the logistic regression method for prediction of landslide susceptibility sites in the Laonong River watershed, which was devastated by Typhoon Morakot in 2009. Simulations of landslide susceptibility under the critical rainfall (300, 600, and 900 mm) were designed to verify the model&rsquo;s sensitivity. Due to the orographic effect, rainfall was concentrated at the low mountainous and middle elevation areas in the southern Laonong River watershed. Landslide change analysis indicates that the landslide ratio increased from 1.5% to 7.0% after Typhoon Morakot in 2009. Subsequently, the landslide ratio fluctuated between 3.5% and 4.5% after 2012, which indicates that the recovery of landslide areas is still in progress. The validation results showed that the calibrated model of 2005 is preferred in the general period, with an accuracy of 78%. For extreme rainfall typhoons, the calibrated model of 2009 would perform better (72%). This study presented that the integration of multi-temporal landslide inventories in a logistic regression model is capable of predicting rainfall-triggered landslide risk under climate change.},
DOI = {10.3390/rs12233855}
}



@Article{s20236732,
AUTHOR = {Qi, Haixia and Zhu, Bingyu and Wu, Zeyu and Liang, Yu and Li, Jianwen and Wang, Leidi and Chen, Tingting and Lan, Yubin and Zhang, Lei},
TITLE = {Estimation of Peanut Leaf Area Index from Unmanned Aerial Vehicle Multispectral Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6732},
URL = {https://www.mdpi.com/1424-8220/20/23/6732},
ISSN = {1424-8220},
ABSTRACT = {Leaf area index (LAI) is used to predict crop yield, and unmanned aerial vehicles (UAVs) provide new ways to monitor LAI. In this study, we used a fixed-wing UAV with multispectral cameras for remote sensing monitoring. We conducted field experiments with two peanut varieties at different planting densities to estimate LAI from multispectral images and establish a high-precision LAI prediction model. We used eight vegetation indices (VIs) and developed simple regression and artificial neural network (BPN) models for LAI and spectral VIs. The empirical model was calibrated to estimate peanut LAI, and the best model was selected from the coefficient of determination and root mean square error. The red (660 nm) and near-infrared (790 nm) bands effectively predicted peanut LAI, and LAI increased with planting density. The predictive accuracy of the multiple regression model was higher than that of the single linear regression models, and the correlations between Modified Red-Edge Simple Ratio Index (MSR), Ratio Vegetation Index (RVI), Normalized Difference Vegetation Index (NDVI), and LAI were higher than the other indices. The combined VI BPN model was more accurate than the single VI BPN model, and the BPN model accuracy was higher. Planting density affects peanut LAI, and reflectance-based vegetation indices can help predict LAI.},
DOI = {10.3390/s20236732}
}



@Article{robotics9040100,
AUTHOR = {Roy, Raphaëlle N. and Drougard, Nicolas and Gateau, Thibault and Dehais, Frédéric and Chanel, Caroline P. C.},
TITLE = {How Can Physiological Computing Benefit Human-Robot Interaction?},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {100},
URL = {https://www.mdpi.com/2218-6581/9/4/100},
ISSN = {2218-6581},
ABSTRACT = {As systems grow more automatized, the human operator is all too often overlooked. Although human-robot interaction (HRI) can be quite demanding in terms of cognitive resources, the mental states (MS) of the operators are not yet taken into account by existing systems. As humans are no providential agents, this lack can lead to hazardous situations. The growing number of neurophysiology and machine learning tools now allows for efficient operators&rsquo; MS monitoring. Sending feedback on MS in a closed-loop solution is therefore at hand. Involving a consistent automated planning technique to handle such a process could be a significant asset. This perspective article was meant to provide the reader with a synthesis of the significant literature with a view to implementing systems that adapt to the operator&rsquo;s MS to improve human-robot operations&rsquo; safety and performance. First of all, the need for this approach is detailed regarding remote operation, an example of HRI. Then, several MS identified as crucial for this type of HRI are defined, along with relevant electrophysiological markers. A focus is made on prime degraded MS linked to time-on-task and task demands, as well as collateral MS linked to system outputs (i.e., feedback and alarms). Lastly, the principle of symbiotic HRI is detailed and one solution is proposed to include the operator state vector into the system using a mixed-initiative decisional framework to drive such an interaction.},
DOI = {10.3390/robotics9040100}
}



@Article{en13236250,
AUTHOR = {Ayele, Yonas Zewdu and Aliyari, Mostafa and Griffiths, David and Droguett, Enrique Lopez},
TITLE = {Automatic Crack Segmentation for UAV-Assisted Bridge Inspection},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6250},
URL = {https://www.mdpi.com/1996-1073/13/23/6250},
ISSN = {1996-1073},
ABSTRACT = {Bridges are a critical piece of infrastructure in the network of road and rail transport system. Many of the bridges in Norway (in Europe) are at the end of their lifespan, therefore regular inspection and maintenance are critical to ensure the safety of their operations. However, the traditional inspection procedures and resources required are so time consuming and costly that there exists a significant maintenance backlog. The central thrust of this paper is to demonstrate the significant benefits of adapting a Unmanned Aerial Vehicle (UAV)-assisted inspection to reduce the time and costs of bridge inspection and established the research needs associated with the processing of the (big) data produced by such autonomous technologies. In this regard, a methodology is proposed for analysing the bridge damage that comprises three key stages, (i) data collection and model training, where one performs experiments and trials to perfect drone flights for inspection using case study bridges to inform and provide necessary (big) data for the second key stage, (ii) 3D construction, where one built 3D models that offer a permanent record of element geometry for each bridge asset, which could be used for navigation and control purposes, (iii) damage identification and analysis, where deep learning-based data analytics and modelling are applied for processing and analysing UAV image data and to perform bridge damage performance assessment. The proposed methodology is exemplified via UAV-assisted inspection of Skodsberg bridge, a 140 m prestressed concrete bridge, in the Viken county in eastern Norway.},
DOI = {10.3390/en13236250}
}



@Article{rs12233892,
AUTHOR = {Egli, Sebastian and Höpke, Martin},
TITLE = {CNN-Based Tree Species Classification Using High Resolution RGB Image Data from Automated UAV Observations},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3892},
URL = {https://www.mdpi.com/2072-4292/12/23/3892},
ISSN = {2072-4292},
ABSTRACT = {Data on the distribution of tree species are often requested by forest managers, inventory agencies, foresters as well as private and municipal forest owners. However, the automated detection of tree species based on passive remote sensing data from aerial surveys is still not sufficiently developed to achieve reliable results independent of the phenological stage, time of day, season, tree vitality and prevailing atmospheric conditions. Here, we introduce a novel tree species classification approach based on high resolution RGB image data gathered during automated UAV flights that overcomes these insufficiencies. For the classification task, a computationally lightweight convolutional neural network (CNN) was designed. We show that with the chosen CNN model architecture, average classification accuracies of 92% can be reached independently of the illumination conditions and the phenological stages of four different tree species. We also show that a minimal ground sampling density of 1.6 cm/px is needed for the classification model to be able to make use of the spatial-structural information in the data. Finally, to demonstrate the applicability of the presented approach to derive spatially explicit tree species information, a gridded product is generated that yields an average classification accuracy of 88%.},
DOI = {10.3390/rs12233892}
}



@Article{rs12233925,
AUTHOR = {Pilaš, Ivan and Gašparović, Mateo and Novkinić, Alan and Klobučar, Damir},
TITLE = {Mapping of the Canopy Openings in Mixed Beech–Fir Forest at Sentinel-2 Subpixel Level Using UAV and Machine Learning Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3925},
URL = {https://www.mdpi.com/2072-4292/12/23/3925},
ISSN = {2072-4292},
ABSTRACT = {The presented study demonstrates a bi-sensor approach suitable for rapid and precise up-to-date mapping of forest canopy gaps for the larger spatial extent. The approach makes use of Unmanned Aerial Vehicle (UAV) red, green and blue (RGB) images on smaller areas for highly precise forest canopy mask creation. Sentinel-2 was used as a scaling platform for transferring information from the UAV to a wider spatial extent. Various approaches to an improvement in the predictive performance were examined: (I) the highest R2 of the single satellite index was 0.57, (II) the highest R2 using multiple features obtained from the single-date, S-2 image was 0.624, and (III) the highest R2 on the multitemporal set of S-2 images was 0.697. Satellite indices such as Atmospherically Resistant Vegetation Index (ARVI), Infrared Percentage Vegetation Index (IPVI), Normalized Difference Index (NDI45), Pigment-Specific Simple Ratio Index (PSSRa), Modified Chlorophyll Absorption Ratio Index (MCARI), Color Index (CI), Redness Index (RI), and Normalized Difference Turbidity Index (NDTI) were the dominant predictors in most of the Machine Learning (ML) algorithms. The more complex ML algorithms such as the Support Vector Machines (SVM), Random Forest (RF), Stochastic Gradient Boosting (GBM), Extreme Gradient Boosting (XGBoost), and Catboost that provided the best performance on the training set exhibited weaker generalization capabilities. Therefore, a simpler and more robust Elastic Net (ENET) algorithm was chosen for the final map creation.},
DOI = {10.3390/rs12233925}
}



@Article{rs12233937,
AUTHOR = {Graf, Lukas and Bach, Heike and Tiede, Dirk},
TITLE = {Semantic Segmentation of Sentinel-2 Imagery for Mapping Irrigation Center Pivots},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3937},
URL = {https://www.mdpi.com/2072-4292/12/23/3937},
ISSN = {2072-4292},
ABSTRACT = {Estimating the number and size of irrigation center pivot systems (CPS) from remotely sensed data, using artificial intelligence (AI), is a potential information source for assessing agricultural water use. In this study, we identified two technical challenges in the neural-network-based classification: Firstly, an effective reduction of the feature space of the remote sensing data to shorten training times and increase classification accuracy is required. Secondly, the geographical transferability of the AI algorithms is a pressing issue if AI is to replace human mapping efforts one day. Therefore, we trained the semantic image segmentation algorithm U-NET on four spectral channels (U-NET SPECS) and the first three principal components (U-NET principal component analysis (PCA)) of ESA/Copernicus Sentinel-2 images on a study area in Texas, USA, and assessed the geographic transferability of the trained models to two other sites: the Duero basin, in Spain, and South Africa. U-NET SPECS outperformed U-NET PCA at all three study areas, with the highest f1-score at Texas (0.87, U-NET PCA: 0.83), and a value of 0.68 (U-NET PCA: 0.43) in South Africa. At the Duero, both models showed poor classification accuracy (f1-score U-NET PCA: 0.08; U-NET SPECS: 0.16) and segmentation quality, which was particularly evident in the incomplete representation of the center pivot geometries. In South Africa and at the Duero site, a high rate of false positive and false negative was observed, which made the model less useful, especially at the Duero test site. Thus, geographical invariance is not an inherent model property and seems to be mainly driven by the complexity of land-use pattern. We do not consider PCA a suited spectral dimensionality reduction measure in this. However, shorter training times and a more stable training process indicate promising prospects for reducing computational burdens. We therefore conclude that effective dimensionality reduction and geographic transferability are important prospects for further research towards the operational usage of deep learning algorithms, not only regarding the mapping of CPS.},
DOI = {10.3390/rs12233937}
}



@Article{en13236357,
AUTHOR = {Cipriani, Giovanni and D’Amico, Antonino and Guarino, Stefania and Manno, Donatella and Traverso, Marzia and Di Dio, Vincenzo},
TITLE = {Convolutional Neural Network for Dust and Hotspot Classification in PV Modules},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6357},
URL = {https://www.mdpi.com/1996-1073/13/23/6357},
ISSN = {1996-1073},
ABSTRACT = {This paper proposes an innovative approach to classify the losses related to photovoltaic (PV) systems, through the use of thermographic non-destructive tests (TNDTs) supported by artificial intelligence techniques. Low electricity production in PV systems can be caused by an efficiency decrease in PV modules due to abnormal operating conditions such as failures or malfunctions. The most common performance decreases are due to the presence of dirt on the surface of the module, the impact of which depends on many parameters and conditions, and can be identified through the use of the TNDTs. The proposed approach allows one to automatically classify the thermographic images from the convolutional neural network (CNN) of the system, achieving an accuracy of 98% in tests that last a couple of minutes. This approach, compared to approaches in literature, offers numerous advantages, including speed of execution, speed of diagnosis, reduced costs, reduction in electricity production losses.},
DOI = {10.3390/en13236357}
}



@Article{sym12121994,
AUTHOR = {Li, Ping and Ni, Zhiwei and Zhu, Xuhui and Song, Juan and Wu, Wenying},
TITLE = {Optimal Transport with Dimensionality Reduction for Domain Adaptation},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1994},
URL = {https://www.mdpi.com/2073-8994/12/12/1994},
ISSN = {2073-8994},
ABSTRACT = {Domain adaptation manages to learn a robust classifier for target domain, using the source domain, but they often follow different distributions. To bridge distribution shift between the two domains, most of previous works aim to align their feature distributions through feature transformation, of which optimal transport for domain adaptation has attract researchers&rsquo; interest, as it can exploit the local information of the two domains in the process of mapping the source instances to the target ones by minimizing Wasserstein distance between their feature distributions. However, it may weaken the feature discriminability of source domain, thus degrade domain adaptation performance. To address this problem, this paper proposes a two-stage feature-based adaptation approach, referred to as optimal transport with dimensionality reduction (OTDR). In the first stage, we apply the dimensionality reduction with intradomain variant maximization but source intraclass compactness minimization, to separate data samples as much as possible and enhance the feature discriminability of the source domain. In the second stage, we leverage optimal transport-based technique to preserve the local information of the two domains. Notably, the desirable properties in the first stage can mitigate the degradation of feature discriminability of the source domain in the second stage. Extensive experiments on several cross-domain image datasets validate that OTDR is superior to its competitors in classification accuracy.},
DOI = {10.3390/sym12121994}
}



@Article{rs12233967,
AUTHOR = {Li, Zhen and Zhao, Baojun and Wang, Wenzheng},
TITLE = {An Efficient Spectral Feature Extraction Framework for Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3967},
URL = {https://www.mdpi.com/2072-4292/12/23/3967},
ISSN = {2072-4292},
ABSTRACT = {Extracting diverse spectral features from hyperspectral images has become a hot topic in recent years. However, these models are time consuming for training and test and suffer from a poor discriminative ability, resulting in low classification accuracy. In this paper, we design an effective feature extracting framework for the spectra of hyperspectral data. We construct a structured dictionary to encode spectral information and apply learning machine to map coding coefficients. To reduce training and testing time, the sparsity constraint is replaced by a block-diagonal constraint to accelerate the iteration, and an efficient extreme learning machine is employed to fit the spectral characteristics. To optimize the discriminative ability of our model, we first add spectral convolution to extract abundant spectral information. Then, we design shared constraints for subdictionaries so that the common features of subdictionaries can be expressed more effectively, and the discriminative and reconstructive ability of dictionary will be improved. The experimental results on diverse databases show that the proposed feature extraction framework can not only greatly reduce the training and testing time, but also lead to very competitive accuracy performance compared with deep learning models.},
DOI = {10.3390/rs12233967}
}



@Article{su122310150,
AUTHOR = {Zhu, Yongyan and Jeon, Seongwoo and Sung, Hyunchan and Kim, Yoonji and Park, Chiyoung and Cha, Sungeun and Jo, Hyun-woo and Lee, Woo-kyun},
TITLE = {Developing UAV-Based Forest Spatial Information and Evaluation Technology for Efficient Forest Management},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {10150},
URL = {https://www.mdpi.com/2071-1050/12/23/10150},
ISSN = {2071-1050},
ABSTRACT = {Forest spatial information is regularly established and managed as basic data for national forest planning and forest policy establishment. Among them, the grade of vegetation conservation shall be investigated and evaluated according to the value of vegetation conservation. As the collection of field data over large or remote areas is difficult, unmanned aerial vehicles (UAVs) are increasingly being used for this purpose. Consequently, there is a need for research on UAV-monitoring and three-dimensional (3D) image generation techniques. In this study, a new method that can efficiently collect and analyze UAV spatial data to survey and assess forests was developed. Both UAV-based and LiDAR imaging methods were evaluated in conjunction with the ground control point measurement method for forest surveys. In addition, by fusing the field survey database of each target site and the UAV optical and LiDAR images, the Gongju, Samcheok, and Seogwipo regions were analyzed based on deep learning. The kappa value showed 0.59, 0.47, and 0.78 accuracy for each of the sites in terms of vegetation type (artificial or natural), and 0.68, 0.53, and 0.62 accuracy in terms of vegetation layer structure. The results of comparative analysis with ecological natural maps by establishing vegetation conservation levels show that about 83.9% of the areas are consistent. The findings verified the applicability of this UAV-based approach for the construction of geospatial information on forests. The proposed method can be useful for improving the efficiency of the Vegetation Conservation Classification system and for conducting high-resolution monitoring in forests worldwide.},
DOI = {10.3390/su122310150}
}



@Article{logistics4040033,
AUTHOR = {Haji, Mona and Kerbache, Laoucine and Muhammad, Mahaboob and Al-Ansari, Tareq},
TITLE = {Roles of Technology in Improving Perishable Food Supply Chains},
JOURNAL = {Logistics},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {33},
URL = {https://www.mdpi.com/2305-6290/4/4/33},
ISSN = {2305-6290},
ABSTRACT = {Food supply chains are considered to be more complex systems than other types of supply chains. This complexity is due to the continuous changes taking place, particularly in ensuring the quality of food products throughout the entire supply chain, from growing, procurement of resources, production, and management of stock, to distribution to the final consumers. For that, food supply chain markets have become more highly developed in the use of modern technologies, and have begun to implement them in their logistical systems to satisfy their customers&rsquo; needs. The main objectives of this review are to identify the different technological implementations in different phases of the food supply chain processes and point out the key factors for using technologies to improve the characteristics of the perishable food supply chain. A total number of 137 articles were analyzed in this research to achieve these review objectives. Some of the various technologies found in different phases of the food supply chain were radio frequency identification (RFID), the Internet of Things (IoT), blockchain, three-dimensional printing (3DP), autonomous vehicles, and unmanned aerial vehicles (UAVs). These technologies were found in different phases of the food supply chain and improved the efficiency of supplying perishable foods. The review identified different characteristics of the perishable food supply chain. The main finding indicated that technological implementation enhances the efficiency and sustainability of the food supply chains and helps to retain perishable food characteristics.},
DOI = {10.3390/logistics4040033}
}



@Article{rs12234000,
AUTHOR = {Nevavuori, Petteri and Narra, Nathaniel and Linna, Petri and Lipping, Tarmo},
TITLE = {Crop Yield Prediction Using Multitemporal UAV Data and Spatio-Temporal Deep Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {4000},
URL = {https://www.mdpi.com/2072-4292/12/23/4000},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51% mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17% MAPE with four weekly frames from the beginning of the season.},
DOI = {10.3390/rs12234000}
}



@Article{s20247061,
AUTHOR = {Yang, Zhao and Tang, Rong and Bao, Jie and Lu, Jiahuan and Zhang, Zhijie},
TITLE = {A Real-Time Trajectory Prediction Method of Small-Scale Quadrotors Based on GPS Data and Neural Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7061},
URL = {https://www.mdpi.com/1424-8220/20/24/7061},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a real-time trajectory prediction method for quadrotors based on a bidirectional gated recurrent unit model. Historical trajectory data of ten types of quadrotors were obtained. The bidirectional gated recurrent units were constructed and utilized to learn the historic data. The prediction results were compared with the traditional gated recurrent unit method to test its prediction performance. The efficiency of the proposed algorithm was investigated by comparing the training loss and training time. The results over the testing datasets showed that the proposed model produced better prediction results than the baseline models for all scenarios of the testing datasets. It was also found that the proposed model can converge to a stable state faster than the traditional gated recurrent unit model. Moreover, various types of training samples were applied and compared. With the same randomly selected test datasets, the performance of the prediction model can be improved by selecting the historical trajectory samples of the quadrotors close to the weight or volume of the target quadrotor for training. In addition, the performance of stable trajectory samples is significantly better than that with unstable trajectory segments with a frequent change of speed and direction with large angles.},
DOI = {10.3390/s20247061}
}



@Article{rs12244070,
AUTHOR = {Ellsäßer, Florian and Röll, Alexander and Ahongshangbam, Joyson and Waite, Pierre-André and Hendrayanto and Schuldt, Bernhard and Hölscher, Dirk},
TITLE = {Predicting Tree Sap Flux and Stomatal Conductance from Drone-Recorded Surface Temperatures in a Mixed Agroforestry System—A Machine Learning Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4070},
URL = {https://www.mdpi.com/2072-4292/12/24/4070},
ISSN = {2072-4292},
ABSTRACT = {Plant transpiration is a key element in the hydrological cycle. Widely used methods for its assessment comprise sap flux techniques for whole-plant transpiration and porometry for leaf stomatal conductance. Recently emerging approaches based on surface temperatures and a wide range of machine learning techniques offer new possibilities to quantify transpiration. The focus of this study was to predict sap flux and leaf stomatal conductance based on drone-recorded and meteorological data and compare these predictions with in-situ measured transpiration. To build the prediction models, we applied classical statistical approaches and machine learning algorithms. The field work was conducted in an oil palm agroforest in lowland Sumatra. Random forest predictions yielded the highest congruence with measured sap flux (r2 = 0.87 for trees and r2 = 0.58 for palms) and confidence intervals for intercept and slope of a Passing-Bablok regression suggest interchangeability of the methods. Differences in model performance are indicated when predicting different tree species. Predictions for stomatal conductance were less congruent for all prediction methods, likely due to spatial and temporal offsets of the measurements. Overall, the applied drone and modelling scheme predicts whole-plant transpiration with high accuracy. We conclude that there is large potential in machine learning approaches for ecological applications such as predicting transpiration.},
DOI = {10.3390/rs12244070}
}



@Article{rs12244080,
AUTHOR = {Kavats, Olena and Khramov, Dmitriy and Sergieieva, Kateryna and Vasyliev, Volodymyr},
TITLE = {Monitoring of Sugarcane Harvest in Brazil Based on Optical and SAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4080},
URL = {https://www.mdpi.com/2072-4292/12/24/4080},
ISSN = {2072-4292},
ABSTRACT = {The algorithms for determining sugarcane harvest dates are proposed; the algorithms allow the ability to monitor large areas and are based on the publicly available Synthetic Aperture Radar (SAR) and optical satellite data. Algorithm 1 uses the NDVI (Normalized Difference Vegetation Index) time series derived from Sentinel-2 data. Sharp and continuous decrease in the NDVI values is the main sign of sugarcane harvest. The NDVI time series allows the ability to determine most harvest dates. The best estimates of the sugarcane areas harvested per month have been obtained from March to August 2018 when cloudy pixel percentage is less than 45% of the image area. Algorithm 2 of the harvest monitoring uses the coherence time series derived from Sentinel-1 Single Look Complex (SLC) images and optical satellite data. Low coherence, demonstrating sharp growth upon the harvest completion, corresponds to the harvest period. The NDVI time series trends were used to refine the algorithm. It is supposed that the descending NDVI trend corresponds to harvest. The algorithms were used to identify the harvest dates and calculate the harvested areas of the reference sample of 574 sugarcane parcels with a total area of 3745 ha in the state of S&atilde;o Paulo, Brazil. The harvested areas identified by visual interpretation coincide with the optical-data algorithm (algorithm 1) by 97%; the coincidence with the algorithm based on SAR and optical data (algorithm 2) is 90%. The main practical applications of the algorithms are harvest monitoring and identification of the harvested fields to estimate the harvested area.},
DOI = {10.3390/rs12244080}
}



@Article{ijerph17249333,
AUTHOR = {Zhang, Xuechen and Shen, Huanfeng and Li, Tongwen and Zhang, Liangpei},
TITLE = {The Effects of Fireworks Discharge on Atmospheric PM2.5 Concentration in the Chinese Lunar New Year},
JOURNAL = {International Journal of Environmental Research and Public Health},
VOLUME = {17},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {9333},
URL = {https://www.mdpi.com/1660-4601/17/24/9333},
PubMedID = {33322228},
ISSN = {1660-4601},
ABSTRACT = {Discharging fireworks during the Chinese Lunar New Year celebrations is a deep-rooted custom in China. In this paper, we analyze the effect of this cultural activity on PM2.5 concentration using both ground observations and satellite data. By combining remote sensing data, the problem of uneven spatial distribution of ground monitoring has been compensated, and the research time span has been expanded. The results show that the extensive firework displays on New Year&rsquo;s Eve lead to a remarkable increase in nationwide PM2.5 concentration, which were 159~223% of the average level, indicating the instantaneous effect far exceeds that of any other factor over the whole year. However, the averaged PM2.5 concentrations of the celebration period were 0.99~16.32 &mu;g/m3 lower compared to the average values of the corresponding pre-celebration period and post-celebration period, indicating the sustained effect is not very significant. The implementation of firework prohibition policies can greatly reduce the instantaneous PM2.5 increase, but no obvious air quality improvement is observed over the entire celebration period. Combining these findings and the cultural significance of this activity, we recommend that this custom is actively maintained, using new technologies and scientific governance programs to minimize the negative effects.},
DOI = {10.3390/ijerph17249333}
}



@Article{rs12244104,
AUTHOR = {Chadwick, Andrew J. and Goodbody, Tristan R. H. and Coops, Nicholas C. and Hervieux, Anne and Bater, Christopher W. and Martens, Lee A. and White, Barry and Röeser, Dominik},
TITLE = {Automatic Delineation and Height Measurement of Regenerating Conifer Crowns under Leaf-Off Conditions Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4104},
URL = {https://www.mdpi.com/2072-4292/12/24/4104},
ISSN = {2072-4292},
ABSTRACT = {The increasing use of unmanned aerial vehicles (UAV) and high spatial resolution imagery from associated sensors necessitates the continued advancement of efficient means of image processing to ensure these tools are utilized effectively. This is exemplified in the field of forest management, where the extraction of individual tree crown information stands to benefit operational budgets. We explored training a region-based convolutional neural network (Mask R-CNN) to automatically delineate individual tree crown (ITC) polygons in regenerating forests (14 years after harvest) using true colour red-green-blue (RGB) imagery with an average ground sampling distance (GSD) of 3 cm. We predicted ITC polygons to extract height information using canopy height models generated from digital aerial photogrammetric (DAP) point clouds. Our approach yielded an average precision of 0.98, an average recall of 0.85, and an average F1 score of 0.91 for the delineation of ITC. Remote height measurements were strongly correlated with field height measurements (r2 = 0.93, RMSE = 0.34 m). The mean difference between DAP-derived and field-collected height measurements was &minus;0.37 m and &minus;0.24 m for white spruce (Picea glauca) and lodgepole pine (Pinus contorta), respectively. Our results show that accurate ITC delineation in young, regenerating stands is possible with fine-spatial resolution RGB imagery and that predicted ITC can be used in combination with DAP to estimate tree height.},
DOI = {10.3390/rs12244104}
}



@Article{electronics9122156,
AUTHOR = {Li, Shunli and Zhang, Qiuyi and Li, Jinlun and Zhao, Hongxin and Yin, Xiaoxing and Yang, Mei},
TITLE = {Monopulse Antenna Based on Singular Spoof Surface Plasmon Polariton Structure for Angle Measurement},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2156},
URL = {https://www.mdpi.com/2079-9292/9/12/2156},
ISSN = {2079-9292},
ABSTRACT = {Direction finding and target tracking make demanding requirements on the measurement of incoming angles of electromagnetic waves. A monopulse antenna, based on the singular symmetric spoof surface plasmon polariton (SSPP) structure, is proposed for high-accuracy angle sensing. The singular SSPP structure is composed of periodic corrugated grooves for the confinement of the electromagnetic fields. Due to the microstrip&ndash;coplanar waveguide transition, the fields along both sides of the SSPP add constructively to form the endfire beam at the sum port and destructively to form the null radiation in the endfire direction at the difference port. An optimization based on the team progress algorithm is adopted to facilitate this antenna design. A prototype is designed and fabricated to validate the design principle, and measured results agree with the simulation. The proposed antenna shows a wide bandwidth ranging from 5.0 GHz to 7.5 GHz for both the sum and difference ports with the return loss greater than 10 dB, realizing a relative bandwidth of 40%. The isolation for the sum and difference ports is higher than 21 dB, and the null depth is larger than 20 dB over the entire operating range, which is favorable for the high accuracy angle sensing and measurement. This monopulse antenna has broad prospect in angle measuring systems such as direction finding and radar tracking scenes.},
DOI = {10.3390/electronics9122156}
}



@Article{electronics9122162,
AUTHOR = {Sun, Changqi and Zhang, Cong and Xiong, Naixue},
TITLE = {Infrared and Visible Image Fusion Techniques Based on Deep Learning: A Review},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2162},
URL = {https://www.mdpi.com/2079-9292/9/12/2162},
ISSN = {2079-9292},
ABSTRACT = {Infrared and visible image fusion technologies make full use of different image features obtained by different sensors, retain complementary information of the source images during the fusion process, and use redundant information to improve the credibility of the fusion image. In recent years, many researchers have used deep learning methods (DL) to explore the field of image fusion and found that applying DL has improved the time-consuming efficiency of the model and the fusion effect. However, DL includes many branches, and there is currently no detailed investigation of deep learning methods in image fusion. In this work, this survey reports on the development of image fusion algorithms based on deep learning in recent years. Specifically, this paper first conducts a detailed investigation on the fusion method of infrared and visible images based on deep learning, compares the existing fusion algorithms qualitatively and quantitatively with the existing fusion quality indicators, and discusses various fusions. The main contribution, advantages, and disadvantages of the algorithm. Finally, the research status of infrared and visible image fusion is summarized, and future work has prospected. This research can help us realize many image fusion methods in recent years and lay the foundation for future research work.},
DOI = {10.3390/electronics9122162}
}



@Article{agronomy10121989,
AUTHOR = {Armenta-Medina, Dagoberto and Ramirez-delReal, Tania A. and Villanueva-Vásquez, Daniel and Mejia-Aguirre, Cristian},
TITLE = {Trends on Advanced Information and Communication Technologies for Improving Agricultural Productivities: A Bibliometric Analysis},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1989},
URL = {https://www.mdpi.com/2073-4395/10/12/1989},
ISSN = {2073-4395},
ABSTRACT = {In this work, an exhaustive revision is given of the literature associated with advanced information and communication technologies in agriculture within a window of 25 years using bibliometric tools enabled to detect of the main actors, structure, and dynamics in the scientific papers. The main findings are a trend of growth in the dynamics of publications associated with advanced information and communication technologies in agriculture productivity. Another assertion is that countries, like the USA, China, and Brazil, stand out in many publications due to allocating more resources to research, development, and agricultural productivity. In addition, the collaboration networks between countries are frequently in regions with closer cultural and idiomatic ties; additionally, terms&rsquo; occurrence are obtained with Louvain algorithm predominating four clusters: precision agriculture, smart agriculture, remote sensing, and climate smart agriculture. Finally, the thematic-map characterization with Callon&rsquo;s density and centrality is applied in three periods. The first period of thematic analysis shows a transition in detecting the variability of a nutrient, such as nitrogen, through the help of immature georeferenced techniques, towards greater remote sensing involvement. In the transition from the second to the third stage, the maturation of technologies, such as unmanned aerial vehicles, wireless sensor networks, and the machine learning area, is observed.},
DOI = {10.3390/agronomy10121989}
}



@Article{rs12244149,
AUTHOR = {Samarin, Maxim and Zweifel, Lauren and Roth, Volker and Alewell, Christine},
TITLE = {Identifying Soil Erosion Processes in Alpine Grasslands on Aerial Imagery with a U-Net Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4149},
URL = {https://www.mdpi.com/2072-4292/12/24/4149},
ISSN = {2072-4292},
ABSTRACT = {Erosion in alpine grasslands is a major threat to ecosystem services of alpine soils. Natural causes for the occurrence of soil erosion are steep topography and prevailing climate conditions in combination with soil fragility. To increase our understanding of ongoing erosion processes and support sustainable land-use management, there is a need to acquire detailed information on spatial occurrence and temporal trends. Existing approaches to identify these trends are typically laborious, have lack of transferability to other regions, and are consequently only applicable to smaller regions. In order to overcome these limitations and create a sophisticated erosion monitoring tool capable of large-scale analysis, we developed a model based on U-Net, a fully convolutional neural network, to map different erosion processes on high-resolution aerial images (RGB, 0.25&ndash;0.5 m). U-Net was trained on a high-quality data set consisting of labeled erosion sites mapped with object-based image analysis (OBIA) for the Urseren Valley (Central Swiss Alps) for five aerial images (16 year period). We used the U-Net model to map the same study area and conduct quality assessments based on a held-out test region and a temporal transferability test on new images. Erosion classes are assigned according to their type (shallow landslide and sites with reduced vegetation affected by sheet erosion) or land-use impacts (livestock trails and larger management affected areas). We show that results obtained by OBIA and U-Net follow similar linear trends for the 16 year study period, exhibiting increases in total degraded area of 167% and 201%, respectively. Segmentations of eroded sites are generally in good agreement, but also display method-specific differences, which lead to an overall precision of 73%, a recall of 84%, and a F1-score of 78%. Our results show that U-Net is transferable to spatially (within our study area) and temporally unseen data (data from new years) and is therefore a method suitable to efficiently and successfully capture the temporal trends and spatial heterogeneity of degradation in alpine grasslands. Additionally, U-Net is a powerful and robust tool to map erosion sites in a predictive manner utilising large amounts of new aerial imagery.},
DOI = {10.3390/rs12244149}
}



@Article{rs12244143,
AUTHOR = {Hassanijalilian, Oveis and Igathinathane, C. and Bajwa, Sreekala and Nowatzki, John},
TITLE = {Rating Iron Deficiency in Soybean Using Image Processing and Decision-Tree Based Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4143},
URL = {https://www.mdpi.com/2072-4292/12/24/4143},
ISSN = {2072-4292},
ABSTRACT = {The most efficient way of soybean (Glycine max (L.) Merrill) iron deficiency chlorosis (IDC) management is to select a tolerant cultivar suitable for the specific growing condition. These cultivars are selected by field experts based on IDC visual ratings. However, this visual rating method is laborious, expensive, time-consuming, subjective, and impractical on larger scales. Therefore, a modern digital image-based method using tree-based machine learning classifier models for rating soybean IDC at plot-scale was developed. Data were collected from soybean IDC cultivar trial plots. Images were processed with MATLAB and corrected for light intensity by using a standard color board in the image. The three machine learning models used in this study were decision tree (DT), random forest (RF), and adaptive boosting (AdaBoost). Calculated indices from images, such as dark green color index (DGCI), canopy size, and pixel counts into DGCI ranges and IDC visual scoring were used as input and target variables to train these models. Metrics such as precision, recall, and f1-score were used to assess the performance of the classifier models. Among all three models, AdaBoost had the best performance (average f1-score = 0.75) followed by RF and DT the least. Therefore, a ready-to-use methodology of image processing with AdaBoost model for soybean IDC rating was recommended. The developed method can be easily adapted to smartphone applications or scaled-up using images from aerial platforms.},
DOI = {10.3390/rs12244143}
}



@Article{ijgi9120759,
AUTHOR = {Zang, Yufu and Li, Bijun and Xiao, Xiongwu and Zhu, Jianfeng and Meng, Fancong},
TITLE = {An Efficient Probabilistic Registration Based on Shape Descriptor for Heritage Field Inspection},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {759},
URL = {https://www.mdpi.com/2220-9964/9/12/759},
ISSN = {2220-9964},
ABSTRACT = {Heritage documentation is implemented by digitally recording historical artifacts for the conservation and protection of these cultural heritage objects. As efficient spatial data acquisition tools, laser scanners have been widely used to collect highly accurate three-dimensional (3D) point clouds without damaging the original structure and the environment. To ensure the integrity and quality of the collected data, field inspection (i.e., on-spot checking the data quality) should be carried out to determine the need for additional measurements (i.e., extra laser scanning for areas with quality issues such as data missing and quality degradation). To facilitate inspection of all collected point clouds, especially checking the quality issues in overlaps between adjacent scans, all scans should be registered together. Thus, a point cloud registration method that is able to register scans fast and robustly is required. To fulfill the aim, this study proposes an efficient probabilistic registration for free-form cultural heritage objects by integrating the proposed principal direction descriptor and curve constraints. We developed a novel shape descriptor based on a local frame of principal directions. Within the frame, its density and distance feature images were generated to describe the shape of the local surface. We then embedded the descriptor into a probabilistic framework to reject ambiguous matches. Spatial curves were integrated as constraints to delimit the solution space. Finally, a multi-view registration was used to refine the position and orientation of each scan for the field inspection. Comprehensive experiments show that the proposed method was able to perform well in terms of rotation error, translation error, robustness, and runtime and outperformed some commonly used approaches.},
DOI = {10.3390/ijgi9120759}
}



