
@Article{rs11192225,
AUTHOR = {Agrafiotis, Panagiotis and Skarlatos, Dimitrios and Georgopoulos, Andreas and Karantzalos, Konstantinos},
TITLE = {DepthLearn: Learning to Correct the Refraction on Point Clouds Derived from Aerial Imagery for Accurate Dense Shallow Water Bathymetry Based on SVMs-Fusion with LiDAR Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2225},
URL = {https://www.mdpi.com/2072-4292/11/19/2225},
ISSN = {2072-4292},
ABSTRACT = {The determination of accurate bathymetric information is a key element for near offshore activities; hydrological studies, such as coastal engineering applications, sedimentary processes, hydrographic surveying, archaeological mapping and biological research. Through structure from motion (SfM) and multi-view-stereo (MVS) techniques, aerial imagery can provide a low-cost alternative compared to bathymetric LiDAR (Light Detection and Ranging) surveys, as it offers additional important visual information and higher spatial resolution. Nevertheless, water refraction poses significant challenges on depth determination. Till now, this problem has been addressed through customized image-based refraction correction algorithms or by modifying the collinearity equation. In this article, in order to overcome the water refraction errors in a massive and accurate way, we employ machine learning tools, which are able to learn the systematic underestimation of the estimated depths. In particular, an SVR (support vector regression) model was developed, based on known depth observations from bathymetric LiDAR surveys, which is able to accurately recover bathymetry from point clouds derived from SfM-MVS procedures. Experimental results and validation were based on datasets derived from different test-sites, and demonstrated the high potential of our approach. Moreover, we exploited the fusion of LiDAR and image-based point clouds towards addressing challenges of both modalities in problematic areas.},
DOI = {10.3390/rs11192225}
}



@Article{agronomy9100581,
AUTHOR = {Pádua, Luís and Marques, Pedro and Adão, Telmo and Guimarães, Nathalie and Sousa, António and Peres, Emanuel and Sousa, Joaquim João},
TITLE = {Vineyard Variability Analysis through UAV-Based Vigour Maps to Assess Climate Change Impacts},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {581},
URL = {https://www.mdpi.com/2073-4395/9/10/581},
ISSN = {2073-4395},
ABSTRACT = {Climate change is projected to be a key influence on crop yields across the globe. Regarding viticulture, primary climate vectors with a significant impact include temperature, moisture stress, and radiation. Within this context, it is of foremost importance to monitor soils&rsquo; moisture levels, as well as to detect pests, diseases, and possible problems with irrigation equipment. Regular monitoring activities will enable timely measures that may trigger field interventions that are used to preserve grapevines&rsquo; phytosanitary state, saving both time and money, while assuring a more sustainable activity. This study employs unmanned aerial vehicles (UAVs) to acquire aerial imagery, using RGB, multispectral and thermal infrared sensors in a vineyard located in the Portuguese Douro wine region. Data acquired enabled the multi-temporal characterization of the vineyard development throughout a season through the computation of the normalized difference vegetation index, crop surface models, and the crop water stress index. Moreover, vigour maps were computed in three classes (high, medium, and low) with different approaches: (1) considering the whole vineyard, including inter-row vegetation and bare soil; (2) considering only automatically detected grapevine vegetation; and (3) also considering grapevine vegetation by only applying a normalization process before creating the vigour maps. Results showed that vigour maps considering only grapevine vegetation provided an accurate representation of the vineyard variability. Furthermore, significant spatial associations can be gathered through (i) a multi-temporal analysis of vigour maps, and (ii) by comparing vigour maps with both height and water stress estimation. This type of analysis can assist, in a significant way, the decision-making processes in viticulture.},
DOI = {10.3390/agronomy9100581}
}



@Article{rs11192243,
AUTHOR = {Liu, Weiquan and Wang, Cheng and Bian, Xuesheng and Chen, Shuting and Li, Wei and Lin, Xiuhong and Li, Yongchuan and Weng, Dongdong and Lai, Shang-Hong and Li, Jonathan},
TITLE = {AE-GAN-Net: Learning Invariant Feature Descriptor to Match Ground Camera Images and a Large-Scale 3D Image-Based Point Cloud for Outdoor Augmented Reality},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2243},
URL = {https://www.mdpi.com/2072-4292/11/19/2243},
ISSN = {2072-4292},
ABSTRACT = {Establishing the spatial relationship between 2D images captured by real cameras and 3D models of the environment (2D and 3D space) is one way to achieve the virtual&ndash;real registration for Augmented Reality (AR) in outdoor environments. In this paper, we propose to match the 2D images captured by real cameras and the rendered images from the 3D image-based point cloud to indirectly establish the spatial relationship between 2D and 3D space. We call these two kinds of images as cross-domain images, because their imaging mechanisms and nature are quite different. However, unlike real camera images, the rendered images from the 3D image-based point cloud are inevitably contaminated with image distortion, blurred resolution, and obstructions, which makes image matching with the handcrafted descriptors or existing feature learning neural networks very challenging. Thus, we first propose a novel end-to-end network, AE-GAN-Net, consisting of two AutoEncoders (AEs) with Generative Adversarial Network (GAN) embedding, to learn invariant feature descriptors for cross-domain image matching. Second, a domain-consistent loss function, which balances image content and consistency of feature descriptors for cross-domain image pairs, is introduced to optimize AE-GAN-Net. AE-GAN-Net effectively captures domain-specific information, which is embedded into the learned feature descriptors, thus making the learned feature descriptors robust against image distortion, variations in viewpoints, spatial resolutions, rotation, and scaling. Experimental results show that AE-GAN-Net achieves state-of-the-art performance for image patch retrieval with the cross-domain image patch dataset, which is built from real camera images and the rendered images from 3D image-based point cloud. Finally, by evaluating virtual&ndash;real registration for AR on a campus by using the cross-domain image matching results, we demonstrate the feasibility of applying the proposed virtual&ndash;real registration to AR in outdoor environments.},
DOI = {10.3390/rs11192243}
}



@Article{rs11192252,
AUTHOR = {Castaño, Fernando and Strzelczak, Stanisław and Villalonga, Alberto and Haber, Rodolfo E. and Kossakowska, Joanna},
TITLE = {Sensor Reliability in Cyber-Physical Systems Using Internet-of-Things Data: A Review and Case Study},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2252},
URL = {https://www.mdpi.com/2072-4292/11/19/2252},
ISSN = {2072-4292},
ABSTRACT = {Nowadays, reliability of sensors is one of the most important challenges for widespread application of Internet-of-things data in key emerging fields such as the automotive and manufacturing sectors. This paper presents a brief review of the main research and innovation actions at the European level, as well as some on-going research related to sensor reliability in cyber-physical systems (CPS). The research reported in this paper is also focused on the design of a procedure for evaluating the reliability of Internet-of-Things sensors in a cyber-physical system. The results of a case study of sensor reliability assessment in an autonomous driving scenario for the automotive sector are also shown. A co-simulation framework is designed in order to enable real-time interaction between virtual and real sensors. The case study consists of an IoT LiDAR-based collaborative map in order to assess the CPS-based co-simulation framework. Specifically, the sensor chosen is the Ibeo Lux 4-layer LiDAR sensor with IoT added capabilities. The modeling library for predicting error with machine learning methods is implemented at a local level, and a self-learning-procedure for decision-making based on Q-learning runs at a global level. The study supporting the experimental evaluation of the co-simulation framework is presented using simulated and real data. The results demonstrate the effectiveness of the proposed method for increasing sensor reliability in cyber-physical systems using Internet-of-Things data.},
DOI = {10.3390/rs11192252}
}



@Article{rs11192260,
AUTHOR = {Halls, Joanne Nancie and Magolan, Jessica Lynn},
TITLE = {A Methodology to Assess Land Use Development, Flooding, and Wetland Change as Indicators of Coastal Vulnerability},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2260},
URL = {https://www.mdpi.com/2072-4292/11/19/2260},
ISSN = {2072-4292},
ABSTRACT = {Coastal areas around the world are becoming increasingly urban, which has increased stress to both natural and anthropogenic systems. In the United States, 52% of the population lives along the coast, and North Carolina is in the top 10 fastest growing states. Within North Carolina, the southeastern coast is the fastest growing region in the state. Therefore, this research has developed a methodology that investigates the complex relationship between urbanization, land cover change, and potential flood risk and tested the approach in a rapidly urbanizing region. A variety of data, including satellite (PlanetScope) and airborne imagery (NAIP and Lidar) and vector data (C-CAP, FEMA floodplains, and building permits), were used to assess changes through space and time. The techniques consisted of (1) matrix change analysis, (2) a new approach to analyzing shorelines by computing adjacency statistics for changes in wetland and urban development, and (3) calculating risk using a fishnet, or tessellation, where hexagons of equal size (15 ha) were ranked into high, medium, and low risk and comparing these results with the amount of urbanization. As other research has shown, there was a significant relationship between residential development and wetland loss. Where urban development has yet to occur, most of the remaining area is at risk to flooding. Importantly, the combined methods used in this study have identified at-risk areas and places where wetlands have migrated/transgressed in relationship to urban development. The combination of techniques developed here has resulted in data that local government planners are using to evaluate current development regulations and incorporating into the new long-range plan for the County that will include smart growth and identification of risk. Additionally, results from this study area are being utilized in an application to the Federal Emergency Management Agency&rsquo;s Community Response System which will provide residents with lower flood insurance costs.},
DOI = {10.3390/rs11192260}
}



@Article{rs11192276,
AUTHOR = {Lee, Jae-Hun and Sull, Sanghoon},
TITLE = {Regression Tree CNN for Estimation of Ground Sampling Distance Based on Floating-Point Representation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2276},
URL = {https://www.mdpi.com/2072-4292/11/19/2276},
ISSN = {2072-4292},
ABSTRACT = {The estimation of ground sampling distance (GSD) from a remote sensing image enables measurement of the size of an object as well as more accurate segmentation in the image. In this paper, we propose a regression tree convolutional neural network (CNN) for estimating the value of GSD from an input image. The proposed regression tree CNN consists of a feature extraction CNN and a binomial tree layer. The proposed network first extracts features from an input image. Based on the extracted features, it predicts the GSD value that is represented by the floating-point number with the exponent and its mantissa. They are computed by coarse scale classification and finer scale regression, respectively, resulting in improved results. Experimental results with a Google Earth aerial image dataset and a mixed dataset consisting of eight remote sensing image public datasets with different GSDs show that the proposed network reduces the GSD prediction error rate by 25% compared to a baseline network that directly estimates the GSD.},
DOI = {10.3390/rs11192276}
}



@Article{s19194244,
AUTHOR = {Liu, Xiaomin and Li, Jun-Bao and Pan, Jeng-Shyang},
TITLE = {Feature Point Matching Based on Distinct Wavelength Phase Congruency and Log-Gabor Filters in Infrared and Visible Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4244},
URL = {https://www.mdpi.com/1424-8220/19/19/4244},
ISSN = {1424-8220},
ABSTRACT = {Infrared and visible image matching methods have been rising in popularity with the emergence of more kinds of sensors, which provide more applications in visual navigation, precision guidance, image fusion, and medical image analysis. In such applications, image matching is utilized for location, fusion, image analysis, and so on. In this paper, an infrared and visible image matching approach, based on distinct wavelength phase congruency (DWPC) and log-Gabor filters, is proposed. Furthermore, this method is modified for non-linear image matching with different physical wavelengths. Phase congruency (PC) theory is utilized to obtain PC images with intrinsic and affluent image features for images containing complex intensity changes or noise. Then, the maximum and minimum moments of the PC images are computed to obtain the corners in the matched images. In order to obtain the descriptors, log-Gabor filters are utilized and overlapping subregions are extracted in a neighborhood of certain pixels. In order to improve the accuracy of the algorithm, the moments of PCs in the original image and a Gaussian smoothed image are combined to detect the corners. Meanwhile, it is improper that the two matched images have the same PC wavelengths, due to the images having different physical wavelengths. Thus, in the experiment, the wavelength of the PC is changed for different physical wavelengths. For realistic application, BiDimRegression method is proposed to compute the similarity between two points set in infrared and visible images. The proposed approach is evaluated on four data sets with 237 pairs of visible and infrared images, and its performance is compared with state-of-the-art approaches: the edge-oriented histogram descriptor (EHD), phase congruency edge-oriented histogram descriptor (PCEHD), and log-Gabor histogram descriptor (LGHD) algorithms. The experimental results indicate that the accuracy rate of the proposed approach is 50% higher than the traditional approaches in infrared and visible images.},
DOI = {10.3390/s19194244}
}



@Article{s19194252,
AUTHOR = {Pan, Zhichen and Chen, Haoyao and Li, Silin and Liu, Yunhui},
TITLE = {ClusterMap Building and Relocalization in Urban Environments for Unmanned Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4252},
URL = {https://www.mdpi.com/1424-8220/19/19/4252},
ISSN = {1424-8220},
ABSTRACT = {Map building and map-based relocalization techniques are important for unmanned vehicles operating in urban environments. The existing approaches require expensive high-density laser range finders and suffer from relocalization problems in long-term applications. This study proposes a novel map format called the ClusterMap, on the basis of which an approach to achieving relocalization is developed. The ClusterMap is generated by segmenting the perceived point clouds into different point clusters and filtering out clusters belonging to dynamic objects. A location descriptor associated with each cluster is designed for differentiation. The relocalization in the global map is achieved by matching cluster descriptors between local and global maps. The solution does not require high-density point clouds and high-precision segmentation algorithms. In addition, it prevents the effects of environmental changes on illumination intensity, object appearance, and observation direction. A consistent ClusterMap without any scale problem is built by utilizing a 3D visual&ndash;LIDAR simultaneous localization and mapping solution by fusing LIDAR and visual information. Experiments on the KITTI dataset and our mobile vehicle illustrates the effectiveness of the proposed approach.},
DOI = {10.3390/s19194252}
}



@Article{rs11192326,
AUTHOR = {Fricker, Geoffrey A. and Ventura, Jonathan D. and Wolf, Jeffrey A. and North, Malcolm P. and Davis, Frank W. and Franklin, Janet},
TITLE = {A Convolutional Neural Network Classifier Identifies Tree Species in Mixed-Conifer Forest from Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2326},
URL = {https://www.mdpi.com/2072-4292/11/19/2326},
ISSN = {2072-4292},
ABSTRACT = {In this study, we automate tree species classification and mapping using field-based training data, high spatial resolution airborne hyperspectral imagery, and a convolutional neural network classifier (CNN). We tested our methods by identifying seven dominant trees species as well as dead standing trees in a mixed-conifer forest in the Southern Sierra Nevada Mountains, CA (USA) using training, validation, and testing datasets composed of spatially-explicit transects and plots sampled across a single strip of imaging spectroscopy. We also used a three-band &lsquo;Red-Green-Blue&rsquo; pseudo true-color subset of the hyperspectral imagery strip to test the classification accuracy of a CNN model without the additional non-visible spectral data provided in the hyperspectral imagery. Our classifier is pixel-based rather than object based, although we use three-dimensional structural information from airborne Light Detection and Ranging (LiDAR) to identify trees (points &gt; 5 m above the ground) and the classifier was applied to image pixels that were thus identified as tree crowns. By training a CNN classifier using field data and hyperspectral imagery, we were able to accurately identify tree species and predict their distribution, as well as the distribution of tree mortality, across the landscape. Using a window size of 15 pixels and eight hidden convolutional layers, a CNN model classified the correct species of 713 individual trees from hyperspectral imagery with an average F-score of 0.87 and F-scores ranging from 0.67&ndash;0.95 depending on species. The CNN classification model performance increased from a combined F-score of 0.64 for the Red-Green-Blue model to a combined F-score of 0.87 for the hyperspectral model. The hyperspectral CNN model captures the species composition changes across ~700 meters (1935 to 2630 m) of elevation from a lower-elevation mixed oak conifer forest to a higher-elevation fir-dominated coniferous forest. High resolution tree species maps can support forest ecosystem monitoring and management, and identifying dead trees aids landscape assessment of forest mortality resulting from drought, insects and pathogens. We publicly provide our code to apply deep learning classifiers to tree species identification from geospatial imagery and field training data.},
DOI = {10.3390/rs11192326}
}



@Article{s19194332,
AUTHOR = {Opromolla, Roberto and Inchingolo, Giuseppe and Fasano, Giancarmine},
TITLE = {Airborne Visual Detection and Tracking of Cooperative UAVs Exploiting Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4332},
URL = {https://www.mdpi.com/1424-8220/19/19/4332},
ISSN = {1424-8220},
ABSTRACT = {The performance achievable by using Unmanned Aerial Vehicles (UAVs) for a large variety of civil and military applications, as well as the extent of applicable mission scenarios, can significantly benefit from the exploitation of formations of vehicles able to fly in a coordinated manner (swarms). In this respect, visual cameras represent a key instrument to enable coordination by giving each UAV the capability to visually monitor the other members of the formation. Hence, a related technological challenge is the development of robust solutions to detect and track cooperative targets through a sequence of frames. In this framework, this paper proposes an innovative approach to carry out this task based on deep learning. Specifically, the You Only Look Once (YOLO) object detection system is integrated within an original processing architecture in which the machine-vision algorithms are aided by navigation hints available thanks to the cooperative nature of the formation. An experimental flight test campaign, involving formations of two multirotor UAVs, is conducted to collect a database of images suitable to assess the performance of the proposed approach. Results demonstrate high-level accuracy, and robustness against challenging conditions in terms of illumination, background and target-range variability.},
DOI = {10.3390/s19194332}
}



@Article{s19194337,
AUTHOR = {Lyu, Pin and Wang, Bingqing and Lai, Jizhou and Liu, Shichao and Li, Zhimin},
TITLE = {A Drag Model-LIDAR-IMU Fault-Tolerance Fusion Method for Quadrotors},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4337},
URL = {https://www.mdpi.com/1424-8220/19/19/4337},
ISSN = {1424-8220},
ABSTRACT = {In this paper, a drag model-aided fault-tolerant state estimation method is presented for quadrotors. Firstly, the drag model accuracy was improved by modeling an angular rate related item and an angular acceleration related item, which are related with flight maneuver. Then the drag model, light detection and ranging (LIDAR), and inertial measurement unit (IMU) were fused based on the Federal Kalman filter frame. In the filter, the LIDAR estimation fault was detected and isolated, and the disturbance to the drag model was estimated and compensated. Some experiments were carried out, showing that the velocity and position estimation were improved compared with the traditional LIDAR/IMU fusion scheme.},
DOI = {10.3390/s19194337}
}



@Article{agronomy9100618,
AUTHOR = {Hassler, Samuel C. and Baysal-Gurel, Fulya},
TITLE = {Unmanned Aircraft System (UAS) Technology and Applications in Agriculture},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {618},
URL = {https://www.mdpi.com/2073-4395/9/10/618},
ISSN = {2073-4395},
ABSTRACT = {Numerous sensors have been developed over time for precision agriculture; though, only recently have these sensors been incorporated into the new realm of unmanned aircraft systems (UAS). This UAS technology has allowed for a more integrated and optimized approach to various farming tasks such as field mapping, plant stress detection, biomass estimation, weed management, inventory counting, and chemical spraying, among others. These systems can be highly specialized depending on the particular goals of the researcher or farmer, yet many aspects of UAS are similar. All systems require an underlying platform&mdash;or unmanned aerial vehicle (UAV)&mdash;and one or more peripherals and sensing equipment such as imaging devices (RGB, multispectral, hyperspectral, near infra-red, RGB depth), gripping tools, or spraying equipment. Along with these wide-ranging peripherals and sensing equipment comes a great deal of data processing. Common tools to aid in this processing include vegetation indices, point clouds, machine learning models, and statistical methods. With any emerging technology, there are also a few considerations that need to be analyzed like legal constraints, economic trade-offs, and ease of use. This review then concludes with a discussion on the pros and cons of this technology, along with a brief outlook into future areas of research regarding UAS technology in agriculture.},
DOI = {10.3390/agronomy9100618}
}



@Article{s19204363,
AUTHOR = {Sun, Jie and Di, Liping and Sun, Ziheng and Shen, Yonglin and Lai, Zulong},
TITLE = {County-Level Soybean Yield Prediction Using Deep CNN-LSTM Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4363},
URL = {https://www.mdpi.com/1424-8220/19/20/4363},
ISSN = {1424-8220},
ABSTRACT = {Yield prediction is of great significance for yield mapping, crop market planning, crop insurance, and harvest management. Remote sensing is becoming increasingly important in crop yield prediction. Based on remote sensing data, great progress has been made in this field by using machine learning, especially the Deep Learning (DL) method, including Convolutional Neural Network (CNN) or Long Short-Term Memory (LSTM). Recent experiments in this area suggested that CNN can explore more spatial features and LSTM has the ability to reveal phenological characteristics, which both play an important role in crop yield prediction. However, very few experiments combining these two models for crop yield prediction have been reported. In this paper, we propose a deep CNN-LSTM model for both end-of-season and in-season soybean yield prediction in CONUS at the county-level. The model was trained by crop growth variables and environment variables, which include weather data, MODIS Land Surface Temperature (LST) data, and MODIS Surface Reflectance (SR) data; historical soybean yield data were employed as labels. Based on the Google Earth Engine (GEE), all these training data were combined and transformed into histogram-based tensors for deep learning. The results of the experiment indicate that the prediction performance of the proposed CNN-LSTM model can outperform the pure CNN or LSTM model in both end-of-season and in-season. The proposed method shows great potential in improving the accuracy of yield prediction for other crops like corn, wheat, and potatoes at fine scales in the future.},
DOI = {10.3390/s19204363}
}



@Article{rs11202346,
AUTHOR = {Muñoz, David F. and Cissell, Jordan R. and Moftakhari, Hamed},
TITLE = {Adjusting Emergent Herbaceous Wetland Elevation with Object-Based Image Analysis, Random Forest and the 2016 NLCD},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2346},
URL = {https://www.mdpi.com/2072-4292/11/20/2346},
ISSN = {2072-4292},
ABSTRACT = {Emergent herbaceous wetlands are characterized by complex salt marsh ecosystems that play a key role in diverse coastal processes including carbon storage, nutrient cycling, flood attenuation and shoreline protection. Surface elevation characterization and spatiotemporal distribution of these ecosystems are commonly obtained from LiDAR measurements as this low-cost airborne technique has a wide range of applicability and usefulness in coastal environments. LiDAR techniques, despite significant advantages, show poor performance in generation of digital elevation models (DEMs) in tidal salt marshes due to large vertical errors. In this study, we present a methodology to (i) update emergent herbaceous wetlands (i.e., the ones delineated in the 2016 National Land Cover Database) to present-day conditions; and (ii) automate salt marsh elevation correction in estuarine systems. We integrate object-based image analysis and random forest technique with surface reflectance Landsat imagery to map three emergent U.S. wetlands in Weeks Bay, Alabama, Savannah Estuary, Georgia and Fire Island, New York. Conducting a hyperparameter tuning of random forest and following a hierarchical approach with three nomenclature levels for land cover classification, we are able to better map wetlands and improve overall accuracies in Weeks Bay (0.91), Savannah Estuary (0.97) and Fire Island (0.95). We then develop a tool in ArcGIS to automate salt marsh elevation correction. We use this ‘DEM-correction’ tool to modify an existing DEM (model input) with the calculated elevation correction over salt marsh regions. Our method and tool are validated with real-time kinematic elevation data and helps correct overestimated salt marsh elevation up to 0.50 m in the studied estuaries. The proposed tool can be easily adapted to different vegetation species in wetlands, and thus help provide accurate DEMs for flood inundation mapping in estuarine systems.},
DOI = {10.3390/rs11202346}
}



@Article{s19204378,
AUTHOR = {Franchetti, Benjamin and Ntouskos, Valsamis and Giuliani, Pierluigi and Herman, Tiara and Barnes, Luke and Pirri, Fiora},
TITLE = {Vision Based Modeling of Plants Phenotyping in Vertical Farming under Artificial Lighting},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4378},
URL = {https://www.mdpi.com/1424-8220/19/20/4378},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we present a novel method for vision based plants phenotyping in indoor vertical farming under artificial lighting. The method combines 3D plants modeling and deep segmentation of the higher leaves, during a period of 25&ndash;30 days, related to their growth. The novelty of our approach is in providing 3D reconstruction, leaf segmentation, geometric surface modeling, and deep network estimation for weight prediction to effectively measure plant growth, under three relevant phenotype features: height, weight and leaf area. Together with the vision based measurements, to verify the soundness of our proposed method, we also harvested the plants at specific time periods to take manual measurements, collecting a great amount of data. In particular, we manually collected 2592 data points related to the plant phenotype and 1728 images of the plants. This allowed us to show with a good number of experiments that the vision based methods ensure a quite accurate prediction of the considered features, providing a way to predict plant behavior, under specific conditions, without any need to resort to human measurements.},
DOI = {10.3390/s19204378}
}



@Article{rs11202351,
AUTHOR = {Aimaiti, Yusupujiang and Liu, Wen and Yamazaki, Fumio and Maruyama, Yoshihisa},
TITLE = {Earthquake-Induced Landslide Mapping for the 2018 Hokkaido Eastern Iburi Earthquake Using PALSAR-2 Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2351},
URL = {https://www.mdpi.com/2072-4292/11/20/2351},
ISSN = {2072-4292},
ABSTRACT = {Timely information about landslides during or immediately after an event is an invaluable source for emergency response and management. Using an active sensor, synthetic aperture radar (SAR) can capture images of the earth&rsquo;s surface regardless of weather conditions and may provide a solution to the problem of mapping landslides when clouds obstruct optical imaging. The 2018 Hokkaido Eastern Iburi earthquake (Mw 6.6) and its aftershocks not only caused major damage with severe loss of life and property but also induced many landslides across the area. To gain a better understanding of the landslides induced by this earthquake, we proposed a method of landslide mapping using pre- and post-event Advanced Land Observation Satellite 2 Phased Array L-band Synthetic Aperture Radar 2 (ALOS-2 PALSAR-2) images acquired from both descending and ascending orbits. Moreover, the accuracy of the classification results was verified by comparisons with high-resolution optical images, and ground truth data (provided by GSI, Japan). The detected landslides show a good match with the reference optical images by visual comparison. The quantitative comparison results showed that a combination of the descending and ascending intensity-based landslide classification had the best accuracy with an overall accuracy and kappa coefficient of 80.1% and 0.45, respectively.},
DOI = {10.3390/rs11202351}
}



@Article{rs11202356,
AUTHOR = {Lausch, Angela and Baade, Jussi and Bannehr, Lutz and Borg, Erik and Bumberger, Jan and Chabrilliat, Sabine and Dietrich, Peter and Gerighausen, Heike and Glässer, Cornelia and Hacker, Jorg M. and Haase, Dagmar and Jagdhuber, Thomas and Jany, Sven and Jung, András and Karnieli, Arnon and Kraemer, Roland and Makki, Mohsen and Mielke, Christian and Möller, Markus and Mollenhauer, Hannes and Montzka, Carsten and Pause, Marion and Rogass, Christian and Rozenstein, Offer and Schmullius, Christiane and Schrodt, Franziska and Schrön, Martin and Schulz, Karsten and Schütze, Claudia and Schweitzer, Christian and Selsam, Peter and Skidmore, Andrew K. and Spengler, Daniel and Thiel, Christian and Truckenbrodt, Sina C. and Vohland, Michael and Wagner, Robert and Weber, Ute and Werban, Ulrike and Wollschläger, Ute and Zacharias, Steffen and Schaepman, Michael E.},
TITLE = {Linking Remote Sensing and Geodiversity and Their Traits Relevant to Biodiversity—Part I: Soil Characteristics},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2356},
URL = {https://www.mdpi.com/2072-4292/11/20/2356},
ISSN = {2072-4292},
ABSTRACT = {In the face of rapid global change it is imperative to preserve geodiversity for the overall conservation of biodiversity. Geodiversity is important for understanding complex biogeochemical and physical processes and is directly and indirectly linked to biodiversity on all scales of ecosystem organization. Despite the great importance of geodiversity, there is a lack of suitable monitoring methods. Compared to conventional in-situ techniques, remote sensing (RS) techniques provide a pathway towards cost-effective, increasingly more available, comprehensive, and repeatable, as well as standardized monitoring of continuous geodiversity on the local to global scale. This paper gives an overview of the state-of-the-art approaches for monitoring soil characteristics and soil moisture with unmanned aerial vehicles (UAV) and air- and spaceborne remote sensing techniques. Initially, the definitions for geodiversity along with its five essential characteristics are provided, with an explanation for the latter. Then, the approaches of spectral traits (ST) and spectral trait variations (STV) to record geodiversity using RS are defined. LiDAR (light detection and ranging), thermal and microwave sensors, multispectral, and hyperspectral RS technologies to monitor soil characteristics and soil moisture are also presented. Furthermore, the paper discusses current and future satellite-borne sensors and missions as well as existing data products. Due to the prospects and limitations of the characteristics of different RS sensors, only specific geotraits and geodiversity characteristics can be recorded. The paper provides an overview of those geotraits.},
DOI = {10.3390/rs11202356}
}



@Article{electronics8101152,
AUTHOR = {Barral, Valentín and Suárez-Casal, Pedro and Escudero, Carlos J. and García-Naya, José A.},
TITLE = {Multi-Sensor Accurate Forklift Location and Tracking Simulation in Industrial Indoor Environments},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1152},
URL = {https://www.mdpi.com/2079-9292/8/10/1152},
ISSN = {2079-9292},
ABSTRACT = {Location and tracking needs are becoming more prominent in industrial environments nowadays. Process optimization, traceability or safety are some of the topics where a positioning system can operate to improve and increase the productivity of a factory or warehouse. Among the different options, solutions based on ultra-wideband (UWB) have emerged during recent years as a good choice to obtain highly accurate estimations in indoor scenarios. However, the typical harsh wireless channel conditions found inside industrial environments, together with interferences caused by workers and machinery, constitute a challenge for this kind of system. This paper describes a real industrial problem (location and tracking of forklift trucks) that requires precise internal positioning and presents a study on the feasibility of meeting this challenge using UWB technology. To this end, a simulator of this technology was created based on UWB measurements from a set of real sensors. This simulator was used together with a location algorithm and a physical model of the forklift to obtain estimations of position in different scenarios with different obstacles. Together with the simulated UWB sensor, an additional inertial sensor and optical sensor were modeled in order to test its effect on supporting the location based on UWB. All the software created for this work is published under an open-source license and is publicly available.},
DOI = {10.3390/electronics8101152}
}



@Article{info10100312,
AUTHOR = {Alghamdi, Ibrahim and Anagnostopoulos, Christos and P. Pezaros, Dimitrios},
TITLE = {Delay-Tolerant Sequential Decision Making for Task Offloading in Mobile Edge Computing Environments},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {312},
URL = {https://www.mdpi.com/2078-2489/10/10/312},
ISSN = {2078-2489},
ABSTRACT = {In recent years, there has been a significant increase in the use of mobile devices and their applications. Meanwhile, cloud computing has been considered as the latest generation of computing infrastructure. There has also been a transformation in cloud computing ideas and their implementation so as to meet the demand for the latest applications. mobile edge computing (MEC) is a computing paradigm that provides cloud services near to the users at the edge of the network. Given the movement of mobile nodes between different MEC servers, the main aim would be the connection to the best server and at the right time in terms of the load of the server in order to optimize the quality of service (QoS) of the mobile nodes. We tackle the offloading decision making problem by adopting the principles of optimal stopping theory (OST) to minimize the execution delay in a sequential decision manner. A performance evaluation is provided using real world data sets with baseline deterministic and stochastic offloading models. The results show that our approach significantly minimizes the execution delay for task execution and the results are closer to the optimal solution than other offloading methods.},
DOI = {10.3390/info10100312}
}



@Article{s19204416,
AUTHOR = {Yang, Baohua and Wang, Mengxuan and Sha, Zhengxia and Wang, Bing and Chen, Jianlin and Yao, Xia and Cheng, Tao and Cao, Weixing and Zhu, Yan},
TITLE = {Evaluation of Aboveground Nitrogen Content of Winter Wheat Using Digital Imagery of Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4416},
URL = {https://www.mdpi.com/1424-8220/19/20/4416},
ISSN = {1424-8220},
ABSTRACT = {Nitrogen (N) content is an important basis for the precise management of wheat fields. The application of unmanned aerial vehicles (UAVs) in agriculture provides an easier and faster way to monitor nitrogen content. Previous studies have shown that the features acquired from UAVs yield favorable results in monitoring wheat growth. However, since most of them are based on different vegetation indices, it is difficult to meet the requirements of accurate image interpretation. Moreover, resampling also easily ignores the structural features of the image information itself. Therefore, a spectral-spatial feature is proposed combining vegetation indices (VIs) and wavelet features (WFs), especially the acquisition of wavelet features from the UAV image, which was transformed from the spatial domain to the frequency domain with a wavelet transformation. In this way, the complete spatial information of different scales can be obtained to realize good frequency localization, scale transformation, and directional change. The different models based on different features were compared, including partial least squares regression (PLSR), support vector regression (SVR), and particle swarm optimization-SVR (PSO-SVR). The results showed that the accuracy of the model based on the spectral-spatial feature by combining VIs and WFs was higher than that of VIs or WF indices alone. The performance of PSO-SVR was the best (R2 = 0.9025, root mean square error (RMSE) = 0.3287) among the three regression algorithms regardless of the use of all the original features or the combination features. Our results implied that our proposed method could improve the estimation accuracy of aboveground nitrogen content of winter wheat from UAVs with consumer digital cameras, which have greater application potential in predicting other growth parameters.},
DOI = {10.3390/s19204416}
}



@Article{rs11202375,
AUTHOR = {Zhang, Dongyan and Wang, Daoyong and Gu, Chunyan and Jin, Ning and Zhao, Haitao and Chen, Gao and Liang, Hongyi and Liang, Dong},
TITLE = {Using Neural Network to Identify the Severity of Wheat Fusarium Head Blight in the Field Environment},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2375},
URL = {https://www.mdpi.com/2072-4292/11/20/2375},
ISSN = {2072-4292},
ABSTRACT = {Fusarium head blight (FHB), one of the most important diseases of wheat, mainly occurs in the ear. Given that the severity of the disease cannot be accurately identified, the cost of pesticide application increases every year, and the agricultural ecological environment is also polluted. In this study, a neural network (NN) method was proposed based on the red-green-blue (RGB) image to segment wheat ear and disease spot in the field environment, and then to determine the disease grade. Firstly, a segmentation dataset of single wheat ear was constructed to provide a benchmark for the segmentation of the wheat ear. Secondly, a segmentation model of single wheat ear based on the fully convolutional network (FCN) was established to effectively realize the segmentation of the wheat ear in the field environment. An FHB segmentation algorithm was proposed based on a pulse-coupled neural network (PCNN) with K-means clustering of the improved artificial bee colony (IABC) to segment the diseased spot of wheat ear by automatic optimization of PCNN parameters. Finally, the disease grade was calculated using the ratio of the disease spot to the whole wheat ear. The experimental results show that: (1) the accuracy of the segmentation model for single wheat ear constructed in this study is 0.981. The segmentation time is less than 1 s, indicating that the model can quickly and accurately segment wheat ear in the field environment; (2) the segmentation method of the disease spot performed under each evaluation indicator is improved compared with the traditional segmentation methods, and the accuracy is 0.925 in the disease severity identification. These research results can provide important reference value for grading wheat FHB in the field environment, which also can be beneficial for real-time monitoring of other crops&rsquo; diseases under near-Earth remote sensing.},
DOI = {10.3390/rs11202375}
}



@Article{s19204440,
AUTHOR = {Sun, Hanming and Duo, Bin and Wang, Zhengqiang and Lin, Xiaochen and Gao, Changchun},
TITLE = {Aerial Cooperative Jamming for Cellular-Enabled UAV Secure Communication Network: Joint Trajectory and Power Control Design},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4440},
URL = {https://www.mdpi.com/1424-8220/19/20/4440},
ISSN = {1424-8220},
ABSTRACT = {To improve the secrecy performance of cellular-enabled unmanned aerial vehicle (UAV) communication networks, this paper proposes an aerial cooperative jamming scheme and studies its optimal design to achieve the maximum average secrecy rate. Specifically, a base station (BS) transmits confidential messages to a UAV and meanwhile another UAV performs the role of an aerial jammer by cooperatively sending jamming signals to oppose multiple suspicious eavesdroppers on the ground. As the UAVs have the advantage of the controllable mobility, the objective is to maximize the worst-case average secrecy rate by the joint optimization of the two UAVs&rsquo; trajectories and the BS&rsquo;s/UAV jammer&rsquo;s transmit/jamming power over a given mission period. The objective function of the formulated problem is highly non-linear regarding the optimization variables and the problem has non-convex constraints, which is, in general, difficult to achieve a globally optimal solution. Thus, we divide the original problem into four subproblems and then solve them by applying the successive convex approximation (SCA) and block coordinate descent (BCD) methods. Numerical results demonstrate that the significantly better secrecy performance can be obtained by using the proposed algorithm in comparison with benchmark schemes.},
DOI = {10.3390/s19204440}
}



@Article{app9204312,
AUTHOR = {Xia, Lang and Zhang, Ruirui and Chen, Liping and Huang, Yanbo and Xu, Gang and Wen, Yao and Yi, Tongchuan},
TITLE = {Monitor Cotton Budding Using SVM and UAV Images},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4312},
URL = {https://www.mdpi.com/2076-3417/9/20/4312},
ISSN = {2076-3417},
ABSTRACT = {Monitoring the cotton budding rate is important for growers so that they can replant cotton in a timely fashion at locations at which cotton density is sparse. In this study, a true-color camera was mounted on an unmanned aerial vehicle (UAV) and used to collect images of young cotton plants to estimate the germination of cotton plants. The collected images were preprocessed by stitching them together to obtain the single orthomosaic image. The support-vector machine method and maximum likelihood classification method were conducted to identify the cotton plants in the image. The accuracy evaluation indicated the overall accuracy of the classification for SVM is 96.65% with the Kappa coefficient of 93.99%, while for maximum likelihood classification, the accuracy is 87.85% with a Kappa coefficient of 80.67%. A method based on the morphological characteristics of cotton plants was proposed to identify and count the overlapping cotton plants in this study. The analysis showed that the method can improve the detection accuracy by 6.3% when compared to without it. The validation based on visual interpretation indicated that the method presented an accuracy of 91.13%. The study showed that the minimal resolution of no less than 1.2 cm/pixel in practice for image collection is necessary in order to recognize cotton plants accurately.},
DOI = {10.3390/app9204312}
}



@Article{s19204468,
AUTHOR = {Lee, Joon Yeop and Chung, Albert Y. and Shim, Hooyeop and Joe, Changhwan and Park, Seongjoon and Kim, Hwangnam},
TITLE = {UAV Flight and Landing Guidance System for Emergency Situations †},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4468},
URL = {https://www.mdpi.com/1424-8220/19/20/4468},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) with high mobility can perform various roles such as delivering goods, collecting information, recording videos and more. However, there are many elements in the city that disturb the flight of the UAVs, such as various obstacles and urban canyons which can cause a multi-path effect of GPS signals, which degrades the accuracy of GPS-based localization. In order to empower the safety of the UAVs flying in urban areas, UAVs should be guided to a safe area even in a GPS-denied or network-disconnected environment. Also, UAVs must be able to avoid obstacles while landing in an urban area. For this purpose, we present the UAV detour system for operating UAV in an urban area. The UAV detour system includes a highly reliable laser guidance system to guide the UAVs to a point where they can land, and optical flow magnitude map to avoid obstacles for a safe landing.},
DOI = {10.3390/s19204468}
}



@Article{s19204484,
AUTHOR = {García Rubio, Víctor and Rodrigo Ferrán, Juan Antonio and Menéndez García, Jose Manuel and Sánchez Almodóvar, Nuria and Lalueza Mayordomo, José María and Álvarez, Federico},
TITLE = {Automatic Change Detection System over Unmanned Aerial Vehicle Video Sequences Based on Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4484},
URL = {https://www.mdpi.com/1424-8220/19/20/4484},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the use of unmanned aerial vehicles (UAVs) for surveillance tasks has increased considerably. This technology provides a versatile and innovative approach to the field. However, the automation of tasks such as object recognition or change detection usually requires image processing techniques. In this paper we present a system for change detection in video sequences acquired by moving cameras. It is based on the combination of image alignment techniques with a deep learning model based on convolutional neural networks (CNNs). This approach covers two important topics. Firstly, the capability of our system to be adaptable to variations in the UAV flight. In particular, the difference of height between flights, and a slight modification of the camera&rsquo;s position or movement of the UAV because of natural conditions such as the effect of wind. These modifications can be produced by multiple factors, such as weather conditions, security requirements or human errors. Secondly, the precision of our model to detect changes in diverse environments, which has been compared with state-of-the-art methods in change detection. This has been measured using the Change Detection 2014 dataset, which provides a selection of labelled images from different scenarios for training change detection algorithms. We have used images from dynamic background, intermittent object motion and bad weather sections. These sections have been selected to test our algorithm&rsquo;s robustness to changes in the background, as in real flight conditions. Our system provides a precise solution for these scenarios, as the mean F-measure score from the image analysis surpasses 97%, and a significant precision in the intermittent object motion category, where the score is above 99%.},
DOI = {10.3390/s19204484}
}



@Article{s19204513,
AUTHOR = {Freimuth, Henk and König, Markus},
TITLE = {A Framework for Automated Acquisition and Processing of As-Built Data with Autonomous Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4513},
URL = {https://www.mdpi.com/1424-8220/19/20/4513},
ISSN = {1424-8220},
ABSTRACT = {Planning and scheduling in construction heavily depend on current information about the state of construction processes. However, the acquisition process for visual data requires human personnel to take photographs of construction objects. We propose using unmanned aerial vehicle (UAVs) for automated creation of images and point cloud data of particular construction objects. The method extracts locations of objects that require inspection from Four Dimensional Building Information Modelling (4D-BIM). With this information at hand viable flight missions around the known structures of the construction site are computed. During flight, the UAV uses stereo cameras to detect and avoid any obstacles that are not known to the model, for example moving humans or machinery. The combination of pre-computed waypoint missions and reactive avoidance ensures deterministic routing from takeoff to landing and operational safety for humans and machines. During flight, an additional software component compares the captured point cloud data with the model data, enabling automatic per-object completion checking or reconstruction. The prototype is developed in the Robot Operating System (ROS) and evaluated in Software-In-The-Loop (SITL) simulations for the sake of being executable on real UAVs.},
DOI = {10.3390/s19204513}
}



@Article{en12203960,
AUTHOR = {Zhang, Haitao and Zhou, Ming and Lan, Xudong},
TITLE = {State of Charge Estimation Algorithm for Unmanned Aerial Vehicle Power-Type Lithium Battery Packs Based on the Extended Kalman Filter},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {3960},
URL = {https://www.mdpi.com/1996-1073/12/20/3960},
ISSN = {1996-1073},
ABSTRACT = {The lack of endurance is an important reason restricting further development of unmanned aerial vehicles (UAVs). Accurately estimating the state of charge (SOC) of the Li-Po battery can maximize the battery energy utilization and improve the endurance of UAVs. In this paper, the main current methods for estimating the SOC of vehicles were explored and discussed to unveil their advantages and disadvantages. In addition, the extended Kalman filter algorithm based on an equivalent circuit model was used to estimate SOC of power-type Li-Po batteries at different temperatures. The result showed that the closed-loop control method can effectively improve the battery life of small-sized electric UAVs.},
DOI = {10.3390/en12203960}
}



@Article{rs11202415,
AUTHOR = {Woodget, Amy S. and Dietrich, James T. and Wilson, Robin T.},
TITLE = {Quantifying Below-Water Fluvial Geomorphic Change: The Implications of Refraction Correction, Water Surface Elevations, and Spatially Variable Error},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2415},
URL = {https://www.mdpi.com/2072-4292/11/20/2415},
ISSN = {2072-4292},
ABSTRACT = {Much of the geomorphic work of rivers occurs underwater. As a result, high resolutionquantification of geomorphic change in these submerged areas is important. Currently, to quantify thischange, multiple methods are required to get high resolution data for both the exposed and submergedareas. Remote sensing methods are often limited to the exposed areas due to the challenges imposedby the water, and those remote sensing methods for below the water surface require the collection ofextensive calibration data in-channel, which is time-consuming, labour-intensive, and sometimesprohibitive in dicult-to-access areas. Within this paper, we pioneer a novel approach for quantifyingabove- and below-water geomorphic change using Structure-from-Motion photogrammetry andinvestigate the implications of water surface elevations, refraction correction measures, and thespatial variability of topographic errors. We use two epochs of imagery from a site on the River Teme,Herefordshire, UK, collected using a remotely piloted aircraft system (RPAS) and processed usingStructure-from-Motion (SfM) photogrammetry. For the first time, we show that: (1) Quantification ofsubmerged geomorphic change to levels of accuracy commensurate with exposed areas is possiblewithout the need for calibration data or a dierent method from exposed areas; (2) there is minimaldierence in results produced by dierent refraction correction procedures using predominantlynadir imagery (small angle vs. multi-view), allowing users a choice of software packages/processingcomplexity; (3) improvements to our estimations of water surface elevations are critical for accuratetopographic estimation in submerged areas and can reduce mean elevation error by up to 73%;and (4) we can use machine learning, in the form of multiple linear regressions, and a Gaussian Na&iuml;veBayes classifier, based on the relationship between error and 11 independent variables, to generate ahigh resolution, spatially continuous model of geomorphic change in submerged areas, constrained byspatially variable error estimates. Our multiple regression model is capable of explaining up to 54%of magnitude and direction of topographic error, with accuracies of less than 0.04 m. With on-goingtesting and improvements, this machine learning approach has potential for routine application inspatially variable error estimation within the RPAS&ndash;SfM workflow.},
DOI = {10.3390/rs11202415}
}



@Article{electronics8101188,
AUTHOR = {Tang, Tao and Hong, Tao and Hong, Haohui and Ji, Senyuan and Mumtaz, Shahid and Cheriet, Mohamed},
TITLE = {An Improved UAV-PHD Filter-Based Trajectory Tracking Algorithm for Multi-UAVs in Future 5G IoT Scenarios},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1188},
URL = {https://www.mdpi.com/2079-9292/8/10/1188},
ISSN = {2079-9292},
ABSTRACT = {The 5G cellular network is expected to provide core service platform for the expanded Internet of Things (IoT) by supporting enhanced mobile broadband (eMBB), massive machine-type communication (mMTC), and ultra-reliable low latency communications (URLLC). Unmanned aerial vehicles (UAVs), also known as drones, provide civil, commercial, and government services in various fields. Particularly in a 5G IoT scenario, UAV-aided network communications will fulfill an increasingly important role and will require the tracking of multiple UAV targets. As UAVs move quickly, maintaining the stability of the communication connection in 5G will be a challenge. Therefore, it is necessary to track the trajectory of UAVs. At present, the GM-PHD filter has a problem that the new target intensity must be known, and it cannot obtain the moving target trajectory and the influence of the clutter is likely to cause false alarm. A UAV-PHD filter is proposed in this work to improve the traditional GM-PHD filter by applying machine learning to the emergency detection and trajectory tracking of UAV targets. An out-of-sight detection algorithm for multiple UAVs is then presented to improve tracking performance. The method is assessed by simulation using MATLAB, and OSPA distance is utilized as an evaluation indicator. The simulation results illustrate that the proposed method can be applied to the tracking of multiple UAV targets in future 5G-IoT scenarios, and the performance is superior to the traditional GM-PHD filter.},
DOI = {10.3390/electronics8101188}
}



@Article{rs11202448,
AUTHOR = {Pinheiro, Helena S. K. and Barbosa, Theresa P. R. and Antunes, Mauro A. H. and Carvalho, Daniel Costa de and Nummer, Alexis R. and Carvalho Junior, Waldir de and Chagas, Cesar da Silva and Fernandes-Filho, Elpídio I. and Pereira, Marcos Gervasio},
TITLE = {Assessment of Phytoecological Variability by Red-Edge Spectral Indices and Soil-Landscape Relationships},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2448},
URL = {https://www.mdpi.com/2072-4292/11/20/2448},
ISSN = {2072-4292},
ABSTRACT = {There is a relation of vegetation physiognomies with soil and geological conditions that can be represented spatially with the support of remote sensing data. The goal of this research was to map vegetation physiognomies in a mountainous area by using Sentinel-2 Multispectral Instrument (MSI) data and morphometrical covariates through data mining techniques. The research was based on red-edge (RE) bands, and indices, to classify phytophysiognomies at two taxonomic levels. The input data was pixel sampled based on field sample sites. Data mining procedures comprised covariate selection and supervised classification through the Random Forest model. Results showed the potential of bands 3, 5, and 6 to map phytophysiognomies for both seasons, as well as Green Chlorophyll (CLg) and SAVI indices. NDVI indices were important, particularly those calculated with bands 6, 7, 8, and 8A, which were placed at the RE position. The model performance showed reasonable success to Kappa index 0.72 and 0.56 for the first and fifth taxonomic level, respectively. The model presented confusion between Broadleaved dwarf-forest, Parkland Savanna, and Bushy grassland. Savanna formations occurred variably in the area while Bushy grasslands strictly occur in certain landscape positions. Broadleaved forests presented the best performance (first taxonomic level), and among its variation (fifth level) the model could precisely capture the pattern for those on deep soils from gneiss parent material. The approach was thus useful to capture intrinsic soil-plant relationships and its relation with remote sensing data, showing potential to map phytophysiognomies in two distinct taxonomic levels in poorly accessible areas.},
DOI = {10.3390/rs11202448}
}



@Article{rs11202455,
AUTHOR = {He, Zhi and He, Dan and Mei, Xiangqin and Hu, Saihan},
TITLE = {Wetland Classification Based on a New Efficient Generative Adversarial Network and Jilin-1 Satellite Image},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2455},
URL = {https://www.mdpi.com/2072-4292/11/20/2455},
ISSN = {2072-4292},
ABSTRACT = {Recent studies have shown that deep learning methods provide useful tools for wetland classification. However, it is difficult to perform species-level classification with limited labeled samples. In this paper, we propose a semi-supervised method for wetland species classification by using a new efficient generative adversarial network (GAN) and Jilin-1 satellite image. The main contributions of this paper are twofold. First, the proposed method, namely ShuffleGAN, requires only a small number of labeled samples. ShuffleGAN is composed of two neural networks (i.e., generator and discriminator), which perform an adversarial game in the training phase and ShuffleNet units are added in both generator and discriminator to obtain speed-accuracy tradeoff. Second, ShuffleGAN can perform species-level wetland classification. In addition to distinguishing the wetland areas from non-wetlands, different tree species located in the wetland are also identified, thus providing a more detailed distribution of the wetland land-covers. Experiments are conducted on the Haizhu Lake wetland data acquired by the Jilin-1 satellite. Compared with existing GAN, the improvement in overall accuracy (OA) of the proposed ShuffleGAN is more than 2%. This work can not only deepen the application of deep learning in wetland classification but also promote the study of fine classification of wetland land-covers.},
DOI = {10.3390/rs11202455}
}



@Article{rs11202456,
AUTHOR = {Zhu, Wanxue and Sun, Zhigang and Huang, Yaohuan and Lai, Jianbin and Li, Jing and Zhang, Junqiang and Yang, Bin and Li, Binbin and Li, Shiji and Zhu, Kangying and Li, Yang and Liao, Xiaohan},
TITLE = {Improving Field-Scale Wheat LAI Retrieval Based on UAV Remote-Sensing Observations and Optimized VI-LUTs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2456},
URL = {https://www.mdpi.com/2072-4292/11/20/2456},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is a key biophysical parameter for monitoring crop growth status, predicting crop yield, and quantifying crop variability in agronomic applications. Mapping the LAI at the field scale using multispectral cameras onboard unmanned aerial vehicles (UAVs) is a promising precision-agriculture application with specific requirements: The LAI retrieval method should be (1) robust so that crop LAI can be estimated with similar accuracy and (2) easy to use so that it can be applied to the adjustment of field management practices. In this study, three UAV remote-sensing missions (UAVs with Micasense RedEdge-M and Cubert S185 cameras) were carried out over six experimental plots from 2018 to 2019 to investigate the performance of reflectance-based lookup tables (LUTs) and vegetation index (VI)-based LUTs generated from the PROSAIL model for wheat LAI retrieval. The effects of the central wavelengths and bandwidths for the VI calculations on the LAI retrieval were further examined. We found that the VI-LUT strategy was more robust and accurate than the reflectance-LUT strategy. The differences in the LAI retrieval accuracy among the four VI-LUTs were small, although the improved modified chlorophyll absorption ratio index-lookup table (MCARI2-LUT) and normalized difference vegetation index-lookup table (NDVI-LUT) performed slightly better. We also found that both of the central wavelengths and bandwidths of the VIs had effects on the LAI retrieval. The VI-LUTs with optimized central wavelengths (red = 612 nm, near-infrared (NIR) = 756 nm) and narrow bandwidths (~4 nm) improved the wheat LAI retrieval accuracy (R2 &ge; 0.75). The results of this study provide an alternative method for retrieving crop LAI, which is robust and easy use for precision-agriculture applications and may be helpful for designing UAV multispectral cameras for agricultural monitoring.},
DOI = {10.3390/rs11202456}
}



@Article{rs11212479,
AUTHOR = {Li, Huiying and Jia, Mingming and Zhang, Rong and Ren, Yongxing and Wen, Xin},
TITLE = {Incorporating the Plant Phenological Trajectory into Mangrove Species Mapping with Dense Time Series Sentinel-2 Imagery and the Google Earth Engine Platform},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2479},
URL = {https://www.mdpi.com/2072-4292/11/21/2479},
ISSN = {2072-4292},
ABSTRACT = {Information on mangrove species composition and distribution is key to studying functions of mangrove ecosystems and securing sustainable mangrove conservation. Even though remote sensing technology is developing rapidly currently, mapping mangrove forests at the species level based on freely accessible images is still a great challenge. This study built a Sentinel-2 normalized difference vegetation index (NDVI) time series (from 2017-01-01 to 2018-12-31) to represent phenological trajectories of mangrove species and then demonstrated the feasibility of phenology-based mangrove species classification using the random forest algorithm in the Google Earth Engine platform. It was found that (i) in Zhangjiang estuary, the phenological trajectories (NDVI time series) of different mangrove species have great differences; (ii) the overall accuracy and Kappa confidence of the classification map is 84% and 0.84, respectively; and (iii) Months in late winter and early spring play critical roles in mangrove species mapping. This is the first study to use phonological signatures in discriminating mangrove species. The methodology presented can be used as a practical guideline for the mapping of mangrove or other vegetation species in other regions. However, future work should pay attention to various phenological trajectories of mangrove species in different locations.},
DOI = {10.3390/rs11212479}
}



@Article{rs11212494,
AUTHOR = {Gebremedhin, Alem and Badenhorst, Pieter and Wang, Junping and Giri, Khageswor and Spangenberg, German and Smith, Kevin},
TITLE = {Development and Validation of a Model to Combine NDVI and Plant Height for High-Throughput Phenotyping of Herbage Yield in a Perennial Ryegrass Breeding Program},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2494},
URL = {https://www.mdpi.com/2072-4292/11/21/2494},
ISSN = {2072-4292},
ABSTRACT = {Sensor-based phenotyping technologies may offer a non-destructive, high-throughput and efficient assessment of herbage yield (HY) to replace current inefficient phenotyping methods. This paper assesses the feasibility of combining normalised difference vegetative index (NDVI) from multispectral imaging and ultrasonic sonar estimates of plant height to estimate HY of single plants in a large perennial ryegrass breeding program. For sensor calibration, fresh HY (FHY) and dry HY (DHY) were acquired destructively, and plant height was measured at four dates each in 2017 and 2018 from a selected subset of 480 plants. Global multiple linear regression models based on K-fold and random split cross-validation methods were used to evaluate the relationship between observed vs. predicted HY. The coefficient of determination (R2) = 0.67&ndash;0.68 and a root mean square error (RMSE) between 5.43&ndash;7.60 g was obtained for the validation of predicted vs. observed DHY. The mean absolute error (MAE) and mean percentage error (MPE) ranged between 3.59&ndash;5.44 g and 22&ndash;28%, respectively. For the FHY, R2 values ranged from 0.63 to 0.70, with an RMSE between 23.50 and 33 g, MAE between 15.11 and 24.34 g and MPE between ~22% and 31%. Combining NDVI and plant height is a robust method to enable high-throughput phenotyping of herbage yield in perennial ryegrass breeding programs.},
DOI = {10.3390/rs11212494}
}



@Article{rs11212495,
AUTHOR = {Bohnenkamp, David and Behmann, Jan and Mahlein, Anne-Katrin},
TITLE = {In-Field Detection of Yellow Rust in Wheat on the Ground Canopy and UAV Scale},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2495},
URL = {https://www.mdpi.com/2072-4292/11/21/2495},
ISSN = {2072-4292},
ABSTRACT = {The application of hyperspectral imaging technology for plant disease detection in the field is still challenging. Existing equipment and analysis algorithms are adapted to highly controlled environmental conditions in the laboratory. However, only real time information from the field scale is able to guide plant protection measures and to optimize the use of resources. At the field scale, many parameters such as the optimal measurement distance, informative feature sets, and suitable algorithms have not been investigated. In this study, the hyperspectral detection and quantification of yellow rust in wheat was evaluated using two measurement platforms: a ground-based vehicle and an unmanned aerial vehicle (UAV). Different disease development stages and disease severities were provided in a plot-based field experiment. Measurements were performed weekly during the vegetation period. Data analysis was performed by three prediction algorithms with a focus on the selection of optimal feature sets. In this context, the across-scale application of optimized feature sets, an approach of information transfer between scales, was also evaluated. Relevant aspects for an on-line disease assessment in the field integrating affordable sensor technology, sensor spatial resolution, compact analysis models, and fast evaluation have been outlined and reflected upon. For the first time, a hyperspectral imaging observation experiment of a plant disease was comparatively performed at two scales, ground canopy and UAV.},
DOI = {10.3390/rs11212495}
}



@Article{rs11212499,
AUTHOR = {Xin, Jiang and Zhang, Xinchang and Zhang, Zhiqiang and Fang, Wu},
TITLE = {Road Extraction of High-Resolution Remote Sensing Images Derived from DenseUNet},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2499},
URL = {https://www.mdpi.com/2072-4292/11/21/2499},
ISSN = {2072-4292},
ABSTRACT = {Road network extraction is one of the significant assignments for disaster emergency response, intelligent transportation systems, and real-time updating road network. Road extraction base on high-resolution remote sensing images has become a hot topic. Presently, most of the researches are based on traditional machine learning algorithms, which are complex and computational because of impervious surfaces such as roads and buildings that are discernible in the images. Given the above problems, we propose a new method to extract the road network from remote sensing images using a DenseUNet model with few parameters and robust characteristics. DenseUNet consists of dense connection units and skips connections, which strengthens the fusion of different scales by connections at various network layers. The performance of the advanced method is validated on two datasets of high-resolution images by comparison with three classical semantic segmentation methods. The experimental results show that the method can be used for road extraction in complex scenes.},
DOI = {10.3390/rs11212499}
}



@Article{app9214543,
AUTHOR = {Seo, Dae Kyo and Eo, Yang Dam},
TITLE = {Multilayer Perceptron-Based Phenological and Radiometric Normalization for High-Resolution Satellite Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4543},
URL = {https://www.mdpi.com/2076-3417/9/21/4543},
ISSN = {2076-3417},
ABSTRACT = {Radiometric normalization is an essential preprocessing step that must be performed to detect changes in multi-temporal satellite images and, in general, relative radiometric normalization is utilized. However, most relative radiometric normalization methods assume a linear relationship and they cannot take into account nonlinear properties, such as the distribution of the earth&rsquo;s surface or phenological differences that are caused by the growth of vegetation. Thus, this paper proposes a novel method that assumes a nonlinear relationship and it uses a representative nonlinear regression model&mdash;multilayer perceptron (MLP). The proposed method performs radiometric resolution compression while considering both the complexity and time cost, and radiometric control set samples are extracted based on a no-change set method. Subsequently, the spectral index is selected for each band to compensate for the phenological properties, phenological normalization is performed based on MLP, and the global radiometric properties are adjusted through postprocessing. Finally, a performance evaluation is conducted by comparing the results herein with those from conventional relative radiometric normalization algorithms. The experimental results show that the proposed method outperforms conventional methods in terms of both visual inspection and quantitative evaluation. In other words, the applicability of the proposed method to the normalization of multi-temporal images with nonlinear properties is confirmed.},
DOI = {10.3390/app9214543}
}



@Article{agronomy9110682,
AUTHOR = {Pagay, Vinay and Kidman, Catherine M.},
TITLE = {Evaluating Remotely-Sensed Grapevine (Vitis vinifera L.) Water Stress Responses Across a Viticultural Region},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {682},
URL = {https://www.mdpi.com/2073-4395/9/11/682},
ISSN = {2073-4395},
ABSTRACT = {The evolving spatial and temporal knowledge about vineyard performance through the use of remote sensing offers new perspectives for vine water status studies. This paper describes the application of aerial thermal imaging to evaluate vine water status to improve irrigation scheduling decisions, water use efficiency, and overall winegrape quality in the Coonawarra viticultural region of South Australia. Airborne infrared images were acquired during the 2016 and 2017 growing seasons in the region of Coonawarra, South Australia. Several thermal indices of crop water status (CWSI, Ig, (Tc-Ta)) were calculated that correlated with conventional soil and vine water status measures (&Psi;pd, &Psi;s, gs). CWSI and Ig could discriminate between the two cultivars used in this study, Cabernet Sauvignon (CAS) and Shiraz (SHI), as did the conventional water stress measures. The relationship between conventional vine water status measures appeared stronger with CWSI in the warmer and drier season (2016) compared to the cooler and wetter season (2017), where Ig and (Tc-Ta) showed stronger correlations. The study identified CWSI, Ig and (Tc-Ta) to be reliable indicators of vine water status under a variety of environmental conditions. This is the first study to report on high resolution vine water status at a regional scale in Australia using a combination of remote and direct sensing methods. This methodology is promising for aerial surveillance of vine water status across multiple blocks and cultivars to inform irrigation scheduling.},
DOI = {10.3390/agronomy9110682}
}



@Article{rs11212511,
AUTHOR = {Kerle, Norman and Ghaffarian, Saman and Nawrotzki, Raphael and Leppert, Gerald and Lech, Malte},
TITLE = {Evaluating Resilience-Centered Development Interventions with Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2511},
URL = {https://www.mdpi.com/2072-4292/11/21/2511},
ISSN = {2072-4292},
ABSTRACT = {Natural disasters are projected to increase in number and severity, in part due to climate change. At the same time a growing number of disaster risk reduction (DRR) and climate change adaptation measures are being implemented by governmental and non-governmental organizations, and substantial post-disaster donations are frequently pledged. At the same time there has been increasing demand for transparency and accountability, and thus evidence of those measures having a positive effect. We hypothesized that resilience-enhancing interventions should result in less damage during a hazard event, or at least quicker recovery. In this study we assessed recovery over a 3 year period of seven municipalities in the central Philippines devastated by Typhoon Haiyan in 2013. We used very high resolution optical images (&lt;1 m), and created detailed land cover and land use maps for four epochs before and after the event, using a machine learning approach with extreme gradient boosting. The spatially and temporally highly variable recovery maps were then statistically related to detailed questionnaire data acquired by DEval in 2012 and 2016, whose principal aim was to assess the impact of a 10 year land-planning intervention program by the German agency for technical cooperation (GIZ). The survey data allowed very detailed insights into DRR-related perspectives, motivations and drivers of the affected population. To some extent they also helped to overcome the principal limitation of remote sensing, which can effectively describe but not explain the reasons for differential recovery. However, while a number of causal links between intervention parameters and reconstruction was found, the common notion that a resilient community should recover better and more quickly could not be confirmed. The study also revealed a number of methodological limitations, such as the high cost for commercial image data not matching the spatially extensive but also detailed scale of field evaluations, the remote sensing analysis likely overestimating damage and thus providing incorrect recovery metrics, and image data catalogues especially for more remote communities often being incomplete. Nevertheless, the study provides a valuable proof of concept for the synergies resulting from an integration of socio-economic survey data and remote sensing imagery for recovery assessment.},
DOI = {10.3390/rs11212511}
}



@Article{app9214552,
AUTHOR = {Chuang, Hsiu-Min and He, Dongqing and Namiki, Akio},
TITLE = {Autonomous Target Tracking of UAV Using High-Speed Visual Feedback},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4552},
URL = {https://www.mdpi.com/2076-3417/9/21/4552},
ISSN = {2076-3417},
ABSTRACT = {Most current unmanned aerial vehicles (UAVs) primarily use a global positioning system (GPS) and an inertial measurement unit (IMU) for position estimation. However, compared to birds and insects, the abilities of current UAVs to recognize the environment are not sufficient. To achieve autonomous flight of UAVs, like birds, the UAVs should be able to process and respond to information from their surrounding environment immediately. Therefore, in this paper, we propose a direct visual servoing system for UAVs, using an onboard high-speed monocular camera. There are two advantages of this system. First, the high image sampling rates help to improve the ability to recognize the environment. Second, the issue of control latency can be effectively solved because the position control signals are transmitted to the flight controller directly. In the experiment, the UAV could recognize a target at update rates of about 350 Hz, and a target tracking task was successfully realized.},
DOI = {10.3390/app9214552}
}



@Article{s19214674,
AUTHOR = {Botina, Deivid and Franco, Ricardo and Murillo, Javier and Galeano, July and Zarzycki, Artur and Torres-Madronero, Maria C. and Bermúdez, Camilo and Montaño, Jaime and Garzón, Johnson and Marzani, Franck and Robledo, Sara M.},
TITLE = {Estimation of Biological Parameters of Cutaneous Ulcers Caused by Leishmaniasis in an Animal Model Using Diffuse Reflectance Spectroscopy},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4674},
URL = {https://www.mdpi.com/1424-8220/19/21/4674},
ISSN = {1424-8220},
ABSTRACT = {Cutaneous leishmaniasis (CL) is a neglected tropical disease that requires novel tools for its understanding, diagnosis, and treatment follow-up. In the cases of other cutaneous pathologies, such as cancer or cutaneous ulcers due to diabetes, optical diffuse reflectance-based tools and methods are widely used for the investigation of those illnesses. These types of tools and methods offer the possibility to develop portable diagnosis and treatment follow-up systems. In this article, we propose the use of a three-layer diffuse reflectance model for the study of the formation of cutaneous ulcers caused by CL. The proposed model together with an inverse-modeling procedure were used in the evaluation of diffuse-reflectance spectral signatures acquired from cutaneous ulcers formed in the dorsal area of 21 golden hamsters inoculated with Leishmanisis braziliensis. As result, the quantification of the model&rsquo;s variables related to the main biological parameters of skin were obtained, such as: diameter and volumetric fraction of keratinocytes, collagen; volumetric fraction of hemoglobin, and oxygen saturation. Those parameters show statistically significant differences among the different stages of the CL ulcer formation. We found that these differences are coherent with histopathological manifestations reported in the literature for the main phases of CL formation.},
DOI = {10.3390/s19214674}
}



@Article{rs11212523,
AUTHOR = {Xia, Wei and Ma, Caihong and Liu, Jianbo and Liu, Shibin and Chen, Fu and Yang, Zhi and Duan, Jianbo},
TITLE = {High-Resolution Remote Sensing Imagery Classification of Imbalanced Data Using Multistage Sampling Method and Deep Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2523},
URL = {https://www.mdpi.com/2072-4292/11/21/2523},
ISSN = {2072-4292},
ABSTRACT = {Class imbalance is a key issue for the application of deep learning for remote sensing image classification because a model generated by imbalanced samples training has low classification accuracy for minority classes. In this study, an accurate classification approach using the multistage sampling method and deep neural networks was proposed to classify imbalanced data. We first balance samples by multistage sampling to obtain the training sets. Then, a state-of-the-art model is adopted by combining the advantages of atrous spatial pyramid pooling (ASPP) and Encoder-Decoder for pixel-wise classification, which are two different types of fully convolutional networks (FCNs) that can obtain contextual information of multiple levels in the Encoder stage. The details and spatial dimensions of targets are restored using such information during the Decoder stage. We employ four deep learning-based classification algorithms (basic FCN, FCN-8S, ASPP, and Encoder-Decoder with ASPP of our approach) on multistage training sets (original, MUS1, and MUS2) of WorldView-3 images in southeastern Qinghai-Tibet Plateau and GF-2 images in northeastern Beijing for comparison. The experiments show that, compared with existing sets (original, MUS1, and identical) and existing method (cost weighting), the MUS2 training set of multistage sampling significantly enhance the classification performance for minority classes. Our approach shows distinct advantages for imbalanced data.},
DOI = {10.3390/rs11212523}
}



@Article{s19214695,
AUTHOR = {Sousa, Daniela and Hernandez, Diego and Oliveira, Francisco and Luís, Miguel and Sargento, Susana},
TITLE = {A Platform of Unmanned Surface Vehicle Swarms for Real Time Monitoring in Aquaculture Environments},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4695},
URL = {https://www.mdpi.com/1424-8220/19/21/4695},
ISSN = {1424-8220},
ABSTRACT = {The Internet of Things (IoT) is a rapidly evolving technology that is changing almost every business, and aquaculture is no exception. In this work we present an integrated IoT platform for the acquisition of environmental data and the monitoring of aquaculture environments, supported by a real-time communication and processing network. The complete monitoring platform consists of environmental sensors equipped in a swarm of mobile Unmanned Surface Vehicles (USVs) and Buoys, capable of collecting aquatic and outside information, and sending it to a central station where it will be stored and processed. The sensing platform, formed by the USVs and Buoys, are equipped with multi-communication technology: IEEE 802.11n (Wi-Fi) and Bluetooth for short range communication, for mission delegation and the transmission of data collection, and LoRa for periodic report. On the back-end side, supported by FIWARE technology, an interactive web-based platform can be used to define sensing missions and for data visualization. Results on the sensing platform lifetime, mission control and delay processing time are presented to assess the performance of the aquatic monitoring system.},
DOI = {10.3390/s19214695}
}



@Article{drones3040080,
AUTHOR = {Otsu, Kaori and Pla, Magda and Duane, Andrea and Cardil, Adrián and Brotons, Lluís},
TITLE = {Estimating the Threshold of Detection on Tree Crown Defoliation Using Vegetation Indices from UAS Multispectral Imagery},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {80},
URL = {https://www.mdpi.com/2504-446X/3/4/80},
ISSN = {2504-446X},
ABSTRACT = {Periodical outbreaks of Thaumetopoea pityocampa feeding on pine needles may pose a threat to Mediterranean coniferous forests by causing severe tree defoliation, growth reduction, and eventually mortality. To cost&ndash;effectively monitor the temporal and spatial damages in pine&ndash;oak mixed stands using unmanned aerial systems (UASs) for multispectral imagery, we aimed at developing a simple thresholding classification tool for forest practitioners as an alternative method to complex classifiers such as Random Forest. The UAS flights were performed during winter 2017&ndash;2018 over four study areas in Catalonia, northeastern Spain. To detect defoliation and further distinguish pine species, we conducted nested histogram thresholding analyses with four UAS-derived vegetation indices (VIs) and evaluated classification accuracy. The normalized difference vegetation index (NDVI) and NDVI red edge performed the best for detecting defoliation with an overall accuracy of 95% in the total study area. For discriminating pine species, accuracy results of 93&ndash;96% were only achievable with green NDVI in the partial study area, where the Random Forest classification combined for defoliation and tree species resulted in 91&ndash;93%. Finally, we achieved to estimate the average thresholds of VIs for detecting defoliation over the total area, which may be applicable across similar Mediterranean pine stands for monitoring regional forest health on a large scale.},
DOI = {10.3390/drones3040080}
}



@Article{rs11212533,
AUTHOR = {Jensen, Daniel and Cavanaugh, Kyle C. and Simard, Marc and Okin, Gregory S. and Castañeda-Moya, Edward and McCall, Annabeth and Twilley, Robert R.},
TITLE = {Integrating Imaging Spectrometer and Synthetic Aperture Radar Data for Estimating Wetland Vegetation Aboveground Biomass in Coastal Louisiana},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2533},
URL = {https://www.mdpi.com/2072-4292/11/21/2533},
ISSN = {2072-4292},
ABSTRACT = {Aboveground biomass (AGB) plays a critical functional role in coastal wetland ecosystem stability, with high biomass vegetation contributing to organic matter production, sediment accretion potential, and the surface elevation&rsquo;s ability to keep pace with relative sea level rise. Many remote sensing studies have employed either imaging spectrometer or synthetic aperture radar (SAR) for AGB estimation in various environments for assessing ecosystem health and carbon storage. This study leverages airborne data from NASA&rsquo;s Airborne Visible/Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) and Uninhabited Aerial Vehicle Synthetic Aperture Radar (UAVSAR) to assess their unique capabilities in combination to estimate AGB in coastal deltaic wetlands. Here we develop AGB models for emergent herbaceous and forested wetland vegetation in coastal Louisiana. In addition to horizontally emitted, vertically received (HV) backscatter, SAR parameters are expressed by the Freeman&ndash;Durden polarimetric decomposition components representing volume and double-bounce scattering. The imaging spectrometer parameters include normalized difference vegetation index (NDVI), reflectance from 290 visible-shortwave infrared (VSWIR) bands, the first derivatives from those bands, or partial least squares (PLS) x-scores derived from those data. Model metrics and cross-validation indicate that the integrated models using the Freeman-Durden components and PLS x-scores improve AGB estimates for both wetland vegetation types. In our study domain over Louisiana&rsquo;s Wax Lake Delta (WLD), we estimated a mean herbaceous wetland AGB of 3.58 Megagrams/hectare (Mg/ha) and a total of 3551.31 Mg over 9.92 km2, and a mean forested wetland AGB of 294.78 Mg/ha and a total of 27,499.14 Mg over 0.93 km2. While the addition of SAR-derived values to imaging spectrometer data provides a nominal error decrease for herbaceous wetland AGB, this combination significantly improves forested wetland AGB prediction. This integrative approach is particularly effective in forested wetlands as canopy-level biochemical characteristics are captured by the imaging spectrometer in addition to the variable structural information measured by the SAR.},
DOI = {10.3390/rs11212533}
}



@Article{s19214699,
AUTHOR = {Barreto, M. Alejandra P. and Johansen, Kasper and Angel, Yoseline and McCabe, Matthew F.},
TITLE = {Radiometric Assessment of a UAV-Based Push-Broom Hyperspectral Camera},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4699},
URL = {https://www.mdpi.com/1424-8220/19/21/4699},
ISSN = {1424-8220},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) for Earth and environmental sensing has increased significantly in recent years. This is particularly true for multi- and hyperspectral sensing, with a variety of both push-broom and snap-shot systems becoming available. However, information on their radiometric performance and stability over time is often lacking. The authors propose the use of a general protocol for sensor evaluation to characterize the data retrieval and radiometric performance of push-broom hyperspectral cameras, and illustrate the workflow with the Nano-Hyperspec (Headwall Photonics, Boston USA) sensor. The objectives of this analysis were to: (1) assess dark current and white reference consistency, both temporally and spatially; (2) evaluate spectral fidelity; and (3) determine the relationship between sensor-recorded radiance and spectroradiometer-derived reflectance. Both the laboratory-based dark current and white reference evaluations showed an insignificant increase over time (&lt;2%) across spatial pixels and spectral bands for &gt;99.5% of pixel–waveband combinations. Using a mercury/argon (Hg/Ar) lamp, the hyperspectral wavelength bands exhibited a slight shift of 1-3 nm against 29 Hg/Ar wavelength emission lines. The relationship between the Nano-Hyperspec radiance values and spectroradiometer-derived reflectance was found to be highly linear for all spectral bands. The developed protocol for assessing UAV-based radiometric performance of hyperspectral push-broom sensors showed that the Nano-Hyperspec data were both time-stable and spectrally sound.},
DOI = {10.3390/s19214699}
}



@Article{s19214717,
AUTHOR = {Liu, Yuxuan and Aleksandrov, Mitko and Zlatanova, Sisi and Zhang, Junjun and Mo, Fan and Chen, Xiaojian},
TITLE = {Classification of Power Facility Point Clouds from Unmanned Aerial Vehicles Based on Adaboost and Topological Constraints},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4717},
URL = {https://www.mdpi.com/1424-8220/19/21/4717},
ISSN = {1424-8220},
ABSTRACT = {Machine learning algorithms can be well suited to LiDAR point cloud classification, but when they are applied to the point cloud classification of power facilities, many problems such as a large number of computational features and low computational efficiency can be encountered. To solve these problems, this paper proposes the use of the Adaboost algorithm and different topological constraints. For different objects, the top five features with the best discrimination are selected and combined into a strong classifier by the Adaboost algorithm, where coarse classification is performed. For power transmission lines, the optimum scales are selected automatically, and the coarse classification results are refined. For power towers, it is difficult to distinguish the tower from vegetation points by only using spatial features due to the similarity of their proposed key features. Therefore, the topological relationship between the power line and power tower is introduced to distinguish the power tower from vegetation points. The experimental results show that the classification of power transmission lines and power towers by our method can achieve the accuracy of manual classification results and even be more efficient.},
DOI = {10.3390/s19214717}
}



@Article{drones3040081,
AUTHOR = {Buters, Todd M. and Belton, David and Cross, Adam T.},
TITLE = {Multi-Sensor UAV Tracking of Individual Seedlings and Seedling Communities at Millimetre Accuracy},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {81},
URL = {https://www.mdpi.com/2504-446X/3/4/81},
ISSN = {2504-446X},
ABSTRACT = {The increasing spatial and temporal scales of ecological recovery projects demand more rapid and accurate methods of predicting restoration trajectory. Unmanned aerial vehicles (UAVs) offer greatly improved rapidity and efficiency compared to traditional biodiversity monitoring surveys and are increasingly employed in the monitoring of ecological restoration. However, the applicability of UAV-based remote sensing in the identification of small features of interest from captured imagery (e.g., small individual plants, &lt;100 cm2) remains untested and the potential of UAVs to track the performance of individual plants or the development of seedlings remains unexplored. This study utilised low-altitude UAV imagery from multi-sensor flights (Red-Green-Blue and multispectral sensors) and an automated object-based image analysis software to detect target seedlings from among a matrix of non-target grasses in order to track the performance of individual target seedlings and the seedling community over a 14-week period. Object-based Image Analysis (OBIA) classification effectively and accurately discriminated among target and non-target seedling objects and these groups exhibited distinct spectral signatures (six different visible-spectrum and multispectral indices) that responded differently over a 24-day drying period. OBIA classification from captured imagery also allowed for the accurate tracking of individual target seedling objects through time, clearly illustrating the capacity of UAV-based monitoring to undertake plant performance monitoring of individual plants at very fine spatial scales.},
DOI = {10.3390/drones3040081}
}



@Article{rs11212560,
AUTHOR = {Marchetti, Francesca and Waske, Björn and Arbelo, Manuel and Moreno-Ruíz, Jose A. and Alonso-Benito, Alfonso},
TITLE = {Mapping Chestnut Stands Using Bi-Temporal VHR Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2560},
URL = {https://www.mdpi.com/2072-4292/11/21/2560},
ISSN = {2072-4292},
ABSTRACT = {This study analyzes the potential of very high resolution (VHR) remote sensing images and extended morphological profiles for mapping Chestnut stands on Tenerife Island (Canary Islands, Spain). Regarding their relevance for ecosystem services in the region (cultural and provisioning services) the public sector demand up-to-date information on chestnut and a simple straight-forward approach is presented in this study. We used two VHR WorldView images (March and May 2015) to cover different phenological phases. Moreover, we included spatial information in the classification process by extended morphological profiles (EMPs). Random forest is used for the classification process and we analyzed the impact of the bi-temporal information as well as of the spatial information on the classification accuracies. The detailed accuracy assessment clearly reveals the benefit of bi-temporal VHR WorldView images and spatial information, derived by EMPs, in terms of the mapping accuracy. The bi-temporal classification outperforms or at least performs equally well when compared to the classification accuracies achieved by the mono-temporal data. The inclusion of spatial information by EMPs further increases the classification accuracy by 5% and reduces the quantity and allocation disagreements on the final map. Overall the new proposed classification strategy proves useful for mapping chestnut stands in a heterogeneous and complex landscape, such as the municipality of La Orotava, Tenerife.},
DOI = {10.3390/rs11212560}
}



@Article{rs11212561,
AUTHOR = {Gong, Chizhang and Buddenbaum, Henning and Retzlaff, Rebecca and Udelhoven, Thomas},
TITLE = {An Empirical Assessment of Angular Dependency for RedEdge-M in Sloped Terrain Viticulture},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2561},
URL = {https://www.mdpi.com/2072-4292/11/21/2561},
ISSN = {2072-4292},
ABSTRACT = {For grape canopy pixels captured by an unmanned aerial vehicle (UAV) tilt-mounted RedEdge-M multispectral sensor in a sloped vineyard, an in situ Walthall model can be established with purely image-based methods. This was derived from RedEdge-M directional reflectance and a vineyard 3D surface model generated from the same imagery. The model was used to correct the angular effects in the reflectance images to form normalized difference vegetation index (NDVI) orthomosaics of different view angles. The results showed that the effect could be corrected to a certain scope, but not completely. There are three drawbacks that might restrict a successful angular model construction and correction: (1) the observable micro shadow variation on the canopy enabled by the high resolution; (2) the complexity of vine canopies that causes an inconsistency between reflectance and canopy geometry, including effects such as micro shadows and near-infrared (NIR) additive effects; and (3) the resolution limit of a 3D model to represent the accurate real-world optical geometry. The conclusion is that grape canopies might be too inhomogeneous for the tested method to perform the angular correction in high quality.},
DOI = {10.3390/rs11212561}
}



@Article{beverages5040062,
AUTHOR = {Gonzalez Viejo, Claudia and Torrico, Damir D. and Dunshea, Frank R. and Fuentes, Sigfredo},
TITLE = {Emerging Technologies Based on Artificial Intelligence to Assess the Quality and Consumer Preference of Beverages},
JOURNAL = {Beverages},
VOLUME = {5},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {62},
URL = {https://www.mdpi.com/2306-5710/5/4/62},
ISSN = {2306-5710},
ABSTRACT = {Beverages is a broad and important category within the food industry, which is comprised of a wide range of sub-categories and types of drinks with different levels of complexity for their manufacturing and quality assessment. Traditional methods to evaluate the quality traits of beverages consist of tedious, time-consuming, and costly techniques, which do not allow researchers to procure results in real-time. Therefore, there is a need to test and implement emerging technologies in order to automate and facilitate those analyses within this industry. This paper aimed to present the most recent publications and trends regarding the use of low-cost, reliable, and accurate, remote or non-contact techniques using robotics, machine learning, computer vision, biometrics and the application of artificial intelligence, as well as to identify the research gaps within the beverage industry. It was found that there is a wide opportunity in the development and use of robotics and biometrics for all types of beverages, but especially for hot and non-alcoholic drinks. Furthermore, there is a lack of knowledge and clarity within the industry, and research about the concepts of artificial intelligence and machine learning, as well as that concerning the correct design and interpretation of modeling related to the lack of inclusion of relevant data, additional to presenting over- or under-fitted models.},
DOI = {10.3390/beverages5040062}
}



@Article{app9214656,
AUTHOR = {Alhichri, Haikel and Bazi, Yakoub and Alajlan, Naif and Bin Jdira, Bilel},
TITLE = {Helping the Visually Impaired See via Image Multi-labeling Based on SqueezeNet CNN},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4656},
URL = {https://www.mdpi.com/2076-3417/9/21/4656},
ISSN = {2076-3417},
ABSTRACT = {This work presents a deep learning method for scene description. (1) Background: This method is part of a larger system, called BlindSys, that assists the visually impaired in an indoor environment. The method detects the presence of certain objects, regardless of their position in the scene. This problem is also known as image multi-labeling. (2) Methods: Our proposed deep learning solution is based on a light-weight pre-trained CNN called SqueezeNet. We improved the SqueezeNet architecture by resetting the last convolutional layer to free weights, replacing its activation function from a rectified linear unit (ReLU) to a LeakyReLU, and adding a BatchNormalization layer thereafter. We also replaced the activation functions at the output layer from softmax to linear functions. These adjustments make up the main contributions in this work. (3) Results: The proposed solution is tested on four image multi-labeling datasets representing different indoor environments. It has achieved results better than state-of-the-art solutions both in terms of accuracy and processing time. (4) Conclusions: The proposed deep CNN is an effective solution for predicting the presence of objects in a scene and can be successfully used as a module within BlindSys.},
DOI = {10.3390/app9214656}
}



@Article{rs11212575,
AUTHOR = {Tavakkoli Piralilou, Sepideh and Shahabi, Hejar and Jarihani, Ben and Ghorbanzadeh, Omid and Blaschke, Thomas and Gholamnia, Khalil and Meena, Sansar Raj and Aryal, Jagannath},
TITLE = {Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2575},
URL = {https://www.mdpi.com/2072-4292/11/21/2575},
ISSN = {2072-4292},
ABSTRACT = {Landslides represent a severe hazard in many areas of the world. Accurate landslide maps are needed to document the occurrence and extent of landslides and to investigate their distribution, types, and the pattern of slope failures. Landslide maps are also crucial for determining landslide susceptibility and risk. Satellite data have been widely used for such investigations—next to data from airborne or unmanned aerial vehicle (UAV)-borne campaigns and Digital Elevation Models (DEMs). We have developed a methodology that incorporates object-based image analysis (OBIA) with three machine learning (ML) methods, namely, the multilayer perceptron neural network (MLP-NN) and random forest (RF), for landslide detection. We identified the optimal scale parameters (SP) and used them for multi-scale segmentation and further analysis. We evaluated the resulting objects using the object pureness index (OPI), object matching index (OMI), and object fitness index (OFI) measures. We then applied two different methods to optimize the landslide detection task: (a) an ensemble method of stacking that combines the different ML methods for improving the performance, and (b) Dempster–Shafer theory (DST), to combine the multi-scale segmentation and classification results. Through the combination of three ML methods and the multi-scale approach, the framework enhanced landslide detection when it was tested for detecting earthquake-triggered landslides in Rasuwa district, Nepal. PlanetScope optical satellite images and a DEM were used, along with the derived landslide conditioning factors. Different accuracy assessment measures were used to compare the results against a field-based landslide inventory. All ML methods yielded the highest overall accuracies ranging from 83.3% to 87.2% when using objects with the optimal SP compared to other SPs. However, applying DST to combine the multi-scale results of each ML method significantly increased the overall accuracies to almost 90%. Overall, the integration of OBIA with ML methods resulted in appropriate landslide detections, but using the optimal SP and ML method is crucial for success.},
DOI = {10.3390/rs11212575}
}



@Article{su11216116,
AUTHOR = {Mangewa, Lazaro J. and Ndakidemi, Patrick A. and Munishi, Linus K.},
TITLE = {Integrating UAV Technology in an Ecological Monitoring System for Community Wildlife Management Areas in Tanzania},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {6116},
URL = {https://www.mdpi.com/2071-1050/11/21/6116},
ISSN = {2071-1050},
ABSTRACT = {Unmanned aerial vehicles (UAV) have recently emerged as a new remote sensing aerial platform, and they are seemingly advancing real-time data generation. Nonetheless, considerable uncertainties remain in the extent to which wildlife managers can integrate UAVs into ecological monitoring systems for wildlife and their habitats. In this review, we discuss the recent progress and gaps in UAV use in wildlife conservation and management. The review notes that there is scanty information on UAV use in ecological monitoring of medium-to-large mammals found in groups in heterogeneous habitats. We also explore the need and extent to which the technology can be integrated into ecological monitoring systems for mammals in heterogeneous habitats and in topographically-challenging community wildlife-management areas, as a complementary platform to the traditional techniques. Based on its ability to provide high-resolution images in real-time, further experiments on its wider use in the ecological monitoring of wildlife on a spatiotemporal scale are important. The experimentation outputs will make the UAV a very reliable remote sensing platform that addresses the challenges facing conventional techniques.},
DOI = {10.3390/su11216116}
}



@Article{rs11212579,
AUTHOR = {Carvajal-Ramírez, Fernando and Serrano, João Manuel Pereira Ramalho and Agüera-Vega, Francisco and Martínez-Carricondo, Patricio},
TITLE = {A Comparative Analysis of Phytovolume Estimation Methods Based on UAV-Photogrammetry and Multispectral Imagery in a Mediterranean Forest},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2579},
URL = {https://www.mdpi.com/2072-4292/11/21/2579},
ISSN = {2072-4292},
ABSTRACT = {Management and control operations are crucial for preventing forest fires, especially in Mediterranean forest areas with dry climatic periods. One of them is prescribed fires, in which the biomass fuel present in the controlled plot area must be accurately estimated. The most used methods for estimating biomass are time-consuming and demand too much manpower. Unmanned aerial vehicles (UAVs) carrying multispectral sensors can be used to carry out accurate indirect measurements of terrain and vegetation morphology and their radiometric characteristics. Based on the UAV-photogrammetric project products, four estimators of phytovolume were compared in a Mediterranean forest area, all obtained using the difference between a digital surface model (DSM) and a digital terrain model (DTM). The DSM was derived from a UAV-photogrammetric project based on the structure from a motion algorithm. Four different methods for obtaining a DTM were used based on an unclassified dense point cloud produced through a UAV-photogrammetric project (FFU), an unsupervised classified dense point cloud (FFC), a multispectral vegetation index (FMI), and a cloth simulation filter (FCS). Qualitative and quantitative comparisons determined the ability of the phytovolume estimators for vegetation detection and occupied volume. The results show that there are no significant differences in surface vegetation detection between all the pairwise possible comparisons of the four estimators at a 95% confidence level, but FMI presented the best kappa value (0.678) in an error matrix analysis with reference data obtained from photointerpretation and supervised classification. Concerning the accuracy of phytovolume estimation, only FFU and FFC presented differences higher than two standard deviations in a pairwise comparison, and FMI presented the best RMSE (12.3 m) when the estimators were compared to 768 observed data points grouped in four 500 m2 sample plots. The FMI was the best phytovolume estimator of the four compared for low vegetation height in a Mediterranean forest. The use of FMI based on UAV data provides accurate phytovolume estimations that can be applied on several environment management activities, including wildfire prevention. Multitemporal phytovolume estimations based on FMI could help to model the forest resources evolution in a very realistic way.},
DOI = {10.3390/rs11212579}
}



@Article{s19214794,
AUTHOR = {Rodriguez-Ramos, Alejandro and Alvarez-Fernandez, Adrian and Bavle, Hriday and Campoy, Pascual and How, Jonathan P.},
TITLE = {Vision-Based Multirotor Following Using Synthetic Learning Techniques},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4794},
URL = {https://www.mdpi.com/1424-8220/19/21/4794},
ISSN = {1424-8220},
ABSTRACT = {Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights).},
DOI = {10.3390/s19214794}
}



@Article{machines7040069,
AUTHOR = {Iannace, Gino and Ciaburro, Giuseppe and Trematerra, Amelia},
TITLE = {Wind Turbine Noise Prediction Using Random Forest Regression},
JOURNAL = {Machines},
VOLUME = {7},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/2075-1702/7/4/69},
ISSN = {2075-1702},
ABSTRACT = {Wind energy is one of the most widely used renewable energy sources in the world and has grown rapidly in recent years. However, the wind towers generate a noise that is perceived as an annoyance by the population living near the wind farms. It is therefore important to new tools that can help wind farm builders and the administrations. In this study, the measurements of the noise emitted by a wind farm and the data recorded by the supervisory control and data acquisition (SCADA) system were used to construct a prediction model. First, acoustic measurements and control system data have been analyzed to characterize the phenomenon. An appropriate number of observations were then extracted, and these data were pre-processed. Subsequently two models of prediction of sound pressure levels were built at the receiver: a model based on multiple linear regression, and a model based on Random Forest algorithm. As predictors wind speeds measured near the wind turbines and the active power of the turbines were selected. Both data were measured by the SCADA system of wind turbines. The model based on the Random Forest algorithm showed high values of the Pearson correlation coefficient (0.981), indicating a high number of correct predictions. This model can be extremely useful, both for the receiver and for the wind farm manager. Through the results of the model it will be possible to establish for which wind speed values the noise produced by wind turbines become dominant. Furthermore, the predictive model can give an overview of the noise produced by the receiver from the system in different operating conditions. Finally, the prediction model does not require the shutdown of the plant, a very expensive procedure due to the consequent loss of production.},
DOI = {10.3390/machines7040069}
}



@Article{s19224851,
AUTHOR = {Zhou, Jun and Tian, Yichen and Yuan, Chao and Yin, Kai and Yang, Guang and Wen, Meiping},
TITLE = {Improved UAV Opium Poppy Detection Using an Updated YOLOv3 Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4851},
URL = {https://www.mdpi.com/1424-8220/19/22/4851},
ISSN = {1424-8220},
ABSTRACT = {Rapid detection of illicit opium poppy plants using UAV (unmanned aerial vehicle) imagery has become an important means to prevent and combat crimes related to drug cultivation. However, current methods rely on time-consuming visual image interpretation. Here, the You Only Look Once version 3 (YOLOv3) network structure was used to assess the influence that different backbone networks have on the average precision and detection speed of an UAV-derived dataset of poppy imagery, with MobileNetv2 (MN) selected as the most suitable backbone network. A Spatial Pyramid Pooling (SPP) unit was introduced and Generalized Intersection over Union (GIoU) was used to calculate the coordinate loss. The resulting SPP-GIoU-YOLOv3-MN model improved the average precision by 1.62% (from 94.75% to 96.37%) without decreasing speed and achieved an average precision of 96.37%, with a detection speed of 29 FPS using an RTX 2080Ti platform. The sliding window method was used for detection in complete UAV images, which took approximately 2.2 sec/image, approximately 10&times; faster than visual interpretation. The proposed technique significantly improved the efficiency of poppy detection in UAV images while also maintaining a high detection accuracy. The proposed method is thus suitable for the rapid detection of illicit opium poppy cultivation in residential areas and farmland where UAVs with ordinary visible light cameras can be operated at low altitudes (relative height &lt; 200 m).},
DOI = {10.3390/s19224851}
}



@Article{s19224859,
AUTHOR = {Li, Mingfeng and Zhao, Lichen and Tan, Ding and Tong, Xiaozhe},
TITLE = {BLE Fingerprint Indoor Localization Algorithm Based on Eight-Neighborhood Template Matching},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4859},
URL = {https://www.mdpi.com/1424-8220/19/22/4859},
ISSN = {1424-8220},
ABSTRACT = {Aiming at the problem of indoor environment, signal non-line-of-sight propagation and other factors affect the accuracy of indoor locating, an algorithm of indoor fingerprint localization based on the eight-neighborhood template is proposed. Based on the analysis of the signal strength of adjacent reference points in the fingerprint database, the methods for the eight-neighborhood template matching and generation were studied. In this study, the indoor environment was divided into four quadrants for each access point and the expected values of the received signal strength indication (RSSI) difference between the center points and their eight-neighborhoods in different quadrants were chosen as the generation parameters. Then different templates were generated for different access points, and the unknown point was located by the Euclidean distance for the correlation of RSSI between each template and its coverage area in the fingerprint database. With the spatial correlation of fingerprint data taken into account, the influence of abnormal fingerprint on locating accuracy is reduced. The experimental results show that the locating error is 1.0 m, which is about 0.2 m less than both K-nearest neighbor (KNN) and weighted K-nearest neighbor (WKNN) algorithms.},
DOI = {10.3390/s19224859}
}



@Article{info10110348,
AUTHOR = {Triantafyllou, Anna and Sarigiannidis, Panagiotis and Bibi, Stamatia},
TITLE = {Precision Agriculture: A Remote Sensing Monitoring System Architecture},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {348},
URL = {https://www.mdpi.com/2078-2489/10/11/348},
ISSN = {2078-2489},
ABSTRACT = {Smart Farming is a development that emphasizes on the use of modern technologies in the cyber-physical field management cycle. Technologies such as the Internet of Things (IoT) and Cloud Computing have accelerated the digital transformation of the conventional agricultural practices promising increased production rate and product quality. The adoption of smart farming though is hampered because of the lack of models providing guidance to practitioners regarding the necessary components that constitute IoT-based monitoring systems. To guide the process of designing and implementing Smart farming monitoring systems, in this paper we propose a generic reference architecture model, taking also into consideration a very important non-functional requirement, the energy consumption restriction. Moreover, we present and discuss the technologies that incorporate the seven layers of the architecture model that are the Sensor Layer, the Link Layer, the Encapsulation Layer, the Middleware Layer, the Configuration Layer, the Management Layer and the Application Layer. Furthermore, the proposed Reference Architecture model is exemplified in a real-world application for surveying Saffron agriculture in Kozani, Greece.},
DOI = {10.3390/info10110348}
}



@Article{s19224895,
AUTHOR = {Silva, Maurício R. and Souza, Elitelma S. and Alsina, Pablo J. and Leite, Deyvid L. and Morais, Mateus R. and Pereira, Diego S. and Nascimento, Luís B. P. and Medeiros, Adelardo A. D. and Junior, Francisco H. Cunha and Nogueira, Marcelo B. and Albuquerque, Glauberto L. A. and Dantas, João B. D.},
TITLE = {Performance Evaluation of Multi-UAV Network Applied to Scanning Rocket Impact Area},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4895},
URL = {https://www.mdpi.com/1424-8220/19/22/4895},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a communication network for a squadron of unmanned aerial vehicles (UAVs) to be used in the scanning rocket impact area for Barreira do Inferno Launch Center&mdash;CLBI (Rio Grande do Norte, Brazil), aiming at detecting intruder boats. The main features of communication networks associated with multi-UAV systems are presented. This system sends information through Wireless Sensor Networks (WSN). After comparing and analyzing area scanning strategies, it presents the specification of a data communication network architecture for a squadron of UAVs within a sensor network using XBee Pro 900HP S3B modules. A brief description is made about the initial information from the construction of the system. The embedded hardware and the design procedure of a dedicated communication antenna to the XBee modules are presented. In order to evaluate the performance of the proposed architecture in terms of robustness and reliability, a set of experimental tests in different communication scenarios is carried out. Network management software is employed to measure the throughput, packet loss and other performance indicators in the communication links between the different network nodes. Experimental results allow verifying the quality and performance of the network nodes, as well as the reliability of the communication links, assessing signal received quality, range and latency.},
DOI = {10.3390/s19224895}
}



@Article{app9224826,
AUTHOR = {Tian, Furui and Zhao, Ying and Che, Xiangqian and Zhao, Yagebai and Xin, Dabo},
TITLE = {Concrete Crack Identification and Image Mosaic Based on Image Processing},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4826},
URL = {https://www.mdpi.com/2076-3417/9/22/4826},
ISSN = {2076-3417},
ABSTRACT = {Crack assessment is an essential process in bridge detection. In general, most non-contact crack detection techniques are not suitable for widespread use. The reason for this is that they all need to position the ruler at the inspection site in advance or calibrate the camera unit pixel size at a certain distance in a very intricate process. However, the object distance method in this paper can complete the calculation using only the crack image and the working distance, which are provided by an acquisition system equipped with a camera and laser range finder. First, the object distance method and the scale method are compared by calculating the crack width, and the results show that the object distance method is the more accurate method. Then, a double edge pixel statistical method is proposed to calculate the crack length, which solves the problem of redundant and missing pixels. In addition, the conventional mosaic algorithm is improved to realize an image mosaic for the more efficient splicing of crack images. Finally, a series of laboratory tests were conducted to verify the proposed approach. The experiments showed that the precision of crack length extraction can reach 92%, and the improved algorithm stitching precision can reach 98%.},
DOI = {10.3390/app9224826}
}



@Article{info10110349,
AUTHOR = {Tsouros, Dimosthenis C. and Bibi, Stamatia and Sarigiannidis, Panagiotis G.},
TITLE = {A Review on UAV-Based Applications for Precision Agriculture},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {349},
URL = {https://www.mdpi.com/2078-2489/10/11/349},
ISSN = {2078-2489},
ABSTRACT = {Emerging technologies such as Internet of Things (IoT) can provide significant potential in Smart Farming and Precision Agriculture applications, enabling the acquisition of real-time environmental data. IoT devices such as Unmanned Aerial Vehicles (UAVs) can be exploited in a variety of applications related to crops management, by capturing high spatial and temporal resolution images. These technologies are expected to revolutionize agriculture, enabling decision-making in days instead of weeks, promising significant reduction in cost and increase in the yield. Such decisions enable the effective application of farm inputs, supporting the four pillars of precision agriculture, i.e., apply the right practice, at the right place, at the right time and with the right quantity. However, the actual proliferation and exploitation of UAVs in Smart Farming has not been as robust as expected mainly due to the challenges confronted when selecting and deploying the relevant technologies, including the data acquisition and image processing methods. The main problem is that still there is no standardized workflow for the use of UAVs in such applications, as it is a relatively new area. In this article, we review the most recent applications of UAVs for Precision Agriculture. We discuss the most common applications, the types of UAVs exploited and then we focus on the data acquisition methods and technologies, appointing the benefits and drawbacks of each one. We also point out the most popular processing methods of aerial imagery and discuss the outcomes of each method and the potential applications of each one in the farming operations.},
DOI = {10.3390/info10110349}
}



@Article{ijgi8110511,
AUTHOR = {Yu, Hao and Wang, Lei and Wang, Zongming and Ren, Chunying and Zhang, Bai},
TITLE = {Using Landsat OLI and Random Forest to Assess Grassland Degradation with Aboveground Net Primary Production and Electrical Conductivity Data},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {511},
URL = {https://www.mdpi.com/2220-9964/8/11/511},
ISSN = {2220-9964},
ABSTRACT = {Grassland coverage, aboveground net primary production (ANPP), and species composition are used as indicators of grassland degradation. However, soil salinization deficiency, which is also a factor of grassland degradation, is rarely used in grassland degradation assessment in semiarid regions. We assessed grassland degradation by its quality, quantity, and spatial pattern over semiarid west Jilin, China. Considering soil salinization in west Jilin, electrical conductivity (EC) is used as an index with ANPP to assess grassland degradation. First, the spatial distribution of the grassland was measured with information mined from multi-temporal remote sensing images using an object-based image analysis combined with classification and decision tree methods. Second, with 166 field samples, we utilized the random forest (RF) algorithm as the variable selection and regression method for predicting EC and ANPP. Finally, we created a new grassland degradation model (GDM) based on ANPP and EC. The results showed the R2 (0.91) and RMSE (0.057 mS/cm) of the EC model were generally highest and lowest when the ntree was 400; the ANPP model was optimal (R2 = 0.85 and RMSE = 15.81 gC/m2) when the ntree was 600. Grassland area of west Jilin was 609.67 &times; 103 ha in 2017, there were 373.79 &times; 103 ha of degraded grassland, with 210.47 &times; 103 ha being intensively degraded. This paper surpasses past limitations of excessive reliance on vegetation index to construct a grassland degradation model which considers the characteristics of the study area and soil salinity. The results confirm the positive influence of the ecological conservation projects sponsored by the government. The research outcome could offer supporting data for decision making to help alleviate grassland degradation and promote the rehabilitation of grassland vegetation.},
DOI = {10.3390/ijgi8110511}
}



@Article{rs11222641,
AUTHOR = {Zhao, Longcai and Li, Qiangzi and Zhang, Yuan and Wang, Hongyan and Du, Xin},
TITLE = {Integrating the Continuous Wavelet Transform and a Convolutional Neural Network to Identify Vineyard Using Time Series Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2641},
URL = {https://www.mdpi.com/2072-4292/11/22/2641},
ISSN = {2072-4292},
ABSTRACT = {Grape is an economic crop of great importance and is widely cultivated in China. With the development of remote sensing, abundant data sources strongly guarantee that researchers can identify crop types and map their spatial distributions. However, to date, only a few studies have been conducted to identify vineyards using satellite image data. In this study, a vineyard is identified using satellite images, and a new approach is proposed that integrates the continuous wavelet transform (CWT) and a convolutional neural network (CNN). Specifically, the original time series of the normalized difference vegetation index (NDVI), enhanced vegetation index (EVI), and green chlorophyll vegetation index (GCVI) are reconstructed by applying an iterated Savitzky-Golay (S-G) method to form a daily time series for a full year; then, the CWT is applied to three reconstructed time series to generate corresponding scalograms; and finally, CNN technology is used to identify vineyards based on the stacked scalograms. In addition to our approach, a traditional and common approach that uses a random forest (RF) to identify crop types based on multi-temporal images is selected as the control group. The experimental results demonstrated the following: (i) the proposed approach was comprehensively superior to the RF approach; it improved the overall accuracy by 9.87% (up to 89.66%); (ii) the CWT had a stable and effective influence on the reconstructed time series, and the scalograms fully represented the unique time-related frequency pattern of each of the planting conditions; and (iii) the convolution and max pooling processing of the CNN captured the unique and subtle distribution patterns of the scalograms to distinguish vineyards from other crops. Additionally, the proposed approach is considered as able to be applied to other practical scenarios, such as using time series data to identify crop types, map landcover/land use, and is recommended to be tested in future practical applications.},
DOI = {10.3390/rs11222641}
}



@Article{s19224945,
AUTHOR = {Liu, Wenlei and Wu, Sentang and Wu, Zhongbo and Wu, Xiaolong},
TITLE = {Incremental Pose Map Optimization for Monocular Vision SLAM Based on Similarity Transformation},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4945},
URL = {https://www.mdpi.com/1424-8220/19/22/4945},
ISSN = {1424-8220},
ABSTRACT = {The novel contribution of this paper is to propose an incremental pose map optimization for monocular vision simultaneous localization and mapping (SLAM) based on similarity transformation, which can effectively solve the scale drift problem of SLAM for monocular vision and eliminate the cumulative error by global optimization. With the method of mixed inverse depth estimation based on a probability graph, the problem of the uncertainty of depth estimation is effectively solved and the robustness of depth estimation is improved. Firstly, this paper proposes a method combining the sparse direct method based on histogram equalization and the feature point method for front-end processing, and the mixed inverse depth estimation method based on a probability graph is used to estimate the depth information. Then, a bag-of-words model based on the mean initialization K-means is proposed for closed-loop feature detection. Finally, the incremental pose map optimization method based on similarity transformation is proposed to process the back end to optimize the pose and depth information of the camera. When the closed loop is detected, global optimization is carried out to effectively eliminate the cumulative error of the system. In this paper, indoor and outdoor environmental experiments are carried out using open data sets, such as TUM and KITTI, which fully proves the effectiveness of this method. Closed-loop detection experiments using hand-held cameras verify the importance of closed-loop detection. This method can effectively solve the scale drift problem of monocular vision SLAM and has strong robustness.},
DOI = {10.3390/s19224945}
}



@Article{app9224871,
AUTHOR = {Liu, Quan and Feng, Chen and Song, Zida and Louis, Joseph and Zhou, Jian},
TITLE = {Deep Learning Model Comparison for Vision-Based Classification of Full/Empty-Load Trucks in Earthmoving Operations},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4871},
URL = {https://www.mdpi.com/2076-3417/9/22/4871},
ISSN = {2076-3417},
ABSTRACT = {Earthmoving is an integral civil engineering operation of significance, and tracking its productivity requires the statistics of loads moved by dump trucks. Since current truck loads&rsquo; statistics methods are laborious, costly, and limited in application, this paper presents the framework of a novel, automated, non-contact field earthmoving quantity statistics (FEQS) for projects with large earthmoving demands that use uniform and uncovered trucks. The proposed FEQS framework utilizes field surveillance systems and adopts vision-based deep learning for full/empty-load truck classification as the core work. Since convolutional neural network (CNN) and its transfer learning (TL) forms are popular vision-based deep learning models and numerous in type, a comparison study is conducted to test the framework&rsquo;s core work feasibility and evaluate the performance of different deep learning models in implementation. The comparison study involved 12 CNN or CNN-TL models in full/empty-load truck classification, and the results revealed that while several provided satisfactory performance, the VGG16-FineTune provided the optimal performance. This proved the core work feasibility of the proposed FEQS framework. Further discussion provides model choice suggestions that CNN-TL models are more feasible than CNN prototypes, and models that adopt different TL methods have advantages in either working accuracy or speed for different tasks.},
DOI = {10.3390/app9224871}
}



@Article{s19225012,
AUTHOR = {Arshad, Bilal and Ogie, Robert and Barthelemy, Johan and Pradhan, Biswajeet and Verstaevel, Nicolas and Perez, Pascal},
TITLE = {Computer Vision and IoT-Based Sensors in Flood Monitoring and Mapping: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {5012},
URL = {https://www.mdpi.com/1424-8220/19/22/5012},
ISSN = {1424-8220},
ABSTRACT = {Floods are amongst the most common and devastating of all natural hazards. The alarming number of flood-related deaths and financial losses suffered annually across the world call for improved response to flood risks. Interestingly, the last decade has presented great opportunities with a series of scholarly activities exploring how camera images and wireless sensor data from Internet-of-Things (IoT) networks can improve flood management. This paper presents a systematic review of the literature regarding IoT-based sensors and computer vision applications in flood monitoring and mapping. The paper contributes by highlighting the main computer vision techniques and IoT sensor approaches utilised in the literature for real-time flood monitoring, flood modelling, mapping and early warning systems including the estimation of water level. The paper further contributes by providing recommendations for future research. In particular, the study recommends ways in which computer vision and IoT sensor techniques can be harnessed to better monitor and manage coastal lagoons&mdash;an aspect that is under-explored in the literature.},
DOI = {10.3390/s19225012}
}



@Article{rs11222700,
AUTHOR = {Wang, Wantian and Tang, Ziyue and Chen, Yichang and Zhang, Yuanpeng and Sun, Yongjian},
TITLE = {Aircraft Target Classification for Conventional Narrow-Band Radar with Multi-Wave Gates Sparse Echo Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2700},
URL = {https://www.mdpi.com/2072-4292/11/22/2700},
ISSN = {2072-4292},
ABSTRACT = {For a conventional narrow-band radar system, the detectable information of the target is limited, and it is difficult for the radar to accurately identify the target type. In particular, the classification probability will further decrease when part of the echo data is missed. By extracting the target features in time and frequency domains from multi-wave gates sparse echo data, this paper presents a classification algorithm in conventional narrow-band radar to identify three different types of aircraft target, i.e., helicopter, propeller and jet. Firstly, the classical sparse reconstruction algorithm is utilized to reconstruct the target frequency spectrum with single-wave gate sparse echo data. Then, the micro-Doppler effect caused by rotating parts of different targets is analyzed, and the micro-Doppler based features, such as amplitude deviation coefficient, time domain waveform entropy and frequency domain waveform entropy, are extracted from reconstructed echo data to identify targets. Thirdly, the target features extracted from multi-wave gates reconstructed echo data are weighted and fused to improve the accuracy of classification. Finally, the fused feature vectors are fed into a support vector machine (SVM) model for classification. By contrast with the conventional algorithm of aircraft target classification, the proposed algorithm can effectively process sparse echo data and achieve higher classification probability via weighted features fusion of multi-wave gates echo data. The experiments on synthetic data are carried out to validate the effectiveness of the proposed algorithm.},
DOI = {10.3390/rs11222700}
}



@Article{rs11222701,
AUTHOR = {Zheng, Yuhui and Song, Huihui and Sun, Le and Wu, Zebin and Jeon, Byeungwoo},
TITLE = {Spatiotemporal Fusion of Satellite Images via Very Deep Convolutional Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2701},
URL = {https://www.mdpi.com/2072-4292/11/22/2701},
ISSN = {2072-4292},
ABSTRACT = {Spatiotemporal fusion provides an effective way to fuse two types of remote sensing data featured by complementary spatial and temporal properties (typical representatives are Landsat and MODIS images) to generate fused data with both high spatial and temporal resolutions. This paper presents a very deep convolutional neural network (VDCN) based spatiotemporal fusion approach to effectively handle massive remote sensing data in practical applications. Compared with existing shallow learning methods, especially for the sparse representation based ones, the proposed VDCN-based model has the following merits: (1) explicitly correlating the MODIS and Landsat images by learning a non-linear mapping relationship; (2) automatically extracting effective image features; and (3) unifying the feature extraction, non-linear mapping, and image reconstruction into one optimization framework. In the training stage, we train a non-linear mapping between downsampled Landsat and MODIS data using VDCN, and then we train a multi-scale super-resolution (MSSR) VDCN between the original Landsat and downsampled Landsat data. The prediction procedure contains three layers, where each layer consists of a VDCN-based prediction and a fusion model. These layers achieve non-linear mapping from MODIS to downsampled Landsat data, the two-times SR of downsampled Landsat data, and the five-times SR of downsampled Landsat data, successively. Extensive evaluations are executed on two groups of commonly used Landsat&ndash;MODIS benchmark datasets. For the fusion results, the quantitative evaluations on all prediction dates and the visual effect on one key date demonstrate that the proposed approach achieves more accurate fusion results than sparse representation based methods.},
DOI = {10.3390/rs11222701}
}



@Article{rs11222704,
AUTHOR = {Goian, Abdulrahman and Ashour, Reem and Ahmad, Ubaid and Taha, Tarek and Almoosa, Nawaf and Seneviratne, Lakmal},
TITLE = {Victim Localization in USAR Scenario Exploiting Multi-Layer Mapping Structure},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2704},
URL = {https://www.mdpi.com/2072-4292/11/22/2704},
ISSN = {2072-4292},
ABSTRACT = {Urban search and rescue missions require rapid intervention to locate victims and survivors in the affected environments. To facilitate this activity, Unmanned Aerial Vehicles (UAVs) have been recently used to explore the environment and locate possible victims. In this paper, a UAV equipped with multiple complementary sensors is used to detect the presence of a human in an unknown environment. A novel human localization approach in unknown environments is proposed that merges information gathered from deep-learning-based human detection, wireless signal mapping, and thermal signature mapping to build an accurate global human location map. A next-best-view (NBV) approach with a proposed multi-objective utility function is used to iteratively evaluate the map to locate the presence of humans rapidly. Results demonstrate that the proposed strategy outperforms other methods in several performance measures such as the number of iterations, entropy reduction, and traveled distance.},
DOI = {10.3390/rs11222704}
}



@Article{s19225046,
AUTHOR = {Huang, Lvwen and Guo, Han and Rao, Qinqin and Hou, Zixia and Li, Shuqin and Qiu, Shicheng and Fan, Xinyun and Wang, Hongyan},
TITLE = {Body Dimension Measurements of Qinchuan Cattle with Transfer Learning from LiDAR Sensing},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {5046},
URL = {https://www.mdpi.com/1424-8220/19/22/5046},
ISSN = {1424-8220},
ABSTRACT = {For the time-consuming and stressful body measuring task of Qinchuan cattle and farmers, the demand for the automatic measurement of body dimensions has become more and more urgent. It is necessary to explore automatic measurements with deep learning to improve breeding efficiency and promote the development of industry. In this paper, a novel approach to measuring the body dimensions of live Qinchuan cattle with on transfer learning is proposed. Deep learning of the Kd-network was trained with classical three-dimensional (3D) point cloud datasets (PCD) of the ShapeNet datasets. After a series of processes of PCD sensed by the light detection and ranging (LiDAR) sensor, the cattle silhouettes could be extracted, which after augmentation could be applied as an input layer to the Kd-network. With the output of a convolutional layer of the trained deep model, the output layer of the deep model could be applied to pre-train the full connection network. The TrAdaBoost algorithm was employed to transfer the pre-trained convolutional layer and full connection of the deep model. To classify and recognize the PCD of the cattle silhouette, the average accuracy rate after training with transfer learning could reach up to 93.6%. On the basis of silhouette extraction, the candidate region of the feature surface shape could be extracted with mean curvature and Gaussian curvature. After the computation of the FPFH (fast point feature histogram) of the surface shape, the center of the feature surface could be recognized and the body dimensions of the cattle could finally be calculated. The experimental results showed that the comprehensive error of body dimensions was close to 2%, which could provide a feasible approach to the non-contact observations of the bodies of large physique livestock without any human intervention.},
DOI = {10.3390/s19225046}
}



@Article{f10111047,
AUTHOR = {Sun, Ying and Huang, Jianfeng and Ao, Zurui and Lao, Dazhao and Xin, Qinchuan},
TITLE = {Deep Learning Approaches for the Mapping of Tree Species Diversity in a Tropical Wetland Using Airborne LiDAR and High-Spatial-Resolution Remote Sensing Images},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1047},
URL = {https://www.mdpi.com/1999-4907/10/11/1047},
ISSN = {1999-4907},
ABSTRACT = {The monitoring of tree species diversity is important for forest or wetland ecosystem service maintenance or resource management. Remote sensing is an efficient alternative to traditional field work to map tree species diversity over large areas. Previous studies have used light detection and ranging (LiDAR) and imaging spectroscopy (hyperspectral or multispectral remote sensing) for species richness prediction. The recent development of very high spatial resolution (VHR) RGB images has enabled detailed characterization of canopies and forest structures. In this study, we developed a three-step workflow for mapping tree species diversity, the aim of which was to increase knowledge of tree species diversity assessment using deep learning in a tropical wetland (Haizhu Wetland) in South China based on VHR-RGB images and LiDAR points. Firstly, individual trees were detected based on a canopy height model (CHM, derived from LiDAR points) by the local-maxima-based method in the FUSION software (Version 3.70, Seattle, USA). Then, tree species at the individual tree level were identified via a patch-based image input method, which cropped the RGB images into small patches (the individually detected trees) based on the tree apexes detected. Three different deep learning methods (i.e., AlexNet, VGG16, and ResNet50) were modified to classify the tree species, as they can make good use of the spatial context information. Finally, four diversity indices, namely, the Margalef richness index, the Shannon&ndash;Wiener diversity index, the Simpson diversity index, and the Pielou evenness index, were calculated from the fixed subset with a size of 30 &times; 30 m for assessment. In the classification phase, VGG16 had the best performance, with an overall accuracy of 73.25% for 18 tree species. Based on the classification results, mapping of tree species diversity showed reasonable agreement with field survey data (R2Margalef = 0.4562, root-mean-square error RMSEMargalef = 0.5629; R2Shannon&ndash;Wiener = 0.7948, RMSEShannon&ndash;Wiener = 0.7202; R2Simpson = 0.7907, RMSESimpson = 0.1038; and R2Pielou = 0.5875, RMSEPielou = 0.3053). While challenges remain for individual tree detection and species classification, the deep-learning-based solution shows potential for mapping tree species diversity.},
DOI = {10.3390/f10111047}
}



@Article{rs11232751,
AUTHOR = {Bajocco, Sofia and Raparelli, Elisabetta and Teofili, Tommaso and Bascietto, Marco and Ricotta, Carlo},
TITLE = {Text Mining in Remotely Sensed Phenology Studies: A Review on Research Development, Main Topics, and Emerging Issues},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2751},
URL = {https://www.mdpi.com/2072-4292/11/23/2751},
ISSN = {2072-4292},
ABSTRACT = {As an interdisciplinary field of research, phenology is developing rapidly, and the contents of phenological research have become increasingly abundant. In addition, the potentiality of remote sensing technologies has largely contributed to the growth and complexity of this discipline, in terms of the scale of analysis, techniques of data processing, and a variety of topics. As a consequence, it is increasingly difficult for scientists to get a clear picture of remotely sensed phenology (rs+pheno) research. Bibliometric analysis is increasingly used for the study of a discipline and its conceptual dynamics. This review analyzed the last 40 years (1979&ndash;2018) of publications in the rs+pheno field retrieved from the Scopus database; such publications were investigated by means of a text mining approach, both in terms of bibliographic and text data. Results demonstrated that rs+pheno research is exponentially growing through time; however, it is primarily considered a subset of remote sensing science rather than a branch of phenology. In this framework, in the last decade, agriculture is becoming more and more a standalone science in rs+pheno research, independently from other related topics, e.g., classification. On the contrary, forestry struggles to gain its thematic role in rs+pheno studies and remains strictly connected with climate change issues. Classification and mapping represent the major rs+pheno topic, together with the extraction and the analysis of phenological metrics, like the start of the growing season. To the contrary, forest ecophysiology, in terms of ecosystem respiration and net ecosystem exchange, results as the most relevant new topic, together with the use of the red edge band and SAR (Synthetic Aperture Radar) data in rs+pheno agricultural studies. Some niche emerging rs+pheno topics may be recognized in the ocean and arctic investigations linked to phytoplankton blooming and ice cover dynamics. The findings of this study might be applicable for planning and managing remotely sensed phenology research; scientists involved in such discipline might use this study as a reference to consider their research domain in a broader dynamical network.},
DOI = {10.3390/rs11232751}
}



@Article{s19235133,
AUTHOR = {Lerro, Angelo and Brandl, Alberto and Battipede, Manuela and Gili, Piero},
TITLE = {Preliminary Design of a Model-Free Synthetic Sensor for Aerodynamic Angle Estimation for Commercial Aviation},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5133},
URL = {https://www.mdpi.com/1424-8220/19/23/5133},
ISSN = {1424-8220},
ABSTRACT = {Heterogeneity of the small aircraft category (e.g., small air transport (SAT), urban air mobility (UAM), unmanned aircraft system (UAS)), modern avionic solution (e.g., fly-by-wire (FBW)) and reduced aircraft (A/C) size require more compact, integrated, digital and modular air data system (ADS) able to measure data from the external environment. The MIDAS project, funded in the frame of the Clean Sky 2 program, aims to satisfy those recent requirements with an ADS certified for commercial applications. The main pillar lays on a smart fusion between COTS solutions and analytical sensors (patented technology) for the identification of the aerodynamic angles. The identification involves both flight dynamic relationships and data-driven state observer(s) based on neural techniques, which are deterministic once the training is completed. As this project will bring analytical sensors on board of civil aircraft as part of a redundant system for the very first time, design activities documented in this work have a particular focus on airworthiness certification aspects. At this maturity level, simulated data are used, real flight test data will be used in the next stages. Data collection is described both for the training and test aspects. Training maneuvers are defined aiming to excite all dynamic modes, whereas test maneuvers are collected aiming to validate results independently from the training set and all autopilot configurations. Results demonstrate that an alternate solution is possible enabling significant savings in terms of computational effort and lines of codes but they show, at the same time, that a better training strategy may be beneficial to cope with the new neural network architecture.},
DOI = {10.3390/s19235133}
}



@Article{rs11232757,
AUTHOR = {Ashapure, Akash and Jung, Jinha and Chang, Anjin and Oh, Sungchan and Maeda, Murilo and Landivar, Juan},
TITLE = {A Comparative Study of RGB and Multispectral Sensor-Based Cotton Canopy Cover Modelling Using Multi-Temporal UAS Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2757},
URL = {https://www.mdpi.com/2072-4292/11/23/2757},
ISSN = {2072-4292},
ABSTRACT = {This study presents a comparative study of multispectral and RGB (red, green, and blue) sensor-based cotton canopy cover modelling using multi-temporal unmanned aircraft systems (UAS) imagery. Additionally, a canopy cover model using an RGB sensor is proposed that combines an RGB-based vegetation index with morphological closing. The field experiment was established in 2017 and 2018, where the whole study area was divided into approximately 1 x 1 m size grids. Grid-wise percentage canopy cover was computed using both RGB and multispectral sensors over multiple flights during the growing season of the cotton crop. Initially, the normalized difference vegetation index (NDVI)-based canopy cover was estimated, and this was used as a reference for the comparison with RGB-based canopy cover estimations. To test the maximum achievable performance of RGB-based canopy cover estimation, a pixel-wise classification method was implemented. Later, four RGB-based canopy cover estimation methods were implemented using RGB images, namely Canopeo, the excessive greenness index, the modified red green vegetation index and the red green blue vegetation index. The performance of RGB-based canopy cover estimation was evaluated using NDVI-based canopy cover estimation. The multispectral sensor-based canopy cover model was considered to be a more stable and accurately estimating canopy cover model, whereas the RGB-based canopy cover model was very unstable and failed to identify canopy when cotton leaves changed color after canopy maturation. The application of a morphological closing operation after the thresholding significantly improved the RGB-based canopy cover modeling. The red green blue vegetation index turned out to be the most efficient vegetation index to extract canopy cover with very low average root mean square error (2.94% for the 2017 dataset and 2.82% for the 2018 dataset), with respect to multispectral sensor-based canopy cover estimation. The proposed canopy cover model provides an affordable alternate of the multispectral sensors which are more sensitive and expensive.},
DOI = {10.3390/rs11232757}
}



@Article{rs11232765,
AUTHOR = {Nex, Francesco and Duarte, Diogo and Tonolo, Fabio Giulio and Kerle, Norman},
TITLE = {Structural Building Damage Detection with Deep Learning: Assessment of a State-of-the-Art CNN in Operational Conditions},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2765},
URL = {https://www.mdpi.com/2072-4292/11/23/2765},
ISSN = {2072-4292},
ABSTRACT = {Remotely sensed data can provide the basis for timely and efficient building damage maps that are of fundamental importance to support the response activities following disaster events. However, the generation of these maps continues to be mainly based on the manual extraction of relevant information in operational frameworks. Considering the identification of visible structural damages caused by earthquakes and explosions, several recent works have shown that Convolutional Neural Networks (CNN) outperform traditional methods. However, the limited availability of publicly available image datasets depicting structural disaster damages, and the wide variety of sensors and spatial resolution used for these acquisitions (from space, aerial and UAV platforms), have limited the clarity of how these networks can effectively serve First Responder needs and emergency mapping service requirements. In this paper, an advanced CNN for visible structural damage detection is tested to shed some light on what deep learning networks can currently deliver, and its adoption in realistic operational conditions after earthquakes and explosions is critically discussed. The heterogeneous and large datasets collected by the authors covering different locations, spatial resolutions and platforms were used to assess the network performances in terms of transfer learning with specific regard to geographical transferability of the trained network to imagery acquired in different locations. The computational time needed to deliver these maps is also assessed. Results show that quality metrics are influenced by the composition of training samples used in the network. To promote their wider use, three pre-trained networks&mdash;optimized for satellite, airborne and UAV image spatial resolutions and viewing angles&mdash;are made freely available to the scientific community.},
DOI = {10.3390/rs11232765}
}



@Article{s19235149,
AUTHOR = {Gao, Bingbing and Hu, Gaoge and Zhu, Xinhe and Zhong, Yongmin},
TITLE = {A Robust Cubature Kalman Filter with Abnormal Observations Identification Using the Mahalanobis Distance Criterion for Vehicular INS/GNSS Integration},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5149},
URL = {https://www.mdpi.com/1424-8220/19/23/5149},
ISSN = {1424-8220},
ABSTRACT = {INS/GNSS (inertial navigation system/global navigation satellite system) integration is a promising solution of vehicle navigation for intelligent transportation systems. However, the observation of GNSS inevitably involves uncertainty due to the vulnerability to signal blockage in many urban/suburban areas, leading to the degraded navigation performance for INS/GNSS integration. This paper develops a novel robust CKF with scaling factor by combining the emerging cubature Kalman filter (CKF) with the concept of Mahalanobis distance criterion to address the above problem involved in nonlinear INS/GNSS integration. It establishes a theory of abnormal observations identification using the Mahalanobis distance criterion. Subsequently, a robust factor (scaling factor), which is calculated via the Mahalanobis distance criterion, is introduced into the standard CKF to inflate the observation noise covariance, resulting in a decreased filtering gain in the presence of abnormal observations. The proposed robust CKF can effectively resist the influence of abnormal observations on navigation solution and thus improves the robustness of CKF for vehicular INS/GNSS integration. Simulation and experimental results have demonstrated the effectiveness of the proposed robust CKF for vehicular navigation with INS/GNSS integration.},
DOI = {10.3390/s19235149}
}



@Article{rs11232787,
AUTHOR = {Zhang, Xiaokang and Shi, Wenzhong and Lv, Zhiyong and Peng, Feifei},
TITLE = {Land Cover Change Detection from High-Resolution Remote Sensing Imagery Using Multitemporal Deep Feature Collaborative Learning and a Semi-supervised Chan–Vese Model},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2787},
URL = {https://www.mdpi.com/2072-4292/11/23/2787},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a novel approach for automatically detecting land cover changes from multitemporal high-resolution remote sensing images in the deep feature space. This is accomplished by using multitemporal deep feature collaborative learning and a semi-supervised Chan&ndash;Vese (SCV) model. The multitemporal deep feature collaborative learning model is developed to obtain the multitemporal deep feature representations in the same high-level feature space and to improve the separability between changed and unchanged patterns. The deep difference feature map at the object-level is then extracted through a feature similarity measure. Based on the deep difference feature map, the SCV model is proposed to detect changes in which labeled patterns automatically derived from uncertainty analysis are integrated into the energy functional to efficiently drive the contour towards accurate boundaries of changed objects. The experimental results obtained on the four data sets acquired by different high-resolution sensors corroborate the effectiveness of the proposed approach.},
DOI = {10.3390/rs11232787}
}



@Article{s19235170,
AUTHOR = {Bithas, Petros S. and Michailidis, Emmanouel T. and Nomikos, Nikolaos and Vouyioukas, Demosthenes and Kanatas, Athanasios G.},
TITLE = {A Survey on Machine-Learning Techniques for UAV-Based Communications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5170},
URL = {https://www.mdpi.com/1424-8220/19/23/5170},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) will be an integral part of the next generation wireless communication networks. Their adoption in various communication-based applications is expected to improve coverage and spectral efficiency, as compared to traditional ground-based solutions. However, this new degree of freedom that will be included in the network will also add new challenges. In this context, the machine-learning (ML) framework is expected to provide solutions for the various problems that have already been identified when UAVs are used for communication purposes. In this article, we provide a detailed survey of all relevant research works, in which ML techniques have been used on UAV-based communications for improving various design and functional aspects such as channel modeling, resource management, positioning, and security.},
DOI = {10.3390/s19235170}
}



@Article{rs11232828,
AUTHOR = {Zhao, Nan and Ma, Ailong and Zhong, Yanfei and Zhao, Ji and Cao, Liqin},
TITLE = {Self-Training Classification Framework with Spatial-Contextual Information for Local Climate Zones},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2828},
URL = {https://www.mdpi.com/2072-4292/11/23/2828},
ISSN = {2072-4292},
ABSTRACT = {Local climate zones (LCZ) have become a generic criterion for climate analysis among global cities, as they can describe not only the urban climate but also the morphology inside the city. LCZ mapping based on the remote sensing classification method is a fundamental task, and the protocol proposed by the World Urban Database and Access Portal Tools (WUDAPT) project, which consists of random forest classification and filter-based spatial smoothing, is the most common approach. However, the classification and spatial smoothing lack a unified framework, which causes the appearance of small, isolated areas in the LCZ maps. In this paper, a spatial-contextual information-based self-training classification framework (SCSF) is proposed to solve this LCZ classification problem. In SCSF, conditional random field (CRF) is used to integrate the classification and spatial smoothing processing into one model and a self-training method is adopted, considering that the lack of sufficient expert-labeled training samples is always a big issue, especially for the complex LCZ scheme. Moreover, in the unary potentials of CRF modeling, pseudo-label selection using a self-training process is used to train the classifier, which fuses the regional spatial information through segmentation and the local neighborhood information through moving windows to provide a more reliable probabilistic classification map. In the pairwise potential function, SCSF can effectively improve the classification accuracy by integrating the spatial-contextual information through CRF. The experimental results prove that the proposed framework is efficient when compared to the traditional mapping product of WUDAPT in LCZ classification.},
DOI = {10.3390/rs11232828}
}



@Article{app9235187,
AUTHOR = {Zhou, Qiang and Li, Xin},
TITLE = {STN-Homography: Direct Estimation of Homography Parameters for Image Pairs},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5187},
URL = {https://www.mdpi.com/2076-3417/9/23/5187},
ISSN = {2076-3417},
ABSTRACT = {Estimating a 2D homography from a pair of images is a fundamental task in computer vision. Contrary to most convolutional neural network-based homography estimation methods that use alternative four-point homography parameterization schemes, in this study, we directly estimate the     3 &times; 3     homography matrix value. We show that after coordinate normalization, the magnitude difference and variance of the elements of the normalized     3 &times; 3     homography matrix is very small. Accordingly, we present STN-Homography, a neural network based on spatial transformer network (STN), to directly estimate the normalized homography matrix of an image pair. To decrease the homography estimation error, we propose hierarchical STN-Homography and sequence STN-homography models in which the sequence STN-Homography can be trained in an end-to-end manner. The effectiveness of the proposed methods is demonstrated based on experiments on the Microsoft common objects in context (MSCOCO) dataset, and it is shown that they significantly outperform the current state-of-the-art. The average processing time of the three-stage hierarchical STN-Homography and the three-stage sequence STN-Homography models on a GPU are 17.85 ms and 13.85 ms, respectively. Both models satisfy the real-time processing requirements of most potential applications.},
DOI = {10.3390/app9235187}
}



@Article{s19235270,
AUTHOR = {Wang, Yantian and Li, Haifeng and Jia, Peng and Zhang, Guo and Wang, Taoyang and Hao, Xiaoyun},
TITLE = {Multi-Scale DenseNets-Based Aircraft Detection from Remote Sensing Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5270},
URL = {https://www.mdpi.com/1424-8220/19/23/5270},
ISSN = {1424-8220},
ABSTRACT = {Deep learning-based aircraft detection methods have been increasingly implemented in recent years. However, due to the multi-resolution imaging modes, aircrafts in different images show very wide diversity on size, view and other visual features, which brings great challenges to detection. Although standard deep convolution neural networks (DCNN) can extract rich semantic features, they destroy the bottom-level location information. The features of small targets may also be submerged by redundant top-level features, resulting in poor detection. To address these problems, we proposed a compact multi-scale dense convolutional neural network (MS-DenseNet) for aircraft detection in remote sensing images. Herein, DenseNet was utilized for feature extraction, which enhances the propagation and reuse of the bottom-level high-resolution features. Subsequently, we combined feature pyramid network (FPN) with DenseNet to form a MS-DenseNet for learning multi-scale features, especially features of small objects. Finally, by compressing some of the unnecessary convolution layers of each dense block, we designed three new compact architectures: MS-DenseNet-41, MS-DenseNet-65, and MS-DenseNet-77. Comparative experiments showed that the compact MS-DenseNet-65 obtained a noticeable improvement in detecting small aircrafts and achieved state-of-the-art performance with a recall of 94% and an F1-score of 92.7% and cost less computational time. Furthermore, the experimental results on robustness of UCAS-AOD and RSOD datasets also indicate the good transferability of our method.},
DOI = {10.3390/s19235270}
}



@Article{s19235287,
AUTHOR = {Moreno-Armendáriz, Marco A. and Calvo, Hiram and Duchanoy, Carlos A. and López-Juárez, Anayantzin P. and Vargas-Monroy, Israel A. and Suarez-Castañon, Miguel Santiago},
TITLE = {Deep Green Diagnostics: Urban Green Space Analysis Using Deep Learning and Drone Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5287},
URL = {https://www.mdpi.com/1424-8220/19/23/5287},
ISSN = {1424-8220},
ABSTRACT = {Nowadays, more than half of the world’s population lives in urban areas, and this number continues increasing. Consequently, there are more and more scientific publications that analyze health problems of people associated with living in these highly urbanized locations. In particular, some of the recent work has focused on relating people’s health to the quality and quantity of urban green areas. In this context, and considering the huge amount of land area in large cities that must be supervised, our work seeks to develop a deep learning-based solution capable of determining the level of health of the land and to assess whether it is contaminated. The main purpose is to provide health institutions with software capable of creating updated maps that indicate where these phenomena are presented, as this information could be very useful to guide public health goals in large cities. Our software is released as open source code, and the data used for the experiments presented in this paper are also freely available.},
DOI = {10.3390/s19235287}
}



@Article{jmse7120438,
AUTHOR = {Wang, Le and Wu, Qing and Liu, Jialun and Li, Shijie and Negenborn, Rudy R.},
TITLE = {State-of-the-Art Research on Motion Control of Maritime Autonomous Surface Ships},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {7},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {438},
URL = {https://www.mdpi.com/2077-1312/7/12/438},
ISSN = {2077-1312},
ABSTRACT = {At present, with the development of waterborne transport vehicles, research on ship faces a new round of challenges in terms of intelligence and autonomy. The concept of maritime autonomous surface ships (MASS) has been put forward by the International Maritime Organization in 2017, in which MASS become the new focus of the waterborne transportation industry. This paper elaborates on the state-of-the-art research on motion control of MASS. Firstly, the characteristics and current research status of unmanned surface vessels in MASS and conventional ships are summarized, and the system composition of MASS is analyzed. In order to better realize the self-adaptability of the MASS motion control, the theory and algorithm of ship motion control-related systems are emphatically analyzed under the condition of classifying ship motion control. Especially, the application of intelligent algorithms in the ship control field is summarized and analyzed. Finally, this paper summarizes the challenges faced by MASS in the model establishment, motion control algorithms, and real ship experiments, and proposes the composition of MASS motion control system based on variable autonomous control strategy. Future researches on the accuracy and diversity of developments and applications to MASS motion control are suggested.},
DOI = {10.3390/jmse7120438}
}



@Article{rs11232858,
AUTHOR = {Ci, Tianyu and Liu, Zhen and Wang, Ying},
TITLE = {Assessment of the Degree of Building Damage Caused by Disaster Using Convolutional Neural Networks in Combination with Ordinal Regression},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2858},
URL = {https://www.mdpi.com/2072-4292/11/23/2858},
ISSN = {2072-4292},
ABSTRACT = {We propose a new convolutional neural networks method in combination with ordinal regression aiming at assessing the degree of building damage caused by earthquakes with aerial imagery. The ordinal regression model and a deep learning algorithm are incorporated to make full use of the information to improve the accuracy of the assessment. A new loss function was introduced in this paper to combine convolutional neural networks and ordinal regression. Assessing the level of damage to buildings can be considered as equivalent to predicting the ordered labels of buildings to be assessed. In the existing research, the problem has usually been simplified as a problem of pure classification to be further studied and discussed, which ignores the ordinal relationship between different levels of damage, resulting in a waste of information. Data accumulated throughout history are used to build network models for assessing the level of damage, and models for assessing levels of damage to buildings based on deep learning are described in detail, including model construction, implementation methods, and the selection of hyperparameters, and verification is conducted by experiments. When categorizing the damage to buildings into four types, we apply the method proposed in this paper to aerial images acquired from the 2014 Ludian earthquake and achieve an overall accuracy of 77.39%; when categorizing damage to buildings into two types, the overall accuracy of the model is 93.95%, exceeding such values in similar types of theories and methods.},
DOI = {10.3390/rs11232858}
}



@Article{su11236829,
AUTHOR = {Hasan, Umut and Sawut, Mamat and Chen, Shuisen},
TITLE = {Estimating the Leaf Area Index of Winter Wheat Based on Unmanned Aerial Vehicle RGB-Image Parameters},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {6829},
URL = {https://www.mdpi.com/2071-1050/11/23/6829},
ISSN = {2071-1050},
ABSTRACT = {The leaf area index (LAI) is not only an important parameter for monitoring crop growth, but also an important input parameter for crop yield prediction models and hydrological and climatic models. Several studies have recently been conducted to estimate crop LAI using unmanned aerial vehicle (UAV) multispectral and hyperspectral data. However, there are few studies on estimating the LAI of winter wheat using unmanned aerial vehicle (UAV) RGB images. In this study, we estimated the LAI of winter wheat at the jointing stage on simple farmland in Xinjiang, China, using parameters derived from UAV RGB images. According to gray correlation analysis, UAV RGB-image parameters such as the Visible Atmospherically Resistant Index (VARI), the Red Green Blue Vegetation Index (RGBVI), the Digital Number (DN) of Blue Channel (B) and the Green Leaf Algorithm (GLA) were selected to develop models for estimating the LAI of winter wheat. The results showed that it is feasible to use UAV RGB images for inverting and mapping the LAI of winter wheat at the jointing stage on the field scale, and the partial least squares regression (PLSR) model based on the VARI, RGBVI, B and GLA had the best prediction accuracy (R2 = 0.776, root mean square error (RMSE) = 0.468, residual prediction deviation (RPD) = 1.838) among all the regression models. To conclude, UAV RGB images not only have great potential in estimating the LAI of winter wheat, but also can provide more reliable and accurate data for precision agriculture management.},
DOI = {10.3390/su11236829}
}



@Article{rs11232869,
AUTHOR = {Cogato, Alessia and Pagay, Vinay and Marinello, Francesco and Meggio, Franco and Grace, Peter and De Antoni Migliorati, Massimiliano},
TITLE = {Assessing the Feasibility of Using Sentinel-2 Imagery to Quantify the Impact of Heatwaves on Irrigated Vineyards},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2869},
URL = {https://www.mdpi.com/2072-4292/11/23/2869},
ISSN = {2072-4292},
ABSTRACT = {Heatwaves are common in many viticultural regions of Australia. We evaluated the potential of satellite-based remote sensing to detect the effects of high temperatures on grapevines in a South Australian vineyard over the 2016&ndash;2017 and 2017&ndash;2018 seasons. The study involved: (i) comparing the normalized difference vegetation index (NDVI) from medium- and high-resolution satellite images; (ii) determining correlations between environmental conditions and vegetation indices (Vis); and (iii) identifying VIs that best indicate heatwave effects. Pearson&rsquo;s correlation and Bland&ndash;Altman testing showed a significant agreement between the NDVI of high- and medium-resolution imagery (R = 0.74, estimated difference &minus;0.093). The band and the VI most sensitive to changes in environmental conditions were 705 nm and enhanced vegetation index (EVI), both of which correlated with relative humidity (R = 0.65 and R = 0.62, respectively). Conversely, SWIR (short wave infrared, 1610 nm) exhibited a negative correlation with growing degree days (R = &minus;0.64). The analysis of heat stress showed that green and red edge bands&mdash;the chlorophyll absorption ratio index (CARI) and transformed chlorophyll absorption ratio index (TCARI)&mdash;were negatively correlated with thermal environmental parameters such as air and soil temperature and growing degree days (GDDs). The red and red edge bands&mdash;the soil-adjusted vegetation index (SAVI) and CARI2&mdash;were correlated with relative humidity. To the best of our knowledge, this is the first study demonstrating the effectiveness of using medium-resolution imagery for the detection of heat stress on grapevines in irrigated vineyards.},
DOI = {10.3390/rs11232869}
}



@Article{rs11232873,
AUTHOR = {Kayad, Ahmed and Sozzi, Marco and Gatto, Simone and Marinello, Francesco and Pirotti, Francesco},
TITLE = {Monitoring Within-Field Variability of Corn Yield using Sentinel-2 and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2873},
URL = {https://www.mdpi.com/2072-4292/11/23/2873},
ISSN = {2072-4292},
ABSTRACT = {Monitoring and prediction of within-field crop variability can support farmers to make the right decisions in different situations. The current advances in remote sensing and the availability of high resolution, high frequency, and free Sentinel-2 images improve the implementation of Precision Agriculture (PA) for a wider range of farmers. This study investigated the possibility of using vegetation indices (VIs) derived from Sentinel-2 images and machine learning techniques to assess corn (Zea mays) grain yield spatial variability within the field scale. A 22-ha study field in North Italy was monitored between 2016 and 2018; corn yield was measured and recorded by a grain yield monitor mounted on the harvester machine recording more than 20,000 georeferenced yield observation points from the study field for each season. VIs from a total of 34 Sentinel-2 images at different crop ages were analyzed for correlation with the measured yield observations. Multiple regression and two different machine learning approaches were also tested to model corn grain yield. The three main results were the following: (i) the Green Normalized Difference Vegetation Index (GNDVI) provided the highest R2 value of 0.48 for monitoring within-field variability of corn grain yield; (ii) the most suitable period for corn yield monitoring was a crop age between 105 and 135 days from the planting date (R4&ndash;R6); (iii) Random Forests was the most accurate machine learning approach for predicting within-field variability of corn yield, with an R2 value of almost 0.6 over an independent validation set of half of the total observations. Based on the results, within-field variability of corn yield for previous seasons could be investigated from archived Sentinel-2 data with GNDVI at crop stage (R4&ndash;R6).},
DOI = {10.3390/rs11232873}
}



@Article{s19245364,
AUTHOR = {Nagy, Balázs and Botzheim, János and Korondi, Péter},
TITLE = {Magnetic Angular Rate and Gravity Sensor Based Supervised Learning for Positioning Tasks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5364},
URL = {https://www.mdpi.com/1424-8220/19/24/5364},
ISSN = {1424-8220},
ABSTRACT = {This paper deals with sensor fusion of magnetic, angular rate and gravity sensor (MARG). The main contribution of this paper is the sensor fusion performed by supervised learning, which means parallel processing of the different kinds of measured data and estimating the position in periodic and non-periodic cases. During the learning phase, the position estimated by sensor fusion is compared with position data of a motion capture system. The main challenge is avoiding the error caused by the implicit integral calculation of MARG. There are several filter based signal processing methods for disturbance and noise estimation, which are calculated for each sensor separately. These classical methods can be used for disturbance and noise reduction and extracting hidden information from it as well. This paper examines the different types of noises and proposes a machine learning-based method for calculation of position and orientation directly from nine separate sensors. This method includes the disturbance and noise reduction in addition to sensor fusion. The proposed method was validated by experiments which provided promising results on periodic and translational motion as well.},
DOI = {10.3390/s19245364}
}



@Article{rs11242912,
AUTHOR = {Liu, Wei and Yang, MengYuan and Xie, Meng and Guo, Zihui and Li, ErZhu and Zhang, Lianpeng and Pei, Tao and Wang, Dong},
TITLE = {Accurate Building Extraction from Fused DSM and UAV Images Using a Chain Fully Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2912},
URL = {https://www.mdpi.com/2072-4292/11/24/2912},
ISSN = {2072-4292},
ABSTRACT = {Accurate extraction of buildings using high spatial resolution imagery is essential to a wide range of urban applications. However, it is difficult to extract semantic features from a variety of complex scenes (e.g., suburban, urban and urban village areas) because various complex man-made objects usually appear heterogeneous with large intra-class and low inter-class variations. The automatic extraction of buildings is thus extremely challenging. The fully convolutional neural networks (FCNs) developed in recent years have performed well in the extraction of urban man-made objects due to their ability to learn state-of-the-art features and to label pixels end-to-end. One of the most successful FCNs used in building extraction is U-net. However, the commonly used skip connection and feature fusion refinement modules in U-net often ignore the problem of feature selection, and the ability to extract smaller buildings and refine building boundaries needs to be improved. In this paper, we propose a trainable chain fully convolutional neural network (CFCN), which fuses high spatial resolution unmanned aerial vehicle (UAV) images and the digital surface model (DSM) for building extraction. Multilevel features are obtained from the fusion data, and an improved U-net is used for the coarse extraction of the building. To solve the problem of incomplete extraction of building boundaries, a U-net network is introduced by chain, which is used for the introduction of a coarse building boundary constraint, hole filling, and "speckle" removal. Typical areas such as suburban, urban, and urban villages were selected for building extraction experiments. The results show that the CFCN achieved recall of 98.67%, 98.62%, and 99.52% and intersection over union (IoU) of 96.23%, 96.43%, and 95.76% in suburban, urban, and urban village areas, respectively. Considering the IoU in conjunction with the CFCN and U-net resulted in improvements of 6.61%, 5.31%, and 6.45% in suburban, urban, and urban village areas, respectively. The proposed method can extract buildings with higher accuracy and with clearer and more complete boundaries.},
DOI = {10.3390/rs11242912}
}



@Article{rs11242925,
AUTHOR = {Prado Osco, Lucas and Marques Ramos, Ana Paula and Roberto Pereira, Danilo and Akemi Saito Moriya, Érika and Nobuhiro Imai, Nilton and Takashi Matsubara, Edson and Estrabis, Nayara and de Souza, Maurício and Marcato Junior, José and Gonçalves, Wesley Nunes and Li, Jonathan and Liesenberg, Veraldo and Eduardo Creste, José},
TITLE = {Predicting Canopy Nitrogen Content in Citrus-Trees Using Random Forest Algorithm Associated to Spectral Vegetation Indices from UAV-Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2925},
URL = {https://www.mdpi.com/2072-4292/11/24/2925},
ISSN = {2072-4292},
ABSTRACT = {The traditional method of measuring nitrogen content in plants is a time-consuming and labor-intensive task. Spectral vegetation indices extracted from unmanned aerial vehicle (UAV) images and machine learning algorithms have been proved effective in assisting nutritional analysis in plants. Still, this analysis has not considered the combination of spectral indices and machine learning algorithms to predict nitrogen in tree-canopy structures. This paper proposes a new framework to infer the nitrogen content in citrus-tree at a canopy-level using spectral vegetation indices processed with the random forest algorithm. A total of 33 spectral indices were estimated from multispectral images acquired with a UAV-based sensor. Leaf samples were gathered from different planting-fields and the leaf nitrogen content (LNC) was measured in the laboratory, and later converted into the canopy nitrogen content (CNC). To evaluate the robustness of the proposed framework, we compared it with other machine learning algorithms. We used 33,600 citrus trees to evaluate the performance of the machine learning models. The random forest algorithm had higher performance in predicting CNC than all models tested, reaching an R2 of 0.90, MAE of 0.341 g&middot;kg&minus;1 and MSE of 0.307 g&middot;kg&minus;1. We demonstrated that our approach is able to reduce the need for chemical analysis of the leaf tissue and optimizes citrus orchard CNC monitoring.},
DOI = {10.3390/rs11242925}
}



@Article{rs11242928,
AUTHOR = {Mondal, Pinki and Liu, Xue and Fatoyinbo, Temilola E. and Lagomasino, David},
TITLE = {Evaluating Combinations of Sentinel-2 Data and Machine-Learning Algorithms for Mangrove Mapping in West Africa},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2928},
URL = {https://www.mdpi.com/2072-4292/11/24/2928},
ISSN = {2072-4292},
ABSTRACT = {Creating a national baseline for natural resources, such as mangrove forests, and monitoring them regularly often requires a consistent and robust methodology. With freely available satellite data archives and cloud computing resources, it is now more accessible to conduct such large-scale monitoring and assessment. Yet, few studies examine the reproducibility of such mangrove monitoring frameworks, especially in terms of generating consistent spatial extent. Our objective was to evaluate a combination of image processing approaches to classify mangrove forests along the coast of Senegal and The Gambia. We used freely available global satellite data (Sentinel-2), and cloud computing platform (Google Earth Engine) to run two machine learning algorithms, random forest (RF), and classification and regression trees (CART). We calibrated and validated the algorithms using 800 reference points collected using high-resolution images. We further re-ran 10 iterations for each algorithm, utilizing unique subsets of the initial training data. While all iterations resulted in thematic mangrove maps with over 90% accuracy, the mangrove extent ranges between 827&ndash;2807 km2 for Senegal and 245&ndash;1271 km2 for The Gambia with one outlier for each country. We further report &ldquo;Places of Agreement&rdquo; (PoA) to identify areas where all iterations for both methods agree (506.6 km2 and 129.6 km2 for Senegal and The Gambia, respectively), thus have a high confidence in predicting mangrove extent. While we acknowledge the time- and cost-effectiveness of such methods for the landscape managers, we recommend utilizing them with utmost caution, as well as post-classification on-the-ground checks, especially for decision making.},
DOI = {10.3390/rs11242928}
}



@Article{rs11242934,
AUTHOR = {Zhou, Tao and Geng, Yajun and Chen, Jie and Sun, Chuanliang and Haase, Dagmar and Lausch, Angela},
TITLE = {Mapping of Soil Total Nitrogen Content in the Middle Reaches of the Heihe River Basin in China Using Multi-Source Remote Sensing-Derived Variables},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2934},
URL = {https://www.mdpi.com/2072-4292/11/24/2934},
ISSN = {2072-4292},
ABSTRACT = {Soil total nitrogen (STN) is an important indicator of soil quality and plays a key role in global nitrogen cycling. Accurate prediction of STN content is essential for the sustainable use of soil resources. Synthetic aperture radar (SAR) provides a promising source of data for soil monitoring because of its all-weather, all-day monitoring, but it has rarely been used for STN mapping. In this study, we explored the potential of multi-temporal Sentinel-1 data to predict STN by evaluating and comparing the performance of boosted regression trees (BRTs), random forest (RF), and support vector machine (SVM) models in STN mapping in the middle reaches of the Heihe River Basin in northwestern China. Fifteen predictor variables were used to construct models, including land use/land cover, multi-source remote sensing-derived variables, and topographic and climatic variables. We evaluated the prediction accuracy of the models based on a cross-validation procedure. Results showed that tree-based models (RF and BRT) outperformed SVM. Compared to the model that only used optical data, the addition of multi-temporal Sentinel-1A data using the BRT method improved the root mean square error (RMSE) and the mean absolute error (MAE) by 17.2% and 17.4%, respectively. Furthermore, the combination of all predictor variables using the BRT model had the best predictive performance, explaining 57% of the variation in STN, with the highest R2 (0.57) value and the lowest RMSE (0.24) and MAE (0.18) values. Remote sensing variables were the most important environmental variables for STN mapping, with 59% and 50% relative importance in the RF and BRT models, respectively. Our results show the potential of using multi-temporal Sentinel-1 data to predict STN, broadening the data source for future digital soil mapping. In addition, we propose that the SVM, RF, and BRT models should be calibrated and evaluated to obtain the best results for STN content mapping in similar landscapes.},
DOI = {10.3390/rs11242934}
}



@Article{rs11242942,
AUTHOR = {Blanco-Sacristán, Javier and Panigada, Cinzia and Tagliabue, Giulia and Gentili, Rodolfo and Colombo, Roberto and Ladrón de Guevara, Mónica and Maestre, Fernando T. and Rossini, Micol},
TITLE = {Spectral Diversity Successfully Estimates the α-Diversity of Biocrust-Forming Lichens},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2942},
URL = {https://www.mdpi.com/2072-4292/11/24/2942},
ISSN = {2072-4292},
ABSTRACT = {Biocrusts, topsoil communities formed by mosses, lichens, liverworts, algae, and cyanobacteria, are a key biotic component of dryland ecosystems worldwide. Experiments carried out with lichen- and moss-dominated biocrusts indicate that climate change may dramatically reduce their cover and diversity. Therefore, the development of reproducible methods to monitor changes in biocrust diversity and abundance across multiple spatio-temporal scales is key for evaluating how climate change may impact biocrust communities and the myriad of ecosystem functions and services that rely on them. In this study, we collected lichen-dominated biocrust samples from a semi-arid ecosystem in central Spain. Their &alpha;-diversity was then evaluated using very high spatial resolution hyperspectral images (pixel size of 0.091 mm) measured in laboratory under controlled conditions. Support vector machines were used to map the biocrust composition. Traditional &alpha;-diversity metrics (i.e., species richness, Shannon&rsquo;s, Simpson&rsquo;s, and Pielou&rsquo;s indices) were calculated using lichen fractional cover data derived from their classifications in the hyperspectral imagery. Spectral diversity was calculated at different wavelength ranges as the coefficient of variation of different regions of the reflectance spectra of lichens and as the standard deviation of the continuum removal algorithm (SD_CR). The accuracy of the classifications of the images obtained was close to 100%. The results showed the best coefficient of determination (r2 = 0.47) between SD_CR calculated at 680 nm and the &alpha;-diversity calculated as the Simpson&rsquo;s index, which includes species richness and their evenness. These findings indicate that this spectral diversity index could be used to track spatio-temporal changes in lichen-dominated biocrust communities. Thus, they are the first step to monitor &alpha;-diversity of biocrust-forming lichens at the ecosystem and regional levels, a key task for any program aiming to evaluate changes in biodiversity and associated ecosystem services in drylands.},
DOI = {10.3390/rs11242942}
}



@Article{rs11242952,
AUTHOR = {Natarajan, Sijesh and Basnayake, Jayampathi and Wei, Xianming and Lakshmanan, Prakash},
TITLE = {High-Throughput Phenotyping of Indirect Traits for Early-Stage Selection in Sugarcane Breeding},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2952},
URL = {https://www.mdpi.com/2072-4292/11/24/2952},
ISSN = {2072-4292},
ABSTRACT = {One of the major limitations for sugarcane genetic improvement is the low heritability of yield in the early stages of breeding, mainly due to confounding inter-plot competition effects. In this study, we investigate an indirect selection index (Si), developed based on traits correlated to yield (indirect traits) that were measured using an unmanned aerial vehicle (UAV), to improve clonal assessment in early stages of sugarcane breeding. A single-row early-stage clonal assessment trial, involving 2134 progenies derived from 245 crosses, and a multi-row experiment representative of pure-stand conditions, with an unrelated population of 40 genotypes, were used in this study. Both experiments were screened at several stages using visual, multispectral, and thermal sensors mounted on a UAV for indirect traits, including canopy cover, canopy height, canopy temperature, and normalised difference vegetation index (NDVI). To construct the indirect selection index, phenotypic and genotypic variance-covariances were estimated in the single-row and multi-row experiment, respectively. Clonal selection from the indirect selection index was compared to single-row yield-based selection. Ground observations of stalk number and plant height at six months after planting made from a subset of 75 clones within the single-row experiment were highly correlated to canopy cover (rg = 0.72) and canopy height (rg = 0.69), respectively. The indirect traits had high heritability and strong genetic correlation with cane yield in both the single-row and multi-row experiments. Only 45% of the clones were common between the indirect selection index and single-row yield based selection, and the expected efficiency of correlated response to selection for pure-stand yield based on indirect traits (44%&ndash;73%) was higher than that based on single-row yield (45%). These results highlight the potential of high-throughput phenotyping of indirect traits combined in an indirect selection index for improving early-stage clonal selections in sugarcane breeding.},
DOI = {10.3390/rs11242952}
}



@Article{rs11242953,
AUTHOR = {Rist, Florian and Gabriel, Doreen and Mack, Jennifer and Steinhage, Volker and Töpfer, Reinhard and Herzog, Katja},
TITLE = {Combination of an Automated 3D Field Phenotyping Workflow and Predictive Modelling for High-Throughput and Non-Invasive Phenotyping of Grape Bunches},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2953},
URL = {https://www.mdpi.com/2072-4292/11/24/2953},
ISSN = {2072-4292},
ABSTRACT = {In grapevine breeding, loose grape bunch architecture is one of the most important selection traits, contributing to an increased resilience towards Botrytis bunch rot. Grape bunch architecture is mainly influenced by the berry number, berry size, the total berry volume, and bunch width and length. For an objective, precise, and high-throughput assessment of these architectural traits, the 3D imaging sensor Artec&reg; Spider was applied to gather dense point clouds of the visible side of grape bunches directly in the field. Data acquisition in the field is much faster and non-destructive in comparison to lab applications but results in incomplete point clouds and, thus, mostly incomplete phenotypic values. Therefore, lab scans of whole bunches (360&deg;) were used as ground truth. We observed strong correlations between field and lab data but also shifts in mean and max values, especially for the berry number and total berry volume. For this reason, the present study is focused on the training and validation of different predictive regression models using 3D data from approximately 2000 different grape bunches in order to predict incomplete bunch traits from field data. Modeling concepts included simple linear regression and machine learning-based approaches. The support vector machine was the best and most robust regression model, predicting the phenotypic traits with an R2 of 0.70&ndash;0.91. As a breeding orientated proof-of-concept, we additionally performed a Quantitative Trait Loci (QTL)-analysis with both the field modeled and lab data. All types of data resulted in joint QTL regions, indicating that this innovative, fast, and non-destructive phenotyping method is also applicable for molecular marker development and grapevine breeding research.},
DOI = {10.3390/rs11242953}
}



@Article{s19245436,
AUTHOR = {Barbedo, Jayme Garcia Arnal and Koenigkan, Luciano Vieira and Santos, Thiago Teixeira and Santos, Patrícia Menezes},
TITLE = {A Study on the Detection of Cattle in UAV Images Using Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/1424-8220/19/24/5436},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are being increasingly viewed as valuable tools to aid the management of farms. This kind of technology can be particularly useful in the context of extensive cattle farming, as production areas tend to be expansive and animals tend to be more loosely monitored. With the advent of deep learning, and convolutional neural networks (CNNs) in particular, extracting relevant information from aerial images has become more effective. Despite the technological advancements in drone, imaging and machine learning technologies, the application of UAVs for cattle monitoring is far from being thoroughly studied, with many research gaps still remaining. In this context, the objectives of this study were threefold: (1) to determine the highest possible accuracy that could be achieved in the detection of animals of the Canchim breed, which is visually similar to the Nelore breed (Bos taurus indicus); (2) to determine the ideal ground sample distance (GSD) for animal detection; (3) to determine the most accurate CNN architecture for this specific problem. The experiments involved 1853 images containing 8629 samples of animals, and 15 different CNN architectures were tested. A total of 900 models were trained (15 CNN architectures &times; 3 spacial resolutions &times; 2 datasets &times; 10-fold cross validation), allowing for a deep analysis of the several aspects that impact the detection of cattle using aerial images captured using UAVs. Results revealed that many CNN architectures are robust enough to reliably detect animals in aerial images even under far from ideal conditions, indicating the viability of using UAVs for cattle monitoring.},
DOI = {10.3390/s19245436}
}



@Article{rs11242974,
AUTHOR = {Zhang, Youqiang and Cao, Guo and Li, Xuesong and Wang, Bisheng and Fu, Peng},
TITLE = {Active Semi-Supervised Random Forest for Hyperspectral Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2974},
URL = {https://www.mdpi.com/2072-4292/11/24/2974},
ISSN = {2072-4292},
ABSTRACT = {Random forest (RF) has obtained great success in hyperspectral image (HSI) classification. However, RF cannot leverage its full potential in the case of limited labeled samples. To address this issue, we propose a unified framework that embeds active learning (AL) and semi-supervised learning (SSL) into RF (ASSRF). Our aim is to utilize AL and SSL simultaneously to improve the performance of RF. The objective of the proposed method is to use a small number of manually labeled samples to train classifiers with relative high classification accuracy. To achieve this goal, a new query function is designed to query the most informative samples for manual labeling, and a new pseudolabeling strategy is introduced to select some samples for pseudolabeling. Compared with other AL- and SSL-based methods, the proposed method has several advantages. First, ASSRF utilizes the spatial information to construct a query function for AL, which can select more informative samples. Second, in addition to providing more labeled samples for SSL, the proposed pseudolabeling method avoids bias caused by AL-labeled samples. Finally, the proposed model retains the advantages of RF. To demonstrate the effectiveness of ASSRF, we conducted experiments on three real hyperspectral data sets. The experimental results have shown that our proposed method outperforms other state-of-the-art methods.},
DOI = {10.3390/rs11242974}
}



@Article{rs11242984,
AUTHOR = {Bishop-Taylor, Robbi and Sagar, Stephen and Lymburner, Leo and Alam, Imam and Sixsmith, Joshua},
TITLE = {Sub-Pixel Waterline Extraction: Characterising Accuracy and Sensitivity to Indices and Spectra},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2984},
URL = {https://www.mdpi.com/2072-4292/11/24/2984},
ISSN = {2072-4292},
ABSTRACT = {Accurately mapping the boundary between land and water (the &lsquo;waterline&rsquo;) is critical for tracking change in vulnerable coastal zones, and managing increasingly threatened water resources. Previous studies have largely relied on mapping waterlines at the pixel scale, or employed computationally intensive sub-pixel waterline extraction methods that are impractical to implement at scale. There is a pressing need for operational methods for extracting information from freely available medium resolution satellite imagery at spatial scales relevant to coastal and environmental management. In this study, we present a comprehensive evaluation of a promising method for mapping waterlines at sub-pixel accuracy from satellite remote sensing data. By combining a synthetic landscape approach with high resolution WorldView-2 satellite imagery, it was possible to rapidly assess the performance of the method across multiple coastal environments with contrasting spectral characteristics (sandy beaches, artificial shorelines, rocky shorelines, wetland vegetation and tidal mudflats), and under a range of water indices (Normalised Difference Water Index, Modified Normalised Difference Water Index, and the Automated Water Extraction Index) and thresholding approaches (optimal, zero and automated Otsu&rsquo;s method). The sub-pixel extraction method shows a strong ability to reproduce both absolute waterline positions and relative shape at a resolution that far exceeds that of traditional whole-pixel methods, particularly in environments without extreme contrast between the water and land (e.g., accuracies of up to 1.50&ndash;3.28 m at 30 m Landsat resolution using optimal water index thresholds). We discuss key challenges and limitations associated with selecting appropriate water indices and thresholds for sub-pixel waterline extraction, and suggest future directions for improving the accuracy and reliability of extracted waterlines. The sub-pixel waterline extraction method has a low computational overhead and is made available as an open-source tool, making it suitable for operational continental-scale or full time-depth analyses aimed at accurately mapping and monitoring dynamic waterlines through time and space.},
DOI = {10.3390/rs11242984}
}



@Article{s19245477,
AUTHOR = {Siebring, Jasper and Valente, João and Domingues Franceschini, Marston Heracles and Kamp, Jan and Kooistra, Lammert},
TITLE = {Object-Based Image Analysis Applied to Low Altitude Aerial Imagery for Potato Plant Trait Retrieval and Pathogen Detection},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5477},
URL = {https://www.mdpi.com/1424-8220/19/24/5477},
ISSN = {1424-8220},
ABSTRACT = {There is a growing demand in both food quality and quantity, but as of now, one-third of all food produced for human consumption is lost due to pests and other pathogens accounting for roughly 40% of pre-harvest loss in potatoes. Pathogens in potato plants, like the Erwinia bacteria and the PVYNTN virus for example, exhibit symptoms of varying severity that are not easily captured by pixel-based classes (as these ignore shape, texture, and context in general). The aim of this research is to develop an object-based image analysis (OBIA) method for trait retrieval of individual potato plants that maximizes information output from Unmanned Aerial Vehicle (UAV) RGB very high resolution (VHR) imagery and its derivatives, to be used for disease detection of the Solanum tuberosum. The approach proposed can be split in two steps: (1) object-based mapping of potato plants using an optimized implementation of large scale mean-shift segmentation (LSMSS), and (2) classification of disease using a random forest (RF) model for a set of morphological traits computed from their associative objects. The approach was proven viable as the associative RF model detected presence of Erwinia and PVY pathogens with a maximum F1 score of 0.75 and an average Matthews Correlation Coefficient (MCC) score of 0.47. It also shows that low-altitude imagery acquired with a commercial UAV is a viable off-the-shelf tool for precision farming, and potato pathogen detection.},
DOI = {10.3390/s19245477}
}



@Article{electronics8121532,
AUTHOR = {Wubben, Jamie and Fabra, Francisco and Calafate, Carlos T. and Krzeszowski, Tomasz and Marquez-Barja, Johann M. and Cano, Juan-Carlos and Manzoni, Pietro},
TITLE = {Accurate Landing of Unmanned Aerial Vehicles Using Ground Pattern Recognition},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1532},
URL = {https://www.mdpi.com/2079-9292/8/12/1532},
ISSN = {2079-9292},
ABSTRACT = {Over the last few years, several researchers have been developing protocols and applications in order to autonomously land unmanned aerial vehicles (UAVs). However, most of the proposed protocols rely on expensive equipment or do not satisfy the high precision needs of some UAV applications such as package retrieval and delivery or the compact landing of UAV swarms. Therefore, in this work, a solution for high precision landing based on the use of ArUco markers is presented. In the proposed solution, a UAV equipped with a low-cost camera is able to detect ArUco markers sized     56 &times; 56     cm from an altitude of up to 30 m. Once the marker is detected, the UAV changes its flight behavior in order to land on the exact position where the marker is located. The proposal was evaluated and validated using both the ArduSim simulation platform and real UAV flights. The results show an average offset of only 11 cm from the target position, which vastly improves the landing accuracy compared to the traditional GPS-based landing, which typically deviates from the intended target by 1 to 3 m.},
DOI = {10.3390/electronics8121532}
}



@Article{s19245479,
AUTHOR = {Rahnemoonfar, Maryam and Johnson, Jimmy and Paden, John},
TITLE = {AI Radar Sensor: Creating Radar Depth Sounder Images Based on Generative Adversarial Network},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5479},
URL = {https://www.mdpi.com/1424-8220/19/24/5479},
ISSN = {1424-8220},
ABSTRACT = {Significant resources have been spent in collecting and storing large and heterogeneous radar datasets during expensive Arctic and Antarctic fieldwork. The vast majority of data available is unlabeled, and the labeling process is both time-consuming and expensive. One possible alternative to the labeling process is the use of synthetically generated data with artificial intelligence. Instead of labeling real images, we can generate synthetic data based on arbitrary labels. In this way, training data can be quickly augmented with additional images. In this research, we evaluated the performance of synthetically generated radar images based on modified cycle-consistent adversarial networks. We conducted several experiments to test the quality of the generated radar imagery. We also tested the quality of a state-of-the-art contour detection algorithm on synthetic data and different combinations of real and synthetic data. Our experiments show that synthetic radar images generated by generative adversarial network (GAN) can be used in combination with real images for data augmentation and training of deep neural networks. However, the synthetic images generated by GANs cannot be used solely for training a neural network (training on synthetic and testing on real) as they cannot simulate all of the radar characteristics such as noise or Doppler effects. To the best of our knowledge, this is the first work in creating radar sounder imagery based on generative adversarial network.},
DOI = {10.3390/s19245479}
}



@Article{s19245480,
AUTHOR = {Abbas, Syed Muhammad and Aslam, Salman and Berns, Karsten and Muhammad, Abubakr},
TITLE = {Analysis and Improvements in AprilTag Based State Estimation},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5480},
URL = {https://www.mdpi.com/1424-8220/19/24/5480},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we analyzed the accuracy and precision of AprilTag as a visual fiducial marker in detail. We have analyzed error propagation along two horizontal axes along with the effect of angular rotation about the vertical axis. We have identified that the angular rotation of the camera (yaw angle) about its vertical axis is the primary source of error that decreases the precision to the point where the marker system is not potentially viable for sub-decimeter precise tasks. Other factors are the distance and viewing angle of the camera from the AprilTag. Based on these observations, three improvement steps have been proposed. One is the trigonometric correction of the yaw angle to point the camera towards the center of the tag. Second, the use of a custom-built yaw-axis gimbal, which tracks the center of the tag in real-time. Third, we have presented for the first time a pose-indexed probabilistic sensor error model of the AprilTag using a Gaussian Processes based regression of experimental data, validated by particle filter tracking. Our proposed approach, which can be deployed with all three improvement steps, increases the system&rsquo;s overall accuracy and precision by manifolds with a slight trade-off with execution time over commonly available AprilTag library. These proposed improvements make AprilTag suitable to be used as precision localization systems for outdoor and indoor applications.},
DOI = {10.3390/s19245480}
}



@Article{ijgi8120585,
AUTHOR = {Hadavandsiri, Zahra and Lichti, Derek D. and Jahraus, Adam and Jarron, David},
TITLE = {Concrete Preliminary Damage Inspection by Classification of Terrestrial Laser Scanner Point Clouds through Systematic Threshold Definition},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {585},
URL = {https://www.mdpi.com/2220-9964/8/12/585},
ISSN = {2220-9964},
ABSTRACT = {This paper presents a novel approach for automatic, preliminary detection of damage in concrete structures using ground-based terrestrial laser scanners. The method is based on computation of defect-sensitive features such as the surface curvature, since the surface roughness changes strongly if an area is affected by damage. A robust version of principal component analysis (PCA) classification is proposed to distinguish between structural damage and outliers present in the laser scanning data. Numerical simulations were conducted to develop a systematic point-wise defect classifier that automatically diagnoses the location of superficial damage on the investigated region. The method provides a complete picture of the surface health of concrete structures. It has been tested on two real datasets: a concrete heritage aqueduct in Brooks, Alberta, Canada; and a civil pedestrian concrete structure. The experiment results demonstrate the validity and accuracy of the proposed systematic framework for detecting and localizing areas of damage as small as 1 cm or less.},
DOI = {10.3390/ijgi8120585}
}



@Article{app9245477,
AUTHOR = {Ahn, Hyojung and Choi, Han-Lim and Kang, Minguk and Moon, SungTae},
TITLE = {Learning-Based Anomaly Detection and Monitoring for Swarm Drone Flights},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5477},
URL = {https://www.mdpi.com/2076-3417/9/24/5477},
ISSN = {2076-3417},
ABSTRACT = {This paper addresses anomaly detection and monitoring for swarm drone flights. While the current practice of swarm flight typically relies on the operator&rsquo;s naked eyes to monitor health of the multiple vehicles, this work proposes a machine learning-based framework to enable detection of abnormal behavior of a large number of flying drones on the fly. The method works in two steps: a sequence of two unsupervised learning procedures reduces the dimensionality of the real flight test data and labels them as normal and abnormal cases; then, a deep neural network classifier with one-dimensional convolution layers followed by fully connected multi-layer perceptron extracts the associated features and distinguishes the anomaly from normal conditions. The proposed anomaly detection scheme is validated on the real flight test data, highlighting its capability of online implementation.},
DOI = {10.3390/app9245477}
}



@Article{rs11242997,
AUTHOR = {Dechesne, Clément and Lefèvre, Sébastien and Vadaine, Rodolphe and Hajduch, Guillaume and Fablet, Ronan},
TITLE = {Ship Identification and Characterization in Sentinel-1 SAR Images with Multi-Task Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2997},
URL = {https://www.mdpi.com/2072-4292/11/24/2997},
ISSN = {2072-4292},
ABSTRACT = {The monitoring and surveillance of maritime activities are critical issues in both military and civilian fields, including among others fisheries&rsquo; monitoring, maritime traffic surveillance, coastal and at-sea safety operations, and tactical situations. In operational contexts, ship detection and identification is traditionally performed by a human observer who identifies all kinds of ships from a visual analysis of remotely sensed images. Such a task is very time consuming and cannot be conducted at a very large scale, while Sentinel-1 SAR data now provide a regular and worldwide coverage. Meanwhile, with the emergence of GPUs, deep learning methods are now established as state-of-the-art solutions for computer vision, replacing human intervention in many contexts. They have been shown to be adapted for ship detection, most often with very high resolution SAR or optical imagery. In this paper, we go one step further and investigate a deep neural network for the joint classification and characterization of ships from SAR Sentinel-1 data. We benefit from the synergies between AIS (Automatic Identification System) and Sentinel-1 data to build significant training datasets. We design a multi-task neural network architecture composed of one joint convolutional network connected to three task specific networks, namely for ship detection, classification, and length estimation. The experimental assessment shows that our network provides promising results, with accurate classification and length performance (classification overall accuracy: 97.25%, mean length error: 4.65 m &plusmn; 8.55 m).},
DOI = {10.3390/rs11242997}
}



@Article{agronomy9120885,
AUTHOR = {Méndez, Valeriano and Pérez-Romero, Antonio and Sola-Guirado, Rubén and Miranda-Fuentes, Antonio and Manzano-Agugliaro, Francisco and Zapata-Sierra, Antonio and Rodríguez-Lizana, Antonio},
TITLE = {In-Field Estimation of Orange Number and Size by 3D Laser Scanning},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {885},
URL = {https://www.mdpi.com/2073-4395/9/12/885},
ISSN = {2073-4395},
ABSTRACT = {The estimation of fruit load of an orchard prior to harvest is useful for planning harvest logistics and trading decisions. The manual fruit counting and the determination of the harvesting capacity of the field results are expensive and time-consuming. The automatic counting of fruits and their geometry characterization with 3D LiDAR models can be an interesting alternative. Field research has been conducted in the province of Cordoba (Southern Spain) on 24 &lsquo;Salustiana&rsquo; variety orange trees&mdash;Citrus sinensis (L.) Osbeck&mdash;(12 were pruned and 12 unpruned). Harvest size and the number of each fruit were registered. Likewise, the unitary weight of the fruits and their diameter were determined (N = 160). The orange trees were also modelled with 3D LiDAR with colour capture for their subsequent segmentation and fruit detection by using a K-means algorithm. In the case of pruned trees, a significant regression was obtained between the real and modelled fruit number (R2 = 0.63, p = 0.01). The opposite case occurred in the unpruned ones (p = 0.18) due to a leaf occlusion problem. The mean diameters proportioned by the algorithm (72.15 &plusmn; 22.62 mm) did not present significant differences (p = 0.35) with the ones measured on fruits (72.68 &plusmn; 5.728 mm). Even though the use of 3D LiDAR scans is time-consuming, the harvest size estimation obtained in this research is very accurate.},
DOI = {10.3390/agronomy9120885}
}



@Article{rs11243001,
AUTHOR = {Abdalla, Alwaseela and Cen, Haiyan and Abdel-Rahman, Elfatih and Wan, Liang and He, Yong},
TITLE = {Color Calibration of Proximal Sensing RGB Images of Oilseed Rape Canopy via Deep Learning Combined with K-Means Algorithm},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {3001},
URL = {https://www.mdpi.com/2072-4292/11/24/3001},
ISSN = {2072-4292},
ABSTRACT = {Plant color is a key feature for estimating parameters of the plant grown under different conditions using remote sensing images. In this case, the variation in plant color should be only due to the influence of the growing conditions and not due to external confounding factors like a light source. Hence, the impact of the light source in plant color should be alleviated using color calibration algorithms. This study aims to develop an efficient, robust, and cutting-edge approach for automatic color calibration of three-band (red green blue: RGB) images. Specifically, we combined the k-means model and deep learning for accurate color calibration matrix (CCM) estimation. A dataset of 3150 RGB images for oilseed rape was collected by a proximal sensing technique under varying illumination conditions and used to train, validate, and test our proposed framework. Firstly, we manually derived CCMs by mapping RGB color values of each patch of a color chart obtained in an image to standard RGB (sRGB) color values of that chart. Secondly, we grouped the images into clusters according to the CCM assigned to each image using the unsupervised k-means algorithm. Thirdly, the images with the new cluster labels were used to train and validate the deep learning convolutional neural network (CNN) algorithm for an automatic CCM estimation. Finally, the estimated CCM was applied to the input image to obtain an image with a calibrated color. The performance of our model for estimating CCM was evaluated using the Euclidean distance between the standard and the estimated color values of the test dataset. The experimental results showed that our deep learning framework can efficiently extract useful low-level features for discriminating images with inconsistent colors and achieved overall training and validation accuracies of 98.00% and 98.53%, respectively. Further, the final CCM provided an average Euclidean distance of 16.23 &Delta;&Epsilon; and outperformed the previously reported methods. This proposed technique can be used in real-time plant phenotyping at multiscale levels.},
DOI = {10.3390/rs11243001}
}



@Article{s19245507,
AUTHOR = {Jenal, Alexander and Bareth, Georg and Bolten, Andreas and Kneer, Caspar and Weber, Immanuel and Bongartz, Jens},
TITLE = {Development of a VNIR/SWIR Multispectral Imaging System for Vegetation Monitoring with Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5507},
URL = {https://www.mdpi.com/1424-8220/19/24/5507},
ISSN = {1424-8220},
ABSTRACT = {Short-wave infrared (SWIR) imaging systems with unmanned aerial vehicles (UAVs) are rarely used for remote sensing applications, like for vegetation monitoring. The reasons are that in the past, sensor systems covering the SWIR range were too expensive, too heavy, or not performing well enough, as, in contrast, it is the case in the visible and near-infrared range (VNIR). Therefore, our main objective is the development of a novel modular two-channel multispectral imaging system with a broad spectral sensitivity from the visible to the short-wave infrared spectrum (approx. 400 nm to 1700 nm) that is compact, lightweight and energy-efficient enough for UAV-based remote sensing applications. Various established vegetation indices (VIs) for mapping vegetation traits can then be set up by selecting any suitable filter combination. The study describes the selection of the individual components, starting with suitable camera modules, the optical as well as the control and storage parts. Special bandpass filters are used to select the desired wavelengths to be captured. A unique flange system has been developed, which also allows the filters to be interchanged quickly in order to adapt the system to a new application in a short time. The characterization of the system was performed in the laboratory with an integrating sphere and a climatic chamber. Finally, the integration of the novel modular VNIR/SWIR imaging system into a UAV and a subsequent first outdoor test flight, in which the functionality was tested, are described.},
DOI = {10.3390/s19245507}
}



@Article{w11122633,
AUTHOR = {Yang, Shengtian and Wang, Juan and Wang, Pengfei and Gong, Tongliang and Liu, Huiping},
TITLE = {Low Altitude Unmanned Aerial Vehicles (UAVs) and Satellite Remote Sensing Are Used to Calculated River Discharge Attenuation Coefficients of Ungauged Catchments in Arid Desert},
JOURNAL = {Water},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2633},
URL = {https://www.mdpi.com/2073-4441/11/12/2633},
ISSN = {2073-4441},
ABSTRACT = {The arid desert ecosystem is very fragile, and the change of its river discharge has a direct impact on irrigation and natural environment. River discharge attenuation coefficients is a key index to reveal the stability of desert river ecosystem. However, due to the harsh conditions in desert areas, it is difficult to establish a hydrological station to obtain data and calculate the attenuation coefficients, so it is urgent to develop new methods to master the attenuation coefficients of rivers. In this study, Taklamakan desert river was selected as the research area, and the river discharge of the desert river were estimated by combining low-altitude UAV and satellite remote sensing technology, so as to calculate the attenuation status of the river in its natural state. Combined with satellite remote sensing, the surface runoff in the desert reaches of the Hotan River from 1993 to 2017 were estimated. The results showed that the base of runoff attenuation in the lower reaches of the Hotan River is 40%. Coupled UAV and satellite remote sensing technology can provide technical support for the study of surface runoff in desert rivers within ungauged basins. Using UAV and satellite remote sensing can monitor surface runoff effectively providing important reference for river discharge monitoring in ungauged catchments.},
DOI = {10.3390/w11122633}
}



@Article{rs11243012,
AUTHOR = {Zhao, Licheng and Shi, Yun and Liu, Bin and Hovis, Ciara and Duan, Yulin and Shi, Zhongchao},
TITLE = {Finer Classification of Crops by Fusing UAV Images and Sentinel-2A Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {3012},
URL = {https://www.mdpi.com/2072-4292/11/24/3012},
ISSN = {2072-4292},
ABSTRACT = {Accurate crop distribution maps provide important information for crop censuses, yield monitoring and agricultural insurance assessments. Most existing studies apply low spatial resolution satellite images for crop distribution mapping, even in areas with a fragmented landscape. Unmanned aerial vehicle (UAV) imagery provides an alternative imagery source for crop mapping, yet its spectral resolution is usually lower than satellite images. In order to produce more accurate maps without losing any spatial heterogeneity (e.g., the physical boundary of land parcel), this study fuses Sentinel-2A and UAV images to map crop distribution at a finer spatial scale (i.e., land parcel scale) in an experimental site with various cropping patterns in Heilongjiang Province, Northeast China. Using a random forest algorithm, the original, as well as the fused images, are classified into 10 categories: rice, corn, soybean, buckwheat, other vegetations, greenhouses, bare land, water, roads and houses. In addition, we test the effect of UAV image choice by fusing Sentinel-2A with different UAV images at multiples spatial resolutions: 0.03 m, 0.10 m, 0.50 m, 1.00 m and 3.00 m. Overall, the fused images achieved higher classification accuracies, ranging between 10.58% and 16.39%, than the original images. However, the fused image based on the finest UAV image (i.e., 0.03 m) does not result in the highest accuracy. Instead, the 0.10 m spatial resolution UAV image produced the most accurate map. When the spatial resolution is less than 0.10 m, accuracy decreases gradually as spatial resolution decreases. The results of this paper not only indicate the possibility of combining satellite images and UAV images for land parcel level crop mapping for fragmented landscapes, but it also implies a potential scheme to exploit optimal choice of spatial resolution in fusing UAV images and Sentinel-2A, with little to no adverse side-effects.},
DOI = {10.3390/rs11243012}
}



@Article{s19245527,
AUTHOR = {Korošak, Žiga and Suhadolnik, Nejc and Pleteršek, Anton},
TITLE = {The Implementation of a Low Power Environmental Monitoring and Soil Moisture Measurement System Based on UHF RFID},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5527},
URL = {https://www.mdpi.com/1424-8220/19/24/5527},
ISSN = {1424-8220},
ABSTRACT = {A smart sensor label based on the integration of ultra high frequency (UHF) radio frequency identification (RFID) technology and sensors is presented. The label is composed of a semi-active system that measures temperature, light, relative humidity and gravimetric water content (GWC) in the soil. The deployed system provides a simple, cost effective solution to monitor and control the growing of plants in modern agriculture and is intended be a part of a smart wireless sensor network (WSN) for agricultural monitoring. This paper is focused on analysis and development of a moisture sensor to measure GWC. It is based on a capacitance measurement solution, the accuracy of which is enhanced using several sensor driving frequencies. Thanks to the cancellation of supply voltage variations, the modeling of the GWC sensor and readout circuit was correct. The results we measured were close to modeled values. The maximum measurement resolution of the capacitive moisture sensor was 0.07 pF. To get the GWC from measured capacitance, a scale was used to weigh the mass of water in the soil. The comparison between capacitance measurement and calculated soil GWC is presented. The RFID measurement system has energy harvesting capabilities and an ultra-low power microcontroller, which uses embedded software to control the measurement properties. The microcontroller has to choose the appropriate model depending on the measured amplitude and chosen frequency to calculate the actual voltage on the sensing capacitor.},
DOI = {10.3390/s19245527}
}



@Article{s19245529,
AUTHOR = {Tan, Xiaopeng and Su, Shaojing and Zuo, Zhen and Guo, Xiaojun and Sun, Xiaoyong},
TITLE = {Intrusion Detection of UAVs Based on the Deep Belief Network Optimized by PSO},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5529},
URL = {https://www.mdpi.com/1424-8220/19/24/5529},
ISSN = {1424-8220},
ABSTRACT = {With the rapid development of information technology, the problem of the network security of unmanned aerial vehicles (UAVs) has become increasingly prominent. In order to solve the intrusion detection problem of massive, high-dimensional, and nonlinear data, this paper proposes an intrusion detection method based on the deep belief network (DBN) optimized by particle swarm optimization (PSO). First, a classification model based on the DBN is constructed, and the PSO algorithm is then used to optimize the number of hidden layer nodes of the DBN, to obtain the optimal DBN structure. The simulations are conducted on a benchmark intrusion dataset, and the results show that the accuracy of the DBN-PSO algorithm reaches 92.44%, which is higher than those of the support vector machine (SVM), artificial neural network (ANN), deep neural network (DNN), and Adaboost. It can be seen from comparative experiments that the optimization effect of PSO is better than those of the genetic algorithm, simulated annealing algorithm, and Bayesian optimization algorithm. The method of PSO-DBN provides an effective solution to the problem of intrusion detection of UAV networks.},
DOI = {10.3390/s19245529}
}



@Article{app9245524,
AUTHOR = {Cao, Dongju and Yang, Wendong and Xu, Gangyi},
TITLE = {Joint Trajectory and Communication Design for Buffer-Aided Multi-UAV Relaying Networks},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5524},
URL = {https://www.mdpi.com/2076-3417/9/24/5524},
ISSN = {2076-3417},
ABSTRACT = {With the rapid development and evolvement of unmanned aerial vehicle (UAV) technology, UAV aided wireless communication technology has been widely studied recently. In this paper, a buffer aided multi-UAV relaying network is investigated to assist blocked ground communication. According to the mobility and implementation flexibility of UAV relays, it is assumed that the communication link between air-to-ground is the Rician fading channel. On the basis of information causality, we derive the state change of the information in the buffer of UAV relays and maximize the end-to-end average throughput by join the relay selection, UAV transmit power, and UAV trajectory optimization. However, the considered problem is a mixed integer non-convex optimization problem, and therefore, it is difficult to solve directly with general optimization methods. In order to make the problem tractable, an efficient iterative algorithm based on the block coordinate descent and the successive convex optimization techniques is proposed. The convergence of the proposed algorithm will be verified analytically at the end of this paper. The simulation results show that by alternately optimizing the relay selection, UAV transmit power, and UAV trajectory, the proposed algorithm is able to achieve convergence quickly and significantly improve the average throughput, as compared to other benchmark schemes.},
DOI = {10.3390/app9245524}
}



@Article{s19245547,
AUTHOR = {Kram, Sebastian and Stahlke, Maximilian and Feigl, Tobias and Seitz, Jochen and Thielecke, Jörn},
TITLE = {UWB Channel Impulse Responses for Positioning in Complex Environments: A Detailed Feature Analysis},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5547},
URL = {https://www.mdpi.com/1424-8220/19/24/5547},
ISSN = {1424-8220},
ABSTRACT = {Radio signal-based positioning in environments with complex propagation paths is a challenging task for classical positioning methods. For example, in a typical industrial environment, objects such as machines and workpieces cause reflections, diffractions, and absorptions, which are not taken into account by classical lateration methods and may lead to erroneous positions. Only a few data-driven methods developed in recent years can deal with these irregularities in the propagation paths or use them as additional information for positioning. These methods exploit the channel impulse responses (CIR) that are detected by ultra-wideband radio systems for positioning. These CIRs embed the signal properties of the underlying propagation paths that represent the environment. This article describes a feature-based localization approach that exploits machine-learning to derive characteristic information of the CIR signal for positioning. The approach is complete without highly time-synchronized receiver or arrival times. Various features were investigated based on signal propagation models for complex environments. These features were then assessed qualitatively based on their spatial relationship to objects and their contribution to a more accurate position estimation. Three datasets collected in environments of varying degrees of complexity were analyzed. The evaluation of the experiments showed that a clear relationship between the features and the environment indicates that features in complex propagation environments improve positional accuracy. A quantitative assessment of the features was made based on a hierarchical classification of stratified regions within the environment. Classification accuracies of over 90% could be achieved for region sizes of about 0.1 m     2    . An application-driven evaluation was made to distinguish between different screwing processes on a car door based on CIR measures. While in a static environment, even with a single infrastructure tag, nearly error-free classification could be achieved, the accuracy of changes in the environment decreases rapidly. To adapt to changes in the environment, the models were retrained with a small amount of CIR data. This increased performance considerably. The proposed approach results in highly accurate classification, even with a reduced infrastructure of one or two tags, and is easily adaptable to new environments. In addition, the approach does not require calibration or synchronization of the positioning system or the installation of a reference system.},
DOI = {10.3390/s19245547}
}



@Article{s19245558,
AUTHOR = {Chen, Yayong and Hou, Chaojun and Tang, Yu and Zhuang, Jiajun and Lin, Jintian and He, Yong and Guo, Qiwei and Zhong, Zhenyu and Lei, Huan and Luo, Shaoming},
TITLE = {Citrus Tree Segmentation from UAV Images Based on Monocular Machine Vision in a Natural Orchard Environment},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5558},
URL = {https://www.mdpi.com/1424-8220/19/24/5558},
ISSN = {1424-8220},
ABSTRACT = {The segmentation of citrus trees in a natural orchard environment is a key technology for achieving the fully autonomous operation of agricultural unmanned aerial vehicles (UAVs). Therefore, a tree segmentation method based on monocular machine vision technology and a support vector machine (SVM) algorithm are proposed in this paper to segment citrus trees precisely under different brightness and weed coverage conditions. To reduce the sensitivity to environmental brightness, a selective illumination histogram equalization method was developed to compensate for the illumination, thereby improving the brightness contrast for the foreground without changing its hue and saturation. To accurately differentiate fruit trees from different weed coverage backgrounds, a chromatic aberration segmentation algorithm and the Otsu threshold method were combined to extract potential fruit tree regions. Then, 14 color features, five statistical texture features, and local binary pattern features of those regions were calculated to establish an SVM segmentation model. The proposed method was verified on a dataset with different brightness and weed coverage conditions, and the results show that the citrus tree segmentation accuracy reached 85.27% &plusmn; 9.43%; thus, the proposed method achieved better performance than two similar methods.},
DOI = {10.3390/s19245558}
}



@Article{rs12010024,
AUTHOR = {Liu, Meng and Popescu, Sorin and Malambo, Lonesome},
TITLE = {Feasibility of Burned Area Mapping Based on ICESAT−2 Photon Counting Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {24},
URL = {https://www.mdpi.com/2072-4292/12/1/24},
ISSN = {2072-4292},
ABSTRACT = {Accurately mapping burned areas is crucial for the analysis of carbon emissions and wildfire risk as well as understanding the effects of climate change on forest structure. Burned areas have predominantly been mapped using optical remote sensing images. However, the structural changes due to fire also offer opportunities for mapping burned areas using three-dimensional (3D) datasets such as Light detection and ranging (LiDAR). This study focuses on the feasibility of using photon counting LiDAR data from National Aeronautics and Space Administration’s (NASA) Ice, Cloud, and land Elevation Satellite-2 (ICESat−2) mission to differentiate vegetation structure in burned and unburned areas and ultimately classify burned areas along mapped ground tracks. The ICESat−2 mission (launched in September 2018) provides datasets such as geolocated photon data (ATL03), which comprises precise latitude, longitude and elevation of each point where a photon interacts with land surface, and derivative products such as the Land Water Vegetation Elevation product (ATL08), which comprises estimated terrain and canopy height information. For analysis, 24 metrics such as the average, median and standard deviation of canopy height were derived from ATL08 data over forests burned by recent fires in 2018 in northern California and western New Mexico. A reference burn map was derived from Sentinel−2 images based on the differenced Normalized Burn Ratio (dNBR) index. A landcover map based on Sentinel−2 images was employed to remove non-forest classes. Landsat 8 based dNBR image and landcover map were also used for comparison. Next, ICESat−2 data of forest samples were classified into burned and unburned ATL08 100-m segments by both Random Forest classification and logistic regression. Both Sentinel−2 derived and Landsat 8 derived ATL08 samples got high classification accuracy, 83% versus 76%. Moreover, the resulting classification accuracy by Random Forest and logistic regression reached 83% and 74%, respectively. Among the 24 ICESat−2 metrics, apparent surface reflectance and the number of canopy photons were the most important. Furthermore, burn severity of each ATL08 segment was also estimated with Random Forest regression. R2 of predicted burn severity to observed dNBR is 0.61 with significant linear relationship and moderate correlation (r = 0.78). Overall, the reasonably high accuracies achieved in this study demonstrate the feasibility of employing ICESat−2 data in burned forest classification, opening avenues for improved estimation of burned biomass and carbon emissions from a 3D perspective.},
DOI = {10.3390/rs12010024}
}



@Article{s20010038,
AUTHOR = {Khan, Muhammad Fahad and Yau, Kok-Lim Alvin and Noor, Rafidah Md and Imran, Muhammad Ali},
TITLE = {Routing Schemes in FANETs: A Survey},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {38},
URL = {https://www.mdpi.com/1424-8220/20/1/38},
ISSN = {1424-8220},
ABSTRACT = {Flying ad hoc network (FANET) is a self-organizing wireless network that enables inexpensive, flexible, and easy-to-deploy flying nodes, such as unmanned aerial vehicles (UAVs), to communicate among themselves in the absence of fixed network infrastructure. FANET is one of the emerging networks that has an extensive range of next-generation applications. Hence, FANET plays a significant role in achieving application-based goals. Routing enables the flying nodes to collaborate and coordinate among themselves and to establish routes to radio access infrastructure, particularly FANET base station (BS). With a longer route lifetime, the effects of link disconnections and network partitions reduce. Routing must cater to two main characteristics of FANETs that reduce the route lifetime. Firstly, the collaboration nature requires the flying nodes to exchange messages and to coordinate among themselves, causing high energy consumption. Secondly, the mobility pattern of the flying nodes is highly dynamic in a three-dimensional space and they may be spaced far apart, causing link disconnection. In this paper, we present a comprehensive survey of the limited research work of routing schemes in FANETs. Different aspects, including objectives, challenges, routing metrics, characteristics, and performance measures, are covered. Furthermore, we present open issues.},
DOI = {10.3390/s20010038}
}



@Article{s20010043,
AUTHOR = {Ilyas, Naveed and Shahzad, Ahsan and Kim, Kiseon},
TITLE = {Convolutional-Neural Network-Based Image Crowd Counting: Review, Categorization, Analysis, and Performance Evaluation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {43},
URL = {https://www.mdpi.com/1424-8220/20/1/43},
ISSN = {1424-8220},
ABSTRACT = {Traditional handcrafted crowd-counting techniques in an image are currently transformed via machine-learning and artificial-intelligence techniques into intelligent crowd-counting techniques. This paradigm shift offers many advanced features in terms of adaptive monitoring and the control of dynamic crowd gatherings. Adaptive monitoring, identification/recognition, and the management of diverse crowd gatherings can improve many crowd-management-related tasks in terms of efficiency, capacity, reliability, and safety. Despite many challenges, such as occlusion, clutter, and irregular object distribution and nonuniform object scale, convolutional neural networks are a promising technology for intelligent image crowd counting and analysis. In this article, we review, categorize, analyze (limitations and distinctive features), and provide a detailed performance evaluation of the latest convolutional-neural-network-based crowd-counting techniques. We also highlight the potential applications of convolutional-neural-network-based crowd-counting techniques. Finally, we conclude this article by presenting our key observations, providing strong foundation for future research directions while designing convolutional-neural-network-based crowd-counting techniques. Further, the article discusses new advancements toward understanding crowd counting in smart cities using the Internet of Things (IoT).},
DOI = {10.3390/s20010043}
}



@Article{s20010050,
AUTHOR = {Yang, Baohua and Qi, Lin and Wang, Mengxuan and Hussain, Saddam and Wang, Huabin and Wang, Bing and Ning, Jingming},
TITLE = {Cross-Category Tea Polyphenols Evaluation Model Based on Feature Fusion of Electronic Nose and Hyperspectral Imagery},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/1424-8220/20/1/50},
ISSN = {1424-8220},
ABSTRACT = {Tea polyphenols are important ingredients for evaluating tea quality. The rapid development of sensors provides an efficient method for nondestructive detection of tea polyphenols. Previous studies have shown that features obtained from single or multiple sensors yield better results in detecting interior tea quality. However, due to their lack of external features, it is difficult to meet the general evaluation model for the quality of the interior and exterior of tea. In addition, some features do not fully reflect the sensor signals of tea for several categories. Therefore, a feature fusion method based on time and frequency domains from electronic nose (E-nose) and hyperspectral imagery (HSI) is proposed to estimate the polyphenol content of tea for cross-category evaluation. The random forest and the gradient boosting decision tree (GBDT) are used to evaluate the feature importance to obtain the optimized features. Three models based on different features for cross-category tea (black tea, green tea, and yellow tea) were compared, including grid support vector regression (Grid-SVR), random forest (RF), and extreme gradient boosting (XGBoost). The results show that the accuracy of fusion features based on the time and frequency domain from the electronic nose and hyperspectral image system is higher than that of the features from single sensor. Whether based on all original features or optimized features, the performance of XGBoost is the best among the three regression algorithms (R2 = 0.998, RMSE = 0.434). Results indicate that the proposed method in this study can improve the estimation accuracy of tea polyphenol content for cross-category evaluation, which provides a technical basis for predicting other components of tea.},
DOI = {10.3390/s20010050}
}



@Article{rs12010034,
AUTHOR = {Angel, Yoseline and Turner, Darren and Parkes, Stephen and Malbeteau, Yoann and Lucieer, Arko and McCabe, Matthew F.},
TITLE = {Automated Georectification and Mosaicking of UAV-Based Hyperspectral Imagery from Push-Broom Sensors},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {34},
URL = {https://www.mdpi.com/2072-4292/12/1/34},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral systems integrated on unmanned aerial vehicles (UAV) provide unique opportunities to conduct high-resolution multitemporal spectral analysis for diverse applications. However, additional time-consuming rectification efforts in postprocessing are routinely required, since geometric distortions can be introduced due to UAV movements during flight, even if navigation/motion sensors are used to track the position of each scan. Part of the challenge in obtaining high-quality imagery relates to the lack of a fast processing workflow that can retrieve geometrically accurate mosaics while optimizing the ground data collection efforts. To address this problem, we explored a computationally robust automated georectification and mosaicking methodology. It operates effectively in a parallel computing environment and evaluates results against a number of high-spatial-resolution datasets (mm to cm resolution) collected using a push-broom sensor and an associated RGB frame-based camera. The methodology estimates the luminance of the hyperspectral swaths and coregisters these against a luminance RGB-based orthophoto. The procedure includes an improved coregistration strategy by integrating the Speeded-Up Robust Features (SURF) algorithm, with the Maximum Likelihood Estimator Sample Consensus (MLESAC) approach. SURF identifies common features between each swath and the RGB-orthomosaic, while MLESAC fits the best geometric transformation model to the retrieved matches. Individual scanlines are then geometrically transformed and merged into a single spatially continuous mosaic reaching high positional accuracies only with a few number of ground control points (GCPs). The capacity of the workflow to achieve high spatial accuracy was demonstrated by examining statistical metrics such as RMSE, MAE, and the relative positional accuracy at 95% confidence level. Comparison against a user-generated georectification demonstrates that the automated approach speeds up the coregistration process by 85%.},
DOI = {10.3390/rs12010034}
}



@Article{rs12010039,
AUTHOR = {Halladin-Dąbrowska, Anna and Kania, Adam and Kopeć, Dominik},
TITLE = {The t-SNE Algorithm as a Tool to Improve the Quality of Reference Data Used in Accurate Mapping of Heterogeneous Non-Forest Vegetation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2072-4292/12/1/39},
ISSN = {2072-4292},
ABSTRACT = {Supervised classification methods, used for many applications, including vegetation mapping require accurate &ldquo;ground truth&rdquo; to be effective. Nevertheless, it is common for the quality of this data to be poorly verified prior to it being used for the training and validation of classification models. The fact that noisy or erroneous parts of the reference dataset are not removed is usually explained by the relatively high resistance of some algorithms to errors. The objective of this study was to demonstrate the rationale for cleaning the reference dataset used for the classification of heterogeneous non-forest vegetation, and to present a workflow based on the t-distributed stochastic neighbor embedding (t-SNE) algorithm for the better integration of reference data with remote sensing data in order to improve outcomes. The proposed analysis is a new application of the t-SNE algorithm. The effectiveness of this workflow was tested by classifying three heterogeneous non-forest Natura 2000 habitats: Molinia meadows (Molinion caeruleae; code 6410), species-rich Nardus grassland (code 6230) and dry heaths (code 4030), employing two commonly used algorithms: random forest (RF) and AdaBoost (AB), which, according to the literature, differ in their resistance to errors in reference datasets. Polygons collected in the field (on-ground reference data) in 2016 and 2017, containing no intentional errors, were used as the on-ground reference dataset. The remote sensing data used in the classification were obtained in 2017 during the peak growing season by a HySpex sensor consisting of two imaging spectrometers covering spectral ranges of 0.4&ndash;0.9 &mu;m (VNIR-1800) and 0.9&ndash;2.5 &mu;m (SWIR-384). The on-ground reference dataset was gradually cleaned by verifying candidate polygons selected by visual interpretation of t-SNE plots. Around 40&ndash;50% of candidate polygons were ultimately found to contain errors. Altogether, 15% of reference polygons were removed. As a result, the quality of the final map, as assessed by the Kappa and F1 accuracy measures as well as by visual evaluation, was significantly improved. The global map accuracy increased by about 6% (in Kappa coefficient), relative to the baseline classification obtained using random removal of the same number of reference polygons.},
DOI = {10.3390/rs12010039}
}



@Article{s20010057,
AUTHOR = {Wang, Tonghua and Han, Wenting and Zhang, Mengfei and Yao, Xiaomin and Zhang, Liyuan and Peng, Xingshuo and Li, Chaoqun and Dan, Xvjia},
TITLE = {Unmanned Aerial Vehicle-Borne Sensor System for Atmosphere-Particulate-Matter Measurements: Design and Experiments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {57},
URL = {https://www.mdpi.com/1424-8220/20/1/57},
ISSN = {1424-8220},
ABSTRACT = {An unmanned aerial vehicle (UAV) particulate-matter (PM) monitoring system was developed that can perform three-dimensional stereoscopic observation of PM2.5 and PM10 in the atmosphere. The UAV monitoring system was mainly integrated by modules of data acquisition and processing, wireless data transmission, and global positioning system (GPS). Particularly, in this study, a ground measurement-control subsystem was added that can display and store collected data in real time and set up measurement scenarios, data-storage modes, and system sampling frequency as needed. The UAV PM monitoring system was calibrated via comparison with a national air-quality monitoring station; the data of both systems were highly correlated. Since rotation of the UAV propeller affects measured PM concentration, this study specifically tested this effect by setting up another identical monitoring system fixed at a tower as reference. The UAV systems worked simultaneously to collect data for comparison. A correction method for the propeller disturbance was proposed. Averaged relative errors for the PM2.5 and PM10 concentrations measured by the two systems were 6.2% and 6.6%, respectively, implying that the UAV system could be used for monitoring PM in an atmosphere environment.},
DOI = {10.3390/s20010057}
}



@Article{rs12010041,
AUTHOR = {Brooke, Christopher and Clutterbuck, Ben},
TITLE = {Mapping Heterogeneous Buried Archaeological Features Using Multisensor Data from Unmanned Aerial Vehicles},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {41},
URL = {https://www.mdpi.com/2072-4292/12/1/41},
ISSN = {2072-4292},
ABSTRACT = {There is a long history of the use of aerial imagery for archaeological research, but the application of multisensor image data has only recently been facilitated by the development of unmanned aerial vehicles (UAVs). Two archaeological sites in the East Midlands U.K. that differ in age and topography were selected for survey using multisensor imaging from a fixed-wing UAV. The aim of this study was to determine optimum methodology for the use of UAVs in examining archaeological sites that have no obvious surface features and examine issues of ground control target design, thermal effects, image processing and advanced filtration. The information derived from the range of sensors used in this study enabled interpretation of buried archaeology at both sites. For any archaeological survey using UAVs, the acquisition of visible colour (RGB), multispectral, and thermal imagery as a minimum are advised, as no single technique is sufficient to attempt to reveal the maximum amount of potential information.},
DOI = {10.3390/rs12010041}
}



@Article{rs12010056,
AUTHOR = {de Castro, Ana I. and Peña, José M. and Torres-Sánchez, Jorge and Jiménez-Brenes, Francisco M. and Valencia-Gredilla, Francisco and Recasens, Jordi and López-Granados, Francisca},
TITLE = {Mapping Cynodon Dactylon Infesting Cover Crops with an Automatic Decision Tree-OBIA Procedure and UAV Imagery for Precision Viticulture},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {56},
URL = {https://www.mdpi.com/2072-4292/12/1/56},
ISSN = {2072-4292},
ABSTRACT = {The establishment and management of cover crops are common practices widely used in irrigated viticulture around the world, as they bring great benefits not only to protect and improve the soil, but also to control vine vigor and improve the yield quality, among others. However, these benefits are often reduced when cover crops are infested by Cynodon dactylon (bermudagrass), which impacts crop production due to its competition for water and nutrients and causes important economic losses for the winegrowers. Therefore, the discrimination of Cynodon dactylon in cover crops would enable site-specific control to be applied and thus drastically mitigate damage to the vineyard. In this context, this research proposes a novel, automatic and robust image analysis algorithm for the quick and accurate mapping of Cynodon dactylon growing in vineyard cover crops. The algorithm was developed using aerial images taken with an Unmanned Aerial Vehicle (UAV) and combined decision tree (DT) and object-based image analysis (OBIA) approaches. The relevance of this work consisted in dealing with the constraint caused by the spectral similarity of these complex scenarios formed by vines, cover crops, Cynodon dactylon, and bare soil. The incorporation of height information from the Digital Surface Model and several features selected by machine learning tools in the DT-OBIA algorithm solved this spectral similarity limitation and allowed the precise design of Cynodon dactylon maps. Another contribution of this work is the short time needed to apply the full process from UAV flights to image analysis, which can enable useful maps to be created on demand (within two days of the farmer&acute;s request) and is thus timely for controlling Cynodon dactylon in the herbicide application window. Therefore, this combination of UAV imagery and a DT-OBIA algorithm would allow winegrowers to apply site-specific control of Cynodon dactylon and maintain cover crop-based management systems and their consequent benefits in the vineyards, and also comply with the European legal framework for the sustainable use of agricultural inputs and implementation of integrated crop management.},
DOI = {10.3390/rs12010056}
}



@Article{rs12010059,
AUTHOR = {Masoud, Khairiya Mudrik and Persello, Claudio and Tolpekin, Valentyn A.},
TITLE = {Delineation of Agricultural Field Boundaries from Sentinel-2 Images Using a Novel Super-Resolution Contour Detector Based on Fully Convolutional Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2072-4292/12/1/59},
ISSN = {2072-4292},
ABSTRACT = {Boundaries of agricultural fields are important features necessary for defining the location, shape, and spatial extent of agricultural units. They are commonly used to summarize production statistics at the field level. In this study, we investigate the delineation of agricultural field boundaries (AFB) from Sentinel-2 satellite images acquired over the Flevoland province, the Netherlands, using a deep learning technique based on fully convolutional networks (FCNs). We designed a multiple dilation fully convolutional network (MD-FCN) for AFB detection from Sentinel-2 images at 10 m resolution. Furthermore, we developed a novel super-resolution semantic contour detection network (named SRC-Net) using a transposed convolutional layer in the FCN architecture to enhance the spatial resolution of the AFB output from 10 m to 5 m resolution. The SRC-Net also improves the AFB maps at 5 m resolution by exploiting the spatial-contextual information in the label space. The results of the proposed SRC-Net outperform alternative upsampling techniques and are only slightly inferior to the results of the MD-FCN for AFB detection from RapidEye images acquired at 5 m resolution.},
DOI = {10.3390/rs12010059}
}



@Article{rs12010063,
AUTHOR = {Abbasi, Mozhgan and Verrelst, Jochem and Mirzaei, Mohsen and Marofi, Safar and Riyahi Bakhtiari, Hamid Reza},
TITLE = {Optimal Spectral Wavelengths for Discriminating Orchard Species Using Multivariate Statistical Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {63},
URL = {https://www.mdpi.com/2072-4292/12/1/63},
ISSN = {2072-4292},
ABSTRACT = {Sustainable management of orchard fields requires detailed information about the tree types, which is a main component of precision agriculture programs. To this end, hyperspectral imagery can play a major role in orchard tree species mapping. Efficient use of hyperspectral data in combination with field measurements requires the development of optimized band selection strategies to separate tree species. In this study, field spectroscopy (350 to 2500 nm) was performed through scanning 165 spectral leaf samples of dominant orchard tree species (almond, walnut, and grape) in Chaharmahal va Bakhtiyari province, Iran. Two multivariable methods were employed to identify the optimum wavelengths: the first includes three-step approach ANOVA, random forest classifier (RFC) and principal component analysis (PCA), and the second employs partial least squares (PLS). For both methods we determined whether tree species can be spectrally separated using discriminant analysis (DA) and then the optimal wavelengths were identified for this purpose. Results indicate that all species express distinct spectral behaviors at the beginning of the visible range (from 350 to 439 nm), the red edge and the near infrared wavelengths (from 701 to 1405 nm). The ANOVA test was able to reduce primary wavelengths (2151) to 792, which had a significant difference (99% confidence level), then the RFC further reduced the wavelengths to 118. By removing the overlapping wavelengths, the PCA represented five components (99.87% of variance) which extracted optimal wavelengths were: 363, 423, 721, 1064, and 1388 nm. The optimal wavelengths for the species discrimination using the best PLS-DA model (100% accuracy) were at 397, 515, 647, 1386, and 1919 nm.},
DOI = {10.3390/rs12010063}
}



@Article{rs12010065,
AUTHOR = {Laso, Francisco J. and Benítez, Fátima L. and Rivas-Torres, Gonzalo and Sampedro, Carolina and Arce-Nazario, Javier},
TITLE = {Land Cover Classification of Complex Agroecosystems in the Non-Protected Highlands of the Galapagos Islands},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {65},
URL = {https://www.mdpi.com/2072-4292/12/1/65},
ISSN = {2072-4292},
ABSTRACT = {The humid highlands of the Galapagos are the islands&rsquo; most biologically productive regions and a key habitat for endemic animal and plant species. These areas are crucial for the region&rsquo;s food security and for the control of invasive plants, but little is known about the spatial distribution of its land cover. We generated a baseline high-resolution land cover map of the agricultural zones and their surrounding protected areas. We combined the high spatial resolution of PlanetScope images with the high spectral resolution of Sentinel-2 images in an object-based classification using a RandomForest algorithm. We used images collected with an unmanned aerial vehicle (UAV) to verify and validate our classified map. Despite the astounding diversity and heterogeneity of the highland landscape, our classification yielded useful results (overall Kappa: 0.7, R2: 0.69) and revealed that across all four inhabited islands, invasive plants cover the largest fraction (28.5%) of the agricultural area, followed by pastures (22.3%), native vegetation (18.6%), food crops (18.3%), and mixed forest and pioneer plants (11.6%). Our results are consistent with historical trajectories of colonization and abandonment of the highlands. The produced dataset is designed to suit the needs of practitioners of both conservation and agriculture and aims to foster collaboration between the two areas.},
DOI = {10.3390/rs12010065}
}



@Article{ijgi9010014,
AUTHOR = {Kerle, Norman and Nex, Francesco and Gerke, Markus and Duarte, Diogo and Vetrivel, Anand},
TITLE = {UAV-Based Structural Damage Mapping: A Review},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {14},
URL = {https://www.mdpi.com/2220-9964/9/1/14},
ISSN = {2220-9964},
ABSTRACT = {Structural disaster damage detection and characterization is one of the oldest remote sensing challenges, and the utility of virtually every type of active and passive sensor deployed on various air- and spaceborne platforms has been assessed. The proliferation and growing sophistication of unmanned aerial vehicles (UAVs) in recent years has opened up many new opportunities for damage mapping, due to the high spatial resolution, the resulting stereo images and derivatives, and the flexibility of the platform. This study provides a comprehensive review of how UAV-based damage mapping has evolved from providing simple descriptive overviews of a disaster science, to more sophisticated texture and segmentation-based approaches, and finally to studies using advanced deep learning approaches, as well as multi-temporal and multi-perspective imagery to provide comprehensive damage descriptions. The paper further reviews studies on the utility of the developed mapping strategies and image processing pipelines for first responders, focusing especially on outcomes of two recent European research projects, RECONASS (Reconstruction and Recovery Planning: Rapid and Continuously Updated Construction Damage, and Related Needs Assessment) and INACHUS (Technological and Methodological Solutions for Integrated Wide Area Situation Awareness and Survivor Localization to Support Search and Rescue Teams). Finally, recent and emerging developments are reviewed, such as recent improvements in machine learning, increasing mapping autonomy, damage mapping in interior, GPS-denied environments, the utility of UAVs for infrastructure mapping and maintenance, as well as the emergence of UAVs with robotic abilities.},
DOI = {10.3390/ijgi9010014}
}



@Article{geosciences10010013,
AUTHOR = {Milewski, Adam and Seyoum, Wondwosen M. and Elkadiri, Racha and Durham, Michael},
TITLE = {Multi-Scale Hydrologic Sensitivity to Climatic and Anthropogenic Changes in Northern Morocco},
JOURNAL = {Geosciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {13},
URL = {https://www.mdpi.com/2076-3263/10/1/13},
ISSN = {2076-3263},
ABSTRACT = {Natural and human-induced impacts on water resources across the globe continue to negatively impact water resources. Characterizing the hydrologic sensitivity to climatic and anthropogenic changes is problematic given the lack of monitoring networks and global-scale model uncertainties. This study presents an integrated methodology combining satellite remote sensing (e.g., GRACE, TRMM), hydrologic modeling (e.g., SWAT), and climate projections (IPCC AR5), to evaluate the impact of climatic and man-made changes on groundwater and surface water resources. The approach was carried out on two scales: regional (Morocco) and watershed (Souss Basin, Morocco) to capture the recent climatic changes in precipitation and total water storage, examine current and projected impacts on total water resources (surface and groundwater), and investigate the link between climate change and groundwater resources. Simulated (1979&ndash;2014) potential renewable groundwater resources obtained from SWAT are ~4.3 &times; 108 m3/yr. GRACE data (2002&ndash;2016) indicates a decline in total water storage anomaly of ~0.019m/yr., while precipitation remains relatively constant through the same time period (2002&ndash;2016), suggesting human interactions as the major underlying cause of depleting groundwater reserves. Results highlight the need for further conservation of diminishing groundwater resources and a more complete understanding of the links and impacts of climate change on groundwater resources.},
DOI = {10.3390/geosciences10010013}
}



@Article{app10010238,
AUTHOR = {Mazzia, Vittorio and Khaliq, Aleem and Chiaberge, Marcello},
TITLE = {Improvement in Land Cover and Crop Classification based on Temporal Features Learning from Sentinel-2 Data Using Recurrent-Convolutional Neural Network (R-CNN)},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {238},
URL = {https://www.mdpi.com/2076-3417/10/1/238},
ISSN = {2076-3417},
ABSTRACT = {Understanding the use of current land cover, along with monitoring change over time, is vital for agronomists and agricultural agencies responsible for land management. The increasing spatial and temporal resolution of globally available satellite images, such as provided by Sentinel-2, creates new possibilities for researchers to use freely available multi-spectral optical images, with decametric spatial resolution and more frequent revisits for remote sensing applications such as land cover and crop classification (LC&amp;CC), agricultural monitoring and management, environment monitoring. Existing solutions dedicated to cropland mapping can be categorized based on per-pixel based and object-based. However, it is still challenging when more classes of agricultural crops are considered at a massive scale. In this paper, a novel and optimal deep learning model for pixel-based LC&amp;CC is developed and implemented based on Recurrent Neural Networks (RNN) in combination with Convolutional Neural Networks (CNN) using multi-temporal sentinel-2 imagery of central north part of Italy, which has diverse agricultural system dominated by economic crop types. The proposed methodology is capable of automated feature extraction by learning time correlation of multiple images, which reduces manual feature engineering and modeling crop phenological stages. Fifteen classes, including major agricultural crops, were considered in this study. We also tested other widely used traditional machine learning algorithms for comparison such as support vector machine SVM, random forest (RF), Kernal SVM, and gradient boosting machine, also called XGBoost. The overall accuracy achieved by our proposed Pixel R-CNN was 96.5%, which showed considerable improvements in comparison with existing mainstream methods. This study showed that Pixel R-CNN based model offers a highly accurate way to assess and employ time-series data for multi-temporal classification tasks.},
DOI = {10.3390/app10010238}
}



@Article{s20010227,
AUTHOR = {Pytka, Jaroslaw and Budzyński, Piotr and Łyszczyk, Tomasz and Józwik, Jerzy and Michałowska, Joanna and Tofil, Arkadiusz and Błażejczak, Dariusz and Laskowski, Jan},
TITLE = {Determining Wheel Forces and Moments on Aircraft Landing Gear with a Dynamometer Sensor},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {227},
URL = {https://www.mdpi.com/1424-8220/20/1/227},
ISSN = {1424-8220},
ABSTRACT = {This paper describes airfield measurement of forces and moments that act on a landing gear wheel. For the measurement, a wheel force sensor was used. The sensor was designed and built based on strain gage technology and was embedded in the left landing gear wheel of a test aircraft. The sensor is capable of measuring simultaneously three perpendicular forces and three moments and sends data to a handheld device wirelessly. For the airfield tests, the sensor was installed on a PZL 104 Wilga 35A multipurpose aircraft. The aircraft was towed at a &ldquo;marching man&rdquo; speed and the measurements were performed at three driving modes: Free rolling, braking, and turning. The paper contains results obtained in the field measurements performed on a grassy runway of the Rzesz&oacute;w Jasionka Aerodrome, Poland. Rolling resistance of aircraft tire, braking friction, as well as aligning moment were analyzed and discussed with respect to surface conditions.},
DOI = {10.3390/s20010227}
}



@Article{rs12010113,
AUTHOR = {Hennessy, Andrew and Clarke, Kenneth and Lewis, Megan},
TITLE = {Hyperspectral Classification of Plants: A Review of Waveband Selection Generalisability},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {113},
URL = {https://www.mdpi.com/2072-4292/12/1/113},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral sensing, measuring reflectance over visible to shortwave infrared wavelengths, has enabled the classification and mapping of vegetation at a range of taxonomic scales, often down to the species level. Classification with hyperspectral measurements, acquired by narrow band spectroradiometers or imaging sensors, has generally required some form of spectral feature selection to reduce the dimensionality of the data to a level suitable for the construction of a classification model. Despite the large number of hyperspectral plant classification studies, an in-depth review of feature selection methods and resultant waveband selections has not yet been performed. Here, we present a review of the last 22 years of hyperspectral vegetation classification literature that evaluates the overall waveband selection frequency, waveband selection frequency variation by taxonomic, structural, or functional group, and the influence of feature selection choice by comparing such methods as stepwise discriminant analysis (SDA), support vector machines (SVM), and random forests (RF). This review determined that all characteristics of hyperspectral plant studies influence the wavebands selected for classification. This includes the taxonomic, structural, and functional groups of the target samples, the methods, and scale at which hyperspectral measurements are recorded, as well as the feature selection method used. Furthermore, these influences do not appear to be consistent. Moreover, the considerable variability in waveband selection caused by the feature selectors effectively masks the analysis of any variability between studies related to plant groupings. Additionally, questions are raised about the suitability of SDA as a feature selection method, with it producing waveband selections at odds with the other feature selectors. Caution is recommended when choosing a feature selector for hyperspectral plant classification: We recommend multiple methods being performed. The resultant sets of selected spectral features can either be evaluated individually by multiple classification models or combined as an ensemble for evaluation by a single classifier. Additionally, we suggest caution when relying upon waveband recommendations from the literature to guide waveband selections or classifications for new plant discrimination applications, as such recommendations appear to be weakly generalizable between studies.},
DOI = {10.3390/rs12010113}
}



@Article{rs12010126,
AUTHOR = {Wijesingha, Jayan and Astor, Thomas and Schulze-Brüninghoff, Damian and Wengert, Matthias and Wachendorf, Michael},
TITLE = {Predicting Forage Quality of Grasslands Using UAV-Borne Imaging Spectroscopy},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {126},
URL = {https://www.mdpi.com/2072-4292/12/1/126},
ISSN = {2072-4292},
ABSTRACT = {The timely knowledge of forage quality of grasslands is vital for matching the demands in animal feeding. Remote sensing (RS) is a promising tool for estimating field-scale forage quality compared with traditional methods, which usually do not provide equally detailed information. However, the applicability of RS prediction models depends on the variability of the underlying calibration data, which can be brought about by the inclusion of a multitude of grassland types and management practices in the model development. Major aims of this study were (i) to build forage quality estimation models for multiple grassland types based on an unmanned aerial vehicle (UAV)-borne imaging spectroscopy and (ii) to generate forage quality distribution maps using the best models obtained. The study examined data from eight grasslands in northern Hesse, Germany, which largely differed in terms of vegetation type and cutting regime. The UAV with a hyperspectral camera on board was utilised to acquire spectral images from the grasslands, and crude protein (CP) and acid detergent fibre (ADF) concentration of the forage was assessed at each cut. Five predictive modelling regression algorithms were applied to develop quality estimation models. Further, grassland forage quality distribution maps were created using the best models developed. The normalised spectral reflectance data showed the strongest relationship with both CP and ADF concentration. From all predictive algorithms, support vector regression provided the highest precision and accuracy for CP estimation (median normalised root mean square error prediction (nRMSEp) = 10.6%), while cubist regression model proved best for ADF estimation (median nRMSEp = 13.4%). The maps generated for both CP and ADF showed a distinct spatial variation in forage quality values for the different grasslands and cutting regimes. Overall, the results disclose that UAV-borne imaging spectroscopy, in combination with predictive modelling, provides a promising tool for accurate forage quality estimation of multiple grasslands.},
DOI = {10.3390/rs12010126}
}



@Article{rs12010133,
AUTHOR = {Dong, Xinyu and Zhang, Zhichao and Yu, Ruiyang and Tian, Qingjiu and Zhu, Xicun},
TITLE = {Extraction of Information about Individual Trees from High-Spatial-Resolution UAV-Acquired Images of an Orchard},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {133},
URL = {https://www.mdpi.com/2072-4292/12/1/133},
ISSN = {2072-4292},
ABSTRACT = {The extraction of information about individual trees is essential to supporting the growing of fruit in orchard management. Data acquired from spectral sensors mounted on unmanned aerial vehicles (UAVs) have very high spatial and temporal resolution. However, an efficient and reliable method for extracting information about individual trees with irregular tree-crown shapes and a complicated background is lacking. In this study, we developed and tested the performance of an approach, based on UAV imagery, to extracting information about individual trees in an orchard with a complicated background that includes apple trees (Plot 1) and pear trees (Plot 2). The workflow involves the construction of a digital orthophoto map (DOM), digital surface models (DSMs), and digital terrain models (DTMs) using the Structure from Motion (SfM) and Multi-View Stereo (MVS) approaches, as well as the calculation of the Excess Green minus Excess Red Index (ExGR) and the selection of various thresholds. Furthermore, a local-maxima filter method and marker-controlled watershed segmentation were used for the detection and delineation, respectively, of individual trees. The accuracy of the proposed method was evaluated by comparing its results with manual estimates of the numbers of trees and the areas and diameters of tree-crowns, all three of which parameters were obtained from the DOM. The results of the proposed method are in good agreement with these manual estimates: The F-scores for the estimated numbers of individual trees were 99.0% and 99.3% in Plot 1 and Plot 2, respectively, while the Producer&rsquo;s Accuracy (PA) and User&rsquo;s Accuracy (UA) for the delineation of individual tree-crowns were above 95% for both of the plots. For the area of individual tree-crowns, root-mean-square error (RMSE) values of 0.72 m2 and 0.48 m2 were obtained for Plot 1 and Plot 2, respectively, while for the diameter of individual tree-crowns, RMSE values of 0.39 m and 0.26 m were obtained for Plot 1 (339 trees correctly identified) and Plot 2 (203 trees correctly identified), respectively. Both the areas and diameters of individual tree-crowns were overestimated to varying degrees.},
DOI = {10.3390/rs12010133}
}



@Article{rs12010127,
AUTHOR = {Mohamed, Hassan and Nadaoka, Kazuo and Nakamura, Takashi},
TITLE = {Towards Benthic Habitat 3D Mapping Using Machine Learning Algorithms and Structures from Motion Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {127},
URL = {https://www.mdpi.com/2072-4292/12/1/127},
ISSN = {2072-4292},
ABSTRACT = {The accurate classification and 3D mapping of benthic habitats in coastal ecosystems are vital for developing management strategies for these valuable shallow water environments. However, both automatic and semiautomatic approaches for deriving ecologically significant information from a towed video camera system are quite limited. In the current study, we demonstrate a semiautomated framework for high-resolution benthic habitat classification and 3D mapping using Structure from Motion and Multi View Stereo (SfM-MVS) algorithms and automated machine learning classifiers. The semiautomatic classification of benthic habitats was performed using several attributes extracted automatically from labeled examples by a human annotator using raw towed video camera image data. The Bagging of Features (BOF), Hue Saturation Value (HSV), and Gray Level Co-occurrence Matrix (GLCM) methods were used to extract these attributes from 3000 images. Three machine learning classifiers (k-nearest neighbor (k-NN), support vector machine (SVM), and bagging (BAG)) were trained by using these attributes, and their outputs were assembled by the fuzzy majority voting (FMV) algorithm. The correctly classified benthic habitat images were then geo-referenced using a differential global positioning system (DGPS). Finally, SfM-MVS techniques used the resulting classified geo-referenced images to produce high spatial resolution digital terrain models and orthophoto mosaics for each category. The framework was tested for the identification and 3D mapping of seven habitats in a portion of the Shiraho area in Japan. These seven habitats were corals (Acropora and Porites), blue corals (H. coerulea), brown algae, blue algae, soft sand, hard sediments (pebble, cobble, and boulders), and seagrass. Using the FMV algorithm, we achieved an overall accuracy of 93.5% in the semiautomatic classification of the seven habitats.},
DOI = {10.3390/rs12010127}
}



@Article{rs12010186,
AUTHOR = {Hu, Yang and Xu, Xuelei and Wu, Fayun and Sun, Zhongqiu and Xia, Haoming and Meng, Qingmin and Huang, Wenli and Zhou, Hua and Gao, Jinping and Li, Weitao and Peng, Daoli and Xiao, Xiangming},
TITLE = {Estimating Forest Stock Volume in Hunan Province, China, by Integrating In Situ Plot Data, Sentinel-2 Images, and Linear and Machine Learning Regression Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {186},
URL = {https://www.mdpi.com/2072-4292/12/1/186},
ISSN = {2072-4292},
ABSTRACT = {The forest stock volume (FSV) is one of the key indicators in forestry resource assessments on local, regional, and national scales. To date, scaling up in situ plot-scale measurements across landscapes is still a great challenge in the estimation of FSVs. In this study, Sentinel-2 imagery, the Google Earth Engine (GEE) cloud computing platform, three base station joint differential positioning technology (TBSJDPT), and three algorithms were used to build an FSV model for forests located in Hunan Province, southern China. The GEE cloud computing platform was used to extract the imagery variables from the Sentinel-2 imagery pixels. The TBSJDPT was put forward and used to provide high-precision positions of the sample plot data. The random forests (RF), support vector regression (SVR), and multiple linear regression (MLR) algorithms were used to estimate the FSV. For each pixel, 24 variables were extracted from the Sentinel-2 images taken in 2017 and 2018. The RF model performed the best in both the training phase (i.e., R2 = 0.91, RMSE = 35.13 m3 ha&minus;1, n = 321) and in the test phase (i.e., R2 = 0.58, RMSE = 65.03 m3 ha&minus;1, and n = 138). This model was followed by the SVR model (R2 = 0.54, RMSE = 65.60 m3 ha&minus;1, n = 321 in training; R2 = 0.54, RMSE = 66.00 m3 ha&minus;1, n = 138 in testing), which was slightly better than the MLR model (R2 = 0.38, RMSE = 75.74 m3 ha&minus;1, and n = 321 in training; R2 = 0.49, RMSE = 70.22 m3 ha&minus;1, and n = 138 in testing) in both the training phase and test phase. The best predictive band was Red-Edge 1 (B5), which performed well both in the machine learning methods and in the MLR method. The Blue band (B2), Green band (B3), Red band (B4), SWIR2 band (B12), and vegetation indices (TCW, NDVI_B5, and TCB) were used in the machine learning models, and only one vegetation index (MSI) was used in the MLR model. We mapped the FSV distribution in Hunan Province (3.50 &times; 108 m3) based on the RF model; it reached a total accuracy of 63.87% compared with the official forest report in 2017 (5.48 &times; 108 m3). The results from this study will help develop and improve satellite-based methods to estimate FSVs on local, regional and national scales.},
DOI = {10.3390/rs12010186}
}



@Article{rs12010198,
AUTHOR = {Hunter, Frederick D.L. and Mitchard, Edward T.A. and Tyrrell, Peter and Russell, Samantha},
TITLE = {Inter-Seasonal Time Series Imagery Enhances Classification Accuracy of Grazing Resource and Land Degradation Maps in a Savanna Ecosystem},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {198},
URL = {https://www.mdpi.com/2072-4292/12/1/198},
ISSN = {2072-4292},
ABSTRACT = {In savannas, mapping grazing resources and indicators of land degradation is important for assessing ecosystem conditions and informing grazing and land management decisions. We investigated the effects of classifiers and used time series imagery&mdash;images acquired within and across seasons&mdash;on the accuracy of plant species maps. The study site was a grazed savanna in southern Kenya. We used Sentinel-2 multi-spectral imagery due to its high spatial (10&ndash;20 m) and temporal (five days) resolution with support vector machine (SVM) and random forest (RF) classifiers. The species mapped were important for grazing livestock and wildlife (three grass species), indicators of land degradation (one tree genus and one invasive shrub), and a fig tree species. The results show that increasing the number of images, including dry season imagery, results in improved classification accuracy regardless of the classifier (average increase in overall accuracy (OA) = 0.1632). SVM consistently outperformed RF, and the most accurate model and was SVM with a radial kernel using imagery from both wet and dry seasons (OA = 0.8217). Maps showed that seasonal grazing areas provide functionally different grazing opportunities and have different vegetation characteristics that are critical to a landscape&rsquo;s ability to support large populations of both livestock and wildlife. This study highlights the potential of multi-spectral satellite imagery for species-level mapping of savannas.},
DOI = {10.3390/rs12010198}
}



@Article{rs12020213,
AUTHOR = {Zhang, Chengming and Chen, Yan and Yang, Xiaoxia and Gao, Shuai and Li, Feng and Kong, Ailing and Zu, Dawei and Sun, Li},
TITLE = {Improved Remote Sensing Image Classification Based on Multi-Scale Feature Fusion},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {213},
URL = {https://www.mdpi.com/2072-4292/12/2/213},
ISSN = {2072-4292},
ABSTRACT = {When extracting land-use information from remote sensing imagery using image segmentation, obtaining fine edges for extracted objects is a key problem that is yet to be solved. In this study, we developed a new weight feature value convolutional neural network (WFCNN) to perform fine remote sensing image segmentation and extract improved land-use information from remote sensing imagery. The WFCNN includes one encoder and one classifier. The encoder obtains a set of spectral features and five levels of semantic features. It uses the linear fusion method to hierarchically fuse the semantic features, employs an adjustment layer to optimize every level of fused features to ensure the stability of the pixel features, and combines the fused semantic and spectral features to form a feature graph. The classifier then uses a Softmax model to perform pixel-by-pixel classification. The WFCNN was trained using a stochastic gradient descent algorithm; the former and two variants were subject to experimental testing based on Gaofen 6 images and aerial images that compared them with the commonly used SegNet, U-NET, and RefineNet models. The accuracy, precision, recall, and F1-Score of the WFCNN were higher than those of the other models, indicating certain advantages in pixel-by-pixel segmentation. The results clearly show that the WFCNN can improve the accuracy and automation level of large-scale land-use mapping and the extraction of other information using remote sensing imagery.},
DOI = {10.3390/rs12020213}
}



@Article{rs12020215,
AUTHOR = {Zha, Hainie and Miao, Yuxin and Wang, Tiantian and Li, Yue and Zhang, Jing and Sun, Weichao and Feng, Zhengqi and Kusnierek, Krzysztof},
TITLE = {Improving Unmanned Aerial Vehicle Remote Sensing-Based Rice Nitrogen Nutrition Index Prediction with Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {215},
URL = {https://www.mdpi.com/2072-4292/12/2/215},
ISSN = {2072-4292},
ABSTRACT = {Optimizing nitrogen (N) management in rice is crucial for China&rsquo;s food security and sustainable agricultural development. Nondestructive crop growth monitoring based on remote sensing technologies can accurately assess crop N status, which may be used to guide the in-season site-specific N recommendations. The fixed-wing unmanned aerial vehicle (UAV)-based remote sensing is a low-cost, easy-to-operate technology for collecting spectral reflectance imagery, an important data source for precision N management. The relationships between many vegetation indices (VIs) derived from spectral reflectance data and crop parameters are known to be nonlinear. As a result, nonlinear machine learning methods have the potential to improve the estimation accuracy. The objective of this study was to evaluate five different approaches for estimating rice (Oryza sativa L.) aboveground biomass (AGB), plant N uptake (PNU), and N nutrition index (NNI) at stem elongation (SE) and heading (HD) stages in Northeast China: (1) single VI (SVI); (2) stepwise multiple linear regression (SMLR); (3) random forest (RF); (4) support vector machine (SVM); and (5) artificial neural networks (ANN) regression. The results indicated that machine learning methods improved the NNI estimation compared to VI-SLR and SMLR methods. The RF algorithm performed the best for estimating NNI (R2 = 0.94 (SE) and 0.96 (HD) for calibration and 0.61 (SE) and 0.79 (HD) for validation). The root mean square errors (RMSEs) were 0.09, and the relative errors were &lt;10% in all the models. It is concluded that the RF machine learning regression can significantly improve the estimation of rice N status using UAV remote sensing. The application machine learning methods offers a new opportunity to better use remote sensing data for monitoring crop growth conditions and guiding precision crop management. More studies are needed to further improve these machine learning-based models by combining both remote sensing data and other related soil, weather, and management information for applications in precision N and crop management.},
DOI = {10.3390/rs12020215}
}



@Article{rs12020221,
AUTHOR = {Zhang, Xiuwei and Jin, Jiaojiao and Lan, Zeze and Li, Chunjiang and Fan, Minhao and Wang, Yafei and Yu, Xin and Zhang, Yanning},
TITLE = {ICENET: A Semantic Segmentation Deep Network for River Ice by Fusing Positional and Channel-Wise Attentive Features},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {221},
URL = {https://www.mdpi.com/2072-4292/12/2/221},
ISSN = {2072-4292},
ABSTRACT = {River ice monitoring is of great significance for river management, ship navigation and ice hazard forecasting in cold-regions. Accurate ice segmentation is one most important pieces of technology in ice monitoring research. It can provide the prerequisite information for the calculation of ice cover density, drift ice speed, ice cover distribution, change detection and so on. Unmanned aerial vehicle (UAV) aerial photography has the advantages of higher spatial and temporal resolution. As UAV technology has become more popular and cheaper, it has been widely used in ice monitoring. So, we focused on river ice segmentation based on UAV remote sensing images. In this study, the NWPU_YRCC dataset was built for river ice segmentation, in which all images were captured by different UAVs in the region of the Yellow River, the most difficult river to manage in the world. To the best of our knowledge, this is the first public UAV image dataset for river ice segmentation. Meanwhile, a semantic segmentation deep convolution neural network by fusing positional and channel-wise attentive features is proposed for river ice semantic segmentation, named ICENET. Experiments demonstrated that the proposed ICENET outperforms the state-of-the-art methods, achieving a superior result on the NWPU_YRCC dataset.},
DOI = {10.3390/rs12020221}
}



@Article{rs12020244,
AUTHOR = {Takahashi Miyoshi, Gabriela and Imai, Nilton Nobuhiro and Garcia Tommaselli, Antonio Maria and Antunes de Moraes, Marcus Vinícius and Honkavaara, Eija},
TITLE = {Evaluation of Hyperspectral Multitemporal Information to Improve Tree Species Identification in the Highly Diverse Atlantic Forest},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {244},
URL = {https://www.mdpi.com/2072-4292/12/2/244},
ISSN = {2072-4292},
ABSTRACT = {The monitoring of forest resources is crucial for their sustainable management, and tree species identification is one of the fundamental tasks in this process. Unmanned aerial vehicles (UAVs) and miniaturized lightweight sensors can rapidly provide accurate monitoring information. The objective of this study was to investigate the use of multitemporal, UAV-based hyperspectral imagery for tree species identification in the highly diverse Brazilian Atlantic forest. Datasets were captured over three years to identify eight different tree species. The study area comprised initial to medium successional stages of the Brazilian Atlantic forest. Images were acquired with a spatial resolution of 10 cm, and radiometric adjustment processing was performed to reduce the variations caused by different factors, such as the geometry of acquisition. The random forest classification method was applied in a region-based classification approach with leave-one-out cross-validation, followed by computing the area under the receiver operating characteristic (AUCROC) curve. When using each dataset alone, the influence of different weather behaviors on tree species identification was evident. When combining all datasets and minimizing illumination differences over each tree crown, the identification of three tree species was improved. These results show that UAV-based, hyperspectral, multitemporal remote sensing imagery is a promising tool for tree species identification in tropical forests.},
DOI = {10.3390/rs12020244}
}



@Article{electronics9010134,
AUTHOR = {Park, Sang-Soo and Chung, Ki-Seok},
TITLE = {CENNA: Cost-Effective Neural Network Accelerator},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {134},
URL = {https://www.mdpi.com/2079-9292/9/1/134},
ISSN = {2079-9292},
ABSTRACT = {Convolutional neural networks (CNNs) are widely adopted in various applications. State-of-the-art CNN models deliver excellent classification performance, but they require a large amount of computation and data exchange because they typically employ many processing layers. Among these processing layers, convolution layers, which carry out many multiplications and additions, account for a major portion of computation and memory access. Therefore, reducing the amount of computation and memory access is the key for high-performance CNNs. In this study, we propose a cost-effective neural network accelerator, named CENNA, whose hardware cost is reduced by employing a cost-centric matrix multiplication that employs both Strassen&rsquo;s multiplication and a na&iuml;ve multiplication. Furthermore, the convolution method using the proposed matrix multiplication can minimize data movement by reusing both the feature map and the convolution kernel without any additional control logic. In terms of throughput, power consumption, and silicon area, the efficiency of CENNA is up to 88 times higher than that of conventional designs for the CNN inference.},
DOI = {10.3390/electronics9010134}
}



@Article{rs12020245,
AUTHOR = {Senthilnath, J. and Varia, Neelanshi and Dokania, Akanksha and Anand, Gaotham and Benediktsson, Jón Atli},
TITLE = {Deep TEC: Deep Transfer Learning with Ensemble Classifier for Road Extraction from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {245},
URL = {https://www.mdpi.com/2072-4292/12/2/245},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) remote sensing has a wide area of applications and in this paper, we attempt to address one such problem&mdash;road extraction from UAV-captured RGB images. The key challenge here is to solve the road extraction problem using the UAV multiple remote sensing scene datasets that are acquired with different sensors over different locations. We aim to extract the knowledge from a dataset that is available in the literature and apply this extracted knowledge on our dataset. The paper focuses on a novel method which consists of deep TEC (deep transfer learning with ensemble classifier) for road extraction using UAV imagery. The proposed deep TEC performs road extraction on UAV imagery in two stages, namely, deep transfer learning and ensemble classifier. In the first stage, with the help of deep learning methods, namely, the conditional generative adversarial network, the cycle generative adversarial network and the fully convolutional network, the model is pre-trained on the benchmark UAV road extraction dataset that is available in the literature. With this extracted knowledge (based on the pre-trained model) the road regions are then extracted on our UAV acquired images. Finally, for the road classified images, ensemble classification is carried out. In particular, the deep TEC method has an average quality of 71%, which is 10% higher than the next best standard deep learning methods. Deep TEC also shows a higher level of performance measures such as completeness, correctness and F1 score measures. Therefore, the obtained results show that the deep TEC is efficient in extracting road networks in an urban region.},
DOI = {10.3390/rs12020245}
}



@Article{app10020515,
AUTHOR = {González-Patiño, David and Villuendas-Rey, Yenny and Argüelles-Cruz, Amadeo José and Camacho-Nieto, Oscar and Yáñez-Márquez, Cornelio},
TITLE = {AISAC: An Artificial Immune System for Associative Classification Applied to Breast Cancer Detection},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {515},
URL = {https://www.mdpi.com/2076-3417/10/2/515},
ISSN = {2076-3417},
ABSTRACT = {Early breast cancer diagnosis is crucial, as it can prevent further complications and save the life of the patient by treating the disease at its most curable stage. In this paper, we propose a new artificial immune system model for associative classification with competitive performance for breast cancer detection. The proposed model has its foundations in the biological immune system; it mimics the detection skills of the immune system to provide correct identification of antigens. The Wilcoxon test was used to identify the statistically significant differences between our proposal and other classification algorithms based on the same bio-inspired model. These statistical tests evidenced the enhanced performance shown by the proposed model by outperforming other immune-based algorithms. The proposed model proved to be competitive with respect to other well-known classification models. In addition, the model benefits from a low computational cost. The success of this model for classification tasks shows that swarm intelligence is useful for this kind of problem, and that it is not limited to optimization tasks.},
DOI = {10.3390/app10020515}
}



@Article{s20020435,
AUTHOR = {Cardim Ferreira Lima, Matheus and Krus, Anne and Valero, Constantino and Barrientos, Antonio and del Cerro, Jaime and Roldán-Gómez, Juan Jesús},
TITLE = {Monitoring Plant Status and Fertilization Strategy through Multispectral Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {435},
URL = {https://www.mdpi.com/1424-8220/20/2/435},
ISSN = {1424-8220},
ABSTRACT = {A crop monitoring system was developed for the supervision of organic fertilization status on tomato plants at early stages. An automatic and nondestructive approach was used to analyze tomato plants with different levels of water-soluble organic fertilizer (3 + 5 NK) and vermicompost. The evaluation system was composed by a multispectral camera with five lenses: green (550 nm), red (660 nm), red edge (735 nm), near infrared (790 nm), RGB, and a computational image processing system. The water-soluble fertilizer was applied weekly in four different treatments: (T0: 0 mL, T1: 6.25 mL, T2: 12.5 mL and T3: 25 mL) and the vermicomposting was added in Weeks 1 and 5. The trial was conducted in a greenhouse and 192 images were taken with each lens. A plant segmentation algorithm was developed and several vegetation indices were calculated. On top of calculating indices, multiple morphological features were obtained through image processing techniques. The morphological features were revealed to be more feasible to distinguish between the control and the organic fertilized plants than the vegetation indices. The system was developed in order to be assembled in a precision organic fertilization robotic platform.},
DOI = {10.3390/s20020435}
}



@Article{rs12020283,
AUTHOR = {Annala, Leevi and Honkavaara, Eija and Tuominen, Sakari and Pölönen, Ilkka},
TITLE = {Chlorophyll Concentration Retrieval by Training Convolutional Neural Network for Stochastic Model of Leaf Optical Properties (SLOP) Inversion},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {283},
URL = {https://www.mdpi.com/2072-4292/12/2/283},
ISSN = {2072-4292},
ABSTRACT = {Miniaturized hyperspectral imaging techniques have developed rapidly in recent years and have become widely available for different applications. Combining calibrated hyperspectral imagery with inverse physically based reflectance models is an interesting approach for estimating chlorophyll concentrations that are good indicators of vegetation health. The objective of this study was to develop a novel approach for retrieving chlorophyll a and b values from remotely sensed data by inverting the stochastic model of leaf optical properties using a one-dimensional convolutional neural network. The inversion results and retrieved values are validated in two ways: A classical machine learning validation dataset and calculating chlorophyll maps from empirical remotely sensed hyperspectral data and comparing them to     TCARI OSAVI    , an index that has strong negative correlation with chlorophyll concentration. With the validation dataset, coefficients of determination (    R 2    ) of 0.97 were obtained for chlorophyll a and 0.95 for chlorophyll b. The chlorophyll maps correlate with the     TCARI OSAVI     map. The correlation coefficient (R) is &minus;0.87 for chlorophyll a and &minus;0.68 for chlorophyll b in selected plots. These results indicate that the approach is highly promising approach for estimating vegetation chlorophyll content.},
DOI = {10.3390/rs12020283}
}



@Article{s20020484,
AUTHOR = {Viseras, Alberto and Xu, Zhe and Merino, Luis},
TITLE = {Distributed Multi-Robot Information Gathering under Spatio-Temporal Inter-Robot Constraints},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {484},
URL = {https://www.mdpi.com/1424-8220/20/2/484},
ISSN = {1424-8220},
ABSTRACT = {Information gathering (IG) algorithms aim to intelligently select the mobile robotic sensor actions required to efficiently obtain an accurate reconstruction of a physical process, such as an occupancy map, a wind field, or a magnetic field. Recently, multiple IG algorithms that benefit from multi-robot cooperation have been proposed in the literature. Most of these algorithms employ discretization of the state and action spaces, which makes them computationally intractable for robotic systems with complex dynamics. Moreover, they cannot deal with inter-robot restrictions such as collision avoidance or communication constraints. This paper presents a novel approach for multi-robot information gathering (MR-IG) that tackles the two aforementioned restrictions: (i) discretization of robot&rsquo;s state space, and (ii) dealing with inter-robot constraints. Here we propose an algorithm that employs: (i) an underlying model of the physical process of interest, (ii) sampling-based planners to plan paths in a continuous domain, and (iii) a distributed decision-making algorithm to enable multi-robot coordination. In particular, we use the max-sum algorithm for distributed decision-making by defining an information-theoretic utility function. This function maximizes IG, while fulfilling inter-robot communication and collision avoidance constraints. We validate our proposed approach in simulations, and in a field experiment where three quadcopters explore a simulated wind field. Results demonstrate the effectiveness and scalability with respect to the number of robots of our approach.},
DOI = {10.3390/s20020484}
}



@Article{app10020666,
AUTHOR = {Jung, Daekyo and Tran Tuan, Vu and Quoc Tran, Dai and Park, Minsoo and Park, Seunghee},
TITLE = {Conceptual Framework of an Intelligent Decision Support System for Smart City Disaster Management},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {666},
URL = {https://www.mdpi.com/2076-3417/10/2/666},
ISSN = {2076-3417},
ABSTRACT = {In order to protect human lives and infrastructure, as well as to minimize the risk of damage, it is important to predict and respond to natural disasters in advance. However, currently, the standardized disaster response system in South Korea still needs further advancement, and the response phase systems need to be improved to ensure that they are properly equipped to cope with natural disasters. Existing studies on intelligent disaster management systems (IDSSs) in South Korea have focused only on storms, floods, and earthquakes, and they have not used past data. This research proposes a new conceptual framework of an IDSS for disaster management, with particular attention paid to wildfires and cold/heat waves. The IDSS uses big data collected from open application programming interface (API) and artificial intelligence (AI) algorithms to help decision-makers make faster and more accurate decisions. In addition, a simple example of the use of a convolutional neural network (CNN) to detect fire in surveillance video has been developed, which can be used for automatic fire detection and provide an appropriate response. The system will also consider connecting to open source intelligence (OSINT) to identify vulnerabilities, mitigate risks, and develop more robust security policies than those currently in place to prevent cyber-attacks.},
DOI = {10.3390/app10020666}
}



@Article{rs12020322,
AUTHOR = {Agrafiotis, Panagiotis and Karantzalos, Konstantinos and Georgopoulos, Andreas and Skarlatos, Dimitrios},
TITLE = {Correcting Image Refraction: Towards Accurate Aerial Image-Based Bathymetry Mapping in Shallow Waters},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {322},
URL = {https://www.mdpi.com/2072-4292/12/2/322},
ISSN = {2072-4292},
ABSTRACT = {Although aerial image-based bathymetric mapping can provide, unlike acoustic or LiDAR (Light Detection and Ranging) sensors, both water depth and visual information, water refraction poses significant challenges for accurate depth estimation. In order to tackle this challenge, we propose an image correction methodology, which first exploits recent machine learning procedures that recover depth from image-based dense point clouds and then corrects refraction on the original imaging dataset. This way, the structure from motion (SfM) and multi-view stereo (MVS) processing pipelines are executed on a refraction-free set of aerial datasets, resulting in highly accurate bathymetric maps. Performed experiments and validation were based on datasets acquired during optimal sea state conditions and derived from four different test-sites characterized by excellent sea bottom visibility and textured seabed. Results demonstrated the high potential of our approach, both in terms of bathymetric accuracy, as well as texture and orthoimage quality.},
DOI = {10.3390/rs12020322}
}



@Article{en13020494,
AUTHOR = {Hossein Motlagh, Naser and Mohammadrezaei, Mahsa and Hunt, Julian and Zakeri, Behnam},
TITLE = {Internet of Things (IoT) and the Energy Sector},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {494},
URL = {https://www.mdpi.com/1996-1073/13/2/494},
ISSN = {1996-1073},
ABSTRACT = {Integration of renewable energy and optimization of energy use are key enablers of sustainable energy transitions and mitigating climate change. Modern technologies such the Internet of Things (IoT) offer a wide number of applications in the energy sector, i.e, in energy supply, transmission and distribution, and demand. IoT can be employed for improving energy efficiency, increasing the share of renewable energy, and reducing environmental impacts of the energy use. This paper reviews the existing literature on the application of IoT in in energy systems, in general, and in the context of smart grids particularly. Furthermore, we discuss enabling technologies of IoT, including cloud computing and different platforms for data analysis. Furthermore, we review challenges of deploying IoT in the energy sector, including privacy and security, with some solutions to these challenges such as blockchain technology. This survey provides energy policy-makers, energy economists, and managers with an overview of the role of IoT in optimization of energy systems.},
DOI = {10.3390/en13020494}
}



@Article{electronics9010193,
AUTHOR = {Sun, Yunlong and Guan, Lianwu and Wu, Menghao and Gao, Yanbin and Chang, Zhanyuan},
TITLE = {Vehicular Navigation Based on the Fusion of 3D-RISS and Machine Learning Enhanced Visual Data in Challenging Environments},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {193},
URL = {https://www.mdpi.com/2079-9292/9/1/193},
ISSN = {2079-9292},
ABSTRACT = {Based on the 3D Reduced Inertial Sensor System (3D-RISS) and the Machine Learning Enhanced Visual Data (MLEVD), an integrated vehicle navigation system is proposed in this paper. In demanding conditions such as outdoor satellite signal interference and indoor navigation, this work incorporates vehicle smooth navigation. Firstly, a landmark is set up and both of its size and position are accurately measured. Secondly, the image with the landmark information is captured quickly by using the machine learning. Thirdly, the template matching method and the Extended Kalman Filter (EKF) are then used to correct the errors of the Inertial Navigation System (INS), which employs the 3D-RISS to reduce the overall cost and ensuring the vehicular positioning accuracy simultaneously. Finally, both outdoor and indoor experiments are conducted to verify the performance of the 3D-RISS/MLEVD integrated navigation technology. Results reveal that the proposed method can effectively reduce the accumulated error of the INS with time while maintaining the positioning error within a few meters.},
DOI = {10.3390/electronics9010193}
}



@Article{rs12020336,
AUTHOR = {Zhang, Yishan and Wu, Lun and Ren, Huazhong and Liu, Yu and Zheng, Yongqian and Liu, Yaowen and Dong, Jiaji},
TITLE = {Mapping Water Quality Parameters in Urban Rivers from Hyperspectral Images Using a New Self-Adapting Selection of Multiple Artificial Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {336},
URL = {https://www.mdpi.com/2072-4292/12/2/336},
ISSN = {2072-4292},
ABSTRACT = {Protection of water environments is an important part of overall environmental protection; hence, many people devote their efforts to monitoring and improving water quality. In this study, a self-adapting selection method of multiple artificial neural networks (ANNs) using hyperspectral remote sensing and ground-measured water quality data is proposed to quantitatively predict water quality parameters, including phosphorus, nitrogen, biochemical oxygen demand (BOD), chemical oxygen demand (COD), and chlorophyll a. Seventy-nine ground measured data samples are used as training data in the establishment of the proposed model, and 30 samples are used as testing data. The proposed method based on traditional ANNs of numerical prediction involves feature selection of bands, self-adapting selection based on multiple selection criteria, stepwise backtracking, and combined weighted correlation. Water quality parameters are estimated with coefficient of determination      R 2      ranging from 0.93 (phosphorus) to 0.98 (nitrogen), which is higher than the value (0.7 to 0.8) obtained by traditional ANNs. MPAE (mean percent of absolute error) values ranging from 5% to 11% are used rather than root mean square error to evaluate the predicting precision of the proposed model because the magnitude of each water quality parameter considerably differs, thereby providing reasonable and interpretable results. Compared with other ANNs with backpropagation, this study proposes an auto-adapting method assisted by the above-mentioned methods to select the best model with all settings, such as the number of hidden layers, number of neurons in each hidden layer, choice of optimizer, and activation function. Different settings for ANNS with backpropagation are important to improve precision and compatibility for different data. Furthermore, the proposed method is applied to hyperspectral remote sensing images collected using an unmanned aerial vehicle for monitoring the water quality in the Shiqi River, Zhongshan City, Guangdong Province, China. Obtained results indicate the locations of pollution sources.},
DOI = {10.3390/rs12020336}
}



@Article{agronomy10020175,
AUTHOR = {Apolo-Apolo, Orly Enrique and Pérez-Ruiz, Manuel and Martínez-Guanter, Jorge and Egea, Gregorio},
TITLE = {A Mixed Data-Based Deep Neural Network to Estimate Leaf Area Index in Wheat Breeding Trials},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {175},
URL = {https://www.mdpi.com/2073-4395/10/2/175},
ISSN = {2073-4395},
ABSTRACT = {Remote and non-destructive estimation of leaf area index (LAI) has been a challenge in the last few decades as the direct and indirect methods available are laborious and time-consuming. The recent emergence of high-throughput plant phenotyping platforms has increased the need to develop new phenotyping tools for better decision-making by breeders. In this paper, a novel model based on artificial intelligence algorithms and nadir-view red green blue (RGB) images taken from a terrestrial high throughput phenotyping platform is presented. The model mixes numerical data collected in a wheat breeding field and visual features extracted from the images to make rapid and accurate LAI estimations. Model-based LAI estimations were validated against LAI measurements determined non-destructively using an allometric relationship obtained in this study. The model performance was also compared with LAI estimates obtained by other classical indirect methods based on bottom-up hemispherical images and gaps fraction theory. Model-based LAI estimations were highly correlated with ground-truth LAI. The model performance was slightly better than that of the hemispherical image-based method, which tended to underestimate LAI. These results show the great potential of the developed model for near real-time LAI estimation, which can be further improved in the future by increasing the dataset used to train the model.},
DOI = {10.3390/agronomy10020175}
}



@Article{s20030743,
AUTHOR = {Haque, Akkas and Elsaharti, Ahmed and Elderini, Tarek and Elsaharty, Mohamed Atef and Neubert, Jeremiah},
TITLE = {UAV Autonomous Localization Using Macro-Features Matching with a CAD Model},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {743},
URL = {https://www.mdpi.com/1424-8220/20/3/743},
ISSN = {1424-8220},
ABSTRACT = {Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has significantly advanced in recent years, mainly due to their relevance in a large variety of commercial, industrial, and military applications. However, UAV navigation in GPS-denied environments continues to be a challenging problem that has been tackled in recent research through sensor-based approaches. This paper presents a novel offline, portable, real-time in-door UAV localization technique that relies on macro-feature detection and matching. The proposed system leverages the support of machine learning, traditional computer vision techniques, and pre-existing knowledge of the environment. The main contribution of this work is the real-time creation of a macro-feature description vector from the UAV captured images which are simultaneously matched with an offline pre-existing vector from a Computer-Aided Design (CAD) model. This results in a quick UAV localization within the CAD model. The effectiveness and accuracy of the proposed system were evaluated through simulations and experimental prototype implementation. Final results reveal the algorithm&rsquo;s low computational burden as well as its ease of deployment in GPS-denied environments.},
DOI = {10.3390/s20030743}
}



@Article{rs12030458,
AUTHOR = {Alganci, Ugur and Soydas, Mehmet and Sertel, Elif},
TITLE = {Comparative Research on Deep Learning Approaches for Airplane Detection from Very High-Resolution Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {458},
URL = {https://www.mdpi.com/2072-4292/12/3/458},
ISSN = {2072-4292},
ABSTRACT = {Object detection from satellite images has been a challenging problem for many years. With the development of effective deep learning algorithms and advancement in hardware systems, higher accuracies have been achieved in the detection of various objects from very high-resolution (VHR) satellite images. This article provides a comparative evaluation of the state-of-the-art convolutional neural network (CNN)-based object detection models, which are Faster R-CNN, Single Shot Multi-box Detector (SSD), and You Look Only Once-v3 (YOLO-v3), to cope with the limited number of labeled data and to automatically detect airplanes in VHR satellite images. Data augmentation with rotation, rescaling, and cropping was applied on the test images to artificially increase the number of training data from satellite images. Moreover, a non-maximum suppression algorithm (NMS) was introduced at the end of the SSD and YOLO-v3 flows to get rid of the multiple detection occurrences near each detected object in the overlapping areas. The trained networks were applied to five independent VHR test images that cover airports and their surroundings to evaluate their performance objectively. Accuracy assessment results of the test regions proved that Faster R-CNN architecture provided the highest accuracy according to the F1 scores, average precision (AP) metrics, and visual inspection of the results. The YOLO-v3 ranked as second, with a slightly lower performance but providing a balanced trade-off between accuracy and speed. The SSD provided the lowest detection performance, but it was better in object localization. The results were also evaluated in terms of the object size and detection accuracy manner, which proved that large- and medium-sized airplanes were detected with higher accuracy.},
DOI = {10.3390/rs12030458}
}



@Article{agronomy10020207,
AUTHOR = {Saiz-Rubio, Verónica and Rovira-Más, Francisco},
TITLE = {From Smart Farming towards Agriculture 5.0: A Review on Crop Data Management},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {207},
URL = {https://www.mdpi.com/2073-4395/10/2/207},
ISSN = {2073-4395},
ABSTRACT = {The information that crops offer is turned into profitable decisions only when efficiently managed. Current advances in data management are making Smart Farming grow exponentially as data have become the key element in modern agriculture to help producers with critical decision-making. Valuable advantages appear with objective information acquired through sensors with the aim of maximizing productivity and sustainability. This kind of data-based managed farms rely on data that can increase efficiency by avoiding the misuse of resources and the pollution of the environment. Data-driven agriculture, with the help of robotic solutions incorporating artificial intelligent techniques, sets the grounds for the sustainable agriculture of the future. This paper reviews the current status of advanced farm management systems by revisiting each crucial step, from data acquisition in crop fields to variable rate applications, so that growers can make optimized decisions to save money while protecting the environment and transforming how food will be produced to sustainably match the forthcoming population growth.},
DOI = {10.3390/agronomy10020207}
}



@Article{rs12030502,
AUTHOR = {Chang, Zhilu and Du, Zhen and Zhang, Fan and Huang, Faming and Chen, Jiawu and Li, Wenbin and Guo, Zizheng},
TITLE = {Landslide Susceptibility Prediction Based on Remote Sensing Images and GIS: Comparisons of Supervised and Unsupervised Machine Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {502},
URL = {https://www.mdpi.com/2072-4292/12/3/502},
ISSN = {2072-4292},
ABSTRACT = {Landslide susceptibility prediction (LSP) has been widely and effectively implemented by machine learning (ML) models based on remote sensing (RS) images and Geographic Information System (GIS). However, comparisons of the applications of ML models for LSP from the perspectives of supervised machine learning (SML) and unsupervised machine learning (USML) have not been explored. Hence, this study aims to compare the LSP performance of these SML and USML models, thus further to explore the advantages and disadvantages of these ML models and to realize a more accurate and reliable LSP result. Two representative SML models (support vector machine (SVM) and CHi-squared Automatic Interaction Detection (CHAID)) and two representative USML models (K-means and Kohonen models) are respectively used to scientifically predict the landslide susceptibility indexes, and then these prediction results are discussed. Ningdu County with 446 recorded landslides obtained through field investigations is introduced as case study. A total of 12 conditioning factors are obtained through procession of Landsat TM 8 images and high-resolution aerial images, topographical and hydrological spatial analysis of Digital Elevation Modeling in GIS software, and government reports. The area value under the curve of receiver operating features (AUC) is applied for evaluating the prediction accuracy of SML models, and the frequency ratio (FR) accuracy is then introduced to compare the remarkable prediction performance differences between SML and USML models. Overall, the receiver operation curve (ROC) results show that the AUC of the SVM is 0.892 and is slightly greater than the AUC of the CHAID model (0.872). The FR accuracy results show that the SVM model has the highest accuracy for LSP (77.80%), followed by the CHAID model (74.50%), the Kohonen model (72.8%) and the K-means model (69.7%), which indicates that the SML models can reach considerably better prediction capability than the USML models. It can be concluded that selecting recorded landslides as prior knowledge to train and test the LSP models is the key reason for the higher prediction accuracy of the SML models, while the lack of a priori knowledge and target guidance is an important reason for the low LSP accuracy of the USML models. Nevertheless, the USML models can also be used to implement LSP due to their advantages of efficient modeling processes, dimensionality reduction and strong scalability.},
DOI = {10.3390/rs12030502}
}



@Article{rs12030508,
AUTHOR = {Fu, Zhaopeng and Jiang, Jie and Gao, Yang and Krienke, Brian and Wang, Meng and Zhong, Kaitai and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
TITLE = {Wheat Growth Monitoring and Yield Estimation based on Multi-Rotor Unmanned Aerial Vehicle},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {508},
URL = {https://www.mdpi.com/2072-4292/12/3/508},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) and leaf dry matter (LDM) are important indices of crop growth. Real-time, nondestructive monitoring of crop growth is instructive for the diagnosis of crop growth and prediction of grain yield. Unmanned aerial vehicle (UAV)-based remote sensing is widely used in precision agriculture due to its unique advantages in flexibility and resolution. This study was carried out on wheat trials treated with different nitrogen levels and seeding densities in three regions of Jiangsu Province in 2018&ndash;2019. Canopy spectral images were collected by the UAV equipped with a multi-spectral camera during key wheat growth stages. To verify the results of the UAV images, the LAI, LDM, and yield data were obtained by destructive sampling. We extracted the wheat canopy reflectance and selected the best vegetation index for monitoring growth and predicting yield. Simple linear regression (LR), multiple linear regression (MLR), stepwise multiple linear regression (SMLR), partial least squares regression (PLSR), artificial neural network (ANN), and random forest (RF) modeling methods were used to construct a model for wheat yield estimation. The results show that the multi-spectral camera mounted on the multi-rotor UAV has a broad application prospect in crop growth index monitoring and yield estimation. The vegetation index combined with the red edge band and the near-infrared band was significantly correlated with LAI and LDM. Machine learning methods (i.e., PLSR, ANN, and RF) performed better for predicting wheat yield. The RF model constructed by normalized difference vegetation index (NDVI) at the jointing stage, heading stage, flowering stage, and filling stage was the optimal wheat yield estimation model in this study, with an R2 of 0.78 and relative root mean square error (RRMSE) of 0.1030. The results provide a theoretical basis for monitoring crop growth with a multi-rotor UAV platform and explore a technical method for improving the precision of yield estimation.},
DOI = {10.3390/rs12030508}
}



@Article{app10031092,
AUTHOR = {Benjdira, Bilel and Ammar, Adel and Koubaa, Anis and Ouni, Kais},
TITLE = {Data-Efficient Domain Adaptation for Semantic Segmentation of Aerial Imagery Using Generative Adversarial Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {1092},
URL = {https://www.mdpi.com/2076-3417/10/3/1092},
ISSN = {2076-3417},
ABSTRACT = {Despite the significant advances noted in semantic segmentation of aerial imagery, a considerable limitation is blocking its adoption in real cases. If we test a segmentation model on a new area that is not included in its initial training set, accuracy will decrease remarkably. This is caused by the domain shift between the new targeted domain and the source domain used to train the model. In this paper, we addressed this challenge and proposed a new algorithm that uses Generative Adversarial Networks (GAN) architecture to minimize the domain shift and increase the ability of the model to work on new targeted domains. The proposed GAN architecture contains two GAN networks. The first GAN network converts the chosen image from the target domain into a semantic label. The second GAN network converts this generated semantic label into an image that belongs to the source domain but conserves the semantic map of the target image. This resulting image will be used by the semantic segmentation model to generate a better semantic label of the first chosen image. Our algorithm is tested on the ISPRS semantic segmentation dataset and improved the global accuracy by a margin up to 24% when passing from Potsdam domain to Vaihingen domain. This margin can be increased by addition of other labeled data from the target domain. To minimize the cost of supervision in the translation process, we proposed a methodology to use these labeled data efficiently.},
DOI = {10.3390/app10031092}
}



@Article{en13030713,
AUTHOR = {Han, Jiaming and Yang, Zhong and Xu, Hao and Hu, Guoxiong and Zhang, Chi and Li, Hongchen and Lai, Shangxiang and Zeng, Huarong},
TITLE = {Search Like an Eagle: A Cascaded Model for Insulator Missing Faults Detection in Aerial Images},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {713},
URL = {https://www.mdpi.com/1996-1073/13/3/713},
ISSN = {1996-1073},
ABSTRACT = {Insulator missing fault is a serious accident of high-voltage transmission lines, which can cause abnormal energy supply. Recently, a lot of vision-based methods are proposed for detecting an insulator missing fault in aerial images. However, these methods usually lack efficiency and robustness due to the effect of the complex background interferences in the aerial images. More importantly, most of these methods cannot address the insulator multi-fault detection. This paper proposes an unprecedented cascaded model to detect insulator multi-fault in the aerial images to solve the existing challenges. Firstly, a total of 764 images are adopted to create a novel insulator missing faults dataset &lsquo;IMF-detection&rsquo;. Secondly, a new network is proposed to locate the insulator string from the complex background. Then, the located region that contains the insulator string is set to be an RoI (region of interest) region. Finally, the YOLO-v3 tiny network is trained and then used to detect the insulator missing faults in the RoI region. Experimental results and analysis validate that the proposed method is more efficient and robust than some previous works. Most importantly, the average running time of the proposed method is about 30ms, which demonstrates that it has the potential to be adopted for the on-line detection of insulator missing faults.},
DOI = {10.3390/en13030713}
}



@Article{sym12020254,
AUTHOR = {Chen, Bin and Wang, Yiduo and Wang, Rongxiao and Zhu, Zhengqiu and Ma, Liang and Qiu, Xiaogang and Dai, Weihui},
TITLE = {The Gray-Box Based Modeling Approach Integrating Both Mechanism-Model and Data-Model: The Case of Atmospheric Contaminant Dispersion},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {254},
URL = {https://www.mdpi.com/2073-8994/12/2/254},
ISSN = {2073-8994},
ABSTRACT = {With the profound understanding of the world, modeling and simulation has been used to solve the problems of complex systems. Generally, mechanism-models are often used to model the engineering systems following the Newton laws, and this kind of modeling approach is called white-box modeling; however, when the internal structure and characteristics of some systems are hard to understand, the black-box modeling based on statistic and data-modeling is often used. For most complex real systems, a single modeling approach can hardly describe the target system accurately. In this paper, we firstly discuss and compare the white-box and black-box modeling approaches. Then, to mitigate the limitations of these two modeling methods in mechanism-partially-observed systems, the gray-box based modeling approach integrating both a mechanism model and data model is proposed. In order to explain the idea of gray-box based modeling, the atmosphere dispersion modeling is studied in practical cases from two symmetric aspects. Specifically, the framework of data assimilation is used to illustrate the modeling from white-box to gray-box, while the Gauss features based Support Vector Regression (SVR) models are used to illustrate the modeling from black-box to gray-box. To verify the feasibility of the gray-box modeling method, we conducted both simulation experiments and real dataset symmetry experiments. The experiment results show the enhanced performance of the gray-box based modeling approach. In the end, we expect that this gray-box based modeling approach will be an alternative modeling approach for different existing systems.},
DOI = {10.3390/sym12020254}
}



@Article{rs12030547,
AUTHOR = {Maxwell, Aaron E. and Pourmohammadi, Pariya and Poyner, Joey D.},
TITLE = {Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {547},
URL = {https://www.mdpi.com/2072-4292/12/3/547},
ISSN = {2072-4292},
ABSTRACT = {Modern elevation-determining remote sensing technologies such as light-detection and ranging (LiDAR) produce a wealth of topographic information that is increasingly being used in a wide range of disciplines, including archaeology and geomorphology. However, automated methods for mapping topographic features have remained a significant challenge. Deep learning (DL) mask regional-convolutional neural networks (Mask R-CNN), which provides context-based instance mapping, offers the potential to overcome many of the difficulties of previous approaches to topographic mapping. We therefore explore the application of Mask R-CNN to extract valley fill faces (VFFs), which are a product of mountaintop removal (MTR) coal mining in the Appalachian region of the eastern United States. LiDAR-derived slopeshades are provided as the only predictor variable in the model. Model generalization is evaluated by mapping multiple study sites outside the training data region. A range of assessment methods, including precision, recall, and F1 score, all based on VFF counts, as well as area- and a fuzzy area-based user&rsquo;s and producer&rsquo;s accuracy, indicate that the model was successful in mapping VFFs in new geographic regions, using elevation data derived from different LiDAR sensors. Precision, recall, and F1-score values were above 0.85 using VFF counts while user&rsquo;s and producer&rsquo;s accuracy were above 0.75 and 0.85 when using the area- and fuzzy area-based methods, respectively, when averaged across all study areas characterized with LiDAR data. Due to the limited availability of LiDAR data until relatively recently, we also assessed how well the model generalizes to terrain data created using photogrammetric methods that characterize past terrain conditions. Unfortunately, the model was not sufficiently general to allow successful mapping of VFFs using photogrammetrically-derived slopeshades, as all assessment metrics were lower than 0.60; however, this may partially be attributed to the quality of the photogrammetric data. The overall results suggest that the combination of Mask R-CNN and LiDAR has great potential for mapping anthropogenic and natural landscape features. To realize this vision, however, research on the mapping of other topographic features is needed, as well as the development of large topographic training datasets including a variety of features for calibrating and testing new methods.},
DOI = {10.3390/rs12030547}
}



@Article{s20030907,
AUTHOR = {da Rosa, Ricardo and Aurelio Wehrmeister, Marco and Brito, Thadeu and Lima, José Luís and Pereira, Ana Isabel Pinheiro Nunes},
TITLE = {Honeycomb Map: A Bioinspired Topological Map for Indoor Search and Rescue Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {907},
URL = {https://www.mdpi.com/1424-8220/20/3/907},
ISSN = {1424-8220},
ABSTRACT = {The use of robots to map disaster-stricken environments can prevent rescuers from being harmed when exploring an unknown space. In addition, mapping a multi-robot environment can help these teams plan their actions with prior knowledge. The present work proposes the use of multiple unmanned aerial vehicles (UAVs) in the construction of a topological map inspired by the way that bees build their hives. A UAV can map a honeycomb only if it is adjacent to a known one. Different metrics to choose the honeycomb to be explored were applied. At the same time, as UAVs scan honeycomb adjacencies, RGB-D and thermal sensors capture other data types, and then generate a 3D view of the space and images of spaces where there may be fire spots, respectively. Simulations in different environments showed that the choice of metric and variation in the number of UAVs influence the number of performed displacements in the environment, consequently affecting exploration time and energy use.},
DOI = {10.3390/s20030907}
}



@Article{app10031144,
AUTHOR = {Zhang, Xueqi and Zhao, Meng and Dong, Rencai},
TITLE = {Time-Series Prediction of Environmental Noise for Urban IoT Based on Long Short-Term Memory Recurrent Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {1144},
URL = {https://www.mdpi.com/2076-3417/10/3/1144},
ISSN = {2076-3417},
ABSTRACT = {Noise pollution is one of the major urban environmental pollutions, and it is increasingly becoming a matter of crucial public concern. Monitoring and predicting environmental noise are of great significance for the prevention and control of noise pollution. With the advent of the Internet of Things (IoT) technology, urban noise monitoring is emerging in the direction of a small interval, long time, and large data amount, which is difficult to model and predict with traditional methods. In this study, an IoT-based noise monitoring system was deployed to acquire the environmental noise data, and a two-layer long short-term memory (LSTM) network was proposed for the prediction of environmental noise under the condition of large data volume. The optimal hyperparameters were selected through testing, and the raw data sets were processed. The urban environmental noise was predicted at time intervals of 1 s, 1 min, 10 min, and 30 min, and their performances were compared with three classic predictive models: random walk (RW), stacked autoencoder (SAE), and support vector machine (SVM). The proposed model outperforms the other three existing classic methods. The time interval of the data set has a close connection with the performance of all models. The results revealed that the LSTM network could reflect changes in noise levels within one day and has good prediction accuracy. Impacts of monitoring point location on prediction results and recommendations for environmental noise management were also discussed in this paper.},
DOI = {10.3390/app10031144}
}



@Article{rs12030577,
AUTHOR = {Perera, Asanka G. and Khanam, Fatema-Tuz-Zohra and Al-Naji, Ali and Chahl, Javaan},
TITLE = {Detection and Localisation of Life Signs from the Air Using Image Registration and Spatio-Temporal Filtering},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {577},
URL = {https://www.mdpi.com/2072-4292/12/3/577},
ISSN = {2072-4292},
ABSTRACT = {In search and rescue operations, it is crucial to rapidly identify those people who are alive from those who are not. If this information is known, emergency teams can prioritize their operations to save more lives. However, in some natural disasters the people may be lying on the ground covered with dust, debris, or ashes making them difficult to detect by video analysis that is tuned to human shapes. We present a novel method to estimate the locations of people from aerial video using image and signal processing designed to detect breathing movements. We have shown that this method can successfully detect clearly visible people and people who are fully occluded by debris. First, the aerial videos were stabilized using the key points of adjacent image frames. Next, the stabilized video was decomposed into tile videos and the temporal frequency bands of interest were motion magnified while the other frequencies were suppressed. Image differencing and temporal filtering were performed on each tile video to detect potential breathing signals. Finally, the detected frequencies were remapped to the image frame creating a life signs map that indicates possible human locations. The proposed method was validated with both aerial and ground recorded videos in a controlled environment. Based on the dataset, the results showed good reliability for aerial videos and no errors for ground recorded videos where the average precision measures for aerial videos and ground recorded videos were 0.913 and 1 respectively.},
DOI = {10.3390/rs12030577}
}



@Article{rs12040597,
AUTHOR = {Anand, Akash and Pandey, Prem Chandra and Petropoulos, George P. and Pavlides, Andrew and Srivastava, Prashant K. and Sharma, Jyoti K. and Malhi, Ramandeep Kaur M.},
TITLE = {Use of Hyperion for Mangrove Forest Carbon Stock Assessment in Bhitarkanika Forest Reserve: A Contribution Towards Blue Carbon Initiative},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {597},
URL = {https://www.mdpi.com/2072-4292/12/4/597},
ISSN = {2072-4292},
ABSTRACT = {Mangrove forest coastal ecosystems contain significant amount of carbon stocks and contribute to approximately 15% of the total carbon sequestered in ocean sediments. The present study aims at exploring the ability of Earth Observation EO-1 Hyperion hyperspectral sensor in estimating aboveground carbon stocks in mangrove forests. Bhitarkanika mangrove forest has been used as case study, where field measurements of the biomass and carbon were acquired simultaneously with the satellite data. The spatial distribution of most dominant mangrove species was identified using the Spectral Angle Mapper (SAM) classifier, which was implemented using the spectral profiles extracted from the hyperspectral data. SAM performed well, identifying the total area that each of the major species covers (overall kappa = 0.81). From the hyperspectral images, the NDVI (Normalized Difference Vegetation Index) and EVI (Enhanced Vegetation Index) were applied to assess the carbon stocks of the various species using machine learning (Linear, Polynomial, Logarithmic, Radial Basis Function (RBF), and Sigmoidal Function) models. NDVI and EVI is generated using covariance matrix based band selection algorithm. All the five machine learning models were tested between the carbon measured in the field sampling and the carbon estimated by the vegetation indices NDVI and EVI was satisfactory (Pearson correlation coefficient, R, of 86.98% for EVI and of 84.1% for NDVI), with the RBF model showing the best results in comparison to other models. As such, the aboveground carbon stocks for species-wise mangrove for the study area was estimated. Our study findings confirm that hyperspectral images such as those from Hyperion can be used to perform species-wise mangrove analysis and assess the carbon stocks with satisfactory accuracy.},
DOI = {10.3390/rs12040597}
}



@Article{rs12040620,
AUTHOR = {Zhang, Jing and Tian, Haiqing and Wang, Di and Li, Haijun and Mouazen, Abdul Mounem},
TITLE = {A Novel Approach for Estimation of Above-Ground Biomass of Sugar Beet Based on Wavelength Selection and Optimized Support Vector Machine},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {620},
URL = {https://www.mdpi.com/2072-4292/12/4/620},
ISSN = {2072-4292},
ABSTRACT = {Timely diagnosis of sugar beet above-ground biomass (AGB) is critical for the prediction of yield and optimal precision crop management. This study established an optimal quantitative prediction model of AGB of sugar beet by using hyperspectral data. Three experiment campaigns in 2014, 2015 and 2018 were conducted to collect ground-based hyperspectral data at three different growth stages, across different sites, for different cultivars and nitrogen (N) application rates. A competitive adaptive reweighted sampling (CARS) algorithm was applied to select the most sensitive wavelengths to AGB. This was followed by developing a novel modified differential evolution grey wolf optimization algorithm (MDE&ndash;GWO) by introducing differential evolution algorithm (DE) and dynamic non-linear convergence factor to grey wolf optimization algorithm (GWO) to optimize the parameters c and &gamma; of a support vector machine (SVM) model for the prediction of AGB. The prediction performance of SVM models under the three GWO, DE&ndash;GWO and MDE&ndash;GWO optimization methods for CARS selected wavelengths and whole spectral data was examined. Results showed that CARS resulted in a huge wavelength reduction of 97.4% for the rapid growth stage of leaf cluster, 97.2% for the sugar growth stage and 97.4% for the sugar accumulation stage. Models resulted after CARS wavelength selection were found to be more accurate than models developed using the entire spectral data. The best prediction accuracy was achieved after the MDE&ndash;GWO optimization of SVM model parameters for the prediction of AGB in sugar beet, independent of growing stage, years, sites and cultivars. The best coefficient of determination (R2), root mean square error (RMSE) and residual prediction deviation (RPD) ranged, respectively, from 0.74 to 0.80, 46.17 to 65.68 g/m2 and 1.42 to 1.97 for the rapid growth stage of leaf cluster, 0.78 to 0.80, 30.16 to 37.03 g/m2 and 1.69 to 2.03 for the sugar growth stage, and 0.69 to 0.74, 40.17 to 104.08 g/m2 and 1.61 to 1.95 for the sugar accumulation stage. It can be concluded that the methodology proposed can be implemented for the prediction of AGB of sugar beet using proximal hyperspectral sensors under a wide range of environmental conditions.},
DOI = {10.3390/rs12040620}
}



@Article{w12020531,
AUTHOR = {Erena, Manuel and Domínguez, José A. and Atenza, Joaquín F. and García-Galiano, Sandra and Soria, Juan and Pérez-Ruzafa, Ángel},
TITLE = {Bathymetry Time Series Using High Spatial Resolution Satellite Images},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {531},
URL = {https://www.mdpi.com/2073-4441/12/2/531},
ISSN = {2073-4441},
ABSTRACT = {The use of the new generation of remote sensors, such as echo sounders and Global Navigation Satellite System (GNSS) receivers with differential correction installed in a drone, allows the acquisition of high-precision data in areas of shallow water, as in the case of the channel of the Encañizadas in the Mar Menor lagoon. This high precision information is the first step to develop the methodology to monitor the bathymetry of the Mar Menor channels. The use of high spatial resolution satellite images is the solution for monitoring many hydrological changes and it is the basis of the three-dimensional (3D) numerical models used to study transport over time, environmental variability, and water ecosystem complexity.},
DOI = {10.3390/w12020531}
}



@Article{app10041275,
AUTHOR = {Wei, Zizhuang and Wang, Yao and Yi, Hongwei and Chen, Yisong and Wang, Guoping},
TITLE = {Semantic 3D Reconstruction with Learning MVS and 2D Segmentation of Aerial Images},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1275},
URL = {https://www.mdpi.com/2076-3417/10/4/1275},
ISSN = {2076-3417},
ABSTRACT = {Semantic modeling is a challenging task that has received widespread attention in recent years. With the help of mini Unmanned Aerial Vehicles (UAVs), multi-view high-resolution aerial images of large-scale scenes can be conveniently collected. In this paper, we propose a semantic Multi-View Stereo (MVS) method to reconstruct 3D semantic models from 2D images. Firstly, 2D semantic probability distribution is obtained by Convolutional Neural Network (CNN). Secondly, the calibrated cameras poses are determined by Structure from Motion (SfM), while the depth maps are estimated by learning MVS. Combining 2D segmentation and 3D geometry information, dense point clouds with semantic labels are generated by a probability-based semantic fusion method. In the final stage, the coarse 3D semantic point cloud is optimized by both local and global refinements. By making full use of the multi-view consistency, the proposed method efficiently produces a fine-level 3D semantic point cloud. The experimental result evaluated by re-projection maps achieves 88.4% Pixel Accuracy on the Urban Drone Dataset (UDD). In conclusion, our graph-based semantic fusion procedure and refinement based on local and global information can suppress and reduce the re-projection error.},
DOI = {10.3390/app10041275}
}



@Article{s20041030,
AUTHOR = {Wang, Jiaquan and Huang, Qijun and Ma, Qiming and Chang, Sheng and He, Jin and Wang, Hao and Zhou, Xiao and Xiao, Fang and Gao, Chao},
TITLE = {Classification of VLF/LF Lightning Signals Using Sensors and Deep Learning Methods},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1030},
URL = {https://www.mdpi.com/1424-8220/20/4/1030},
ISSN = {1424-8220},
ABSTRACT = {Lightning waveform plays an important role in lightning observation, location, and lightning disaster investigation. Based on a large amount of lightning waveform data provided by existing real-time very low frequency/low frequency (VLF/LF) lightning waveform acquisition equipment, an automatic and accurate lightning waveform classification method becomes extremely important. With the widespread application of deep learning in image and speech recognition, it becomes possible to use deep learning to classify lightning waveforms. In this study, 50,000 lightning waveform samples were collected. The data was divided into the following categories: positive cloud ground flash, negative cloud ground flash, cloud ground flash with ionosphere reflection signal, positive narrow bipolar event, negative narrow bipolar event, positive pre-breakdown process, negative pre-breakdown process, continuous multi-pulse cloud flash, bipolar pulse, skywave. A multi-layer one-dimensional convolutional neural network (1D-CNN) was designed to automatically extract VLF/LF lightning waveform features and distinguish lightning waveforms. The model achieved an overall accuracy of 99.11% in the lightning dataset and overall accuracy of 97.55% in a thunderstorm process. Considering its excellent performance, this model could be used in lightning sensors to assist in lightning monitoring and positioning.},
DOI = {10.3390/s20041030}
}



@Article{antiox9020156,
AUTHOR = {Karydas, Christos and Iatrou, Miltiadis and Kouretas, Dimitrios and Patouna, Anastasia and Iatrou, George and Lazos, Nikolaos and Gewehr, Sandra and Tseni, Xanthi and Tekos, Fotis and Zartaloudis, Zois and Mainos, Evangelos and Mourelatos, Spiros},
TITLE = {Prediction of Antioxidant Activity of Cherry Fruits from UAS Multispectral Imagery Using Machine Learning},
JOURNAL = {Antioxidants},
VOLUME = {9},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {156},
URL = {https://www.mdpi.com/2076-3921/9/2/156},
PubMedID = {32075036},
ISSN = {2076-3921},
ABSTRACT = {In this research, a model for the estimation of antioxidant content in cherry fruits from multispectral imagery acquired from drones was developed, based on machine learning methods. For two consecutive cultivation years, the trees were sampled on different dates and then analysed for their fruits&rsquo; radical scavenging activity (DPPH) and Folin&ndash;Ciocalteu (FCR) reducing capacity. Multispectral images from unmanned aerial vehicles were acquired on the same dates with fruit sampling. Soil samples were collected throughout the study fields at the end of the season. Topographic, hydrographic and weather data also were included in modelling. First-year data were used for model-fitting, whereas second-year data for testing. Spatial autocorrelation tests indicated unbiased sampling and, moreover, allowed restriction of modelling input parameters to a smaller group. The optimum model employs 24 input variables resulting in a 6.74 root mean square error. Provided that soil profiles and other ancillary data are known in advance of the cultivation season, capturing drone images in critical growth phases, together with contemporary weather data, can support site- and time-specific harvesting. It could also support site-specific treatments (precision farming) for improving fruit quality in the long-term, with analogous marketing perspectives.},
DOI = {10.3390/antiox9020156}
}



@Article{rs12040638,
AUTHOR = {Hufkens, Koen and de Haulleville, Thalès and Kearsley, Elizabeth and Jacobsen, Kim and Beeckman, Hans and Stoffelen, Piet and Vandelook, Filip and Meeus, Sofie and Amara, Michael and Van Hirtum, Leen and Van den Bulcke, Jan and Verbeeck, Hans and Wingate, Lisa},
TITLE = {Historical Aerial Surveys Map Long-Term Changes of Forest Cover and Structure in the Central Congo Basin},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {638},
URL = {https://www.mdpi.com/2072-4292/12/4/638},
ISSN = {2072-4292},
ABSTRACT = {Given the impact of tropical forest disturbances on atmospheric carbon emissions, biodiversity, and ecosystem productivity, accurate long-term reporting of Land-Use and Land-Cover (LULC) change in the pre-satellite era (&lt;1972) is an imperative. Here, we used a combination of historical (1958) aerial photography and contemporary remote sensing data to map long-term changes in the extent and structure of the tropical forest surrounding Yangambi (DR Congo) in the central Congo Basin. Our study leveraged structure-from-motion and a convolutional neural network-based LULC classifier, using synthetic landscape-based image augmentation to map historical forest cover across a large orthomosaic (~93,431 ha) geo-referenced to ~4.7 ± 4.3 m at submeter resolution. A comparison with contemporary LULC data showed a shift from previously highly regular industrial deforestation of large areas to discrete smallholder farming clearing, increasing landscape fragmentation and providing opportunties for substantial forest regrowth. We estimated aboveground carbon gains through reforestation to range from 811 to 1592 Gg C, partially offsetting historical deforestation (2416 Gg C), in our study area. Efforts to quantify long-term canopy texture changes and their link to aboveground carbon had limited to no success. Our analysis provides methods and insights into key spatial and temporal patterns of deforestation and reforestation at a multi-decadal scale, providing a historical context for past and ongoing forest research in the area.},
DOI = {10.3390/rs12040638}
}



@Article{rs12040644,
AUTHOR = {Du, Ling and McCarty, Gregory W. and Zhang, Xin and Lang, Megan W. and Vanderhoof, Melanie K. and Li, Xia and Huang, Chengquan and Lee, Sangchul and Zou, Zhenhua},
TITLE = {Mapping Forested Wetland Inundation in the Delmarva Peninsula, USA Using Deep Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {644},
URL = {https://www.mdpi.com/2072-4292/12/4/644},
ISSN = {2072-4292},
ABSTRACT = {The Delmarva Peninsula in the eastern United States is partially characterized by thousands of small, forested, depressional wetlands that are highly sensitive to weather variability and climate change, but provide critical ecosystem services. Due to the relatively small size of these depressional wetlands and their occurrence under forest canopy cover, it is very challenging to map their inundation status based on existing remote sensing data and traditional classification approaches. In this study, we applied a state-of-the-art U-Net semantic segmentation network to map forested wetland inundation in the Delmarva area by integrating leaf-off WorldView-3 (WV3) multispectral data with fine spatial resolution light detection and ranging (lidar) intensity and topographic data, including a digital elevation model (DEM) and topographic wetness index (TWI). Wetland inundation labels generated from lidar intensity were used for model training and validation. The wetland inundation map results were also validated using field data, and compared to the U.S. Fish and Wildlife Service National Wetlands Inventory (NWI) geospatial dataset and a random forest output from a previous study. Our results demonstrate that our deep learning model can accurately determine inundation status with an overall accuracy of 95% (Kappa = 0.90) compared to field data and high overlap (IoU = 70%) with lidar intensity-derived inundation labels. The integration of topographic metrics in deep learning models can improve the classification accuracy for depressional wetlands. This study highlights the great potential of deep learning models to improve the accuracy of wetland inundation maps through use of high-resolution optical and lidar remote sensing datasets.},
DOI = {10.3390/rs12040644}
}



@Article{w12020548,
AUTHOR = {Torres-Sanchez, Roque and Navarro-Hellin, Honorio and Guillamon-Frutos, Antonio and San-Segundo, Rubén and Ruiz-Abellón, Maria Carmen and Domingo-Miguel, Rafael},
TITLE = {A Decision Support System for Irrigation Management: Analysis and Implementation of Different Learning Techniques},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {548},
URL = {https://www.mdpi.com/2073-4441/12/2/548},
ISSN = {2073-4441},
ABSTRACT = {Automatic irrigation scheduling systems are highly demanded in the agricultural sector due to their ability to both save water and manage deficit irrigation strategies. Elaborating a functional and efficient automatic irrigation system is a very complex task due to the high number of factors that the technician considers when managing irrigation in an optimal way. Automatic learning systems propose an alternative to traditional irrigation management by means of the automatic elaboration of predictions based on the learning of an agronomist (DSS). The aim of this paper is the study of several learning techniques in order to determine the goodness and error relative to expert decision. Nine orchards were tested during 2018 using linear regression (LR), random forest regression (RFR), and support vector regression (SVR) methods as engines of the irrigation decision support system (IDSS) proposed. The results obtained by the learning methods in three of these orchards have been compared with the decisions made by the agronomist over an entire year. The prediction model errors determined the best fitting regression model. The results obtained lead to the conclusion that these methods are valid engines to develop automatic irrigation scheduling systems.},
DOI = {10.3390/w12020548}
}



@Article{app10041344,
AUTHOR = {Piltan, Farzin and Prosvirin, Alexander E. and Sohaib, Muhammad and Saldivar, Belem and Kim, Jong-Myon},
TITLE = {An SVM-Based Neural Adaptive Variable Structure Observer for Fault Diagnosis and Fault-Tolerant Control of a Robot Manipulator},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1344},
URL = {https://www.mdpi.com/2076-3417/10/4/1344},
ISSN = {2076-3417},
ABSTRACT = {A robot manipulator is a multi-degree-of-freedom and nonlinear system that is used in various applications, including the medical area and automotive industries. Uncertain conditions in which a robot manipulator operates, as well as its nonlinearities, represent challenges for fault diagnosis and fault-tolerant control (FDC) that are addressed through the proposed FDC technique. A machine-learning-based neural adaptive, high-order, variable structure observer for fault diagnosis (FD) and adaptive, modern, fuzzy, backstepping, variable structure control for use in a fault-tolerant control (FC) algorithm, are proposed in this paper. In the first stage, a variable structure observer is proposed as an FD technique for the robot manipulator. The chattering phenomenon associated with the variable structure observer(VSO) is solved using a high-order variable structure observer. Then, the dynamic behavior estimation performance in the high-order variable structure observer is improved by incorporating a neural network algorithm in the FD pipeline. This adaptive technique is also effective in improving the robustness of the fault signal estimation. Moreover, support vector machines (SVMs) that can derive adaptive threshold values are used to categorize faults. To design an effective fault-tolerant controller (FC), an adaptive modern fuzzy backstepping variable structure controller is used in this study. First, a new variable structure controller is designed. Next, to increase robustness and reduce high-frequency oscillations in uncertain conditions, a backstepping algorithm is used in parallel with the variable structure controller to design the backstepping variable structure controller. To design an effective hybrid controller, a fuzzy algorithm is integrated into the backstepping variable structure controller to create a fuzzy backstepping variable structure controller. Then, to improve the robustness and reliability of the FC, a neural adaptive. high-order. variable structure observer is applied to the fuzzy backstepping variable structure controller to design a modern fuzzy backstepping variable structure controller. An adaptive algorithm is used to fine-tune the variable structure coefficients and reduce the effect of faults on the robot manipulator. The effectiveness of the selected algorithm is validated using a PUMA robot manipulator. The neural adaptive. high-order variable structure observer improves the average performance for the identification of various faults by about 27% and 29.2%, compared with the neural high-order variable structure observer and variable structure observer, respectively.},
DOI = {10.3390/app10041344}
}



@Article{rs12040656,
AUTHOR = {Wan, Luoma and Lin, Yinyi and Zhang, Hongsheng and Wang, Feng and Liu, Mingfeng and Lin, Hui},
TITLE = {GF-5 Hyperspectral Data for Species Mapping of Mangrove in Mai Po, Hong Kong},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {656},
URL = {https://www.mdpi.com/2072-4292/12/4/656},
ISSN = {2072-4292},
ABSTRACT = {Hyperspectral data has been widely used in species discrimination of plants with rich spectral information in hundreds of spectral bands, while the availability of hyperspectral data has hindered its applications in many specific cases. The successful operation of the Chinese satellite, Gaofen-5 (GF-5), provides potentially promising new hyperspectral dataset with 330 spectral bands in visible and near infrared range. Therefore, there is much demand for assessing the effectiveness and superiority of GF-5 hyperspectral data in plants species mapping, particularly mangrove species mapping, to better support the efficient mangrove management. In this study, mangrove forest in Mai Po Nature Reserve (MPNR), Hong Kong was selected as the study area. Four dominant native mangrove species were investigated in this study according to the field surveys. Two machine learning methods, Random Forests and Support Vector Machines, were employed to classify mangrove species with Landsat 8, Simulated Hyperion and GF-5 data sets. The results showed that 97 more bands of GF-5 over Hyperion brought a higher over accuracy of 87.12%, in comparison with 86.82% from Hyperion and 73.89% from Landsat 8. The higher spectral resolution of 5 nm in GF-5 was identified as making the major contribution, especially for the mapping of Aegiceras corniculatum. Therefore, GF-5 is likely to improve the classification accuracy of mangrove species mapping via enhancing spectral resolution and thus has promising potential to improve mangrove monitoring at species level to support mangrove management.},
DOI = {10.3390/rs12040656}
}



@Article{s20041101,
AUTHOR = {de Figueiredo, Felipe A. P. and Mennes, Ruben and Jabandžić, Irfan and Jiao, Xianjun and Moerman, Ingrid},
TITLE = {A Baseband Wireless Spectrum Hypervisor for Multiplexing Concurrent OFDM Signals},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1101},
URL = {https://www.mdpi.com/1424-8220/20/4/1101},
ISSN = {1424-8220},
ABSTRACT = {The next generation of wireless and mobile networks will have to handle a significant increase in traffic load compared to the current ones. This situation calls for novel ways to increase the spectral efficiency. Therefore, in this paper, we propose a wireless spectrum hypervisor architecture that abstracts a radio frequency (RF) front-end into a configurable number of virtual RF front ends. The proposed architecture has the ability to enable flexible spectrum access in existing wireless and mobile networks, which is a challenging task due to the limited spectrum programmability, i.e., the capability a system has to change the spectral properties of a given signal to fit an arbitrary frequency allocation. The proposed architecture is a non-intrusive and highly optimized wireless hypervisor that multiplexes the signals of several different and concurrent multi-carrier-based radio access technologies with numerologies that are multiple integers of one another, which are also referred in our work as radio access technologies with correlated numerology. For example, the proposed architecture can multiplex the signals of several Wi-Fi access points, several LTE base stations, several WiMAX base stations, etc. As it able to multiplex the signals of radio access technologies with correlated numerology, it can, for instance, multiplex the signals of LTE, 5G-NR and NB-IoT base stations. It abstracts a radio frequency front-end into a configurable number of virtual RF front ends, making it possible for such different technologies to share the same RF front-end and consequently reduce the costs and increasing the spectral efficiency by employing densification, once several networks share the same infrastructure or by dynamically accessing free chunks of spectrum. Therefore, the main goal of the proposed approach is to improve spectral efficiency by efficiently using vacant gaps in congested spectrum bandwidths or adopting network densification through infrastructure sharing. We demonstrate mathematically how our proposed approach works and present several simulation results proving its functionality and efficiency. Additionally, we designed and implemented an open-source and free proof of concept prototype of the proposed architecture, which can be used by researchers and developers to run experiments or extend the concept to other applications. We present several experimental results used to validate the proposed prototype. We demonstrate that the prototype can easily handle up to 12 concurrent physical layers.},
DOI = {10.3390/s20041101}
}



@Article{app10041370,
AUTHOR = {Benjdira, Bilel and Ouni, Kais and Al Rahhal, Mohamad M. and Albakr, Abdulrahman and Al-Habib, Amro and Mahrous, Emad},
TITLE = {Spinal Cord Segmentation in Ultrasound Medical Imagery},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1370},
URL = {https://www.mdpi.com/2076-3417/10/4/1370},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we study and evaluate the task of semantic segmentation of the spinal cord in ultrasound medical imagery. This task is useful for neurosurgeons to analyze the spinal cord movement during and after the laminectomy surgical operation. Laminectomy is performed on patients that suffer from an abnormal pressure made on the spinal cord. The surgeon operates by cutting the bones of the laminae and the intervening ligaments to relieve this pressure. During the surgery, ultrasound waves can pass through the laminectomy area to give real-time exploitable images of the spinal cord. The surgeon uses them to confirm spinal cord decompression or, occasionally, to assess a tumor adjacent to the spinal cord. The Freely pulsating spinal cord is a sign of adequate decompression. To evaluate the semantic segmentation approaches chosen in this study, we constructed two datasets using images collected from 10 different patients performing the laminectomy surgery. We found that the best solution for this task is Fully Convolutional DenseNets if the spinal cord is already in the train set. If the spinal cord does not exist in the train set, U-Net is the best. We also studied the effect of integrating inside both models some deep learning components like Atrous Spatial Pyramid Pooling (ASPP) and Depthwise Separable Convolution (DSC). We added a post-processing step and detailed the configurations to set for both models.},
DOI = {10.3390/app10041370}
}



@Article{s20041102,
AUTHOR = {Moreno, Hugo and Valero, Constantino and Bengochea-Guevara, José María and Ribeiro, Ángela and Garrido-Izard, Miguel and Andújar, Dionisio},
TITLE = {On-Ground Vineyard Reconstruction Using a LiDAR-Based Automated System},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1102},
URL = {https://www.mdpi.com/1424-8220/20/4/1102},
ISSN = {1424-8220},
ABSTRACT = {Crop 3D modeling allows site-specific management at different crop stages. In recent years, light detection and ranging (LiDAR) sensors have been widely used for gathering information about plant architecture to extract biophysical parameters for decision-making programs. The study reconstructed vineyard crops using light detection and ranging (LiDAR) technology. Its accuracy and performance were assessed for vineyard crop characterization using distance measurements, aiming to obtain a 3D reconstruction. A LiDAR sensor was installed on-board a mobile platform equipped with an RTK-GNSS receiver for crop 2D scanning. The LiDAR system consisted of a 2D time-of-flight sensor, a gimbal connecting the device to the structure, and an RTK-GPS to record the sensor data position. The LiDAR sensor was facing downwards installed on-board an electric platform. It scans in planes perpendicular to the travel direction. Measurements of distance between the LiDAR and the vineyards had a high spatial resolution, providing high-density 3D point clouds. The 3D point cloud was obtained containing all the points where the laser beam impacted. The fusion of LiDAR impacts and the positions of each associated to the RTK-GPS allowed the creation of the 3D structure. Although point clouds were already filtered, discarding points out of the study area, the branch volume cannot be directly calculated, since it turns into a 3D solid cluster that encloses a volume. To obtain the 3D object surface, and therefore to be able to calculate the volume enclosed by this surface, a suitable alpha shape was generated as an outline that envelops the outer points of the point cloud. The 3D scenes were obtained during the winter season when only branches were present and defoliated. The models were used to extract information related to height and branch volume. These models might be used for automatic pruning or relating this parameter to evaluate the future yield at each location. The 3D map was correlated with ground truth, which was manually determined, pruning the remaining weight. The number of scans by LiDAR influenced the relationship with the actual biomass measurements and had a significant effect on the treatments. A positive linear fit was obtained for the comparison between actual dry biomass and LiDAR volume. The influence of individual treatments was of low significance. The results showed strong correlations with actual values of biomass and volume with R2 = 0.75, and when comparing LiDAR scans with weight, the R2 rose up to 0.85. The obtained values show that this LiDAR technique is also valid for branch reconstruction with great advantages over other types of non-contact ranging sensors, regarding a high sampling resolution and high sampling rates. Even narrow branches were properly detected, which demonstrates the accuracy of the system working on difficult scenarios such as defoliated crops.},
DOI = {10.3390/s20041102}
}



@Article{rs12040677,
AUTHOR = {Espriella, Michael C. and Lecours, Vincent and C. Frederick, Peter and V. Camp, Edward and Wilkinson, Benjamin},
TITLE = {Quantifying Intertidal Habitat Relative Coverage in a Florida Estuary Using UAS Imagery and GEOBIA},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {677},
URL = {https://www.mdpi.com/2072-4292/12/4/677},
ISSN = {2072-4292},
ABSTRACT = {Intertidal habitats like oyster reefs and salt marshes provide vital ecosystem services including shoreline erosion control, habitat provision, and water filtration. However, these systems face significant global change as a result of a combination of anthropogenic stressors like coastal development and environmental stressors such as sea-level rise and disease. Traditional intertidal habitat monitoring techniques are cost and time-intensive, thus limiting how frequently resources are mapped in a way that is often insufficient to make informed management decisions. Unoccupied aircraft systems (UASs) have demonstrated the potential to mitigate these costs as they provide a platform to rapidly, safely, and inexpensively collect data in coastal areas. In this study, a UAS was used to survey intertidal habitats along the Gulf of Mexico coastline in Florida, USA. The structure from motion photogrammetry techniques were used to generate an orthomosaic and a digital surface model from the UAS imagery. These products were used in a geographic object-based image analysis (GEOBIA) workflow to classify mudflat, salt marsh, and oyster reef habitats. GEOBIA allows for a more informed classification than traditional techniques by providing textural and geometric context to habitat covers. We developed a ruleset to allow for a repeatable workflow, further decreasing the temporal cost of monitoring. The classification produced an overall accuracy of 79% in classifying habitats in a coastal environment with little spectral and textural separability, indicating that GEOBIA can differentiate intertidal habitats. This method allows for effective monitoring that can inform management and restoration efforts.},
DOI = {10.3390/rs12040677}
}



@Article{su12041606,
AUTHOR = {Barrile, Vincenzo and Fotia, Antonino and Leonardi, Giovanni and Pucinotti, Raffaele},
TITLE = {Geomatics and Soft Computing Techniques for Infrastructural Monitoring},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1606},
URL = {https://www.mdpi.com/2071-1050/12/4/1606},
ISSN = {2071-1050},
ABSTRACT = {Structural Health Monitoring (SHM) allows us to have information about the structure under investigation and thus to create analytical models for the assessment of its state or structural behavior. Exceeded a predetermined danger threshold, the possibility of an early warning would allow us, on the one hand, to suspend risky activities and, on the other, to reduce maintenance costs. The system proposed in this paper represents an integration of multiple traditional systems that integrate data of a different nature (used in the preventive phase to define the various behavior scenarios on the structural model), and then reworking them through machine learning techniques, in order to obtain values to compare with limit thresholds. The risk level depends on several variables, specifically, the paper wants to evaluate the possibility of predicting the structure behavior monitoring only displacement data, transmitted through an experimental transmission control unit. In order to monitor and to make our cities more &ldquo;sustainable&rdquo;, the paper describes some tests on road infrastructure, in this contest through the combination of geomatics techniques and soft computing.},
DOI = {10.3390/su12041606}
}



@Article{rs12040706,
AUTHOR = {Lin, Yan-Ting and Yang, Ming-Der and Han, Jen-Yu and Su, Yuan-Fong and Jang, Jiun-Huei},
TITLE = {Quantifying Flood Water Levels Using Image-Based Volunteered Geographic Information},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {706},
URL = {https://www.mdpi.com/2072-4292/12/4/706},
ISSN = {2072-4292},
ABSTRACT = {Many people use smartphone cameras to record their living environments through captured images, and share aspects of their daily lives on social networks, such as Facebook, Instagram, and Twitter. These platforms provide volunteered geographic information (VGI), which enables the public to know where and when events occur. At the same time, image-based VGI can also indicate environmental changes and disaster conditions, such as flooding ranges and relative water levels. However, little image-based VGI has been applied for the quantification of flooding water levels because of the difficulty of identifying water lines in image-based VGI and linking them to detailed terrain models. In this study, flood detection has been achieved through image-based VGI obtained by smartphone cameras. Digital image processing and a photogrammetric method were presented to determine the water levels. In digital image processing, the random forest classification was applied to simplify ambient complexity and highlight certain aspects of flooding regions, and the HT-Canny method was used to detect the flooding line of the classified image-based VGI. Through the photogrammetric method and a fine-resolution digital elevation model based on the unmanned aerial vehicle mapping technique, the detected flooding lines were employed to determine water levels. Based on the results of image-based VGI experiments, the proposed approach identified water levels during an urban flood event in Taipei City for demonstration. Notably, classified images were produced using random forest supervised classification for a total of three classes with an average overall accuracy of 88.05%. The quantified water levels with a resolution of centimeters (&lt;3-cm difference on average) can validate flood modeling so as to extend point-basis observations to area-basis estimations. Therefore, the limited performance of image-based VGI quantification has been improved to help in flood disasters. Consequently, the proposed approach using VGI images provides a reliable and effective flood-monitoring technique for disaster management authorities.},
DOI = {10.3390/rs12040706}
}



@Article{iot1010002,
AUTHOR = {Spachos, Petros},
TITLE = {Towards a Low-Cost Precision Viticulture System Using Internet of Things Devices},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {1},
PAGES = {5--20},
URL = {https://www.mdpi.com/2624-831X/1/1/2},
ISSN = {2624-831X},
ABSTRACT = {Precision Agriculture (PA) is an ever-expanding field that takes modern technological advancements and applies it to farming practices to reduce waste and increase output. One advancement that can play a significant role in achieving precision agriculture is wireless technology, and specifically the Internet of Things (IoT) devices. Small, inch scale and low-cost devices can be used to monitor great agricultural areas. In this paper, a system for precision viticulture which uses IoT devices for real-time monitoring is proposed. The different components of the system are programmed properly and the interconnection between them is designed to minimize energy consumption. Wireless sensor nodes measure soil moisture and soil temperature in the field and transmit the information to a base station. If the conditions are optimal for a disease or pest to occur, a drone flies towards the area. When the drone is over the node, pictures are captured and then it returns to the base station for further processing. The feasibility of the system is examined through experimentation in a realistic scenario.},
DOI = {10.3390/iot1010002}
}



@Article{met10020288,
AUTHOR = {Branca, Teresa Annunziata and Fornai, Barbara and Colla, Valentina and Murri, Maria Maddalena and Streppa, Eliana and Schröder, Antonius Johannes},
TITLE = {The Challenge of Digitalization in the Steel Sector},
JOURNAL = {Metals},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {288},
URL = {https://www.mdpi.com/2075-4701/10/2/288},
ISSN = {2075-4701},
ABSTRACT = {Digitalization represents a paramount process started some decades ago, but which received a strong acceleration by Industry 4.0 and now directly impacts all the process and manufacturing sectors. It is expected to allow the European industry to increase its production efficiency and its sustainability. In particular, in the energy-intensive industries, such as the steel industry, digitalization concerns the application of the related technologies to the production processes, focusing on two main often overlapping directions: Advanced tools for the optimization of the production chain and specific technologies for low-carbon and sustainable production. Furthermore, the rapid evolution of the technologies in the steel sector require the continuous update of the skills of the industrial workforce. The present review paper, resulting from a recent study developed inside a Blueprint European project, introduces the context of digitalization and some important definitions in both the European industry and the European iron and steel sector. The current technological transformation is depicted, and the main developments funded by European Research Programs are analyzed. Moreover, the impact of digitalization on the steel industry workforce are considered together with the foreseen economic developments.},
DOI = {10.3390/met10020288}
}



@Article{rs12040723,
AUTHOR = {Noguera, Miguel and Millán, Borja and Pérez-Paredes, Juan José and Ponce, Juan Manuel and Aquino, Arturo and Andújar, José Manuel},
TITLE = {A New Low-Cost Device Based on Thermal Infrared Sensors for Olive Tree Canopy Temperature Measurement and Water Status Monitoring},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {723},
URL = {https://www.mdpi.com/2072-4292/12/4/723},
ISSN = {2072-4292},
ABSTRACT = {In recent years, many olive orchards, which are a major crop in the Mediterranean basin, have been converted into intensive or super high-density hedgerow systems. This configuration is more efficient in terms of yield per hectare, but at the same time the water requirements are higher than in traditional grove arrangements. Moreover, irrigation regulations have a high environmental (through water use optimization) impact and influence on crop quality and yield. The mapping of (spatio-temporal) variability with conventional water stress assessment methods is impractical due to time and labor constraints, which often involve staff training. To address this problem, this work presents the development of a new low-cost device based on a thermal infrared (IR) sensor for the measurement of olive tree canopy temperature and monitoring of water status. The performance of the developed device was compared to a commercial thermal camera. Furthermore, the proposed device was evaluated in a commercially managed olive orchard, where two different irrigation treatments were established: a full irrigation treatment (FI) and a regulated deficit irrigation (RDC), aimed at covering 100% and 50% of crop evapotranspiration (ETc), respectively. Predawn leaf water potential (&Psi;PD) and stomatal conductance (gs), two widely accepted indicators for crop water status, were regressed to the measured canopy temperature. The results were promising, reaching a coefficient of determination R2 &ge; 0.80. On the other hand, the crop water stress index (CWSI) was also calculated, resulting in a coefficient of determination R2 &ge; 0.79. The outcomes provided by the developed device support its suitability for fast, low-cost, and reliable estimation of an olive orchard&rsquo;s water status, even suppressing the need for supervised acquisition of reference temperatures. The newly developed device can be used for water management, reducing water usage, and for overall improvements to olive orchard management.},
DOI = {10.3390/rs12040723}
}



@Article{rs12040725,
AUTHOR = {Pastick, Neal J. and Dahal, Devendra and Wylie, Bruce K. and Parajuli, Sujan and Boyte, Stephen P. and Wu, Zhouting},
TITLE = {Characterizing Land Surface Phenology and Exotic Annual Grasses in Dryland Ecosystems Using Landsat and Sentinel-2 Data in Harmony},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {725},
URL = {https://www.mdpi.com/2072-4292/12/4/725},
ISSN = {2072-4292},
ABSTRACT = {Invasive annual grasses, such as cheatgrass (Bromus tectorum L.), have proliferated in dryland ecosystems of the western United States, promoting increased fire activity and reduced biodiversity that can be detrimental to socio-environmental systems. Monitoring exotic annual grass cover and dynamics over large areas requires the use of remote sensing that can support early detection and rapid response initiatives. However, few studies have leveraged remote sensing technologies and computing frameworks capable of providing rangeland managers with maps of exotic annual grass cover at relatively high spatiotemporal resolutions and near real-time latencies. Here, we developed a system for automated mapping of invasive annual grass (%) cover using in situ observations, harmonized Landsat and Sentinel-2 (HLS) data, maps of biophysical variables, and machine learning techniques. A robust and automated cloud, cloud shadow, water, and snow/ice masking procedure (mean overall accuracy &gt;81%) was implemented using time-series outlier detection and data mining techniques prior to spatiotemporal interpolation of HLS data via regression tree models (r = 0.94; mean absolute error (MAE) = 0.02). Weekly, cloud-free normalized difference vegetation index (NDVI) image composites (2016&ndash;2018) were used to construct a suite of spectral and phenological metrics (e.g., start and end of season dates), consistent with information derived from Moderate Resolution Image Spectroradiometer (MODIS) data. These metrics were incorporated into a data mining framework that accurately (r = 0.83; MAE = 11) modeled and mapped exotic annual grass (%) cover throughout dryland ecosystems in the western United States at a native, 30-m spatial resolution. Our results show that inclusion of weekly HLS time-series data and derived indicators improves our ability to map exotic annual grass cover, as compared to distribution models that use MODIS products or monthly, seasonal, or annual HLS composites as primary inputs. This research fills a critical gap in our ability to effectively assess, manage, and monitor drylands by providing a framework that allows for an accurate and timely depiction of land surface phenology and exotic annual grass cover at spatial and temporal resolutions that are meaningful to local resource managers.},
DOI = {10.3390/rs12040725}
}



@Article{rs12040726,
AUTHOR = {Yuan, Weitao and Zhang, Wangle and Lai, Zhongping and Zhang, Jingxiong},
TITLE = {Extraction of Yardang Characteristics Using Object-Based Image Analysis and Canny Edge Detection Methods},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {726},
URL = {https://www.mdpi.com/2072-4292/12/4/726},
ISSN = {2072-4292},
ABSTRACT = {Parameters of geomorphological characteristics are critical for research on yardangs. However, methods which are low-cost, accurate, and automatic or semi-automatic for extracting these parameters are limited. We present here semi-automatic techniques for this purpose. They are object-based image analysis (OBIA) and Canny edge detection (CED), using free, very high spatial resolution images from Google Earth. We chose yardang fields in Dunhuang of west China to test the methods. Our results showed that the extractions registered an overall accuracy of 92.26% with a Kappa coefficient of agreement of 0.82 at a segmentation scale of 52 using the OBIA method, and the exaction of yardangs had the highest accuracy at medium segmentation scales (138, 145). Using CED, we resampled the experimental image subset to a series of lower spatial resolutions for eliminating noise. The total length of yardang boundaries showed a logarithmically decreasing (R2 = 0.904) trend with decreasing spatial resolution, and there was also a linear relationship between yardang median widths and spatial resolutions (R2 = 0.95). Despite the difficulty of identifying shadows, the CED method achieved an overall accuracy of 89.23% with a kappa coefficient of agreement of 0.72, similar to that of the OBIA method at medium segmentation scale (138).},
DOI = {10.3390/rs12040726}
}



@Article{rs12040739,
AUTHOR = {Nogueira, Keiller and L. S. Machado, Gabriel and H. T. Gama, Pedro and C. V. da Silva, Caio and Balaniuk, Remis and A. dos Santos, Jefersson},
TITLE = {Facing Erosion Identification in Railway Lines Using Pixel-Wise Deep-Based Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {739},
URL = {https://www.mdpi.com/2072-4292/12/4/739},
ISSN = {2072-4292},
ABSTRACT = {Soil erosion is considered one of the most expensive natural hazards with a high impact on several infrastructure assets. Among them, railway lines are one of the most likely constructions for the appearance of erosion and, consequently, one of the most troublesome due to the maintenance costs, risks of derailments, and so on. Therefore, it is fundamental to identify and monitor erosion in railway lines to prevent major consequences. Currently, erosion identification is manually performed by humans using huge image sets, a time-consuming and slow task. Hence, automatic machine learning methods appear as an appealing alternative. A crucial step for automatic erosion identification is to create a good feature representation. Towards such objective, deep learning can learn data-driven features and classifiers. In this paper, we propose a novel deep learning-based framework capable of performing erosion identification in railway lines. Six techniques were evaluated and the best one, Dynamic Dilated ConvNet, was integrated into this framework that was then encapsulated into a new ArcGIS plugin to facilitate its use by non-programmer users. To analyze such techniques, we also propose a new dataset, composed of almost 2000 high-resolution images.},
DOI = {10.3390/rs12040739}
}



@Article{rs12050793,
AUTHOR = {Ding, Hu and Liu, Kai and Chen, Xiaozheng and Xiong, Liyang and Tang, Guoan and Qiu, Fang and Strobl, Josef},
TITLE = {Optimized Segmentation Based on the Weighted Aggregation Method for Loess Bank Gully Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {793},
URL = {https://www.mdpi.com/2072-4292/12/5/793},
ISSN = {2072-4292},
ABSTRACT = {The Chinese Loess Plateau suffers severe gully erosion. Gully mapping is a fundamental task for gully erosion monitoring in this region. Among the different gully types in the Loess Plateau, the bank gully is usually regarded as the most important source for the generation of sediment. However, approaches for bank gully extraction are still limited. This study put forward an integrated framework, including segmentation optimization, evaluation and Extreme Gradient Boosting (XGBoost)-based classification, for the bank gully mapping of Zhifanggou catchment in the Chinese Loess Plateau. The approach was conducted using a 1-m resolution digital elevation model (DEM), based on unmanned aerial vehicle (UAV) photogrammetry and WorldView-3 imagery. The methodology first divided the study area into different watersheds. Then, segmentation by weighted aggregation (SWA) was implemented to generate multi-level segments. For achieving an optimum segmentation, area-weighted variance (WV) and Moran&rsquo;s I (MI) were adopted and calculated within each sub-watershed. After that, a new discrepancy metric, the area-number index (ANI), was developed for evaluating the segmentation results, and the results were compared with the multi-resolution segmentation (MRS) algorithm. Finally, bank gully mappings were obtained based on the XGBoost model after fine-tuning. The experiment results demonstrate that the proposed method can achieve superior segmentation compared to MRS. Moreover, the overall accuracy of the bank gully extraction results was 78.57%. The proposed approach provides a credible tool for mapping bank gullies, which could be useful for the catchment-scale gully erosion process.},
DOI = {10.3390/rs12050793}
}



@Article{inventions5010012,
AUTHOR = {Han, Xiongzhe and Thomasson, J. Alex and Wang, Tianyi and Swaminathan, Vaishali},
TITLE = {Autonomous Mobile Ground Control Point Improves Accuracy of Agricultural Remote Sensing through Collaboration with UAV},
JOURNAL = {Inventions},
VOLUME = {5},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2411-5134/5/1/12},
ISSN = {2411-5134},
ABSTRACT = {Ground control points (GCPs) are critical for agricultural remote sensing that require georeferencing and calibration of images collected from an unmanned aerial vehicles (UAV) at different times. However, the conventional stationary GCPs are time-consuming and labor-intensive to measure, distribute, and collect their information in a large field setup. An autonomous mobile GCP and a collaboration strategy to communicate with the UAV were developed to improve the efficiency and accuracy of the UAV-based data collection process. Prior to actual field testing, preliminary tests were conducted using the system to show the capability of automatic path tracking by reducing the root mean square error (RMSE) for lateral deviation from 34.3 cm to 15.6 cm based on the proposed look-ahead tracking method. The tests also indicated the feasibility of moving reflectance reference panels successively along all the waypoints without having detrimental effects on pixel values in the mosaicked images, with the percentage errors in digital number values ranging from &minus;1.1% to 0.1%. In the actual field testing, the autonomous mobile GCP was able to successfully cooperate with the UAV in real-time without any interruption, showing superior performances for georeferencing, radiometric calibration, height calibration, and temperature calibration, compared to the conventional calibration method that has stationary GCPs.},
DOI = {10.3390/inventions5010012}
}



@Article{rs12050814,
AUTHOR = {Vilar, Pedro and Morais, Tiago G. and Rodrigues, Nuno R. and Gama, Ivo and Monteiro, Marta L. and Domingos, Tiago and Teixeira, Ricardo F. M.},
TITLE = {Object-Based Classification Approaches for Multitemporal Identification and Monitoring of Pastures in Agroforestry Regions using Multispectral Unmanned Aerial Vehicle Products},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {814},
URL = {https://www.mdpi.com/2072-4292/12/5/814},
ISSN = {2072-4292},
ABSTRACT = {Sown Biodiverse Pastures (SBP) are the basis of a high-yield grazing system tailored for Mediterranean ecosystems and widely implemented in Southern Portugal. The application of precision farming methods in SBP requires cost-effective monitoring using remote sensing (RS). The main hurdle for the remote monitoring of SBP is the fact that the bulk of the pastures are installed in open Montado agroforestry systems. Sparsely distributed trees cast shadows that hinder the identification of the underlaying pasture using Unmanned Aerial Vehicles (UAV) imagery. Image acquisition in the Spring is made difficult by the presence of flowers that mislead the classification algorithms. Here, we tested multiple procedures for the geographical, object-based image classification (GEOBIA) of SBP, aiming to reduce the effects of tree shadows and flowers in open Montado systems. We used remotely sensed data acquired between November 2017 and May 2018 in three Portuguese farms. We used three machine learning supervised classification algorithms: Random Forests (RF), Support Vector Machine (SVM) and Artificial Neural Networks (ANN). We classified SBP based on: (1) a single-period image for the maximum Normalized Difference Vegetation Index (NDVI) epoch in each of the three farms, and (2) multi-temporal image stacking. RF, SVM and ANN were trained using some visible (red, green and blue bands) and near-infrared (NIR) reflectance bands, plus NDVI and a Digital Surface Model (DSM). We obtained high overall accuracy and kappa index (higher than 79% and 0.60, respectively). The RF algorithm had the highest overall accuracy (more than 92%) for all farms. Multitemporal image classification increased the accuracy of the algorithms. as it helped to correctly identify as SBP the areas covered by tree shadows and flower patches, which would be misclassified using single image classification. This study thus established the first workflow for SBP monitoring based on remotely sensed data, suggesting an operational approach for SBP identification. The workflow can be applied to other types of pastures in agroforestry regions to reduce the effects of shadows and flowering in classification problems.},
DOI = {10.3390/rs12050814}
}



@Article{app10051759,
AUTHOR = {Guo, Han and Zhou, Jun and Liu, Fei and He, Yong and Huang, He and Wang, Hongyan},
TITLE = {Application of Machine Learning Method to Quantitatively Evaluate the Droplet Size and Deposition Distribution of the UAV Spray Nozzle},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1759},
URL = {https://www.mdpi.com/2076-3417/10/5/1759},
ISSN = {2076-3417},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) spray has been used for efficient and adaptive pesticide applications with its low costs. However, droplet drift is the main problem for UAV spray and will induce pesticide waste and safety concerns. Droplet size and deposition distribution are both highly related to droplet drift and spray effect, which are determined by the nozzle. Therefore, it is necessary to propose an evaluating method for a specific UAV spray nozzles. In this paper, four machine learning methods (REGRESS, least squares support vector machines (LS-SVM), extreme learning machine, and radial basis function neural network (RBFNN)) were applied for quantitatively evaluating one type of UAV spray nozzle (TEEJET XR110015VS), and the case of twin nozzles was investigated. The results showed REGRESS and LS-SVM are good candidates for droplet size evaluation with the coefficient of determination in the calibration set above 0.9 and root means square errors of the prediction set around 2 &micro;m. RBFNN achieved the best performance for the evaluation of deposition distribution and showed its potential for determining the droplet size of overlapping area. Overall, this study proved the accuracy and efficiency of using the machine learning method for UAV spray nozzle evaluation. Additionally, the study demonstrated the feasibility of using machine learning model to predict the droplet size in the overlapping area of twin nozzles.},
DOI = {10.3390/app10051759}
}



@Article{s20051420,
AUTHOR = {Sun, Weifeng and Tang, Min and Zhang, Lijun and Huo, Zhiqiang and Shu, Lei},
TITLE = {A Survey of Using Swarm Intelligence Algorithms in IoT},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1420},
URL = {https://www.mdpi.com/1424-8220/20/5/1420},
ISSN = {1424-8220},
ABSTRACT = {With the continuing advancements in technologies (such as machine to machine, wireless telecommunications, artificial intelligence, and big data analysis), the Internet of Things (IoT) aims to connect everything for information sharing and intelligent decision-making. Swarm intelligence (SI) provides the possibility of SI behavior through collaboration in individuals that have limited or no intelligence. Its potential parallelism and distribution characteristics can be used to realize global optimization and solve nonlinear complex problems. This paper reviews representative SI algorithms and summarizes their applications in the IoT. The main focus consists in the analysis of SI-enabled applications to wireless sensor network (WSN) and discussion of related research problems in the WSN. Also, we concluded SI-based applications in other IoT fields, such as SI in UAV-aided wireless network. Finally, possible research prospects and future trends are drawn.},
DOI = {10.3390/s20051420}
}



@Article{s20051437,
AUTHOR = {Durdevic, Petar and Ortiz-Arroyo, Daniel},
TITLE = {A Deep Neural Network Sensor for Visual Servoing in 3D Spaces},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1437},
URL = {https://www.mdpi.com/1424-8220/20/5/1437},
ISSN = {1424-8220},
ABSTRACT = {This paper describes a novel stereo vision sensor based on deep neural networks, that can be used to produce a feedback signal for visual servoing in unmanned aerial vehicles such as drones. Two deep convolutional neural networks attached to the stereo camera in the drone are trained to detect wind turbines in images and stereo triangulation is used to calculate the distance from a wind turbine to the drone. Our experimental results show that the sensor produces data accurate enough to be used for servoing, even in the presence of noise generated when the drone is not being completely stable. Our results also show that appropriate filtering of the signals is needed and that to produce correct results, it is very important to keep the wind turbine within the field of vision of both cameras, so that both deep neural networks could detect it.},
DOI = {10.3390/s20051437}
}



@Article{rs12050852,
AUTHOR = {Pan, Xin and Zhao, Jian and Xu, Jun},
TITLE = {An End-to-End and Localized Post-Processing Method for Correcting High-Resolution Remote Sensing Classification Result Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {852},
URL = {https://www.mdpi.com/2072-4292/12/5/852},
ISSN = {2072-4292},
ABSTRACT = {Since the result images obtained by deep semantic segmentation neural networks are usually not perfect, especially at object borders, the conditional random field (CRF) method is frequently utilized in the result post-processing stage to obtain the corrected classification result image. The CRF method has achieved many successes in the field of computer vision, but when it is applied to remote sensing images, overcorrection phenomena may occur. This paper proposes an end-to-end and localized post-processing method (ELP) to correct the result images of high-resolution remote sensing image classification methods. ELP has two advantages. (1) End-to-end evaluation: ELP can identify which locations of the result image are highly suspected of having errors without requiring samples. This characteristic allows ELP to be adapted to an end-to-end classification process. (2) Localization: Based on the suspect areas, ELP limits the CRF analysis and update area to a small range and controls the iteration termination condition. This characteristic avoids the overcorrections caused by the global processing of the CRF. In the experiments, ELP is used to correct the classification results obtained by various deep semantic segmentation neural networks. Compared with traditional methods, the proposed method more effectively corrects the classification result and improves classification accuracy.},
DOI = {10.3390/rs12050852}
}



@Article{rs12050867,
AUTHOR = {Frey, Julian and Asbeck, Thomas and Bauhus, Jürgen},
TITLE = {Predicting Tree-Related Microhabitats by Multisensor Close-Range Remote Sensing Structural Parameters for the Selection of Retention Elements},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {867},
URL = {https://www.mdpi.com/2072-4292/12/5/867},
ISSN = {2072-4292},
ABSTRACT = {The retention of structural elements such as habitat trees in forests managed for timber production is essential for fulfilling the objectives of biodiversity conservation. This paper seeks to predict tree-related microhabitats (TreMs) by close-range remote sensing parameters. TreMs, such as cavities or crown deadwood, are an established tool to quantify the suitability of habitat trees for biodiversity conservation. The aim to predict TreMs based on remote sensing (RS) parameters is supposed to assist a more objective and efficient selection of retention elements. The RS parameters were collected by the use of terrestrial laser scanning as well as unmanned aerial vehicles structure from motion point cloud generation to provide a 3D distribution of plant tissue. Data was recorded on 135 1-ha plots in Germany. Statistical models were used to test the influence of 28 RS predictors, which described TreM richness (R2: 0.31) and abundance (R2: 0.31) in moderate precision and described a deviance of 44% for the abundance and 38% for richness of TreMs. Our results indicate that multiple RS techniques can achieve moderate predictions of TreM occurrence. This method allows a more efficient and objective selection of retention elements such as habitat trees that are keystone features for biodiversity conservation, even if it cannot be considered a full replacement of TreM inventories due to the moderate statistical relationship at this stage.},
DOI = {10.3390/rs12050867}
}



@Article{s20051487,
AUTHOR = {Gao, Demin and Sun, Quan and Hu, Bin and Zhang, Shuo},
TITLE = {A Framework for Agricultural Pest and Disease Monitoring Based on Internet-of-Things and Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1487},
URL = {https://www.mdpi.com/1424-8220/20/5/1487},
ISSN = {1424-8220},
ABSTRACT = {With the development of information technology, Internet-of-Things (IoT) and low-altitude remote-sensing technology represented by Unmanned Aerial Vehicles (UAVs) are widely used in environmental monitoring fields. In agricultural modernization, IoT and UAV can monitor the incidence of crop diseases and pests from the ground micro and air macro perspectives, respectively. IoT technology can collect real-time weather parameters of the crop growth by means of numerous inexpensive sensor nodes. While depending on spectral camera technology, UAVs can capture the images of farmland, and these images can be utilize for analyzing the occurrence of pests and diseases of crops. In this work, we attempt to design an agriculture framework for providing profound insights into the specific relationship between the occurrence of pests/diseases and weather parameters. Firstly, considering that most farms are usually located in remote areas and far away from infrastructure, making it hard to deploy agricultural IoT devices due to limited energy supplement, a sun tracker device is designed to adjust the angle automatically between the solar panel and the sunlight for improving the energy-harvesting rate. Secondly, for resolving the problem of short flight time of UAV, a flight mode is introduced to ensure the maximum utilization of wind force and prolong the fight time. Thirdly, the images captured by UAV are transmitted to the cloud data center for analyzing the degree of damage of pests and diseases based on spectrum analysis technology. Finally, the agriculture framework is deployed in the Yangtze River Zone of China and the results demonstrate that wheat is susceptible to disease when the temperature is between 14 &deg;C and 16 &deg;C, and high rainfall decreases the spread of wheat powdery mildew.},
DOI = {10.3390/s20051487}
}



@Article{rs12050872,
AUTHOR = {Shang, Ronghua and Zhang, Jiyu and Jiao, Licheng and Li, Yangyang and Marturi, Naresh and Stolkin, Rustam},
TITLE = {Multi-scale Adaptive Feature Fusion Network for Semantic Segmentation in Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {872},
URL = {https://www.mdpi.com/2072-4292/12/5/872},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation of high-resolution remote sensing images is highly challenging due to the presence of a complicated background, irregular target shapes, and similarities in the appearance of multiple target categories. Most of the existing segmentation methods that rely only on simple fusion of the extracted multi-scale features often fail to provide satisfactory results when there is a large difference in the target sizes. Handling this problem through multi-scale context extraction and efficient fusion of multi-scale features, in this paper we present an end-to-end multi-scale adaptive feature fusion network (MANet) for semantic segmentation in remote sensing images. It is a coding and decoding structure that includes a multi-scale context extraction module (MCM) and an adaptive fusion module (AFM). The MCM employs two layers of atrous convolutions with different dilatation rates and global average pooling to extract context information at multiple scales in parallel. MANet embeds the channel attention mechanism to fuse semantic features. The high- and low-level semantic information are concatenated to generate global features via global average pooling. These global features are used as channel weights to acquire adaptive weight information of each channel by the fully connected layer. To accomplish an efficient fusion, these tuned weights are applied to the fused features. Performance of the proposed method has been evaluated by comparing it with six other state-of-the-art networks: fully convolutional networks (FCN), U-net, UZ1, Light-weight RefineNet, DeepLabv3+, and APPD. Experiments performed using the publicly available Potsdam and Vaihingen datasets show that the proposed MANet significantly outperforms the other existing networks, with overall accuracy reaching 89.4% and 88.2%, respectively and with average of F1 reaching 90.4% and 86.7% respectively.},
DOI = {10.3390/rs12050872}
}



