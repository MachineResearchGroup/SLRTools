
@Article{electronics10030222,
AUTHOR = {Zhao, Baigan and Huang, Yingping and Wei, Hongjian and Hu, Xing},
TITLE = {Ego-Motion Estimation Using Recurrent Convolutional Neural Networks through Optical Flow Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {222},
URL = {https://www.mdpi.com/2079-9292/10/3/222},
ISSN = {2079-9292},
ABSTRACT = {Visual odometry (VO) refers to incremental estimation of the motion state of an agent (e.g., vehicle and robot) by using image information, and is a key component of modern localization and navigation systems. Addressing the monocular VO problem, this paper presents a novel end-to-end network for estimation of camera ego-motion. The network learns the latent subspace of optical flow (OF) and models sequential dynamics so that the motion estimation is constrained by the relations between sequential images. We compute the OF field of consecutive images and extract the latent OF representation in a self-encoding manner. A Recurrent Neural Network is then followed to examine the OF changes, i.e., to conduct sequential learning. The extracted sequential OF subspace is used to compute the regression of the 6-dimensional pose vector. We derive three models with different network structures and different training schemes: LS-CNN-VO, LS-AE-VO, and LS-RCNN-VO. Particularly, we separately train the encoder in an unsupervised manner. By this means, we avoid non-convergence during the training of the whole network and allow more generalized and effective feature representation. Substantial experiments have been conducted on KITTI and Malaga datasets, and the results demonstrate that our LS-RCNN-VO outperforms the existing learning-based VO approaches.},
DOI = {10.3390/electronics10030222}
}



@Article{rs13030356,
AUTHOR = {Zhu, Bingxue and Chen, Shengbo and Cao, Yijing and Xu, Zhengyuan and Yu, Yan and Han, Cheng},
TITLE = {A Regional Maize Yield Hierarchical Linear Model Combining Landsat 8 Vegetative Indices and Meteorological Data: Case Study in Jilin Province},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {356},
URL = {https://www.mdpi.com/2072-4292/13/3/356},
ISSN = {2072-4292},
ABSTRACT = {The use of satellite remote sensing could effectively predict maize yield. However, many statistical prediction models using remote sensing data cannot extend to the regional scale without considering the regional climate. This paper first introduced the hierarchical linear modeling (HLM) method to solve maize-yield prediction problems over years and regions. The normalized difference vegetation index (NDVI), calculated by the spectrum of the Landsat 8 operational land imager (OLI), and meteorological data were introduced as input parameters in the maize-yield prediction model proposed in this paper. We built models using 100 samples from 10 areas, and used 101 other samples from 34 areas to evaluate the model&rsquo;s performance in Jilin province. HLM provided higher accuracy with an adjusted determination coefficient equal to 0.75, root mean square error (RMSEV) equal to 0.94 t/ha, and normalized RMSEV equal to 9.79%. Results showed that the HLM approach outperformed linear regression (LR) and multiple LR (MLR) methods. The HLM method based on the Landsat 8 OLI NDVI and meteorological data could flexibly adjust in different regional climatic conditions. They had higher spatiotemporal expansibility than that of widely used yield estimation models (e.g., LR and MLR). This is helpful for the accurate management of maize fields.},
DOI = {10.3390/rs13030356}
}



@Article{app11030953,
AUTHOR = {Hong, Jin and Kwon, Junseok},
TITLE = {Visual Tracking of Small Unmanned Aerial Vehicles Based on Object Proposal Voting},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {953},
URL = {https://www.mdpi.com/2076-3417/11/3/953},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we propose a novel visual tracking method for unmanned aerial vehicles (UAVs) in aerial scenery. To track the UAVs robustly, we present a new object proposal method that can accurately determine the object regions that are likely to exist. The proposed object proposal method is robust to small objects and severe background clutter. For this, we vote on candidate areas of the object and increase or decrease the weight of the area accordingly. Thus, the method can accurately propose the object areas that can be used to track small-sized UAVs with the assumption that their motion is smooth over time. Experimental results verify that UAVs are accurately tracked even when they are very small and the background is complex. The proposed method qualitatively and quantitatively delivers state-of-the-art performance in comparison with conventional object proposal-based methods.},
DOI = {10.3390/app11030953}
}



@Article{drones5010008,
AUTHOR = {Butcher, Paul A. and Colefax, Andrew P. and Gorkin, Robert A. and Kajiura, Stephen M. and López, Naima A. and Mourier, Johann and Purcell, Cormac R. and Skomal, Gregory B. and Tucker, James P. and Walsh, Andrew J. and Williamson, Jane E. and Raoult, Vincent},
TITLE = {The Drone Revolution of Shark Science: A Review},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {8},
URL = {https://www.mdpi.com/2504-446X/5/1/8},
ISSN = {2504-446X},
ABSTRACT = {Over the past decade, drones have become a popular tool for wildlife management and research. Drones have shown significant value for animals that were often difficult or dangerous to study using traditional survey methods. In the past five years drone technology has become commonplace for shark research with their use above, and more recently, below the water helping to minimise knowledge gaps about these cryptic species. Drones have enhanced our understanding of shark behaviour and are critically important tools, not only due to the importance and conservation of the animals in the ecosystem, but to also help minimise dangerous encounters with humans. To provide some guidance for their future use in relation to sharks, this review provides an overview of how drones are currently used with critical context for shark monitoring. We show how drones have been used to fill knowledge gaps around fundamental shark behaviours or movements, social interactions, and predation across multiple species and scenarios. We further detail the advancement in technology across sensors, automation, and artificial intelligence that are improving our abilities in data collection and analysis and opening opportunities for shark-related beach safety. An investigation of the shark-based research potential for underwater drones (ROV/AUV) is also provided. Finally, this review provides baseline observations that have been pioneered for shark research and recommendations for how drones might be used to enhance our knowledge in the future.},
DOI = {10.3390/drones5010008}
}



@Article{s21030742,
AUTHOR = {Nguyen, Canh and Sagan, Vasit and Maimaitiyiming, Matthew and Maimaitijiang, Maitiniyazi and Bhadra, Sourav and Kwasniewski, Misha T.},
TITLE = {Early Detection of Plant Viral Disease Using Hyperspectral Imaging and Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {742},
URL = {https://www.mdpi.com/1424-8220/21/3/742},
PubMedID = {33499335},
ISSN = {1424-8220},
ABSTRACT = {Early detection of grapevine viral diseases is critical for early interventions in order to prevent the disease from spreading to the entire vineyard. Hyperspectral remote sensing can potentially detect and quantify viral diseases in a nondestructive manner. This study utilized hyperspectral imagery at the plant level to identify and classify grapevines inoculated with the newly discovered DNA virus grapevine vein-clearing virus (GVCV) at the early asymptomatic stages. An experiment was set up at a test site at South Farm Research Center, Columbia, MO, USA (38.92 N, &minus;92.28 W), with two grapevine groups, namely healthy and GVCV-infected, while other conditions were controlled. Images of each vine were captured by a SPECIM IQ 400&ndash;1000 nm hyperspectral sensor (Oulu, Finland). Hyperspectral images were calibrated and preprocessed to retain only grapevine pixels. A statistical approach was employed to discriminate two reflectance spectra patterns between healthy and GVCV vines. Disease-centric vegetation indices (VIs) were established and explored in terms of their importance to the classification power. Pixel-wise (spectral features) classification was performed in parallel with image-wise (joint spatial&ndash;spectral features) classification within a framework involving deep learning architectures and traditional machine learning. The results showed that: (1) the discriminative wavelength regions included the 900&ndash;940 nm range in the near-infrared (NIR) region in vines 30 days after sowing (DAS) and the entire visual (VIS) region of 400&ndash;700 nm in vines 90 DAS; (2) the normalized pheophytization index (NPQI), fluorescence ratio index 1 (FRI1), plant senescence reflectance index (PSRI), anthocyanin index (AntGitelson), and water stress and canopy temperature (WSCT) measures were the most discriminative indices; (3) the support vector machine (SVM) was effective in VI-wise classification with smaller feature spaces, while the RF classifier performed better in pixel-wise and image-wise classification with larger feature spaces; and (4) the automated 3D convolutional neural network (3D-CNN) feature extractor provided promising results over the 2D convolutional neural network (2D-CNN) in learning features from hyperspectral data cubes with a limited number of samples.},
DOI = {10.3390/s21030742}
}



@Article{s21030750,
AUTHOR = {Garrido, Iván and Erazo-Aux, Jorge and Lagüela, Susana and Sfarra, Stefano and Ibarra-Castanedo, Clemente and Pivarčiová, Elena and Gargiulo, Gianfranco and Maldague, Xavier and Arias, Pedro},
TITLE = {Introduction of Deep Learning in Thermographic Monitoring of Cultural Heritage and Improvement by Automatic Thermogram Pre-Processing Algorithms},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {750},
URL = {https://www.mdpi.com/1424-8220/21/3/750},
PubMedID = {33499344},
ISSN = {1424-8220},
ABSTRACT = {The monitoring of heritage objects is necessary due to their continuous deterioration over time. Therefore, the joint use of the most up-to-date inspection techniques with the most innovative data processing algorithms plays an important role to apply the required prevention and conservation tasks in each case study. InfraRed Thermography (IRT) is one of the most used Non-Destructive Testing (NDT) techniques in the cultural heritage field due to its advantages in the analysis of delicate objects (i.e., undisturbed, non-contact and fast inspection of large surfaces) and its continuous evolution in both the acquisition and the processing of the data acquired. Despite the good qualitative and quantitative results obtained so far, the lack of automation in the IRT data interpretation predominates, with few automatic analyses that are limited to specific conditions and the technology of the thermographic camera. Deep Learning (DL) is a data processor with a versatile solution for highly automated analysis. Then, this paper introduces the latest state-of-the-art DL model for instance segmentation, Mask Region-Convolution Neural Network (Mask R-CNN), for the automatic detection and segmentation of the position and area of different surface and subsurface defects, respectively, in two different artistic objects belonging to the same family: Marquetry. For that, active IRT experiments are applied to each marquetry. The thermal image sequences acquired are used as input dataset in the Mask R-CNN learning process. Previously, two automatic thermal image pre-processing algorithms based on thermal fundamentals are applied to the acquired data in order to improve the contrast between defective and sound areas. Good detection and segmentation results are obtained regarding state-of-the-art IRT data processing algorithms, which experience difficulty in identifying the deepest defects in the tests. In addition, the performance of the Mask R-CNN is improved by the prior application of the proposed pre-processing algorithms.},
DOI = {10.3390/s21030750}
}



@Article{f12020131,
AUTHOR = {Chen, Xinxin and Jiang, Kang and Zhu, Yushi and Wang, Xiangjun and Yun, Ting},
TITLE = {Individual Tree Crown Segmentation Directly from UAV-Borne LiDAR Data Using the PointNet of Deep Learning},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {131},
URL = {https://www.mdpi.com/1999-4907/12/2/131},
ISSN = {1999-4907},
ABSTRACT = {Accurate individual tree crown (ITC) segmentation from scanned point clouds is a fundamental task in forest biomass monitoring and forest ecology management. Light detection and ranging (LiDAR) as a mainstream tool for forest survey is advancing the pattern of forest data acquisition. In this study, we performed a novel deep learning framework directly processing the forest point clouds belonging to the four forest types (i.e., the nursery base, the monastery garden, the mixed forest, and the defoliated forest) to realize the ITC segmentation. The specific steps of our approach were as follows: first, a voxelization strategy was conducted to subdivide the collected point clouds with various tree species from various forest types into many voxels. These voxels containing point clouds were taken as training samples for the PointNet deep learning framework to identify the tree crowns at the voxel scale. Second, based on the initial segmentation results, we used the height-related gradient information to accurately depict the boundaries of each tree crown. Meanwhile, the retrieved tree crown breadths of individual trees were compared with field measurements to verify the effectiveness of our approach. Among the four forest types, our results revealed the best performance for the nursery base (tree crown detection rate r = 0.90; crown breadth estimation R2 &gt; 0.94 and root mean squared error (RMSE) &lt; 0.2m). A sound performance was also achieved for the monastery garden and mixed forest, which had complex forest structures, complicated intersections of branches and different building types, with r = 0.85, R2 &gt; 0.88 and RMSE &lt; 0.6 m for the monastery garden and r = 0.80, R2 &gt; 0.85 and RMSE &lt; 0.8 m for the mixed forest. For the fourth forest plot type with the distribution of crown defoliation across the woodland, we achieved the performance with r = 0.82, R2 &gt; 0.79 and RMSE &lt; 0.7 m. Our method presents a robust framework inspired by the deep learning technology and computer graphics theory that solves the ITC segmentation problem and retrieves forest parameters under various forest conditions.},
DOI = {10.3390/f12020131}
}



@Article{app11031065,
AUTHOR = {Yang, Yuan and Huang, Yongjiang and Yang, Haoran and Zhang, Tingting and Wang, Zixuan and Liu, Xixiang},
TITLE = {Real-Time Terrain-Following of an Autonomous Quadrotor by Multi-Sensor Fusion and Control},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1065},
URL = {https://www.mdpi.com/2076-3417/11/3/1065},
ISSN = {2076-3417},
ABSTRACT = {For the application of the autonomous guidance of a quadrotor from confined undulant ground, terrain-following is the major issue for flying at a low altitude. This study has modified the open-source autopilot based on the integration of a multi-sensor receiver (a Global Navigation Satellite System (GNSS)), a Lidar-lite (a laser-range-finder device), a barometer and a low-cost inertial navigation system (INS)). These automatically control the position, attitude and height (a constant clearance above the ground) to allow terrain-following and avoid obstacles based on multi-sensors that maintain a constant height above flat ground or with obstacles. The INS/Lidar-lite integration is applied for the attitude and the height stabilization, respectively. The height control is made by the combination of an extended Kalman filter (EKF) estimator and a cascade proportional-integral-derivative (PID) controller that is designed appropriately for the noise characteristics of low accuracy sensors. The proposed terrain-following is tested by both simulations and real-world experiments. The results indicate that the quadrotor can continuously navigate and avoid obstacles at a real-time response of reliable height control with the adjustment time of the cascade PID controller improving over 50% than that of the PID controller.},
DOI = {10.3390/app11031065}
}



@Article{en14030598,
AUTHOR = {Chao, Kuei-Hsiang and Lai, Pei-Lun},
TITLE = {A Fault Diagnosis Mechanism with Power Generation Improvement for a Photovoltaic Module Array},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {598},
URL = {https://www.mdpi.com/1996-1073/14/3/598},
ISSN = {1996-1073},
ABSTRACT = {This paper aims to develop an online diagnostic mechanism, doubling as a maximum power point tracking scheme, for a photovoltaic (PV) module array. In case of malfunction or shadow event occurring to a PV module, the presented diagnostic mechanism is enabled, automatically and immediately, to reconfigure a PV module array for maximum output power operation under arbitrary working conditions. Meanwhile, the malfunctioning or shaded PV module can be located instantly by this diagnostic mechanism according to the array configuration, and a PV module replacement process is made more efficient than ever before for the maintenance crew. In this manner, the intended maximum output power operation can be resumed as soon as possible in consideration of a minimum business loss. Using a particle swarm optimization (PSO)-based algorithm, the PV module array is reconfigured by means of switch manipulations between modules, such that a load is supplied with the maximum amount of output power. For compactness, the PSO-based online diagnostic algorithm is implemented herein using a TMS320F2808 digital signal processor (DSP) and is experimentally validated as successful to identify a malfunctioning PV module at the end of this work.},
DOI = {10.3390/en14030598}
}



@Article{rs13030440,
AUTHOR = {Zhang, Haiming and Wang, Mingchang and Wang, Fengyan and Yang, Guodong and Zhang, Ying and Jia, Junqian and Wang, Siqi},
TITLE = {A Novel Squeeze-and-Excitation W-Net for 2D and 3D Building Change Detection with Multi-Source and Multi-Feature Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {440},
URL = {https://www.mdpi.com/2072-4292/13/3/440},
ISSN = {2072-4292},
ABSTRACT = {Building Change Detection (BCD) is one of the core issues in earth observation and has received extensive attention in recent years. With the rapid development of earth observation technology, the data source of remote sensing change detection is continuously enriched, which provides the possibility to describe the spatial details of the ground objects more finely and to characterize the ground objects with multiple perspectives and levels. However, due to the different physical mechanisms of multi-source remote sensing data, BCD based on heterogeneous data is a challenge. Previous studies mostly focused on the BCD of homogeneous remote sensing data, while the use of multi-source remote sensing data and considering multiple features to conduct 2D and 3D BCD research is sporadic. In this article, we propose a novel and general squeeze-and-excitation W-Net, which is developed from U-Net and SE-Net. Its unique advantage is that it can not only be used for BCD of homogeneous and heterogeneous remote sensing data respectively but also can input both homogeneous and heterogeneous remote sensing data for 2D or 3D BCD by relying on its bidirectional symmetric end-to-end network architecture. Moreover, from a unique perspective, we use image features that are stable in performance and less affected by radiation differences and temporal changes. We innovatively introduced the squeeze-and-excitation module to explicitly model the interdependence between feature channels so that the response between the feature channels is adaptively recalibrated to improve the information mining ability and detection accuracy of the model. As far as we know, this is the first proposed network architecture that can simultaneously use multi-source and multi-feature remote sensing data for 2D and 3D BCD. The experimental results in two 2D data sets and two challenging 3D data sets demonstrate that the promising performances of the squeeze-and-excitation W-Net outperform several traditional and state-of-the-art approaches. Moreover, both visual and quantitative analyses of the experimental results demonstrate competitive performance in the proposed network. This demonstrates that the proposed network and method are practical, physically justified, and have great potential application value in large-scale 2D and 3D BCD and qualitative and quantitative research.},
DOI = {10.3390/rs13030440}
}



@Article{rs13030441,
AUTHOR = {Fu, Han and Fu, Bihong and Shi, Pilong},
TITLE = {An Improved Segmentation Method for Automatic Mapping of Cone Karst from Remote Sensing Data Based on DeepLab V3+ Model},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {441},
URL = {https://www.mdpi.com/2072-4292/13/3/441},
ISSN = {2072-4292},
ABSTRACT = {The South China Karst, a United Nations Educational, Scientific and Cultural Organization (UNESCO) natural heritage site, is one of the world&rsquo;s most spectacular examples of humid tropical to subtropical karst landscapes. The Libo cone karst in the southern Guizhou Province is considered as the world reference site for these types of karst, forming a distinctive and beautiful landscape. Geomorphic information and spatial distribution of cone karst is essential for conservation and management for Libo heritage site. In this study, a deep learning (DL) method based on DeepLab V3+ network was proposed to document the cone karst landscape in Libo by multi-source data, including optical remote sensing images and digital elevation model (DEM) data. The training samples were generated by using Landsat remote sensing images and their combination with satellite derived DEM data. Each group of training dataset contains 898 samples. The input module of DeepLab V3+ network was improved to accept four-channel input data, i.e., combination of Landsat RGB images and DEM data. Our results suggest that the mean intersection over union (MIoU) using the four-channel data as training samples by a new DL-based pixel-level image segmentation approach is the highest, which can reach 95.5%. The proposed method can accomplish automatic extraction of cone karst landscape by self-learning of deep neural network, and therefore it can also provide a powerful and automatic tool for documenting other type of geological landscapes worldwide.},
DOI = {10.3390/rs13030441}
}



@Article{jsan10010007,
AUTHOR = {Benbarrad, Tajeddine and Salhaoui, Marouane and Kenitar, Soukaina Bakhat and Arioua, Mounir},
TITLE = {Intelligent Machine Vision Model for Defective Product Inspection Based on Machine Learning},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {7},
URL = {https://www.mdpi.com/2224-2708/10/1/7},
ISSN = {2224-2708},
ABSTRACT = {Quality control is one of the industrial tasks most susceptible to be improved by implementing technological innovations. As an innovative technology, machine vision enables reliable and fast 24/7 inspections and helps producers to improve the efficiency of manufacturing operations. The accessible data by vision equipment will be used to identify and report defective products, understand the causes of deficiencies and allow rapid and efficient intervention in smart factories. From this perspective, the proposed machine vision model in this paper combines the identification of defective products and the continuous improvement of manufacturing processes by predicting the most suitable parameters of production processes to obtain a defect-free item. The suggested model exploits all generated data by various integrated technologies in the manufacturing chain, thus meeting the requirements of quality management in the context of Industry 4.0, based on predictive analysis to identify patterns in data and suggest corrective actions to ensure product quality. In addition, a comparative study between several machine learning algorithms, both for product classification and process improvement models, is performed in order to evaluate the designed system. The results of this study show that the proposed model largely meets the requirements for the proper implementation of these techniques.},
DOI = {10.3390/jsan10010007}
}



@Article{rs13030457,
AUTHOR = {Zhou, Xixuan and Yang, Liao and Wang, Weisheng and Chen, Baili},
TITLE = {UAV Data as an Alternative to Field Sampling to Monitor Vineyards Using Machine Learning Based on UAV/Sentinel-2 Data Fusion},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {457},
URL = {https://www.mdpi.com/2072-4292/13/3/457},
ISSN = {2072-4292},
ABSTRACT = {Pests and diseases affect the yield and quality of grapes directly and engender noteworthy economic losses. Diagnosing &ldquo;lesions&rdquo; on vines as soon as possible and dynamically monitoring symptoms caused by pests and diseases at a larger scale are essential to pest control. This study has appraised the capabilities of high-resolution unmanned aerial vehicle (UAV) data as an alternative to manual field sampling to obtain sampling canopy sets and to supplement satellite-based monitoring using machine learning models including partial least squared regression (PLSR), support vector regression (SVR), random forest regression (RFR), and extreme learning regression (ELR) with a new activation function. UAV data were acquired from two flights in Turpan to determine disease severity (DS) and disease incidence (DI) and compared with field visual assessments. The UAV-derived canopy structure including canopy height (CH) and vegetation fraction cover (VFC), as well as satellite-based spectral features calculated from Sentinel-2A/B data were analyzed to evaluate the potential of UAV data to replace manual sampling data and predict DI. It was found that SVR slightly outperformed the other methods with a root mean square error (RMSE) of 1.89%. Moreover, the combination of canopy structure (CS) and vegetation index (VIs) improved prediction accuracy compared with single-type features (RMSEcs of 2.86% and RMSEVIs of 1.93%). This study tested the ability of UAV sampling to replace manual sampling on a large scale and introduced opportunities and challenges of fusing different features to monitor vineyards using machine learning. Within this framework, disease incidence can be estimated efficiently and accurately for larger area monitoring operation.},
DOI = {10.3390/rs13030457}
}



@Article{s21030877,
AUTHOR = {Liu, Jian and Xu, Youshuan and Li, Henghui and Guo, Jiao},
TITLE = {Soil Moisture Retrieval in Farmland Areas with Sentinel Multi-Source Data Based on Regression Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {877},
URL = {https://www.mdpi.com/1424-8220/21/3/877},
PubMedID = {33525486},
ISSN = {1424-8220},
ABSTRACT = {As an important component of the earth ecosystem, soil moisture monitoring is of great significance in the fields of crop growth monitoring, crop yield estimation, variable irrigation, and other related applications. In order to mitigate or eliminate the impacts of sparse vegetation covers in farmland areas, this study combines multi-source remote sensing data from Sentinel-1 radar and Sentinel-2 optical satellites to quantitatively retrieve soil moisture content. Firstly, a traditional Oh model was applied to estimate soil moisture content after removing vegetation influence by a water cloud model. Secondly, support vector regression (SVR) and generalized regression neural network (GRNN) models were used to establish the relationships between various remote sensing features and real soil moisture. Finally, a regression convolutional neural network (CNNR) model is constructed to extract deep-level features of remote sensing data to increase soil moisture retrieval accuracy. In addition, polarimetric decomposition features for real Sentinel-1 PolSAR data are also included in the construction of inversion models. Based on the established soil moisture retrieval models, this study analyzes the influence of each input feature on the inversion accuracy in detail. The experimental results show that the optimal combination of R2 and root mean square error (RMSE) for SVR is 0.7619 and 0.0257 cm3/cm3, respectively. The optimal combination of R2 and RMSE for GRNN is 0.7098 and 0.0264 cm3/cm3, respectively. Especially, the CNNR model with optimal feature combination can generate inversion results with the highest accuracy, whose R2 and RMSE reach up to 0.8947 and 0.0208 cm3/cm3, respectively. Compared to other methods, the proposed algorithm improves the accuracy of soil moisture retrieval from synthetic aperture radar (SAR) and optical data. Furthermore, after adding polarization decomposition features, the R2 of CNNR is raised by 0.1524 and the RMSE of CNNR decreased by 0.0019 cm3/cm3 on average, which means that the addition of polarimetric decomposition features effectively improves the accuracy of soil moisture retrieval results.},
DOI = {10.3390/s21030877}
}



@Article{rs13030459,
AUTHOR = {Liao, Wenyue and Deng, Yingbin and Li, Miao and Sun, Meiwei and Yang, Ji and Xu, Jianhui},
TITLE = {Extraction and Analysis of Finer Impervious Surface Classes in Urban Area},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {459},
URL = {https://www.mdpi.com/2072-4292/13/3/459},
ISSN = {2072-4292},
ABSTRACT = {Impervious surfaces (IS), the most common land cover in urban areas, not only provide convenience to the city, but also exert significant negative environmental impacts, thereby affecting the ecological environment carrying capacity of urban agglomerations. Most of the current research considers IS as a single land-cover type, yet this does not fully reflect the complex physical characteristics of various IS types. Therefore, limited information for urban micro-ecology and urban fine management can be provided through one IS land-cover type. This study proposed a finer IS classification scheme and mapped the detailed IS fraction in Guangzhou City, China using Landsat imagery. The IS type was divided into seven finer classes, including blue steel, cement, asphalt, other impervious surface, and other metal, brick, and plastic. Classification results demonstrate that finer IS can be well extracted from the Landsat imagery as all root mean square errors (RMSE) are less than 15%. Specially, the accuracies of asphalt, plastic, and cement are better than other finer IS types with the RMSEs of 7.99%, 8.48%, and 9.92%, respectively. Quantitative analyses illustrate that asphalt, other impervious surface, and brick are the dominant IS types in the study area with the percentages of 9.68%, 6.27%, and 4.45%, respectively, and they are mainly located in Yuexiu, Liwan, Haizhu, and Panyu districts. These results are valuable for research into urban fine management and can support the detailed analysis of urban micro-ecology.},
DOI = {10.3390/rs13030459}
}



@Article{rs13030461,
AUTHOR = {Croce, Valeria and Caroti, Gabriella and De Luca, Livio and Jacquot, Kévin and Piemonte, Andrea and Véron, Philippe},
TITLE = {From the Semantic Point Cloud to Heritage-Building Information Modeling: A Semiautomatic Approach Exploiting Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {461},
URL = {https://www.mdpi.com/2072-4292/13/3/461},
ISSN = {2072-4292},
ABSTRACT = {This work presents a semi-automatic approach to the 3D reconstruction of Heritage-Building Information Models from point clouds based on machine learning techniques. The use of digital information systems leveraging on three-dimensional (3D) representations in architectural heritage documentation and analysis is ever increasing. For the creation of such repositories, reality-based surveying techniques, such as photogrammetry and laser scanning, allow the fast collection of reliable digital replicas of the study objects in the form of point clouds. Besides, their output is raw and unstructured, and the transition to intelligible and semantic 3D representations is still a scarcely automated and time-consuming process requiring considerable human intervention. More refined methods for 3D data interpretation of heritage point clouds are therefore sought after. In tackling these issues, the proposed approach relies on (i) the application of machine learning techniques to semantically label 3D heritage data by identification of relevant geometric, radiometric and intensity features, and (ii) the use of the annotated data to streamline the construction of Heritage-Building Information Modeling (H-BIM) systems, where purely geometric information derived from surveying is associated with semantic descriptors on heritage documentation and management. The &ldquo;Grand-Ducal Cloister&rdquo; dataset, related to the emblematic case study of the Pisa Charterhouse, is discussed.},
DOI = {10.3390/rs13030461}
}



@Article{rs13030472,
AUTHOR = {Chen, Yang and Liu, Guanlan and Xu, Yaming and Pan, Pai and Xing, Yin},
TITLE = {PointNet++ Network Architecture with Individual Point Level and Global Features on Centroid for ALS Point Cloud Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {472},
URL = {https://www.mdpi.com/2072-4292/13/3/472},
ISSN = {2072-4292},
ABSTRACT = {Airborne laser scanning (ALS) point cloud has been widely used in the fields of ground powerline surveying, forest monitoring, urban modeling, and so on because of the great convenience it brings to people&rsquo;s daily life. However, the sparsity and uneven distribution of point clouds increases the difficulty of setting uniform parameters for semantic classification. The PointNet++ network is an end-to-end learning network for irregular point data and highly robust to small perturbations of input points along with corruption. It eliminates the need to calculate costly handcrafted features and provides a new paradigm for 3D understanding. However, each local region in the output is abstracted by its centroid and local feature that encodes the centroid&rsquo;s neighborhood. The feature learned on the centroid point may not contain relevant information of itself for random sampling, especially in large-scale neighborhood balls. Moreover, the centroid point&rsquo;s global-level information in each sample layer is also not marked. Therefore, this study proposed a modified PointNet++ network architecture which concentrates the point-level and global features on the centroid point towards the local features to facilitate classification. The proposed approach also utilizes a modified Focal Loss function to solve the extremely uneven category distribution on ALS point clouds. An elevation- and distance-based interpolation method is also proposed for the objects in ALS point clouds which exhibit discrepancies in elevation distributions. The experiments on the Vaihingen dataset of the International Society for Photogrammetry and Remote Sensing and the GML(B) 3D dataset demonstrate that the proposed method which provides additional contextual information to support classification achieves high accuracy with simple discriminative models and new state-of-the-art performance in power line categories.},
DOI = {10.3390/rs13030472}
}



@Article{app11031240,
AUTHOR = {Casas, Roberto and Hermosa, Arturo and Marco, Álvaro and Blanco, Teresa and Zarazaga-Soria, Francisco Javier},
TITLE = {Real-Time Extensive Livestock Monitoring Using LPWAN Smart Wearable and Infrastructure},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1240},
URL = {https://www.mdpi.com/2076-3417/11/3/1240},
ISSN = {2076-3417},
ABSTRACT = {Extensive unsupervised livestock farming is a habitual technique in many places around the globe. Animal release can be done for months, in large areas and with different species packing and behaving very differently. Nevertheless, the farmer’s needs are similar: where livestock is (and where has been) and how healthy they are. The geographical areas involved usually have difficult access with harsh orography and lack of communications infrastructure. This paper presents the design of a solution for extensive livestock monitoring in these areas. Our proposal is based in a wearable equipped with inertial sensors, global positioning system and wireless communications; and a Low-Power Wide Area Network infrastructure that can run with and without internet connection. Using adaptive analysis and data compression, we provide real-time monitoring and logging of cattle’s position and activities. Hardware and firmware design achieve very low energy consumption allowing months of battery life. We have thoroughly tested the devices in different laboratory setups and evaluated the system performance in real scenarios in the mountains and in the forest.},
DOI = {10.3390/app11031240}
}



@Article{min11020148,
AUTHOR = {Jung, Dahee and Choi, Yosoon},
TITLE = {Systematic Review of Machine Learning Applications in Mining: Exploration, Exploitation, and Reclamation},
JOURNAL = {Minerals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {148},
URL = {https://www.mdpi.com/2075-163X/11/2/148},
ISSN = {2075-163X},
ABSTRACT = {Recent developments in smart mining technology have enabled the production, collection, and sharing of a large amount of data in real time. Therefore, research employing machine learning (ML) that utilizes these data is being actively conducted in the mining industry. In this study, we reviewed 109 research papers, published over the past decade, that discuss ML techniques for mineral exploration, exploitation, and mine reclamation. Research trends, ML models, and evaluation methods primarily discussed in the 109 papers were systematically analyzed. The results demonstrated that ML studies have been actively conducted in the mining industry since 2018, mostly for mineral exploration. Among the ML models, support vector machine was utilized the most, followed by deep learning models. The ML models were evaluated mostly in terms of their root mean square error and coefficient of determination.},
DOI = {10.3390/min11020148}
}



@Article{rs13030504,
AUTHOR = {Yang, Wanting and Zhang, Xianfeng and Luo, Peng},
TITLE = {Transferability of Convolutional Neural Network Models for Identifying Damaged Buildings Due to Earthquake},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {504},
URL = {https://www.mdpi.com/2072-4292/13/3/504},
ISSN = {2072-4292},
ABSTRACT = {The collapse of buildings caused by earthquakes can lead to a large loss of life and property. Rapid assessment of building damage with remote sensing image data can support emergency rescues. However, current studies indicate that only a limited sample set can usually be obtained from remote sensing images immediately following an earthquake. Consequently, the difficulty in preparing sufficient training samples constrains the generalization of the model in the identification of earthquake-damaged buildings. To produce a deep learning network model with strong generalization, this study adjusted four Convolutional Neural Network (CNN) models for extracting damaged building information and compared their performance. A sample dataset of damaged buildings was constructed by using multiple disaster images retrieved from the xBD dataset. Using satellite and aerial remote sensing data obtained after the 2008 Wenchuan earthquake, we examined the geographic and data transferability of the deep network model pre-trained on the xBD dataset. The result shows that the network model pre-trained with samples generated from multiple disaster remote sensing images can extract accurately collapsed building information from satellite remote sensing data. Among the adjusted CNN models tested in the study, the adjusted DenseNet121 was the most robust. Transfer learning solved the problem of poor adaptability of the network model to remote sensing images acquired by different platforms and could identify disaster-damaged buildings properly. These results provide a solution to the rapid extraction of earthquake-damaged building information based on a deep learning network model.},
DOI = {10.3390/rs13030504}
}



@Article{s21030956,
AUTHOR = {Sassu, Alberto and Gambella, Filippo and Ghiani, Luca and Mercenaro, Luca and Caria, Maria and Pazzona, Antonio Luigi},
TITLE = {Advances in Unmanned Aerial System Remote Sensing for Precision Viticulture},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {956},
URL = {https://www.mdpi.com/1424-8220/21/3/956},
PubMedID = {33535445},
ISSN = {1424-8220},
ABSTRACT = {New technologies for management, monitoring, and control of spatio-temporal crop variability in precision viticulture scenarios are numerous. Remote sensing relies on sensors able to provide useful data for the improvement of management efficiency and the optimization of inputs. unmanned aerial systems (UASs) are the newest and most versatile tools, characterized by high precision and accuracy, flexibility, and low operating costs. The work aims at providing a complete overview of the application of UASs in precision viticulture, focusing on the different application purposes, the applied equipment, the potential of technologies combined with UASs for identifying vineyards’ variability. The review discusses the potential of UASs in viticulture by distinguishing five areas of application: rows segmentation and crop features detection techniques; vineyard variability monitoring; estimation of row area and volume; disease detection; vigor and prescription maps creation. Technological innovation and low purchase costs make UASs the core tools for decision support in the customary use by winegrowers. The ability of the systems to respond to the current demands for the acquisition of digital technologies in agricultural fields makes UASs a candidate to play an increasingly important role in future scenarios of viticulture application.},
DOI = {10.3390/s21030956}
}



@Article{s21030974,
AUTHOR = {Rahman, Ehab Ur and Zhang, Yihong and Ahmad, Sohail and Ahmad, Hafiz Ishfaq and Jobaer, Sayed},
TITLE = {Autonomous Vision-Based Primary Distribution Systems Porcelain Insulators Inspection Using UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {974},
URL = {https://www.mdpi.com/1424-8220/21/3/974},
PubMedID = {33540500},
ISSN = {1424-8220},
ABSTRACT = {The early detection of damaged (partially broken) outdoor insulators in primary distribution systems is of paramount importance for continuous electricity supply and public safety. Unmanned aerial vehicles (UAVs) present a safer, autonomous, and efficient way to examine the power system components without closing the power distribution system. In this work, a novel dataset is designed by capturing real images using UAVs and manually generated images collected to overcome the data insufficiency problem. A deep Laplacian pyramid-based super-resolution network is implemented to reconstruct high-resolution training images. To improve the visibility of low-light images, a low-light image enhancement technique is used for the robust exposure correction of the training images. A different fine-tuning strategy is implemented for fine-tuning the object detection model to increase detection accuracy for the specific faulty insulators. Several flight path strategies are proposed to overcome the shuttering effect of insulators, along with providing a less complex and time- and energy-efficient approach for capturing a video stream of the power system components. The performance of different object detection models is presented for selecting the most suitable one for fine-tuning on the specific faulty insulator dataset. For the detection of damaged insulators, our proposed method achieved an F1-score of 0.81 and 0.77 on two different datasets and presents a simple and more efficient flight strategy. Our approach is based on real aerial inspection of in-service porcelain insulators by extensive evaluation of several video sequences showing robust fault recognition and diagnostic capabilities. Our approach is demonstrated on data acquired by a drone in Swat, Pakistan.},
DOI = {10.3390/s21030974}
}



@Article{aerospace8020039,
AUTHOR = {Blasi, Luciano and Borrelli, Mauro and D’Amato, Egidio and di Grazia, Luigi Emanuel and Mattei, Massimiliano and Notaro, Immacolata},
TITLE = {Modeling and Control of a Modular Iron Bird},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2226-4310/8/2/39},
ISSN = {2226-4310},
ABSTRACT = {This paper describes the control architecture and the control laws of a new concept of Modular Iron Bird aimed at reproducing flight loads to test mobile aerodynamic control surface actuators for small and medium size aircraft and Unmanned Aerial Vehicles. The iron bird control system must guarantee the actuation of counteracting forces. On one side, a hydraulic actuator simulates the hinge moments acting on the mobile surface due to aerodynamic and inertial effects during flight; on the other side, the actuator to be tested applies an active hinge moment to control the angular position of the same surface. Reference aerodynamic and inertial loads are generated by a flight simulation module to reproduce more realistic conditions arising during operations. The design of the control action is based on a dynamic model of the hydraulic plant used to generate loads. This system is controlled using a Proportional Integral Derivative control algorithm tuned with an optimization algorithm taking into account the closed loop dynamics of the actuator under testing, uncertainties and disturbances in the controlled plant. Numerical simulations are presented to show the effectiveness of the proposed architecture and control laws.},
DOI = {10.3390/aerospace8020039}
}



@Article{s21030987,
AUTHOR = {Karttunen, Aki and Valkama, Mikko and Talvitie, Jukka},
TITLE = {Influence of Noise-Limited Censored Path Loss on Model Fitting and Path Loss-Based Positioning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {987},
URL = {https://www.mdpi.com/1424-8220/21/3/987},
PubMedID = {33540651},
ISSN = {1424-8220},
ABSTRACT = {Positioning is considered one of the key features in various novel industry verticals in future radio systems. Since path loss (PL) or received signal strength-based measurements are widely available in the majority of wireless standards, PL-based positioning has an important role among positioning technologies. Conventionally, PL-based positioning has two phases—fitting a PL model to training data and positioning based on the link distance estimates. However, in both phases, the maximum measurable PL is limited by measurement noise. Such immeasurable samples are called censored PL data and such noisy data are commonly neglected in both the model fitting and in the positioning phase. In the case of censored PL, the loss is known to be above a known threshold level and that information can be used in model fitting and in the positioning phase. In this paper, we examine and propose how to use censored PL data in PL model-based positioning. Additionally, we demonstrate with several simulations the potential of the proposed approach for considerable improvements in positioning accuracy (23–57%) and improved robustness against PL model fitting errors.},
DOI = {10.3390/s21030987}
}



@Article{safety7010011,
AUTHOR = {Papaioannou, Panagiotis and Papadopoulos, Efthymis and Nikolaidou, Anastasia and Politis, Ioannis and Basbas, Socrates and Kountouri, Eleni},
TITLE = {Dilemma Zone: Modeling Drivers’ Decision at Signalized Intersections against Aggressiveness and Other Factors Using UAV Technology},
JOURNAL = {Safety},
VOLUME = {7},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {11},
URL = {https://www.mdpi.com/2313-576X/7/1/11},
ISSN = {2313-576X},
ABSTRACT = {Intersection safety and drivers’ behavior are strongly interrelated, especially when the latter are located in dilemma zone. This paper explores, among others, the main factors affecting driver behavior, such as distance to stop line, approaching speed and acceleration/deceleration, and two additional factors, namely, driver’s aggressiveness and driver’s relative position at the onset of the yellow signal. Field data were collected using unmanned aerial vehicle (UAV) technology. Two binary choice models were developed, the first relying on observed data and the latter enriched by the latent factor drivers’ aggressiveness and the vehicles’ relative position. Drivers were classified to aggressive and non-aggressive ones using a latent class model that combined approaching speed and acceleration/deceleration data. Drivers were further grouped according to their expected reaction/decision to stop or cross the intersection in relation to their relative position. Both models equally explain drivers’ decisions adequately, but the second one offers additional explanatory power attributed to aggressiveness. Being able to identify the level of aggressiveness among the drivers enables the calculation of the probability that drivers will cross the intersection even if caught in a dilemma zone or in a zone in which the obvious decision is to stop. Such findings can be valuable when designing a signalized intersection and the traffic time settings, as well as the posted speed limit.},
DOI = {10.3390/safety7010011}
}



@Article{app11041403,
AUTHOR = {Kamarudin, Mohd Hider and Ismail, Zool Hilmi and Saidi, Noor Baity},
TITLE = {Deep Learning Sensor Fusion in Plant Water Stress Assessment: A Comprehensive Review},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1403},
URL = {https://www.mdpi.com/2076-3417/11/4/1403},
ISSN = {2076-3417},
ABSTRACT = {Water stress is one of the major challenges to food security, causing a significant economic loss for the nation as well for growers. Accurate assessment of water stress will enhance agricultural productivity through optimization of plant water usage, maximizing plant breeding strategies, and preventing forest wildfire for better ecosystem management. Recent advancements in sensor technologies have enabled high-throughput, non-contact, and cost-efficient plant water stress assessment through intelligence system modeling. The advanced deep learning sensor fusion technique has been reported to improve the performance of the machine learning application for processing the collected sensory data. This paper extensively reviews the state-of-the-art methods for plant water stress assessment that utilized the deep learning sensor fusion approach in their application, together with future prospects and challenges of the application domain. Notably, 37 deep learning solutions fell under six main areas, namely soil moisture estimation, soil water modelling, evapotranspiration estimation, evapotranspiration forecasting, plant water status estimation and plant water stress identification. Basically, there are eight deep learning solutions compiled for the 3D-dimensional data and plant varieties challenge, including unbalanced data that occurred due to isohydric plants, and the effect of variations that occur within the same species but cultivated from different locations.},
DOI = {10.3390/app11041403}
}



@Article{automation2010001,
AUTHOR = {Zenteno-Torres, Jazmín and Cieslak, Jérôme and Dávila, Jorge and Henry, David},
TITLE = {Sliding Mode Control with Application to Fault-Tolerant Control: Assessment and Open Problems},
JOURNAL = {Automation},
VOLUME = {2},
YEAR = {2021},
NUMBER = {1},
PAGES = {1--30},
URL = {https://www.mdpi.com/2673-4052/2/1/1},
ISSN = {2673-4052},
ABSTRACT = {This paper is prepared within a collaboration between the Instituto Politécnico Nacional, which is a Mexican research institute that manages research on sliding-mode control theory, and the ARIA research team of the Intégration du Matériau au Système Lab., a French research group that engages research on model-based fault diagnosis and fault-tolerant control theories. The paper reviews the application of sliding mode control techniques to fault tolerant control and provides perspectives leading to posing some open problems. Operating principles, definitions of the basic concepts are recalled along with the control objectives and design procedures. The evolution of the sliding mode control technique through five generations (as classified by Fridman, Moreno and co-workers) is reviewed. Their respective design procedures, limitations, and robustness properties are also highlighted. The application of the five generations of sliding-mode controllers to fault-tolerant control is discussed. The focus is on some open problems that are judged to commonly be overlooked. Some applications in real-world systems are also presented.},
DOI = {10.3390/automation2010001}
}



@Article{s21041076,
AUTHOR = {Yan, Peng and Jia, Tao and Bai, Chengchao},
TITLE = {Searching and Tracking an Unknown Number of Targets: A Learning-Based Method Enhanced with Maps Merging},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1076},
URL = {https://www.mdpi.com/1424-8220/21/4/1076},
PubMedID = {33557359},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have been widely used in search and rescue (SAR) missions due to their high flexibility. A key problem in SAR missions is to search and track moving targets in an area of interest. In this paper, we focus on the problem of Cooperative Multi-UAV Observation of Multiple Moving Targets (CMUOMMT). In contrast to the existing literature, we not only optimize the average observation rate of the discovered targets, but we also emphasize the fairness of the observation of the discovered targets and the continuous exploration of the undiscovered targets, under the assumption that the total number of targets is unknown. To achieve this objective, a deep reinforcement learning (DRL)-based method is proposed under the Partially Observable Markov Decision Process (POMDP) framework, where each UAV maintains four observation history maps, and maps from different UAVs within a communication range can be merged to enhance UAVs’ awareness of the environment. A deep convolutional neural network (CNN) is used to process the merged maps and generate the control commands to UAVs. The simulation results show that our policy can enable UAVs to balance between giving the discovered targets a fair observation and exploring the search region compared with other methods.},
DOI = {10.3390/s21041076}
}



@Article{plants10020310,
AUTHOR = {Solovchenko, Alexei and Dorokhov, Alexei and Shurygin, Boris and Nikolenko, Alexandr and Velichko, Vitaly and Smirnov, Igor and Khort, Dmitriy and Aksenov, Aleksandr and Kuzin, Andrey},
TITLE = {Linking Tissue Damage to Hyperspectral Reflectance for Non-Invasive Monitoring of Apple Fruit in Orchards},
JOURNAL = {Plants},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {310},
URL = {https://www.mdpi.com/2223-7747/10/2/310},
PubMedID = {33562864},
ISSN = {2223-7747},
ABSTRACT = {Reflected light carries ample information about the biochemical composition, tissue architecture, and physiological condition of plants. Recent technical progress has paved the way for affordable imaging hyperspectrometers (IH) providing spatially resolved spectral information on plants on different levels, from individual plant organs to communities. The extraction of sensible information from hyperspectral images is difficult due to inherent complexity of plant tissue and canopy optics, especially when recorded under ambient sunlight. We report on the changes in hyperspectral reflectance accompanying the accumulation of anthocyanins in healthy apple (cultivars Ligol, Gala, Golden Delicious) fruits as well as in fruits affected by pigment breakdown during sunscald development and phytopathogen attacks. The measurements made outdoors with a snapshot IH were compared with traditional “point-type” reflectance measured with a spectrophotometer under controlled illumination conditions. The spectra captured by the IH were suitable for processing using the approaches previously developed for “point-type” apple fruit and leaf reflectance spectra. The validity of this approach was tested by constructing a novel index mBRI (modified browning reflectance index) for detection of tissue damages on the background of the anthocyanin absorption. The index was suggested in the form of mBRI = (R640−1 + R800−1) − R678−1. Difficulties of the interpretation of fruit hyperspectral reflectance images recorded in situ are discussed with possible implications for plant physiology and precision horticulture practices.},
DOI = {10.3390/plants10020310}
}



@Article{rs13040579,
AUTHOR = {Jiang, Xueqin and Fang, Shenghui and Huang, Xia and Liu, Yanghua and Guo, Linlin},
TITLE = {Rice Mapping and Growth Monitoring Based on Time Series GF-6 Images and Red-Edge Bands},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {579},
URL = {https://www.mdpi.com/2072-4292/13/4/579},
ISSN = {2072-4292},
ABSTRACT = {Accurate rice mapping and growth monitoring are of great significance for ensuring food security and agricultural sustainable development. Remote sensing (RS), as an efficient observation technology, is expected to be useful for rice mapping and growth monitoring. Due to the fragmented distribution of paddy fields and the undulating terrain in Southern China, it is very difficult in rice mapping. Moreover, there are many crops with the same growth period as rice, resulting in low accuracy of rice mapping. We proposed a red-edge decision tree (REDT) method based on the combination of time series GF-6 images and red-edge bands to solve this problem. The red-edge integral and red-edge vegetation index integral were computed by using two red-edge bands derived from GF-6 images to construct the REDT. Meanwhile, the conventional method based on time series normalized difference vegetation index (NDVI), normalized difference water index (NDWI), enhanced vegetation index (EVI) (NNE) was employed to compare the effectiveness of rice mapping. The results indicated that the overall accuracy and Kappa coefficient of REDT ranged from 91%–94% and 0.82–0.87, improving about 7% and 0.15 compared with the NNE method. This proved that the proposed technology was able to efficiently solve the problem of rice mapping on a large scale and regions with fragmented landscapes. Additionally, two red-edge bands of GF-6 images were applied to monitor rice growth. It concluded that the two red-edge bands played different roles in rice growth monitoring. The red-edge bands of GF-6 images were superior in rice mapping and growth monitoring. Further study needs to develop more vegetation indices (VIs) related to the red-edge to make the best use of red-edge characteristics in precision agriculture.},
DOI = {10.3390/rs13040579}
}



@Article{s21041151,
AUTHOR = {Horla, Dariusz and Giernacki, Wojciech and Cieślak, Jacek and Campoy, Pascual},
TITLE = {Altitude Measurement-Based Optimization of the Landing Process of UAVs},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1151},
URL = {https://www.mdpi.com/1424-8220/21/4/1151},
PubMedID = {33562147},
ISSN = {1424-8220},
ABSTRACT = {The paper addresses the loop shaping problem in the altitude control of an unmanned aerial vehicle to land the flying robot with a specific landing scenario adopted. The proposed solution is optimal, in the sense of the selected performance indices, namely minimum-time, minimum-energy, and velocity-penalized related functions, achieving their minimal values, with numerous experiments conducted throughout the development and preparation to the Mohamed Bin Zayed International Robotics Challenge (MBZIRC 2020). A novel approach to generation of a reference altitude trajectory is presented, which is then tracked in a standard, though optimized, control loop. Three landing scenarios are considered, namely: minimum-time, minimum-energy, and velocity-penalized landing scenarios. The experimental results obtained with the use of the Simulink Support Package for Parrot Minidrones, and the OptiTrack motion capture system proved the effectiveness of the proposed approach.},
DOI = {10.3390/s21041151}
}



@Article{rs13040586,
AUTHOR = {Praticò, Salvatore and Solano, Francesco and Di Fazio, Salvatore and Modica, Giuseppe},
TITLE = {Machine Learning Classification of Mediterranean Forest Habitats in Google Earth Engine Based on Seasonal Sentinel-2 Time-Series and Input Image Composition Optimisation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {586},
URL = {https://www.mdpi.com/2072-4292/13/4/586},
ISSN = {2072-4292},
ABSTRACT = {The sustainable management of natural heritage is presently considered a global strategic issue. Owing to the ever-growing availability of free data and software, remote sensing (RS) techniques have been primarily used to map, analyse, and monitor natural resources for conservation purposes. The need to adopt multi-scale and multi-temporal approaches to detect different phenological aspects of different vegetation types and species has also emerged. The time-series composite image approach allows for capturing much of the spectral variability, but presents some criticalities (e.g., time-consuming research, downloading data, and the required storage space). To overcome these issues, the Google Earth engine (GEE) has been proposed, a free cloud-based computational platform that allows users to access and process remotely sensed data at petabyte scales. The application was tested in a natural protected area in Calabria (South Italy), which is particularly representative of the Mediterranean mountain forest environment. In the research, random forest (RF), support vector machine (SVM), and classification and regression tree (CART) algorithms were used to perform supervised pixel-based classification based on the use of Sentinel-2 images. A process to select the best input image (seasonal composition strategies, statistical operators, band composition, and derived vegetation indices (VIs) information) for classification was implemented. A set of accuracy indicators, including overall accuracy (OA) and multi-class F-score (Fm), were computed to assess the results of the different classifications. GEE proved to be a reliable and powerful tool for the classification process. The best results (OA = 0.88 and Fm = 0.88) were achieved using RF with the summer image composite, adding three VIs (NDVI, EVI, and NBR) to the Sentinel-2 bands. SVM and RF produced OAs of 0.83 and 0.80, respectively.},
DOI = {10.3390/rs13040586}
}



@Article{rs13040584,
AUTHOR = {Zhu, Linglong and Zhang, Yonghong and Wang, Jiangeng and Tian, Wei and Liu, Qi and Ma, Guangyi and Kan, Xi and Chu, Ya},
TITLE = {Downscaling Snow Depth Mapping by Fusion of Microwave and Optical Remote-Sensing Data Based on Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {584},
URL = {https://www.mdpi.com/2072-4292/13/4/584},
ISSN = {2072-4292},
ABSTRACT = {Accurate high spatial resolution snow depth mapping in arid and semi-arid regions is of great importance for snow disaster assessment and hydrological modeling. However, due to the complex topography and low spatial-resolution microwave remote-sensing data, the existing snow depth datasets have large errors and uncertainty, and actual spatiotemporal heterogeneity of snow depth cannot be effectively detected. This paper proposed a deep learning approach based on downscaling snow depth retrieval by fusion of satellite remote-sensing data with multiple spatial scales and diverse characteristics. The (Fengyun-3 Microwave Radiation Imager) FY-3 MWRI data were downscaled to 500 m resolution to match Moderate-resolution Imaging Spectroradiometer (MODIS) snow cover, meteorological and geographic data. A deep neural network was constructed to capture detailed spectral and radiation signals and trained to retrieve the higher spatial resolution snow depth from the aforementioned input data and ground observation. Verified by in situ measurements, downscaled snow depth has the lowest root mean square error (RMSE) and mean absolute error (MAE) (8.16 cm, 4.73 cm respectively) among Environmental and Ecological Science Data Center for West China Snow Depth (WESTDC_SD, 9.38 cm and 5.36 cm), the Microwave Radiation Imager (MWRI) Ascend Snow Depth (MWRI_A_SD, 9.45 cm and 5.49 cm) and MWRI Descend Snow Depth (MWRI_D_SD, 10.55 cm and 6.13 cm) in the study area. Meanwhile, downscaled snow depth could provide more detailed information in spatial distribution, which has been used to analyze the decrease of retrieval accuracy by various topography factors.},
DOI = {10.3390/rs13040584}
}



@Article{aerospace8020044,
AUTHOR = {Uzun, Mevlut and Demirezen, Mustafa Umut and Inalhan, Gokhan},
TITLE = {Physics Guided Deep Learning for Data-Driven Aircraft Fuel Consumption Modeling},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {44},
URL = {https://www.mdpi.com/2226-4310/8/2/44},
ISSN = {2226-4310},
ABSTRACT = {This paper presents a physics-guided deep neural network framework to estimate fuel consumption of an aircraft. The framework aims to improve data-driven models’ consistency in flight regimes that are not covered by data. In particular, we guide the neural network with the equations that represent fuel flow dynamics. In addition to the empirical error, we embed this physical knowledge as several extra loss terms. Results show that our proposed model accomplishes correct predictions on the labeled test set, as well as assuring physical consistency in unseen flight regimes. The results indicate that our model, while being applicable to the aircraft’s complete flight envelope, yields lower fuel consumption error measures compared to the model-based approaches and other supervised learning techniques utilizing the same training data sets. In addition, our deep learning model produces fuel consumption trends similar to the BADA4 aircraft performance model, which is widely utilized in real-world operations, in unseen and untrained flight regimes. In contrast, the other supervised learning techniques fail to produce meaningful results. Overall, the proposed methodology enhances the explainability of data-driven models without deteriorating accuracy.},
DOI = {10.3390/aerospace8020044}
}



@Article{su13041821,
AUTHOR = {Islam, Nahina and Rashid, Md Mamunur and Pasandideh, Faezeh and Ray, Biplob and Moore, Steven and Kadel, Rajan},
TITLE = {A Review of Applications and Communication Technologies for Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) Based Sustainable Smart Farming},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1821},
URL = {https://www.mdpi.com/2071-1050/13/4/1821},
ISSN = {2071-1050},
ABSTRACT = {To reach the goal of sustainable agriculture, smart farming is taking advantage of the Unmanned Aerial Vehicles (UAVs) and Internet of Things (IoT) paradigm. These smart farms are designed to be run by interconnected devices and vehicles. Some enormous potentials can be achieved by the integration of different IoT technologies to achieve automated operations with minimum supervision. This paper outlines some major applications of IoT and UAV in smart farming, explores the communication technologies, network functionalities and connectivity requirements for Smart farming. The connectivity limitations of smart agriculture and it’s solutions are analysed with two case studies. In case study-1, we propose and evaluate meshed Long Range Wide Area Network (LoRaWAN) gateways to address connectivity limitations of Smart Farming. While in case study-2, we explore satellite communication systems to provide connectivity to smart farms in remote areas of Australia. Finally, we conclude the paper by identifying future research challenges on this topic and outlining directions to address those challenges.},
DOI = {10.3390/su13041821}
}



@Article{telecom2010005,
AUTHOR = {Kakamoukas, Georgios and Sarigiannidis, Panagiotis and Maropoulos, Andreas and Lagkas, Thomas and Zaralis, Konstantinos and Karaiskou, Chrysoula},
TITLE = {Towards Climate Smart Farming—A Reference Architecture for Integrated Farming Systems},
JOURNAL = {Telecom},
VOLUME = {2},
YEAR = {2021},
NUMBER = {1},
PAGES = {52--74},
URL = {https://www.mdpi.com/2673-4001/2/1/5},
ISSN = {2673-4001},
ABSTRACT = {Climate change is emerging as a major threat to farming, food security and the livelihoods of millions of people across the world. Agriculture is strongly affected by climate change due to increasing temperatures, water shortage, heavy rainfall and variations in the frequency and intensity of excessive climatic events such as floods and droughts. Farmers need to adapt to climate change by developing advanced and sophisticated farming systems instead of simply farming at lower intensity and occupying more land. Integrated agricultural systems constitute a promising solution, as they can lower reliance on external inputs, enhance nutrient cycling and increase natural resource use efficiency. In this context, the concept of Climate-Smart Agriculture (CSA) emerged as a promising solution to secure the resources for the growing world population under climate change conditions. This work proposes a CSA architecture for fostering and supporting integrated agricultural systems, such as Mixed Farming Systems (MFS), by facilitating the design, the deployment and the management of crop–livestock-=forestry combinations towards sustainable, efficient and climate resilient agricultural systems. Propelled by cutting-edge technology solutions in data collection and processing, along with fully autonomous monitoring systems, e.g., smart sensors and unmanned aerial vehicles (UAVs), the proposed architecture called MiFarm-CSA, aims to foster core interactions among animals, forests and crops, while mitigating the high complexity of these interactions, through a novel conceptual framework.},
DOI = {10.3390/telecom2010005}
}



@Article{ai2010004,
AUTHOR = {Espejo-Garcia, Borja and Malounas, Ioannis and Vali, Eleanna and Fountas, Spyros},
TITLE = {Testing the Suitability of Automated Machine Learning for Weeds Identification},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {1},
PAGES = {34--47},
URL = {https://www.mdpi.com/2673-2688/2/1/4},
ISSN = {2673-2688},
ABSTRACT = {In the past years, several machine-learning-based techniques have arisen for providing effective crop protection. For instance, deep neural networks have been used to identify different types of weeds under different real-world conditions. However, these techniques usually require extensive involvement of experts working iteratively in the development of the most suitable machine learning system. To support this task and save resources, a new technique called Automated Machine Learning has started being studied. In this work, a complete open-source Automated Machine Learning system was evaluated with two different datasets, (i) The Early Crop Weeds dataset and (ii) the Plant Seedlings dataset, covering the weeds identification problem. Different configurations, such as the use of plant segmentation, the use of classifier ensembles instead of Softmax and training with noisy data, have been compared. The results showed promising performances of 93.8% and 90.74% F1 score depending on the dataset used. These performances were aligned with other related works in AutoML, but they are far from machine-learning-based systems manually fine-tuned by human experts. From these results, it can be concluded that finding a balance between manual expert work and Automated Machine Learning will be an interesting path to work in order to increase the efficiency in plant protection.},
DOI = {10.3390/ai2010004}
}



@Article{rs13040633,
AUTHOR = {Zhang, Xiuwei and Zhou, Yang and Jin, Jiaojiao and Wang, Yafei and Fan, Minhao and Wang, Ning and Zhang, Yanning},
TITLE = {ICENETv2: A Fine-Grained River Ice Semantic Segmentation Network Based on UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {633},
URL = {https://www.mdpi.com/2072-4292/13/4/633},
ISSN = {2072-4292},
ABSTRACT = {Accurate ice segmentation is one of the most crucial techniques for intelligent ice monitoring. Compared with ice segmentation, it can provide more information for ice situation analysis, change trend prediction, and so on. Therefore, the study of ice segmentation has important practical significance. In this study, we focused on fine-grained river ice segmentation using unmanned aerial vehicle (UAV) images. This has the following difficulties: (1) The scale of river ice varies greatly in different images and even in the same image; (2) the same kind of river ice differs greatly in color, shape, texture, size, and so on; and (3) the appearances of different kinds of river ice sometimes appear similar due to the complex formation and change procedure. Therefore, to perform this study, the NWPU_YRCC2 dataset was built, in which all UAV images were collected in the Ningxia–Inner Mongolia reach of the Yellow River. Then, a novel semantic segmentation method based on deep convolution neural network, named ICENETv2, is proposed. To achieve multiscale accurate prediction, we design a multilevel features fusion framework, in which multi-scale high-level semantic features and lower-level finer features are effectively fused. Additionally, a dual attention module is adopted to highlight distinguishable characteristics, and a learnable up-sampling strategy is further used to improve the segmentation accuracy of the details. Experiments show that ICENETv2 achieves the state-of-the-art on the NWPU_YRCC2 dataset. Finally, our ICENETv2 is also applied to solve a realistic problem, calculating drift ice cover density, which is one of the most important factors to predict the freeze-up data of the river. The results demonstrate that the performance of ICENETv2 meets the actual application demand.},
DOI = {10.3390/rs13040633}
}



@Article{rs13040653,
AUTHOR = {Stojnić, Vladan and Risojević, Vladimir and Muštra, Mario and Jovanović, Vedran and Filipi, Janja and Kezić, Nikola and Babić, Zdenka},
TITLE = {A Method for Detection of Small Moving Objects in UAV Videos},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {653},
URL = {https://www.mdpi.com/2072-4292/13/4/653},
ISSN = {2072-4292},
ABSTRACT = {Detection of small moving objects is an important research area with applications including monitoring of flying insects, studying their foraging behavior, using insect pollinators to monitor flowering and pollination of crops, surveillance of honeybee colonies, and tracking movement of honeybees. However, due to the lack of distinctive shape and textural details on small objects, direct application of modern object detection methods based on convolutional neural networks (CNNs) shows considerably lower performance. In this paper we propose a method for the detection of small moving objects in videos recorded using unmanned aerial vehicles equipped with standard video cameras. The main steps of the proposed method are video stabilization, background estimation and subtraction, frame segmentation using a CNN, and thresholding the segmented frame. However, for training a CNN it is required that a large labeled dataset is available. Manual labelling of small moving objects in videos is very difficult and time consuming, and such labeled datasets do not exist at the moment. To circumvent this problem, we propose training a CNN using synthetic videos generated by adding small blob-like objects to video sequences with real-world backgrounds. The experimental results on detection of flying honeybees show that by using a combination of classical computer vision techniques and CNNs, as well as synthetic training sets, the proposed approach overcomes the problems associated with direct application of CNNs to the given problem and achieves an average F1-score of 0.86 in tests on real-world videos.},
DOI = {10.3390/rs13040653}
}



@Article{rs13040659,
AUTHOR = {Yuval, Matan and Alonso, Iñigo and Eyal, Gal and Tchernov, Dan and Loya, Yossi and Murillo, Ana C. and Treibitz, Tali},
TITLE = {Repeatable Semantic Reef-Mapping through Photogrammetry and Label-Augmentation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {659},
URL = {https://www.mdpi.com/2072-4292/13/4/659},
ISSN = {2072-4292},
ABSTRACT = {In an endeavor to study natural systems at multiple spatial and taxonomic resolutions, there is an urgent need for automated, high-throughput frameworks that can handle plethora of information. The coalescence of remote-sensing, computer-vision, and deep-learning elicits a new era in ecological research. However, in complex systems, such as marine-benthic habitats, key ecological processes still remain enigmatic due to the lack of cross-scale automated approaches (mms to kms) for community structure analysis. We address this gap by working towards scalable and comprehensive photogrammetric surveys, tackling the profound challenges of full semantic segmentation and 3D grid definition. Full semantic segmentation (where every pixel is classified) is extremely labour-intensive and difficult to achieve using manual labeling. We propose using label-augmentation, i.e., propagation of sparse manual labels, to accelerate the task of full segmentation of photomosaics. Photomosaics are synthetic images generated from a projected point-of-view of a 3D model. In the lack of navigation sensors (e.g., a diver-held camera), it is difficult to repeatably determine the slope-angle of a 3D map. We show this is especially important in complex topographical settings, prevalent in coral-reefs. Specifically, we evaluate our approach on benthic habitats, in three different environments in the challenging underwater domain. Our approach for label-augmentation shows human-level accuracy in full segmentation of photomosaics using labeling as sparse as 0.1%, evaluated on several ecological measures. Moreover, we found that grid definition using a leveler improves the consistency in community-metrics obtained due to occlusions and topology (angle and distance between objects), and that we were able to standardise the 3D transformation with two percent error in size measurements. By significantly easing the annotation process for full segmentation and standardizing the 3D grid definition we present a semantic mapping methodology enabling change-detection, which is practical, swift, and cost-effective. Our workflow enables repeatable surveys without permanent markers and specialized mapping gear, useful for research and monitoring, and our code is available online. Additionally, we release the Benthos data-set, fully manually labeled photomosaics from three oceanic environments with over 4500 segmented objects useful for research in computer-vision and marine ecology.},
DOI = {10.3390/rs13040659}
}



@Article{rs13040663,
AUTHOR = {Fan, Runze and Xu, Ting-Bing and Wei, Zhenzhong},
TITLE = {Estimating 6D Aircraft Pose from Keypoints and Structures},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {663},
URL = {https://www.mdpi.com/2072-4292/13/4/663},
ISSN = {2072-4292},
ABSTRACT = {This article addresses the challenge of 6D aircraft pose estimation from a single RGB image during the flight. Many recent works have shown that keypoints-based approaches, which first detect keypoints and then estimate the 6D pose, achieve remarkable performance. However, it is hard to locate the keypoints precisely in complex weather scenes. In this article, we propose a novel approach, called Pose Estimation with Keypoints and Structures (PEKS), which leverages multiple intermediate representations to estimate the 6D pose. Unlike previous works, our approach simultaneously locates keypoints and structures to recover the pose parameter of aircraft through a Perspective-n-Point Structure (PnPS) algorithm. These representations integrate the local geometric information of the object and the topological relationship between components of the target, which effectively improve the accuracy and robustness of 6D pose estimation. In addition, we contribute a dataset for aircraft pose estimation which consists of 3681 real images and 216,000 rendered images. Extensive experiments on our own aircraft pose dataset and multiple open-access pose datasets (e.g., ObjectNet3D, LineMOD) demonstrate that our proposed method can accurately estimate 6D aircraft pose in various complex weather scenes while achieving the comparative performance with the state-of-the-art pose estimation methods.},
DOI = {10.3390/rs13040663}
}



@Article{f12020216,
AUTHOR = {Luo, Mi and Wang, Yifu and Xie, Yunhong and Zhou, Lai and Qiao, Jingjing and Qiu, Siyu and Sun, Yujun},
TITLE = {Combination of Feature Selection and CatBoost for Prediction: The First Application to the Estimation of Aboveground Biomass},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {216},
URL = {https://www.mdpi.com/1999-4907/12/2/216},
ISSN = {1999-4907},
ABSTRACT = {Increasing numbers of explanatory variables tend to result in information redundancy and “dimensional disaster” in the quantitative remote sensing of forest aboveground biomass (AGB). Feature selection of model factors is an effective method for improving the accuracy of AGB estimates. Machine learning algorithms are also widely used in AGB estimation, although little research has addressed the use of the categorical boosting algorithm (CatBoost) for AGB estimation. Both feature selection and regression for AGB estimation models are typically performed with the same machine learning algorithm, but there is no evidence to suggest that this is the best method. Therefore, the present study focuses on evaluating the performance of the CatBoost algorithm for AGB estimation and comparing the performance of different combinations of feature selection methods and machine learning algorithms. AGB estimation models of four forest types were developed based on Landsat OLI data using three feature selection methods (recursive feature elimination (RFE), variable selection using random forests (VSURF), and least absolute shrinkage and selection operator (LASSO)) and three machine learning algorithms (random forest regression (RFR), extreme gradient boosting (XGBoost), and categorical boosting (CatBoost)). Feature selection had a significant influence on AGB estimation. RFE preserved the most informative features for AGB estimation and was superior to VSURF and LASSO. In addition, CatBoost improved the accuracy of the AGB estimation models compared with RFR and XGBoost. AGB estimation models using RFE for feature selection and CatBoost as the regression algorithm achieved the highest accuracy, with root mean square errors (RMSEs) of 26.54 Mg/ha for coniferous forest, 24.67 Mg/ha for broad-leaved forest, 22.62 Mg/ha for mixed forests, and 25.77 Mg/ha for all forests. The combination of RFE and CatBoost had better performance than the VSURF–RFR combination in which random forests were used for both feature selection and regression, indicating that feature selection and regression performed by a single machine learning algorithm may not always ensure optimal AGB estimation. It is promising to extending the application of new machine learning algorithms and feature selection methods to improve the accuracy of AGB estimates.},
DOI = {10.3390/f12020216}
}



@Article{ai2010006,
AUTHOR = {Whitmire, Christopher D. and Vance, Jonathan M. and Rasheed, Hend K. and Missaoui, Ali and Rasheed, Khaled M. and Maier, Frederick W.},
TITLE = {Using Machine Learning and Feature Selection for Alfalfa Yield Prediction},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {1},
PAGES = {71--88},
URL = {https://www.mdpi.com/2673-2688/2/1/6},
ISSN = {2673-2688},
ABSTRACT = {Predicting alfalfa biomass and crop yield for livestock feed is important to the daily lives of virtually everyone, and many features of data from this domain combined with corresponding weather data can be used to train machine learning models for yield prediction. In this work, we used yield data of different alfalfa varieties from multiple years in Kentucky and Georgia, and we compared the impact of different feature selection methods on machine learning (ML) models trained to predict alfalfa yield. Linear regression, regression trees, support vector machines, neural networks, Bayesian regression, and nearest neighbors were all developed with cross validation. The features used included weather data, historical yield data, and the sown date. The feature selection methods that were compared included a correlation-based method, the ReliefF method, and a wrapper method. We found that the best method was the correlation-based method, and the feature set it found consisted of the Julian day of the harvest, the number of days between the sown and harvest dates, cumulative solar radiation since the previous harvest, and cumulative rainfall since the previous harvest. Using these features, the k-nearest neighbor and random forest methods achieved an average R value over 0.95, and average mean absolute error less than 200 lbs./acre. Our top R2 of 0.90 beats a previous work’s best R2 of 0.87. Our primary contribution is the demonstration that ML, with feature selection, shows promise in predicting crop yields even on simple datasets with a handful of features, and that reporting accuracies in R and R2 offers an intuitive way to compare results among various crops.},
DOI = {10.3390/ai2010006}
}



@Article{rs13040705,
AUTHOR = {Kopačková-Strnadová, Veronika and Koucká, Lucie and Jelének, Jan and Lhotáková, Zuzana and Oulehle, Filip},
TITLE = {Canopy Top, Height and Photosynthetic Pigment Estimation Using Parrot Sequoia Multispectral Imagery and the Unmanned Aerial Vehicle (UAV)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {705},
URL = {https://www.mdpi.com/2072-4292/13/4/705},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing is one of the modern methods that have significantly developed over the last two decades and, nowadays, it provides a new means for forest monitoring. High spatial and temporal resolutions are demanded for the accurate and timely monitoring of forests. In this study, multi-spectral Unmanned Aerial Vehicle (UAV) images were used to estimate canopy parameters (definition of crown extent, top, and height, as well as photosynthetic pigment contents). The UAV images in Green, Red, Red-Edge, and Near infrared (NIR) bands were acquired by Parrot Sequoia camera over selected sites in two small catchments (Czech Republic) covered dominantly by Norway spruce monocultures. Individual tree extents, together with tree tops and heights, were derived from the Canopy Height Model (CHM). In addition, the following were tested: (i) to what extent can the linear relationship be established between selected vegetation indexes (Normalized Difference Vegetation Index (NDVI) and NDVIred edge) derived for individual trees and the corresponding ground truth (e.g., biochemically assessed needle photosynthetic pigment contents) and (ii) whether needle age selection as a ground truth and crown light conditions affect the validity of linear models. The results of the conducted statistical analysis show that the two vegetation indexes (NDVI and NDVIred edge) tested here have the potential to assess photosynthetic pigments in Norway spruce forests at a semi-quantitative level; however, the needle-age selection as a ground truth was revealed to be a very important factor. The only usable results were obtained for linear models when using the second year needle pigment contents as a ground truth. On the other hand, the illumination conditions of the crown proved to have very little effect on the model’s validity. No study was found to directly compare these results conducted on coniferous forest stands. This shows that there is a further need for studies dealing with a quantitative estimation of the biochemical variables of nature coniferous forests when employing spectral data that were acquired by the UAV platform at a very high spatial resolution.},
DOI = {10.3390/rs13040705}
}



@Article{s21041386,
AUTHOR = {Liu, Feng and Dai, Shuling and Zhao, Yongjia},
TITLE = {Learning to Have a Civil Aircraft Take Off under Crosswind Conditions by Reinforcement Learning with Multimodal Data and Preprocessing Data},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1386},
URL = {https://www.mdpi.com/1424-8220/21/4/1386},
PubMedID = {33669479},
ISSN = {1424-8220},
ABSTRACT = {Autopilot technology in the field of aviation has developed over many years. However, it is difficult for an autopilot system to autonomously operate a civil aircraft under bad weather conditions. In this paper, we present a reinforcement learning (RL) algorithm using multimodal data and preprocessing data to have a civil aircraft take off autonomously under crosswind conditions. The multimodal data include the common flight status and visual information. The preprocessing is a new design that maps some flight data by nonlinear functions based on the general flight dynamics before these data are fed into the RL model. Extensive experiments under different crosswind conditions with a professional flight simulator demonstrate that the proposed method can effectively control a civil aircraft to take off under various crosswind conditions and achieve better performance than trials without visual information or preprocessing data.},
DOI = {10.3390/s21041386}
}



@Article{rs13040732,
AUTHOR = {Nomura, Ryota and Oki, Kazuo},
TITLE = {Downscaling of MODIS NDVI by Using a Convolutional Neural Network-Based Model with Higher Resolution SAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {732},
URL = {https://www.mdpi.com/2072-4292/13/4/732},
ISSN = {2072-4292},
ABSTRACT = {The normalized difference vegetation index (NDVI) is a simple but powerful indicator, that can be used to observe green live vegetation efficiently. Since its introduction in the 1970s, NDVI has been used widely for land management, food security, and physical models. For these applications, acquiring NDVI in both high spatial resolution and high temporal resolution is preferable. However, there is generally a trade-off between temporal and spatial resolution when using satellite images. To relieve this problem, a convolutional neural network (CNN) based downscaling model was proposed in this research. This model is capable of estimating 10-m high resolution NDVI from MODIS (Moderate Resolution Imaging Spectroradiometer) 250-m resolution NDVI by using Sentinel-1 10-m resolution synthetic aperture radar (SAR) data. First, this downscaling model was trained to estimate Sentinel-2 10-m resolution NDVI from a combination of upscaled 250-m resolution Sentinel-2 NDVI and 10-m resolution Sentinel-1 SAR data, by using data acquired in 2019 in the target area. Then, the generality of this model was validated by applying it to test data acquired in 2020, with the result that the model predicted the NDVI with reasonable accuracy (MAE = 0.090, ρ = 0.734 on average). Next, 250-m NDVI from MODIS data was used as input to confirm this model under conditions replicating an actual application case. Although there were mismatch in the original MODIS and Sentinel-2 NDVI data, the model predicted NDVI with acceptable accuracy (MAE = 0.108, ρ = 0.650 on average). Finally, this model was applied to predict high spatial resolution NDVI using MODIS and Sentinel-1 data acquired in target area from 1 January 2020~31 December 2020. In this experiment, double cropping of cabbage, which was not observable at the original MODIS resolution, was observed by enhanced temporal resolution of high spatial resolution NDVI images (approximately ×2.5). The proposed method enables the production of 10-m resolution NDVI data with acceptable accuracy when cloudless MODIS NDVI and Sentinel-1 SAR data is available, and can enhance the temporal resolution of high resolution 10-m NDVI data.},
DOI = {10.3390/rs13040732}
}



@Article{rs13040733,
AUTHOR = {Gao, Bowen and Chen, Ninghua and Blaschke, Thomas and Wu, Chase Q. and Chen, Jianyu and Xu, Yaochen and Yang, Xiaoping and Du, Zhenhong},
TITLE = {Automated Characterization of Yardangs Using Deep Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {733},
URL = {https://www.mdpi.com/2072-4292/13/4/733},
ISSN = {2072-4292},
ABSTRACT = {The morphological characteristics of yardangs are the direct evidence that reveals the wind and fluvial erosion for lacustrine sediments in arid areas. These features can be critical indicators in reconstructing local wind directions and environment conditions. Thus, the fast and accurate extraction of yardangs is key to studying their regional distribution and evolution process. However, the existing automated methods to characterize yardangs are of limited generalization that may only be feasible for specific types of yardangs in certain areas. Deep learning methods, which are superior in representation learning, provide potential solutions for mapping yardangs with complex and variable features. In this study, we apply Mask region-based convolutional neural networks (Mask R-CNN) to automatically delineate and classify yardangs using very high spatial resolution images from Google Earth. The yardang field in the Qaidam Basin, northwestern China is selected to conduct the experiments and the method yields mean average precisions of 0.869 and 0.671 for intersection of union (IoU) thresholds of 0.5 and 0.75, respectively. The manual validation results on images of additional study sites show an overall detection accuracy of 74%, while more than 90% of the detected yardangs can be correctly classified and delineated. We then conclude that Mask R-CNN is a robust model to characterize multi-scale yardangs of various types and allows for the research of the morphological and evolutionary aspects of aeolian landform.},
DOI = {10.3390/rs13040733}
}



@Article{jsan10010015,
AUTHOR = {Blekos, Kostas and Tsakas, Anastasios and Xouris, Christos and Evdokidis, Ioannis and Alexandropoulos, Dimitris and Alexakos, Christos and Katakis, Sofoklis and Makedonas, Andreas and Theoharatos, Christos and Lalos, Aris},
TITLE = {Analysis, Modeling and Multi-Spectral Sensing for the Predictive Management of Verticillium Wilt in Olive Groves},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2224-2708/10/1/15},
ISSN = {2224-2708},
ABSTRACT = {The intensification and expansion in the cultivation of olives have contributed to the significant spread of Verticillium wilt, which is the most important fungal problem affecting olive trees. Recent studies confirm that practices such as the use of innovative natural minerals (Zeoshell ZF1) and the application of beneficial microorganisms (Micosat F BS WP) restore health in infected trees. However, for their efficient implementation the above methodologies require the marking of trees in the early stages of infestation—a task that is impractical with traditional means (manual labor) but also very difficult, as early stages are difficult to perceive with the naked eye. In this paper, we present the results of the My Olive Grove Coach (MyOGC) project, which used multispectral imaging from unmanned aerial vehicles to develop an olive grove monitoring system based on the autonomous and automatic processing of the multispectral images using computer vision and machine learning techniques. The goal of the system is to monitor and assess the health of olive groves, help in the prediction of Verticillium wilt spread and implement a decision support system that guides the farmer/agronomist.},
DOI = {10.3390/jsan10010015}
}



@Article{mi12020214,
AUTHOR = {Han, Shipeng and Meng, Zhen and Zhang, Xingcheng and Yan, Yuepeng},
TITLE = {Hybrid Deep Recurrent Neural Networks for Noise Reduction of MEMS-IMU with Static and Dynamic Conditions},
JOURNAL = {Micromachines},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {214},
URL = {https://www.mdpi.com/2072-666X/12/2/214},
PubMedID = {33672478},
ISSN = {2072-666X},
ABSTRACT = {Micro-electro-mechanical system inertial measurement unit (MEMS-IMU), a core component in many navigation systems, directly determines the accuracy of inertial navigation system; however, MEMS-IMU system is often affected by various factors such as environmental noise, electronic noise, mechanical noise and manufacturing error. These can seriously affect the application of MEMS-IMU used in different fields. Focus has been on MEMS gyro since it is an essential and, yet, complex sensor in MEMS-IMU which is very sensitive to noises and errors from the random sources. In this study, recurrent neural networks are hybridized in four different ways for noise reduction and accuracy improvement in MEMS gyro. These are two-layer homogenous recurrent networks built on long short term memory (LSTM-LSTM) and gated recurrent unit (GRU-GRU), respectively; and another two-layer but heterogeneous deep networks built on long short term memory-gated recurrent unit (LSTM-GRU) and a gated recurrent unit-long short term memory (GRU-LSTM). Practical implementation with static and dynamic experiments was carried out for a custom MEMS-IMU to validate the proposed networks, and the results show that GRU-LSTM seems to be overfitting large amount data testing for three-dimensional axis gyro in the static test. However, for X-axis and Y-axis gyro, LSTM-GRU had the best noise reduction effect with over 90% improvement in the three axes. For Z-axis gyroscope, LSTM-GRU performed better than LSTM-LSTM and GRU-GRU in quantization noise and angular random walk, while LSTM-LSTM shows better improvement than both GRU-GRU and LSTM-GRU networks in terms of zero bias stability. In the dynamic experiments, the Hilbert spectrum carried out revealed that time-frequency energy of the LSTM-LSTM, GRU-GRU, and GRU-LSTM denoising are higher compared to LSTM-GRU in terms of the whole frequency domain. Similarly, Allan variance analysis also shows that LSTM-GRU has a better denoising effect than the other networks in the dynamic experiments. Overall, the experimental results demonstrate the effectiveness of deep learning algorithms in MEMS gyro noise reduction, among which LSTM-GRU network shows the best noise reduction effect and great potential for application in the MEMS gyroscope area.},
DOI = {10.3390/mi12020214}
}



@Article{app11041861,
AUTHOR = {Rong, Zihao and Wang, Shaofan and Kong, Dehui and Yin, Baocai},
TITLE = {A Cascaded Ensemble of Sparse-and-Dense Dictionaries for Vehicle Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1861},
URL = {https://www.mdpi.com/2076-3417/11/4/1861},
ISSN = {2076-3417},
ABSTRACT = {Vehicle detection as a special case of object detection has practical meaning but faces challenges, such as the difficulty of detecting vehicles of various orientations, the serious influence from occlusion, the clutter of background, etc. In addition, existing effective approaches, like deep-learning-based ones, demand a large amount of training time and data, which causes trouble for their application. In this work, we propose a dictionary-learning-based vehicle detection approach which explicitly addresses these problems. Specifically, an ensemble of sparse-and-dense dictionaries (ESDD) are learned through supervised low-rank decomposition; each pair of sparse-and-dense dictionaries (SDD) in the ensemble is trained to represent either a subcategory of vehicle (corresponding to certain orientation range or occlusion level) or a subcategory of background (corresponding to a cluster of background patterns) and only gives good reconstructions to samples of the corresponding subcategory, making the ESDD capable of classifying vehicles from background even though they exhibit various appearances. We further organize ESDD into a two-level cascade (CESDD) to perform coarse-to-fine two-stage classification for better performance and computation reduction. The CESDD is then coupled with a downstream AdaBoost process to generate robust classifications. The proposed CESDD model is used as a window classifier in a sliding-window scan process over image pyramids to produce multi-scale detections, and an adapted mean-shift-like non-maximum suppression process is adopted to remove duplicate detections. Our CESDD vehicle detection approach is evaluated on KITTI dataset and compared with other strong counterparts; the experimental results exhibit the effectiveness of CESDD-based classification and detection, and the training of CESDD only demands small amount of time and data.},
DOI = {10.3390/app11041861}
}



@Article{s21041492,
AUTHOR = {Li, Guoming and Huang, Yanbo and Chen, Zhiqian and Chesser, Gary D. and Purswell, Joseph L. and Linhoss, John and Zhao, Yang},
TITLE = {Practices and Applications of Convolutional Neural Network-Based Computer Vision Systems in Animal Farming: A Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1492},
URL = {https://www.mdpi.com/1424-8220/21/4/1492},
PubMedID = {33670030},
ISSN = {1424-8220},
ABSTRACT = {Convolutional neural network (CNN)-based computer vision systems have been increasingly applied in animal farming to improve animal management, but current knowledge, practices, limitations, and solutions of the applications remain to be expanded and explored. The objective of this study is to systematically review applications of CNN-based computer vision systems on animal farming in terms of the five deep learning computer vision tasks: image classification, object detection, semantic/instance segmentation, pose estimation, and tracking. Cattle, sheep/goats, pigs, and poultry were the major farm animal species of concern. In this research, preparations for system development, including camera settings, inclusion of variations for data recordings, choices of graphics processing units, image preprocessing, and data labeling were summarized. CNN architectures were reviewed based on the computer vision tasks in animal farming. Strategies of algorithm development included distribution of development data, data augmentation, hyperparameter tuning, and selection of evaluation metrics. Judgment of model performance and performance based on architectures were discussed. Besides practices in optimizing CNN-based computer vision systems, system applications were also organized based on year, country, animal species, and purposes. Finally, recommendations on future research were provided to develop and improve CNN-based computer vision systems for improved welfare, environment, engineering, genetics, and management of farm animals.},
DOI = {10.3390/s21041492}
}



@Article{ijtpp6010002,
AUTHOR = {Mohammadi Doulabi Fard, Seyed Jalal and Jafari, Soheil},
TITLE = {Fuzzy Controller Structures Investigation for Future Gas Turbine Aero-Engines},
JOURNAL = {International Journal of Turbomachinery, Propulsion and Power},
VOLUME = {6},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/2504-186X/6/1/2},
ISSN = {2504-186X},
ABSTRACT = {The Advisory Council for Aeronautics Research in Europe (ACARE) Flight Path 2050 focuses on ambitious and severe targets for the next generation of air travel systems (e.g., 75% reduction in CO2 emissions per passenger kilometer, a 90% reduction in NOx emissions, and 65% reduction in noise emission of flying aircraft relative to the capabilities of typical new aircraft in 2000). In order to meet these requirements, aircraft engines should work very close to their operating limits. Therefore, the importance of advanced control strategies to satisfy all engine control modes simultaneously while protecting them from malfunctions and physical damages is being more crucial these days. In the last three decades, fuzzy controllers (FCs) have been proposed as a high potential solution for performance improvement of the next generation of aircraft engines. Based on an analytic review, this paper divides the trend of FCs design into two main lines including pure FCs (PFC) and min–max FCs (MMFC). These two main architectures are then designed, implemented on hardware, and applied in a case study to analyze the advantages and disadvantages of each structure. The analysis of hardware-in-the-loop (HIL) simulation results shows that the pure FC structure would be a high potential candidate for maneuverability and response time indices improvement (e.g., military applications); while min–max FC architecture has a great potential for future civil aero-engines where the fuel consumption and steady-state responses are more important. The simulation results are also compared with those of industrial min–max controllers to confirm the feasibility and reliability of the fuzzy controllers for real-world application. The results of this paper propose a general roadmap for fuzzy controllers’ structure selection for new and next generation of aircraft engines.},
DOI = {10.3390/ijtpp6010002}
}



@Article{rs13040808,
AUTHOR = {Neupane, Bipul and Horanont, Teerayut and Aryal, Jagannath},
TITLE = {Deep Learning-Based Semantic Segmentation of Urban Features in Satellite Images: A Review and Meta-Analysis},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {808},
URL = {https://www.mdpi.com/2072-4292/13/4/808},
ISSN = {2072-4292},
ABSTRACT = {Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.},
DOI = {10.3390/rs13040808}
}



@Article{app11041950,
AUTHOR = {Qi, Haixia and Liang, Yu and Ding, Quanchen and Zou, Jun},
TITLE = {Automatic Identification of Peanut-Leaf Diseases Based on Stack Ensemble},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1950},
URL = {https://www.mdpi.com/2076-3417/11/4/1950},
ISSN = {2076-3417},
ABSTRACT = {Peanut is an important food crop, and diseases of its leaves can directly reduce its yield and quality. In order to solve the problem of automatic identification of peanut-leaf diseases, this paper uses a traditional machine-learning method to ensemble the output of a deep learning model to identify diseases of peanut leaves. The identification of peanut-leaf diseases included healthy leaves, rust disease on a single leaf, leaf-spot disease on a single leaf, scorch disease on a single leaf, and both rust disease and scorch disease on a single leaf. Three types of data-augmentation methods were used: image flipping, rotation, and scaling. In this experiment, the deep-learning model had a higher accuracy than the traditional machine-learning methods. Moreover, the deep-learning model achieved better performance when using data augmentation and a stacking ensemble. After ensemble by logistic regression, the accuracy of residual network with 50 layers (ResNet50) was as high as 97.59%, and the F1 score of dense convolutional network with 121 layers (DenseNet121) was as high as 90.50. The deep-learning model used in this experiment had the greatest improvement in F1 score after the logistic regression ensemble. Deep-learning networks with deeper network layers like ResNet50 and DenseNet121 performed better in this experiment. This study can provide a reference for the identification of peanut-leaf diseases.},
DOI = {10.3390/app11041950}
}



@Article{land10020223,
AUTHOR = {Binte Mostafiz, Rubaiya and Noguchi, Ryozo and Ahamed, Tofael},
TITLE = {Agricultural Land Suitability Assessment Using Satellite Remote Sensing-Derived Soil-Vegetation Indices},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {223},
URL = {https://www.mdpi.com/2073-445X/10/2/223},
ISSN = {2073-445X},
ABSTRACT = {Satellite remote sensing technologies have a high potential in applications for evaluating land conditions and can facilitate optimized planning for agricultural sectors. However, misinformed land selection decisions limit crop yields and increase production-related costs to farmers. Therefore, the purpose of this research was to develop a land suitability assessment system using satellite remote sensing-derived soil-vegetation indicators. A multicriteria decision analysis was conducted by integrating weighted linear combinations and fuzzy multicriteria analyses in a GIS platform for suitability assessment using the following eight criteria: elevation, slope, and LST vegetation indices (SAVI, ARVI, SARVI, MSAVI, and OSAVI). The relative priorities of the indicators were identified using a fuzzy expert system. Furthermore, the results of the land suitability assessment were evaluated by ground truthed yield data. In addition, a yield estimation method was developed using indices representing influential factors. The analysis utilizing equal weights showed that 43% of the land (1832 km2) was highly suitable, 41% of the land (1747 km2) was moderately suitable, and 10% of the land (426 km2) was marginally suitable for improved yield productions. Alternatively, expert knowledge was also considered, along with references, when using the fuzzy membership function; as a result, 48% of the land (2045 km2) was identified as being highly suitable; 39% of the land (2045 km2) was identified as being moderately suitable, and 7% of the land (298 km2) was identified as being marginally suitable. Additionally, 6% (256 km2) of the land was described as not suitable by both methods. Moreover, the yield estimation using SAVI (R2 = 77.3%), ARVI (R2 = 68.9%), SARVI (R2 = 71.1%), MSAVI (R2 = 74.5%) and OSAVI (R2 = 81.2%) showed a good predictive ability. Furthermore, the combined model using these five indices reported the highest accuracy (R2 = 0.839); this model was then applied to develop yield prediction maps for the corresponding years (2017–2020). This research suggests that satellite remote sensing methods in GIS platforms are an effective and convenient way for agricultural land-use planners and land policy makers to select suitable cultivable land areas with potential for increased agricultural production.},
DOI = {10.3390/land10020223}
}



@Article{rs13040814,
AUTHOR = {Megahed, Yasmine and Shaker, Ahmed and Yan, Wai Yeung},
TITLE = {Fusion of Airborne LiDAR Point Clouds and Aerial Images for Heterogeneous Land-Use Urban Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {814},
URL = {https://www.mdpi.com/2072-4292/13/4/814},
ISSN = {2072-4292},
ABSTRACT = {The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors.},
DOI = {10.3390/rs13040814}
}



@Article{s21051571,
AUTHOR = {Bonci, Andrea and Cen Cheng, Pangcheng  David and Indri, Marina and Nabissi, Giacomo and Sibona, Fiorella},
TITLE = {Human-Robot Perception in Industrial Environments: A Survey},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1571},
URL = {https://www.mdpi.com/1424-8220/21/5/1571},
PubMedID = {33668162},
ISSN = {1424-8220},
ABSTRACT = {Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.},
DOI = {10.3390/s21051571}
}



@Article{rs13050837,
AUTHOR = {Bajić, Milan and Bajić, Milan},
TITLE = {Modeling and Simulation of Very High Spatial Resolution UXOs and Landmines in a Hyperspectral Scene for UAV Survey},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {837},
URL = {https://www.mdpi.com/2072-4292/13/5/837},
ISSN = {2072-4292},
ABSTRACT = {This paper presents methods for the modeling and simulation of explosive target placement in terrain spectral images (i.e., real hyperspectral 90-channel VNIR data), considering unexploded ordnances, landmines, and improvised explosive devices. The models used for landmine detection operate at sub-pixel levels. The presented research uses very fine spatial resolutions, 0.945 × 0.945 mm for targets and 1.868 × 1.868 cm for the scene, where the number of target pixels ranges from 52 to 116. While previous research has used the mean spectral value of the target, it is omitted in this paper. The model considers the probability of detection and its confidence intervals, which are derived and used in the analysis of the considered explosive targets. The detection results are better when decreased target endmembers are used to match the scene resolution, rather than using endmembers at the full resolution of the target. Unmanned aerial vehicles, as carriers of snapshot hyperspectral cameras, enable flexible target resolution selection and good area coverage.},
DOI = {10.3390/rs13050837}
}



@Article{su13052461,
AUTHOR = {Imran and Iqbal, Naeem and Ahmad, Shabir and Kim, Do Hyeun},
TITLE = {Towards Mountain Fire Safety Using Fire Spread Predictive Analytics and Mountain Fire Containment in IoT Environment},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2461},
URL = {https://www.mdpi.com/2071-1050/13/5/2461},
ISSN = {2071-1050},
ABSTRACT = {Mountains are popular tourist destinations due to their climate, fresh atmosphere, breathtaking sceneries, and varied topography. However, they are at times exposed to accidents, such as fire caused due to natural hazards and human activities. Such unforeseen fire accidents have a social, economic, and environmental impact on mountain towns worldwide. Protecting mountains from such fire accidents is also very challenging in terms of the high cost of fire containment resources, tracking fire spread, and evacuating the people at risk. This paper aims to fill this gap and proposes a three-fold methodology for fire safety in the mountains. The first part of the methodology is an optimization model for effective fire containment resource utilization. The second part of the methodology is a novel ensemble model based on machine learning, the heuristic approach, and principal component regression for predictive analytics of fire spread data. The final part of the methodology consists of an Internet of Things-based task orchestration approach to notify fire safety information to safety authorities. The proposed three-fold fire safety approach provides in-time information to safety authorities for making on-time decisions to minimize the damage caused by mountain fire with minimum containment cost. The performance of optimization models is evaluated in terms of execution time and cost. The particle swarm optimization-based model performs better in terms of cost, whereas the bat algorithm performs better in terms of execution time. The prediction models’ performance is evaluated in terms of root mean square error, mean absolute error, and mean absolute percentage error. The proposed ensemble-based prediction model accuracy for fire spread and burned area prediction is higher than that of the state-of-the-art algorithms. It is evident from the results that the proposed fire safety mechanism is a step towards efficient mountain fire safety management.},
DOI = {10.3390/su13052461}
}



@Article{s21051617,
AUTHOR = {Safonova, Anastasiia and Guirado, Emilio and Maglinets, Yuriy and Alcaraz-Segura, Domingo and Tabik, Siham},
TITLE = {Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask R-CNN},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1617},
URL = {https://www.mdpi.com/1424-8220/21/5/1617},
PubMedID = {33668984},
ISSN = {1424-8220},
ABSTRACT = {Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.},
DOI = {10.3390/s21051617}
}



@Article{s21051631,
AUTHOR = {Martini, Bruno Guilherme and Helfer, Gilson Augusto and Barbosa, Jorge Luis Victória and Espinosa Modolo, Regina Célia and da Silva, Marcio Rosa and de Figueiredo, Rodrigo Marques and Mendes, André Sales and Silva, Luís Augusto and Leithardt, Valderi Reis Quietinho},
TITLE = {IndoorPlant: A Model for Intelligent Services in Indoor Agriculture Based on Context Histories},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1631},
URL = {https://www.mdpi.com/1424-8220/21/5/1631},
PubMedID = {33652603},
ISSN = {1424-8220},
ABSTRACT = {The application of ubiquitous computing has increased in recent years, especially due to the development of technologies such as mobile computing, more accurate sensors, and specific protocols for the Internet of Things (IoT). One of the trends in this area of research is the use of context awareness. In agriculture, the context involves the environment, for example, the conditions found inside a greenhouse. Recently, a series of studies have proposed the use of sensors to monitor production and/or the use of cameras to obtain information about cultivation, providing data, reminders, and alerts to farmers. This article proposes a computational model for indoor agriculture called IndoorPlant. The model uses the analysis of context histories to provide intelligent generic services, such as predicting productivity, indicating problems that cultivation may suffer, and giving suggestions for improvements in greenhouse parameters. IndoorPlant was tested in three scenarios of the daily life of farmers with hydroponic production data that were obtained during seven months of cultivation of radicchio, lettuce, and arugula. Finally, the article presents the results obtained through intelligent services that use context histories. The scenarios used services to recommend improvements in cultivation, profiles and, finally, prediction of the cultivation time of radicchio, lettuce, and arugula using the partial least squares (PLS) regression technique. The prediction results were relevant since the following values were obtained: 0.96 (R2, coefficient of determination), 1.06 (RMSEC, square root of the mean square error of calibration), and 1.94 (RMSECV, square root of the mean square error of cross validation) for radicchio; 0.95 (R2), 1.37 (RMSEC), and 3.31 (RMSECV) for lettuce; 0.93 (R2), 1.10 (RMSEC), and 1.89 (RMSECV) for arugula. Eight farmers with different functions on the farm filled out a survey based on the technology acceptance model (TAM). The results showed 92% acceptance regarding utility and 98% acceptance for ease of use.},
DOI = {10.3390/s21051631}
}



@Article{drones5010016,
AUTHOR = {Kwan, Chiman},
TITLE = {Safety Enhancement of UAVs from the Signal Processing’s Perspectives: A Bird’s Eye View},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {16},
URL = {https://www.mdpi.com/2504-446X/5/1/16},
ISSN = {2504-446X},
ABSTRACT = {Unmanned air vehicles (UAVs) or drones have gained popularity in recent years. However, the US Federal Aviation Administration (FAA) is still hesitant to open up the national air space (NAS) to UAVs due to safety concerns because UAVs have several orders of magnitude of more accidents than manned aircraft. To limit the scope in this paper, we focus on large, heavy, and expensive UAVs that can be used for cargo transfer and search and rescue operations, not small radio-controlled toy drones. We first present a general architecture for enhancing the safety of UAVs. We then illustrate how signal processing technologies can help enhance the safety of UAVs. In particular, we provide a bird’s eye view of the application of signal processing algorithms on condition-based maintenance, structural health monitoring, fault diagnostics, and fault mitigation, which all play critical roles in UAV safety. Some practical applications are used to illustrate the importance of the various algorithms.},
DOI = {10.3390/drones5010016}
}



@Article{app11052087,
AUTHOR = {Ahmad, Muhammad and Hussain, Zukhruf Liaqat and Shah, Syed Irtiza Ali and Shams, Taimur Ali},
TITLE = {Estimation of Stability Parameters for Wide Body Aircraft Using Computational Techniques},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2087},
URL = {https://www.mdpi.com/2076-3417/11/5/2087},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we present the procedure of estimating the aerodynamic coefficients for a commercial aviation aircraft from geometric parameters at low-cruise-flight conditions using US DATCOM (United States Data Compendium) and XFLR software. The purpose of this research was to compare the stability parameters from both pieces of software to determine the efficacy of software solution for a wide-body aircraft at the stated flight conditions. During the initial phase of this project, the geometric parameters were acquired from established literature. In the next phase, stability and control coefficients of the aircraft were estimated using both pieces of software in parallel. Results obtained from both pieces of software were compared for any differences and the both pieces of software were validated with analytical correlations as presented in literature. The plots of various parameters with variations of the angle of attack or control surface deflection have also been obtained and presented. The differences between the software solutions and the analytical results can be associated with approximations of techniques used in software (the vortex lattice method is the background theory used in both DATCOM and XFLR). Additionally, from the results, it can be concluded that XFLR is more reliable than DATCOM for longitudinal, directional, and lateral stability/control coefficients. Analyses of a Boeing 747-200 (a wide-body commercial airliner) in DATCOM and XFLR for complete stability/control analysis including all modes in the longitudinal and lateral directions have been presented. DATCOM already has a sample analysis of a previous version of the Boeing 737; however, the Boeing 747-200 is much larger than the former, and complete analysis was, therefore, felt necessary to study its aerodynamics characteristics. Furthermore, in this research, it was concluded that XFLR is more reliable for various categories of aircraft alike in terms of general stability and control coefficients, and hence many aircraft can be dependably modeled and analyzed in this software.},
DOI = {10.3390/app11052087}
}



@Article{app11052105,
AUTHOR = {Papić, Vladan and Šolić, Petar and Milan, Ante and Gotovac, Sven and Polić, Miljenko},
TITLE = {High-Resolution Image Transmission from UAV to Ground Station for Search and Rescue Missions Planning},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2105},
URL = {https://www.mdpi.com/2076-3417/11/5/2105},
ISSN = {2076-3417},
ABSTRACT = {Search and rescue (SAR) missions comprise search for, and provision of aid to people who are in distress or imminent danger. Providing the best possible input for the planners and search teams, up-to-date information about the terrain is of essential importance because every additional hour needed to search a person decreases probability of success. Therefore, availability of aerial images and updated terrain maps as a basis for planning and monitoring SAR missions in real-time is very important for rescuers. In this paper, we present a system for transmission of high-resolution images from an unmanned aerial vehicle (UAV) to the ground station (GS). We define and calculate data rate and transmission distance requirements between the UAV and GS in a mission scenario. Five tests were designed and carried out to confirm the viability of the proposed system architecture and modules. Test results present throughput measurements for various UAV and GS distances, antenna heights and UAV antenna yaw angles. Experimental results from the series of conducted outdoor tests show that the proposed solution using two pMDDL2450 datalinks at 2.4 GHz and a directional antenna on the receiving side can be used for a real-time transmission of high-resolution images acquired with a camera on a UAV. Achieved throughput at a UAV-GS distance of 5 km was 1.4 MB/s (11.2 Mbps). The limitations and possible improvements of the proposed system as well as future work are also discussed.},
DOI = {10.3390/app11052105}
}



@Article{rs13050898,
AUTHOR = {Sadeghi-Tehran, Pouria and Virlet, Nicolas and Hawkesford, Malcolm J.},
TITLE = {A Neural Network Method for Classification of Sunlit and Shaded Components of Wheat Canopies in the Field Using High-Resolution Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {898},
URL = {https://www.mdpi.com/2072-4292/13/5/898},
ISSN = {2072-4292},
ABSTRACT = {(1) Background: Information rich hyperspectral sensing, together with robust image analysis, is providing new research pathways in plant phenotyping. This combination facilitates the acquisition of spectral signatures of individual plant organs as well as providing detailed information about the physiological status of plants. Despite the advances in hyperspectral technology in field-based plant phenotyping, little is known about the characteristic spectral signatures of shaded and sunlit components in wheat canopies. Non-imaging hyperspectral sensors cannot provide spatial information; thus, they are not able to distinguish the spectral reflectance differences between canopy components. On the other hand, the rapid development of high-resolution imaging spectroscopy sensors opens new opportunities to investigate the reflectance spectra of individual plant organs which lead to the understanding of canopy biophysical and chemical characteristics. (2) Method: This study reports the development of a computer vision pipeline to analyze ground-acquired imaging spectrometry with high spatial and spectral resolutions for plant phenotyping. The work focuses on the critical steps in the image analysis pipeline from pre-processing to the classification of hyperspectral images. In this paper, two convolutional neural networks (CNN) are employed to automatically map wheat canopy components in shaded and sunlit regions and to determine their specific spectral signatures. The first method uses pixel vectors of the full spectral features as inputs to the CNN model and the second method integrates the dimension reduction technique known as linear discriminate analysis (LDA) along with the CNN to increase the feature discrimination and improves computational efficiency. (3) Results: The proposed technique alleviates the limitations and lack of separability inherent in existing pre-defined hyperspectral classification methods. It optimizes the use of hyperspectral imaging and ensures that the data provide information about the spectral characteristics of the targeted plant organs, rather than the background. We demonstrated that high-resolution hyperspectral imagery along with the proposed CNN model can be powerful tools for characterizing sunlit and shaded components of wheat canopies in the field. The presented method will provide significant advances in the determination and relevance of spectral properties of shaded and sunlit canopy components under natural light conditions.},
DOI = {10.3390/rs13050898}
}



@Article{s21051682,
AUTHOR = {Tai, Kuan-Chen and Tang, Chih-Wei},
TITLE = {Siamese Networks-Based People Tracking Using Template Update for 360-Degree Videos Using EAC Format},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1682},
URL = {https://www.mdpi.com/1424-8220/21/5/1682},
PubMedID = {33804396},
ISSN = {1424-8220},
ABSTRACT = {Rich information is provided by 360-degree videos. However, non-uniform geometric deformation caused by sphere-to-plane projection significantly decreases tracking accuracy of existing trackers, and the huge amount of data makes it difficult to achieve real-time tracking. Thus, this paper proposes a Siamese networks-based people tracker using template update for 360-degree equi-angular cubemap (EAC) format videos. Face stitching overcomes the problem of content discontinuity of the EAC format and avoids raising new geometric deformation in stitched images. Fully convolutional Siamese networks enable tracking at high speed. Mostly important, to be robust against combination of non-uniform geometric deformation of the EAC format and partial occlusions caused by zero padding in stitched images, this paper proposes a novel Bayes classifier-based timing detector of template update by referring to the linear discriminant feature and statistics of a score map generated by Siamese networks. Experimental results show that the proposed scheme significantly improves tracking accuracy of the fully convolutional Siamese networks SiamFC on the EAC format with operation beyond the frame acquisition rate. Moreover, the proposed score map-based timing detector of template update outperforms state-of-the-art score map-based timing detectors.},
DOI = {10.3390/s21051682}
}



@Article{s21051688,
AUTHOR = {Ali, Luqman and Alnajjar, Fady and Jassmi, Hamad Al and Gocho, Munkhjargal and Khan, Wasif and Serhani, M. Adel},
TITLE = {Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1688},
URL = {https://www.mdpi.com/1424-8220/21/5/1688},
PubMedID = {33804490},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.},
DOI = {10.3390/s21051688}
}



@Article{app11052185,
AUTHOR = {Nakama, Justin and Parada, Ricky and Matos-Carvalho, João P. and Azevedo, Fábio and Pedro, Dário and Campos, Luís},
TITLE = {Autonomous Environment Generator for UAV-Based Simulation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2185},
URL = {https://www.mdpi.com/2076-3417/11/5/2185},
ISSN = {2076-3417},
ABSTRACT = {The increased demand for Unmanned Aerial Vehicles (UAV) has also led to higher demand for realistic and efficient UAV testing environments. The current use of simulated environments has been shown to be a relatively inexpensive, safe, and repeatable way to evaluate UAVs before real-world use. However, the use of generic environments and manually-created custom scenarios leaves more to be desired. In this paper, we propose a new testbed that utilizes machine learning algorithms to procedurally generate, scale, and place 3D models to create a realistic environment. These environments are additionally based on satellite images, thus providing users with a more robust example of real-world UAV deployment. Although certain graphical improvements could be made, this paper serves as a proof of concept for an novel autonomous and relatively-large scale environment generator. Such a testbed could allow for preliminary operational planning and testing worldwide, without the need for on-site evaluation or data collection in the future.},
DOI = {10.3390/app11052185}
}



@Article{rs13050937,
AUTHOR = {Najafi, Payam and Feizizadeh, Bakhtiar and Navid, Hossein},
TITLE = {A Comparative Approach of Fuzzy Object Based Image Analysis and Machine Learning Techniques Which Are Applied to Crop Residue Cover Mapping by Using Sentinel-2 Satellite and UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {937},
URL = {https://www.mdpi.com/2072-4292/13/5/937},
ISSN = {2072-4292},
ABSTRACT = {Conservation tillage methods through leaving the crop residue cover (CRC) on the soil surface protect it from water and wind erosions. Hence, the percentage of the CRC on the soil surface is very critical for the evaluation of tillage intensity. The objective of this study was to develop a new methodology based on the semiautomated fuzzy object based image analysis (fuzzy OBIA) and compare its efficiency with two machine learning algorithms which include: support vector machine (SVM) and artificial neural network (ANN) for the evaluation of the previous CRC and tillage intensity. We also considered the spectral images from two remotely sensed platforms of the unmanned aerial vehicle (UAV) and Sentinel-2 satellite, respectively. The results indicated that fuzzy OBIA for multispectral Sentinel-2 image based on Gaussian membership function with overall accuracy and Cohen’s kappa of 0.920 and 0.874, respectively, surpassed machine learning algorithms and represented the useful results for the classification of tillage intensity. The results also indicated that overall accuracy and Cohen’s kappa for the classification of RGB images from the UAV using fuzzy OBIA method were 0.860 and 0.779, respectively. The semiautomated fuzzy OBIA clearly outperformed machine learning approaches in estimating the CRC and the classification of the tillage methods and also it has the potential to substitute or complement field techniques.},
DOI = {10.3390/rs13050937}
}



@Article{rs13050939,
AUTHOR = {Xue, Yongan and Zhao, Jinling and Zhang, Mingmei},
TITLE = {A Watershed-Segmentation-Based Improved Algorithm for Extracting Cultivated Land Boundaries},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {939},
URL = {https://www.mdpi.com/2072-4292/13/5/939},
ISSN = {2072-4292},
ABSTRACT = {To accurately extract cultivated land boundaries based on high-resolution remote sensing imagery, an improved watershed segmentation algorithm was proposed herein based on a combination of pre- and post-improvement procedures. Image contrast enhancement was used as the pre-improvement, while the color distance of the Commission Internationale de l´Eclairage (CIE) color space, including the Lab and Luv, was used as the regional similarity measure for region merging as the post-improvement. Furthermore, the area relative error criterion (δA), the pixel quantity error criterion (δP), and the consistency criterion (Khat) were used for evaluating the image segmentation accuracy. The region merging in Red–Green–Blue (RGB) color space was selected to compare the proposed algorithm by extracting cultivated land boundaries. The validation experiments were performed using a subset of Chinese Gaofen-2 (GF-2) remote sensing image with a coverage area of 0.12 km2. The results showed the following: (1) The contrast-enhanced image exhibited an obvious gain in terms of improving the image segmentation effect and time efficiency using the improved algorithm. The time efficiency increased by 10.31%, 60.00%, and 40.28%, respectively, in the RGB, Lab, and Luv color spaces. (2) The optimal segmentation and merging scale parameters in the RGB, Lab, and Luv color spaces were C for minimum areas of 2000, 1900, and 2000, and D for a color difference of 1000, 40, and 40. (3) The algorithm improved the time efficiency of cultivated land boundary extraction in the Lab and Luv color spaces by 35.16% and 29.58%, respectively, compared to the RGB color space. The extraction accuracy was compared to the RGB color space using the δA, δP, and Khat, that were improved by 76.92%, 62.01%, and 16.83%, respectively, in the Lab color space, while they were 55.79%, 49.67%, and 13.42% in the Luv color space. (4) Through the visual comparison, time efficiency, and segmentation accuracy, the comprehensive extraction effect using the proposed algorithm was obviously better than that of RGB color-based space algorithm. The established accuracy evaluation indicators were also proven to be consistent with the visual evaluation. (5) The proposed method has a satisfying transferability by a wider test area with a coverage area of 1 km2. In addition, the proposed method, based on the image contrast enhancement, was to perform the region merging in the CIE color space according to the simulated immersion watershed segmentation results. It is a useful attempt for the watershed segmentation algorithm to extract cultivated land boundaries, which provides a reference for enhancing the watershed algorithm.},
DOI = {10.3390/rs13050939}
}



@Article{s21051766,
AUTHOR = {Müezzinoğlu, Taha and Karaköse, Mehmet},
TITLE = {An Intelligent Human–Unmanned Aerial Vehicle Interaction Approach in Real Time Based on Machine Learning Using Wearable Gloves},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1766},
URL = {https://www.mdpi.com/1424-8220/21/5/1766},
PubMedID = {33806388},
ISSN = {1424-8220},
ABSTRACT = {The interactions between humans and unmanned aerial vehicles (UAVs), whose applications are increasing in the civilian field rather than for military purposes, are a popular future research area. Human–UAV interactions are a challenging problem because UAVs move in a three-dimensional space. In this paper, we present an intelligent human–UAV interaction approach in real time based on machine learning using wearable gloves. The proposed approach offers scientific contributions such as a multi-mode command structure, machine-learning-based recognition, task scheduling algorithms, real-time usage, robust and effective use, and high accuracy rates. For this purpose, two wearable smart gloves working in real time were designed. The signal data obtained from the gloves were processed with machine-learning-based methods and classified multi-mode commands were included in the human–UAV interaction process via the interface according to the task scheduling algorithm to facilitate sequential and fast operation. The performance of the proposed approach was verified on a data set created using 25 different hand gestures from 20 different people. In a test using the proposed approach on 49,000 datapoints, process time performance of a few milliseconds was achieved with approximately 98 percent accuracy.},
DOI = {10.3390/s21051766}
}



@Article{rs13050965,
AUTHOR = {Kraft, Marek and Piechocki, Mateusz and Ptak, Bartosz and Walas, Krzysztof},
TITLE = {Autonomous, Onboard Vision-Based Trash and Litter Detection in Low Altitude Aerial Images Collected by an Unmanned Aerial Vehicle},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {965},
URL = {https://www.mdpi.com/2072-4292/13/5/965},
ISSN = {2072-4292},
ABSTRACT = {Public littering and discarded trash are, despite the effort being put to limit it, still a serious ecological, aesthetic, and social problem. The problematic waste is usually localised and picked up by designated personnel, which is a tiresome, time-consuming task. This paper proposes a low-cost solution enabling the localisation of trash and litter objects in low altitude imagery collected by an unmanned aerial vehicle (UAV) during an autonomous patrol mission. The objects of interest are detected in the acquired images and put on the global map using a set of onboard sensors commonly found in typical UAV autopilots. The core object detection algorithm is based on deep, convolutional neural networks. Since the task is domain-specific, a dedicated dataset of images containing objects of interest was collected and annotated. The dataset is made publicly available, and its description is contained in the paper. The dataset was used to test a range of embedded devices enabling the deployment of deep neural networks for inference onboard the UAV. The results of measurements in terms of detection accuracy and processing speed are enclosed, and recommendations for the neural network model and hardware platform are given based on the obtained values. The complete system can be put together using inexpensive, off-the-shelf components, and perform autonomous localisation of discarded trash, relieving human personnel of this burdensome task, and enabling automated pickup planning.},
DOI = {10.3390/rs13050965}
}



@Article{rs13050977,
AUTHOR = {Crusiol, Luís Guilherme Teixeira and Nanni, Marcos Rafael and Furlanetto, Renato Herrig and Sibaldelli, Rubson Natal Ribeiro and Cezar, Everson and Sun, Liang and Foloni, José Salvador Simonetto and Mertz-Henning, Liliane Marcia and Nepomuceno, Alexandre Lima and Neumaier, Norman and Farias, José Renato Bouças},
TITLE = {Yield Prediction in Soybean Crop Grown under Different Levels of Water Availability Using Reflectance Spectroscopy and Partial Least Squares Regression},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {977},
URL = {https://www.mdpi.com/2072-4292/13/5/977},
ISSN = {2072-4292},
ABSTRACT = {Soybean grain yield has regularly been impaired by drought periods, and the future climatic scenarios for soybean production might drastically impact yields worldwide. In this context, the knowledge of soybean yield is extremely important to subsidize government and corporative decisions over technical issues. This paper aimed to predict grain yield in soybean crop grown under different levels of water availability using reflectance spectroscopy and partial least square regression (PLSR). Field experiments were undertaken at Embrapa Soja (Brazilian Agricultural Research Corporation) in the 2016/2017, 2017/2018 and 2018/2019 cropping seasons. The data collected were analyzed following a split plot model in a randomized complete block design, with four blocks. The following water conditions were distributed in the field plots: irrigated (IRR), non-irrigated (NIRR) and water deficit induced at the vegetative (WDV) and reproductive stages (WDR) using rainout shelters. Soybean genotypes with different responses to water deficit were distributed in the subplots. Soil moisture and weather data were monitored daily. A total of 7216 leaf reflectance (from 400 to 2500 nm, measured by the FieldSpec 3 Jr spectroradiometer) was collected at 24 days in the three cropping seasons. The PLSR (p ≤ 0.05) was performed to predict soybean grain yield by its leaf-based reflectance spectroscopy. The results demonstrated the highest accuracy in soybean grain yield prediction at the R5 phenological stage, corresponding to the period when grains are being formed (R2 ranging from 0.731 to 0.924 and the RMSE from 334 to 403 kg ha−1—7.77 to 11.33%). Analyzing the three cropping seasons into a single PLSR model at R5 stage, R2 equal to 0.775, 0.730 and 0.688 were obtained at the calibration, cross-validation and external validation stages, with RMSE lower than 634 kg ha−1 (13.34%). The PLSR demonstrated higher accuracy in plants submitted to water deficit both at the vegetative and reproductive periods in comparison to plants under natural rainfall or irrigation.},
DOI = {10.3390/rs13050977}
}



@Article{rs13050979,
AUTHOR = {Fernández-García, Víctor and Marcos, Elena and Fernández-Guisuraga, José Manuel and Fernández-Manso, Alfonso and Quintano, Carmen and Suárez-Seoane, Susana and Calvo, Leonor},
TITLE = {Multiple Endmember Spectral Mixture Analysis (MESMA) Applied to the Study of Habitat Diversity in the Fine-Grained Landscapes of the Cantabrian Mountains},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {979},
URL = {https://www.mdpi.com/2072-4292/13/5/979},
ISSN = {2072-4292},
ABSTRACT = {Heterogeneous and patchy landscapes where vegetation and abiotic factors vary at small spatial scale (fine-grained landscapes) represent a challenge for habitat diversity mapping using remote sensing imagery. In this context, techniques of spectral mixture analysis may have an advantage over traditional methods of land cover classification because they allow to decompose the spectral signature of a mixed pixel into several endmembers and their respective abundances. In this work, we present the application of Multiple Endmember Spectral Mixture Analysis (MESMA) to quantify habitat diversity and assess the compositional turnover at different spatial scales in the fine-grained landscapes of the Cantabrian Mountains (northwestern Iberian Peninsula). A Landsat-8 OLI scene and high-resolution orthophotographs (25 cm) were used to build a region-specific spectral library of the main types of habitats in this region (arboreal vegetation; shrubby vegetation; herbaceous vegetation; rocks–soil and water bodies). We optimized the spectral library with the Iterative Endmember Selection (IES) method and we applied MESMA to unmix the Landsat scene into five fraction images representing the five defined habitats (root mean square error, RMSE ≤ 0.025 in 99.45% of the pixels). The fraction images were validated by linear regressions using 250 reference plots from the orthophotographs and then used to calculate habitat diversity at the pixel (α-diversity: 30 × 30 m), landscape (γ-diversity: 1 × 1 km) and regional (ε-diversity: 110 × 33 km) scales and the compositional turnover (β- and δ-diversity) according to Simpson’s diversity index. Richness and evenness were also computed. Results showed that fraction images were highly related to reference data (R2 ≥ 0.73 and RMSE ≤ 0.18). In general, our findings indicated that habitat diversity was highly dependent on the spatial scale, with values for the Simpson index ranging from 0.20 ± 0.22 for α-diversity to 0.60 ± 0.09 for γ-diversity and 0.72 ± 0.11 for ε-diversity. Accordingly, we found β-diversity to be higher than δ-diversity. This work contributes to advance in the estimation of ecological diversity in complex landscapes, showing the potential of MESMA to quantify habitat diversity in a comprehensive way using Landsat imagery.},
DOI = {10.3390/rs13050979}
}



@Article{f12030297,
AUTHOR = {Gray, Ross E. J. and Ewers, Robert M.},
TITLE = {Monitoring Forest Phenology in a Changing World},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {297},
URL = {https://www.mdpi.com/1999-4907/12/3/297},
ISSN = {1999-4907},
ABSTRACT = {Plant phenology is strongly interlinked with ecosystem processes and biodiversity. Like many other aspects of ecosystem functioning, it is affected by habitat and climate change, with both global change drivers altering the timings and frequency of phenological events. As such, there has been an increased focus in recent years to monitor phenology in different biomes. A range of approaches for monitoring phenology have been developed to increase our understanding on its role in ecosystems, ranging from the use of satellites and drones to collection traps, each with their own merits and limitations. Here, we outline the trade-offs between methods (spatial resolution, temporal resolution, cost, data processing), and discuss how their use can be optimised in different environments and for different goals. We also emphasise emerging technologies that will be the focus of monitoring in the years to follow and the challenges of monitoring phenology that still need to be addressed. We conclude that there is a need to integrate studies that incorporate multiple monitoring methods, allowing the strengths of one to compensate for the weaknesses of another, with a view to developing robust methods for upscaling phenological observations from point locations to biome and global scales and reconciling data from varied sources and environments. Such developments are needed if we are to accurately quantify the impacts of a changing world on plant phenology.},
DOI = {10.3390/f12030297}
}



@Article{s21051809,
AUTHOR = {Malhotra, Parushi and Singh, Yashwant and Anand, Pooja and Bangotra, Deep Kumar and Singh, Pradeep Kumar and Hong, Wei-Chiang},
TITLE = {Internet of Things: Evolution, Concerns and Security Challenges},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1809},
URL = {https://www.mdpi.com/1424-8220/21/5/1809},
PubMedID = {33807724},
ISSN = {1424-8220},
ABSTRACT = {The escalated growth of the Internet of Things (IoT) has started to reform and reshape our lives. The deployment of a large number of objects adhered to the internet has unlocked the vision of the smart world around us, thereby paving a road towards automation and humongous data generation and collection. This automation and continuous explosion of personal and professional information to the digital world provides a potent ground to the adversaries to perform numerous cyber-attacks, thus making security in IoT a sizeable concern. Hence, timely detection and prevention of such threats are pre-requisites to prevent serious consequences. The survey conducted provides a brief insight into the technology with prime attention towards the various attacks and anomalies and their detection based on the intelligent intrusion detection system (IDS). The comprehensive look-over presented in this paper provides an in-depth analysis and assessment of diverse machine learning and deep learning-based network intrusion detection system (NIDS). Additionally, a case study of healthcare in IoT is presented. The study depicts the architecture, security, and privacy issues and application of learning paradigms in this sector. The research assessment is finally concluded by listing the results derived from the literature. Additionally, the paper discusses numerous research challenges to allow further rectifications in the approaches to deal with unusual complications.},
DOI = {10.3390/s21051809}
}



@Article{s21051812,
AUTHOR = {Uzair, Muhammad and Brinkworth, Russell S. A. and Finn, Anthony},
TITLE = {Detecting Small Size and Minimal Thermal Signature Targets in Infrared Imagery Using Biologically Inspired Vision},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1812},
URL = {https://www.mdpi.com/1424-8220/21/5/1812},
PubMedID = {33807741},
ISSN = {1424-8220},
ABSTRACT = {Thermal infrared imaging provides an effective sensing modality for detecting small moving objects at long range. Typical challenges that limit the efficiency and robustness of the detection performance include sensor noise, minimal target contrast and cluttered backgrounds. These issues become more challenging when the targets are of small physical size and present minimal thermal signatures. In this paper, we experimentally show that a four-stage biologically inspired vision (BIV) model of the flying insect visual system have an excellent ability to overcome these challenges simultaneously. The early two stages of the model suppress spatio-temporal clutter and enhance spatial target contrast while compressing the signal in a computationally manageable bandwidth. The later two stages provide target motion enhancement and sub-pixel motion detection capabilities. To show the superiority of the BIV target detector over existing traditional detection methods, we perform extensive experiments and performance comparisons using high bit-depth, real-world infrared image sequences of small size and minimal thermal signature targets at long ranges. Our results show that the BIV target detector significantly outperformed 10 conventional spatial-only and spatiotemporal methods for infrared small target detection. The BIV target detector resulted in over 25 dB improvement in the median signal-to-clutter-ratio over the raw input and achieved 43% better detection rate than the best performing existing method.},
DOI = {10.3390/s21051812}
}



@Article{jmse9030288,
AUTHOR = {Mannino, Anna Maria and Borfecchia, Flavio and Micheli, Carla},
TITLE = {Tracking Marine Alien Macroalgae in the Mediterranean Sea: The Contribution of Citizen Science and Remote Sensing},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {9},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {288},
URL = {https://www.mdpi.com/2077-1312/9/3/288},
ISSN = {2077-1312},
ABSTRACT = {The accelerating rate of the introduction of non-indigenous species (NIS) and the magnitude of shipping traffic make the Mediterranean Sea a hotspot of biological invasions. For the effective management of NIS, early detection and intensive monitoring over time and space are essential. Here, we present an overview of possible applications of citizen science and remote sensing in monitoring alien seaweeds in the Mediterranean Sea. Citizen science activities, involving the public (e.g., tourists, fishermen, divers) in the collection of data, have great potential for monitoring NIS. The innovative methodologies, based on remote sensing techniques coupled with in situ/laboratory advanced sampling/analysis methods for tracking such species, may be useful and effective tools for easily assessing NIS distribution patterns and monitoring the space/time changes in habitats in order to support the sustainable management of the ecosystems. The reported case studies highlight how these cost-effective systems can be useful complementary tools for monitoring NIS, especially in marine protected areas, which, despite their fundamental role in the conservation of marine biodiversity, are not immune to the introduction of NIS. To ensure effective and long-lasting management strategies, collaborations between researchers, policy makers and citizens are essential.},
DOI = {10.3390/jmse9030288}
}



@Article{f12030308,
AUTHOR = {Ko, Chiung and Lee, Seunghyun and Yim, Jongsu and Kim, Donggeun and Kang, Jintaek},
TITLE = {Comparison of Forest Inventory Methods at Plot-Level between a Backpack Personal Laser Scanning (BPLS) and Conventional Equipment in Jeju Island, South Korea},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {308},
URL = {https://www.mdpi.com/1999-4907/12/3/308},
ISSN = {1999-4907},
ABSTRACT = {In recent years, light detection and ranging (LiDAR) has been increasingly utilized to estimate forest resources. This study was conducted to identify the applicability of a LiDAR sensor for such estimations by comparing data on a tree’s position, height, and diameter at breast height (DBH) obtained using the sensor with those by existing forest inventory methods for a Cryptomeria japonica forest in Jeju Island, South Korea. For this purpose, a backpack personal laser scanning device (BPLS, Greenvalley International, Model D50) was employed in a protected forest, where cutting is not allowed, as a non-invasive means, simultaneously assessing the device’s field applicability. The data collected by the sensor were divided into seven different pathway variations, or “patterns” to consider the density of the sample plots and enhance the efficiency. The accuracy of estimating the variables of each tree was then assessed. The time spent acquiring and processing real-time data was also analyzed for each method, as well as total time and the time required for each measurement. The findings showed that the rate of detection of standing trees by LiDAR was 100%. Additionally, a high statistical accuracy was observed in pattern 5 (DBH: RMSE 1.22 cm, bias—0.90 cm, Height: RMSE 1.66 m, bias—1.18 m) and pattern 7 (DBH: RMSE 1.22 cm, bias—0.92 cm, Height: RMSE 1.48 m, bias—1.23 m) compared to the results from the typical inventory method. A range of 115–162.5 min/ha was required to process the data using the LiDAR, while 322.5–567.5 min was required for the typical inventory method. Thus, the application of a backpack personal LiDAR can lead to higher efficiency when conducting a forest resource inventory in a coniferous plantation with understory vegetation. Further research in various stands is necessary to confirm the efficiency of using backpack personal laser scanning.},
DOI = {10.3390/f12030308}
}



@Article{ijgi10030144,
AUTHOR = {Gebrehiwot, Asmamaw A and Hashemi-Beni, Leila},
TITLE = {Three-Dimensional Inundation Mapping Using UAV Image Segmentation and Digital Surface Model},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {144},
URL = {https://www.mdpi.com/2220-9964/10/3/144},
ISSN = {2220-9964},
ABSTRACT = {Flood occurrence is increasing due to the expansion of urbanization and extreme weather like hurricanes; hence, research on methods of inundation monitoring and mapping has increased to reduce the severe impacts of flood disasters. This research studies and compares two methods for inundation depth estimation using UAV images and topographic data. The methods consist of three main stages: (1) extracting flooded areas and create 2D inundation polygons using deep learning; (2) reconstructing 3D water surface using the polygons and topographic data; and (3) deriving a water depth map using the 3D reconstructed water surface and a pre-flood DEM. The two methods are different at reconstructing the 3D water surface (stage 2). The first method uses structure from motion (SfM) for creating a point cloud of the area from overlapping UAV images, and the water polygons resulted from stage 1 is applied for water point cloud classification. While the second method reconstructs the water surface by intersecting the water polygons and a pre-flood DEM created using the pre-flood LiDAR data. We evaluate the proposed methods for inundation depth mapping over the Town of Princeville during a flooding event during Hurricane Matthew. The methods are compared and validated using the USGS gauge water level data acquired during the flood event. The RMSEs for water depth using the SfM method and integrated method based on deep learning and DEM were 0.34m and 0.26m, respectively.},
DOI = {10.3390/ijgi10030144}
}



@Article{rs13051009,
AUTHOR = {Niu, Yaxiao and Zhang, Huihui and Han, Wenting and Zhang, Liyuan and Chen, Haipeng},
TITLE = {A Fixed-Threshold Method for Estimating Fractional Vegetation Cover of Maize under Different Levels of Water Stress},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1009},
URL = {https://www.mdpi.com/2072-4292/13/5/1009},
ISSN = {2072-4292},
ABSTRACT = {Accurate estimation of fractional vegetation cover (FVC) from digital images taken by commercially available cameras is of great significance in order to monitor the vegetation growth status, especially when plants are under water stress. Two classic threshold-based methods, namely, the intersection method (T1 method) and the equal misclassification probability method (T2 method), have been widely applied to Red-Green-Blue (RGB) images. However, the high coverage and severe water stress of crops in the field make it difficult to extract FVC stably and accurately. To solve this problem, this paper proposes a fixed-threshold method based on the statistical analysis of thresholds obtained from the two classic threshold approaches. Firstly, a Gaussian mixture model (GMM), including the distributions of green vegetation and backgrounds, was fitted on four color features: excessive green index, H channel of the Hue-Saturation-Value (HSV) color space, a* channel of the CIE L*a*b* color space, and the brightness-enhanced a* channel (denoted as a*_I). Secondly, thresholds were calculated by applying the T1 and T2 methods to the GMM of each color feature. Thirdly, based on the statistical analysis of the thresholds with better performance between T1 and T2, the fixed-threshold method was proposed. Finally, the fixed-threshold method was applied to the optimal color feature a*_I to estimate FVC, and was compared with the two classic approaches. Results showed that, for some images with high reference FVC, FVC was seriously underestimated by 0.128 and 0.141 when using the T1 and T2 methods, respectively, but this problem was eliminated by the proposed fixed-threshold method. Compared with the T1 and T2 methods, for images taken in plots under severe water stress, the mean absolute error of FVC obtained by the fixed-threshold method was decreased by 0.043 and 0.193, respectively. Overall, the FVC estimation using the proposed fixed-threshold method has the advantages of robustness, accuracy, and high efficiency, with a coefficient of determination (R2) of 0.99 and root mean squared error (RMSE) of 0.02.},
DOI = {10.3390/rs13051009}
}



@Article{app11052385,
AUTHOR = {Cho, Won and Kong, Joonho},
TITLE = {Memory and Cache Contention Denial-of-Service Attack in Mobile Edge Devices},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2385},
URL = {https://www.mdpi.com/2076-3417/11/5/2385},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we introduce a memory and cache contention denial-of-service attack and its hardware-based countermeasure. Our attack can significantly degrade the performance of the benign programs by hindering the shared resource accesses of the benign programs. It can be achieved by a simple C-based malicious code while degrading the performance of the benign programs by 47.6% on average. As another side-effect, our attack also leads to greater energy consumption of the system by 2.1× on average, which may cause shorter battery life in the mobile edge devices. We also propose detection and mitigation techniques for thwarting our attack. By analyzing L1 data cache miss request patterns, we effectively detect the malicious program for the memory and cache contention denial-of-service attack. For mitigation, we propose using instruction fetch width throttling techniques to restrict the malicious accesses to the shared resources. When employing our malicious program detection with the instruction fetch width throttling technique, we recover the system performance and energy by 92.4% and 94.7%, respectively, which means that the adverse impacts from the malicious programs are almost removed.},
DOI = {10.3390/app11052385}
}



@Article{app11052408,
AUTHOR = {Oñate-López, José and Navarro, Loraine and Quintero M., Christian G. and Pardo, Mauricio},
TITLE = {Intelligent Exploration Approaches Based on Utility Functions Optimization for Multi-Agent Environment Applications},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {2408},
URL = {https://www.mdpi.com/2076-3417/11/5/2408},
ISSN = {2076-3417},
ABSTRACT = {In this work, the problem of exploring an unknown environment with a team of agents and search different targets on it is considered. The key problem to be solved in multiple agents is choosing appropriate target points for the individual agents to simultaneously explore different regions of the environment. An intelligent approach is presented to coordinate several agents using a market-based model to identify the appropriate task for each agent. It is proposed to compare the fitting of the market utility function using neural networks and optimize this function using genetic algorithms to avoid heavy computation in the Non-Polynomial (NP: nondeterministic polynomial time) path-planning problem. An indoor environment inspires the proposed approach with homogeneous physical agents, and its performance is tested in simulations. The results show that the proposed approach allocates agents effectively to the environment and enables them to carry out their mission quickly.},
DOI = {10.3390/app11052408}
}



@Article{app11062458,
AUTHOR = {Roberts, Ronald and Inzerillo, Laura and Di Mino, Gaetano},
TITLE = {Exploiting Data Analytics and Deep Learning Systems to Support Pavement Maintenance Decisions},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2458},
URL = {https://www.mdpi.com/2076-3417/11/6/2458},
ISSN = {2076-3417},
ABSTRACT = {Road networks are critical infrastructures within any region and it is imperative to maintain their conditions for safe and effective movement of goods and services. Road Management, therefore, plays a key role to ensure consistent efficient operation. However, significant resources are required to perform necessary maintenance activities to achieve and maintain high levels of service. Pavement maintenance can typically be very expensive and decisions are needed concerning planning and prioritizing interventions. Data are key towards enabling adequate maintenance planning but in many instances, there is limited available information especially in small or under-resourced urban road authorities. This study develops a roadmap to help these authorities by using flexible data analysis and deep learning computational systems to highlight important factors within road networks, which are used to construct models that can help predict future intervention timelines. A case study in Palermo, Italy was successfully developed to demonstrate how the techniques could be applied to perform appropriate feature selection and prediction models based on limited data sources. The workflow provides a pathway towards more effective pavement maintenance management practices using techniques that can be readily adapted based on different environments. This takes another step towards automating these practices within the pavement management system.},
DOI = {10.3390/app11062458}
}



@Article{rs13061053,
AUTHOR = {Stathopoulou, Elisavet Konstantina and Battisti, Roberto and Cernea, Dan and Remondino, Fabio and Georgopoulos, Andreas},
TITLE = {Semantically Derived Geometric Constraints for MVS Reconstruction of Textureless Areas},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1053},
URL = {https://www.mdpi.com/2072-4292/13/6/1053},
ISSN = {2072-4292},
ABSTRACT = {Conventional multi-view stereo (MVS) approaches based on photo-consistency measures are generally robust, yet often fail in calculating valid depth pixel estimates in low textured areas of the scene. In this study, a novel approach is proposed to tackle this challenge by leveraging semantic priors into a PatchMatch-based MVS in order to increase confidence and support depth and normal map estimation. Semantic class labels on image pixels are used to impose class-specific geometric constraints during multiview stereo, optimising the depth estimation on weakly supported, textureless areas, commonly present in urban scenarios of building facades, indoor scenes, or aerial datasets. Detecting dominant shapes, e.g., planes, with RANSAC, an adjusted cost function is introduced that combines and weighs both photometric and semantic scores propagating, thus, more accurate depth estimates. Being adaptive, it fills in apparent information gaps and smoothing local roughness in problematic regions while at the same time preserves important details. Experiments on benchmark and custom datasets demonstrate the effectiveness of the presented approach.},
DOI = {10.3390/rs13061053}
}



@Article{s21061947,
AUTHOR = {Nemer, Ibrahim and Sheltami, Tarek and Ahmad, Irfan and Yasar, Ansar Ul-Haque and Abdeen, Mohammad A. R.},
TITLE = {RF-Based UAV Detection and Identification Using Hierarchical Learning Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1947},
URL = {https://www.mdpi.com/1424-8220/21/6/1947},
PubMedID = {33802189},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs) are widely available in the current market to be used either for recreation as a hobby or to serve specific industrial requirements, such as agriculture and construction. However, illegitimate and criminal usage of UAVs is also on the rise which introduces their effective identification and detection as a research challenge. This paper proposes a novel machine learning-based for efficient identification and detection of UAVs. Specifically, an improved UAV identification and detection approach is presented using an ensemble learning based on the hierarchical concept, along with pre-processing and feature extraction stages for the Radio Frequency (RF) data. Filtering is applied on the RF signals in the detection approach to improve the output. This approach consists of four classifiers and they are working in a hierarchical way. The sample will pass the first classifier to check the availability of the UAV, and then it will specify the type of the detected UAV using the second classifier. The last two classifiers will handle the sample that is related to Bebop and AR to specify their mode. Evaluation of the proposed approach with publicly available dataset demonstrates better efficiency compared to existing detection systems in the literature. It has the ability to investigate whether a UAV is flying within the area or not, and it can directly identify the type of UAV and then the flight mode of the detected UAV with accuracy around 99%.},
DOI = {10.3390/s21061947}
}



@Article{s21061960,
AUTHOR = {Fotouhi, Azade and Ding, Ming and Hassan, Mahbub},
TITLE = {Deep Q-Learning for Two-Hop Communications of Drone Base Stations},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1960},
URL = {https://www.mdpi.com/1424-8220/21/6/1960},
PubMedID = {33799546},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we address the application of the flying Drone Base Stations (DBS) in order to improve the network performance. Given the high degrees of freedom of a DBS, it can change its position and adapt its trajectory according to the users movements and the target environment. A two-hop communication model, between an end-user and a macrocell through a DBS, is studied in this work. We propose Q-learning and Deep Q-learning based solutions to optimize the drone’s trajectory. Simulation results show that, by employing our proposed models, the drone can autonomously fly and adapts its mobility according to the users’ movements. Additionally, the Deep Q-learning model outperforms the Q-learning model and can be applied in more complex environments.},
DOI = {10.3390/s21061960}
}



@Article{f12030327,
AUTHOR = {Dainelli, Riccardo and Toscano, Piero and Di Gennaro, Salvatore Filippo and Matese, Alessandro},
TITLE = {Recent Advances in Unmanned Aerial Vehicle Forest Remote Sensing—A Systematic Review. Part I: A General Framework},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {327},
URL = {https://www.mdpi.com/1999-4907/12/3/327},
ISSN = {1999-4907},
ABSTRACT = {Natural, semi-natural, and planted forests are a key asset worldwide, providing a broad range of positive externalities. For sustainable forest planning and management, remote sensing (RS) platforms are rapidly going mainstream. In a framework where scientific production is growing exponentially, a systematic analysis of unmanned aerial vehicle (UAV)-based forestry research papers is of paramount importance to understand trends, overlaps and gaps. The present review is organized into two parts (Part I and Part II). Part II inspects specific technical issues regarding the application of UAV-RS in forestry, together with the pros and cons of different UAV solutions and activities where additional effort is needed, such as the technology transfer. Part I systematically analyzes and discusses general aspects of applying UAV in natural, semi-natural and artificial forestry ecosystems in the recent peer-reviewed literature (2018–mid-2020). The specific goals are threefold: (i) create a carefully selected bibliographic dataset that other researchers can draw on for their scientific works; (ii) analyze general and recent trends in RS forest monitoring (iii) reveal gaps in the general research framework where an additional activity is needed. Through double-step filtering of research items found in the Web of Science search engine, the study gathers and analyzes a comprehensive dataset (226 articles). Papers have been categorized into six main topics, and the relevant information has been subsequently extracted. The strong points emerging from this study concern the wide range of topics in the forestry sector and in particular the retrieval of tree inventory parameters often through Digital Aerial Photogrammetry (DAP), RGB sensors, and machine learning techniques. Nevertheless, challenges still exist regarding the promotion of UAV-RS in specific parts of the world, mostly in the tropical and equatorial forests. Much additional research is required for the full exploitation of hyperspectral sensors and for planning long-term monitoring.},
DOI = {10.3390/f12030327}
}



@Article{en14061559,
AUTHOR = {Catalano, Antonio Pio and Scognamillo, Ciro and Guerriero, Pierluigi and Daliento, Santolo and d’Alessandro, Vincenzo},
TITLE = {Using EMPHASIS for the Thermography-Based Fault Detection in Photovoltaic Plants},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1559},
URL = {https://www.mdpi.com/1996-1073/14/6/1559},
ISSN = {1996-1073},
ABSTRACT = {In this paper, an Efficient Method for PHotovoltaic Arrays Study through Infrared Scanning (EMPHASIS) is presented; it is a fast, simple, and trustworthy cell-level diagnosis method for commercial photovoltaic (PV) panels. EMPHASIS processes temperature maps experimentally obtained through IR cameras and is based on a power balance equation. Along with the identification of malfunction events, EMPHASIS offers an innovative feature, i.e., it estimates the electrical powers generated (or dissipated) by the individual cells. A procedure to evaluate the accuracy of the EMPHASIS predictions is proposed, which relies on detailed three-dimensional (3-D) numerical simulations to emulate realistic temperature maps of PV panels under any working condition. Malfunctioning panels were replicated in the numerical environment and the corresponding temperature maps were fed to EMPHASIS. Excellent results were achieved in both the cell- and panel-level power predictions. More specifically, the estimation of the power production of a PV panel with a shunted cell demonstrated an error lower than 1%. In cases of strong nonuniformities as a PV panel in hotspot, an estimation error in the range of 9–16% was quantified.},
DOI = {10.3390/en14061559}
}



@Article{agronomy11030532,
AUTHOR = {Hashim, Izrahayu Che and Shariff, Abdul Rashid Mohamed and Bejo, Siti Khairunniza and Muharam, Farrah Melissa and Ahmad, Khairulmazmi},
TITLE = {Machine-Learning Approach Using SAR Data for the Classification of Oil Palm Trees That Are Non-Infected and Infected with the Basal Stem Rot Disease},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {532},
URL = {https://www.mdpi.com/2073-4395/11/3/532},
ISSN = {2073-4395},
ABSTRACT = {Basal stem rot disease (BSR) in oil palm plants is caused by the Ganoderma boninense (G. boninense) fungus. BSR is a major disease that affects oil palm plantations in Malaysia and Indonesia. As of now, the only available sustaining measure is to prolong the life of oil palm trees since there has been no effective treatment for the BSR disease. This project used an ALOS PALSAR-2 image with dual polarization, Horizontal transmit and Horizontal receive (HH) and Horizontal transmit and Vertical receive (HV). The aims of this study were to (1) identify the potential backscatter variables; and (2) examine the performance of machine learning (ML) classifiers (Multilayer Perceptron (MLP) and Random Forest (RF) to classify oil palm trees that are non-infected and infected by G. boninense. The sample size consisted of 55 uninfected trees and 37 infected trees. We used the imbalance data approach (Synthetic Minority Over-Sampling Technique (SMOTE) in these classifications due to the differing sample sizes. The result showed backscatter variable HV had a higher correct classification for the G. boninense non-infected and infected oil palm trees for both classifiers; the MLP classifier model had a robust success rate, which correctly classified 100% for non-infected and 91.30% for infected G. boninense, and RF had a robust success rate, which correctly classified 94.11% for non-infected and 91.30% for infected G. boninense. In terms of model performance using the most significant variables, HV, the MLP model had a balanced accuracy (BCR) of 95.65% compared to 92.70% for the RF model. Comparison between the MLP model and RF model for the receiver operating characteristics (ROC) curve region, (AUC) gave a value of 0.92 and 0.95, respectively, for the MLP and RF models. Therefore, it can be concluded by using only the HV polarization, that both the MLP and RF can be used to predict BSR disease with a relatively high accuracy.},
DOI = {10.3390/agronomy11030532}
}



@Article{s21061994,
AUTHOR = {Ma, Qian and Han, Wenting and Huang, Shenjin and Dong, Shide and Li, Guang and Chen, Haipeng},
TITLE = {Distinguishing Planting Structures of Different Complexity from UAV Multispectral Images},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1994},
URL = {https://www.mdpi.com/1424-8220/21/6/1994},
PubMedID = {33808967},
ISSN = {1424-8220},
ABSTRACT = {This study explores the classification potential of a multispectral classification model for farmland with planting structures of different complexity. Unmanned aerial vehicle (UAV) remote sensing technology is used to obtain multispectral images of three study areas with low-, medium-, and high-complexity planting structures, containing three, five, and eight types of crops, respectively. The feature subsets of three study areas are selected by recursive feature elimination (RFE). Object-oriented random forest (OB-RF) and object-oriented support vector machine (OB-SVM) classification models are established for the three study areas. After training the models with the feature subsets, the classification results are evaluated using a confusion matrix. The OB-RF and OB-SVM models’ classification accuracies are 97.09% and 99.13%, respectively, for the low-complexity planting structure. The equivalent values are 92.61% and 99.08% for the medium-complexity planting structure and 88.99% and 97.21% for the high-complexity planting structure. For farmland with fragmentary plots and a high-complexity planting structure, as the planting structure complexity changed from low to high, both models’ overall accuracy levels decreased. The overall accuracy of the OB-RF model decreased by 8.1%, and that of the OB-SVM model only decreased by 1.92%. OB-SVM achieves an overall classification accuracy of 97.21%, and a single-crop extraction accuracy of at least 85.65%. Therefore, UAV multispectral remote sensing can be used for classification applications in highly complex planting structures.},
DOI = {10.3390/s21061994}
}



@Article{electronics10060662,
AUTHOR = {Wang, Guoqing and Chen, He and Xie, Yizhuang},
TITLE = {An Efficient Dual-Channel Data Storage and Access Method for Spaceborne Synthetic Aperture Radar Real-Time Processing},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {662},
URL = {https://www.mdpi.com/2079-9292/10/6/662},
ISSN = {2079-9292},
ABSTRACT = {With the development of remote sensing technology and very large-scale integrated circuit (VLSI) technology, the real-time processing of spaceborne Synthetic Aperture Radar (SAR) has greatly improved the ability of Earth observation. However, the characteristics of external memory have led to matrix transposition becoming a technical bottleneck that limits the real-time performance of the SAR imaging system. In order to solve this problem, this paper combines the optimized data mapping method and reasonable hardware architecture to implement a data controller based on the Field-Programmable Gate Array (FPGA). First of all, this paper proposes an optimized dual-channel data storage and access method, so that the two-dimensional data access efficiency can be improved. Then, a hardware architecture is designed with register manager, simplified address generator and dual-channel Double-Data-Rate Three Synchronous Dynamic Random-Access Memory (DDR3 SDRAM) access mode. Finally, the proposed data controller is implemented on the Xilinx XC7VX690T FPGA chip. The experimental results show that the reading efficiency of the data controller proposed is 80% both in the range direction and azimuth direction, and the writing efficiency is 66% both in the range direction and azimuth direction. The results of a comparison with the recent implementations show that the proposed data controller has a higher data bandwidth, is more flexible in its design, and is suitable for use in spaceborne scenarios.},
DOI = {10.3390/electronics10060662}
}



@Article{chemosensors9030055,
AUTHOR = {Elsayed, Salah and El-Hendawy, Salah and Khadr, Mosaad and Elsherbiny, Osama and Al-Suhaibani, Nasser and Dewir, Yaser Hassan and Tahir, Muhammad Usman and Mubushar, Muhammad and Darwish, Waleed},
TITLE = {Integration of Spectral Reflectance Indices and Adaptive Neuro-Fuzzy Inference System for Assessing the Growth Performance and Yield of Potato under Different Drip Irrigation Regimes},
JOURNAL = {Chemosensors},
VOLUME = {9},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {55},
URL = {https://www.mdpi.com/2227-9040/9/3/55},
ISSN = {2227-9040},
ABSTRACT = {Simultaneous and timely assessment of growth and water status-related plant traits is critical for precision irrigation management in arid regions. Here, we used proximal hyperspectral sensing tools to estimate biomass fresh weight (BFW), biomass dry weight (BDW), canopy water content (CWC), and total tuber yield (TTY) of two potato varieties irrigated with 100%, 75%, and 50% of the estimated crop evapotranspiration (ETc). Plant traits were assessed remotely using published and newly constructed vegetation and water spectral reflectance indices (SRIs). We integrated genetic algorithm (GA) and adaptive neuro-fuzzy inference system (ANFIS) models to predict the measured traits based on all SRIs. The different plant traits and SRIs varied significantly (p &lt; 0.05) between the three irrigation regimes for the two varieties. The values of plant traits and majority SRIs showed a continuous decrease from the 100% ETc to the 50% ETc. Water-SRIs performed better than vegetation-SRIs for estimating the four plant traits. Almost all indices of the two SRI types had a weak relationship with the four plant traits (R2 = 0.00–0.37) under each irrigation regime. However, the majority of vegetation-SRIs and all water-SRIs showed strong relationships with BFW, CWC, and TTY (R2 ≥ 0.65) and moderate relationships with BDW (R2 ≥ 0.40) when the data of all irrigation regimes and varieties were analyzed together for each growing season or the data of all irrigation regimes, varieties, and seasons were combined together. The ANFIS-GA model predicted plant traits with satisfactory accuracy in both calibration (R2 = 1.0) and testing (R2 = 0.72–0.97) modes. The results indicate that SRI-based ANFIS models can improve plant trait estimation. This analysis also confirmed the benefits of applying GA to ANFIS to estimate plant responses to different growth conditions.},
DOI = {10.3390/chemosensors9030055}
}



@Article{rs13061081,
AUTHOR = {Liu, Zhen and Wu, Wenxiu and Gu, Xingyu and Li, Shuwei and Wang, Lutai and Zhang, Tianjie},
TITLE = {Application of Combining YOLO Models and 3D GPR Images in Road Detection and Maintenance},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1081},
URL = {https://www.mdpi.com/2072-4292/13/6/1081},
ISSN = {2072-4292},
ABSTRACT = {Improving the detection efficiency and maintenance benefits is one of the greatest challenges in road testing and maintenance. To address this problem, this paper presents a method for combining the you only look once (YOLO) series with 3D ground-penetrating radar (GPR) images to recognize the internal defects in asphalt pavement and compares the effectiveness of traditional detection and GPR detection by evaluating the maintenance benefits. First, traditional detection is conducted to survey and summarize the surface conditions of tested roads, which are missing the internal information. Therefore, GPR detection is implemented to acquire the images of concealed defects. Then, the YOLOv5 model with the most even performance of the six selected models is applied to achieve the rapid identification of road defects. Finally, the benefits evaluation of maintenance programs based on these two detection methods is conducted from economic and environmental perspectives. The results demonstrate that the economic scores are improved and the maintenance cost is reduced by $49,398/km based on GPR detection; the energy consumption and carbon emissions are reduced by 792,106 MJ/km (16.94%) and 56,289 kg/km (16.91%), respectively, all of which indicates the effectiveness of 3D GPR in pavement detection and maintenance.},
DOI = {10.3390/rs13061081}
}



@Article{rs13061094,
AUTHOR = {Peng, Xingshuo and Han, Wenting and Ao, Jianyi and Wang, Yi},
TITLE = {Assimilation of LAI Derived from UAV Multispectral Data into the SAFY Model to Estimate Maize Yield},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1094},
URL = {https://www.mdpi.com/2072-4292/13/6/1094},
ISSN = {2072-4292},
ABSTRACT = {In this study, we develop a method to estimate corn yield based on remote sensing data and ground monitoring data under different water treatments. Spatially explicit information on crop yields is essential for farmers and agricultural agencies to make well-informed decisions. One approach to estimate crop yield with remote sensing is data assimilation, which integrates sequential observations of canopy development from remote sensing into model simulations of crop growth processes. We found that leaf area index (LAI) inversion based on unmanned aerial vehicle (UAV) vegetation index has a high accuracy, with R2 and root mean square error (RMSE) values of 0.877 and 0.609, respectively. Maize yield estimation based on UAV remote sensing data and simple algorithm for yield (SAFY) crop model data assimilation has different yield estimation accuracy under different water treatments. This method can be used to estimate corn yield, where R2 is 0.855 and RMSE is 692.8kg/ha. Generally, the higher the water stress, the lower the estimation accuracy. Furthermore, we perform the yield estimate mapping at 2 m spatial resolution, which has a higher spatial resolution and accuracy than satellite remote sensing. The great potential of incorporating UAV observations with crop data to monitor crop yield, and improve agricultural management is therefore indicated.},
DOI = {10.3390/rs13061094}
}



@Article{w13060791,
AUTHOR = {Liang, Zhongwei and Liu, Xiaochu and Zou, Tao and Xiao, Jinrui},
TITLE = {Adaptive Prediction of Water Droplet Infiltration Effectiveness of Sprinkler Irrigation Using Regularized Sparse Autoencoder–Adaptive Network-Based Fuzzy Inference System (RSAE–ANFIS)},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {791},
URL = {https://www.mdpi.com/2073-4441/13/6/791},
ISSN = {2073-4441},
ABSTRACT = {As the high productive efficiency of sprinkler irrigation is largely based on balanced soil moisture distribution, it is essential to study the exact effectiveness of water droplet infiltration, which provides a theoretical basis for rationally scheduling the circulation efficiency of groundwater in agricultural irrigation performance. This research carried out adaptive prediction of the droplet infiltration effectiveness of sprinkler irrigation by using a novel approach of a regularized sparse autoencoder–adaptive network-based fuzzy inference system (RSAE–ANFIS), for the purpose of quantifying actual water droplet infiltration and effectiveness results of precision irrigation in various environmental conditions. The intelligent prediction experiment we implemented could be phased as: the demonstration of governing equations of droplet infiltration for sprinkler irrigation modeling; the measurement and computation of probability densities in water droplet infiltration; innovative establishment and working analysis of RSAE–ANFIS; and the adaptive prediction of infiltration effectiveness indexes, such as average soil moisture depth increment (θ, mm), irrigation infiltration efficiency (ea, %), irrigation turn duration efficiency (et, mm/min), and the uniformity coefficient of soil moisture infiltration (Cu, %), which were implemented to provide a comprehensive illustration for the effective scheduling of sprinkler irrigation. Result comparisons indicated that when jetting pressure (Pw) was 255.2 kPa, the impinge angle (Wa) was 42.5°, the water flow rate (Fa) was 0.67 kg/min, and continuous irrigation time (Tc) was 32.4 min (error tolerance = ±5%, the same as follows), thereby an optimum and stable effectiveness quality of sprinkler irrigation could be achieved, whereas average soil moisture depth increment (θ) was 57.6 mm, irrigation infiltration efficiency (ea) was 62.5%, irrigation turn duration efficiency (et) was 34.5 mm/min, and the uniformity coefficient of soil moisture infiltration (Cu) was 53.6%, accordingly. It could be concluded that the proposed approach of the regularized sparse autoencoder–adaptive network-based fuzzy inference system has outstanding predictive capability and possesses much better working superiority for infiltration effectiveness in accuracy and efficiency; meanwhile, a high agreement between the adaptive predicted and actual measured values of infiltration effectiveness could be obtained. This novel intelligent prediction system has been promoted constructively to improve the quality uniformity of sprinkler irrigation and, consequently, to facilitate the productive management of sprinkler irrigated agriculture.},
DOI = {10.3390/w13060791}
}



@Article{s21062052,
AUTHOR = {Yang, Xinghai and Wang, Fengjiao and Bai, Zhiquan and Xun, Feifei and Zhang, Yulin and Zhao, Xiuyang},
TITLE = {Deep Learning-Based Congestion Detection at Urban Intersections},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2052},
URL = {https://www.mdpi.com/1424-8220/21/6/2052},
PubMedID = {33803952},
ISSN = {1424-8220},
ABSTRACT = {In this paper, a deep learning-based traffic state discrimination method is proposed to detect traffic congestion at urban intersections. The detection algorithm includes two parts, global speed detection and a traffic state discrimination algorithm. Firstly, the region of interest (ROI) is selected as the road intersection from the input image of the You Only Look Once (YOLO) v3 object detection algorithm for vehicle target detection. The Lucas-Kanade (LK) optical flow method is employed to calculate the vehicle speed. Then, the corresponding intersection state can be obtained based on the vehicle speed and the discrimination algorithm. The detection of the vehicle takes the position information obtained by YOLOv3 as the input of the LK optical flow algorithm and forms an optical flow vector to complete the vehicle speed detection. Experimental results show that the detection algorithm can detect the vehicle speed and traffic state discrimination method can judge the traffic state accurately, which has a strong anti-interference ability and meets the practical application requirements.},
DOI = {10.3390/s21062052}
}



@Article{s21062062,
AUTHOR = {Dias, Pollyanna G. Faria and Silva, Mateus C. and Rocha Filho, Geraldo P. and Vargas, Patrícia A. and Cota, Luciano P. and Pessin, Gustavo},
TITLE = {Swarm Robotics: A Perspective on the Latest Reviewed Concepts and Applications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2062},
URL = {https://www.mdpi.com/1424-8220/21/6/2062},
PubMedID = {33804187},
ISSN = {1424-8220},
ABSTRACT = {Known as an artificial intelligence subarea, Swarm Robotics is a developing study field investigating bio-inspired collaborative control approaches and integrates a huge collection of agents, reasonably plain robots, in a distributed and decentralized manner. It offers an inspiring essential platform for new researchers to be engaged and share new knowledge to examine their concepts in analytical and heuristic strategies. This paper introduces an overview of current activities in Swarm Robotics and examines the present literature in this area to establish to approach between a realistic swarm robotic system and real-world enforcements. First, we review several Swarm Intelligence concepts to define Swarm Robotics systems, reporting their essential qualities and features and contrast them to generic multi-robotic systems. Second, we report a review of the principal projects that allow realistic study of Swarm Robotics. We demonstrate knowledge regarding current hardware platforms and multi-robot simulators. Finally, the forthcoming promissory applications and the troubles to surpass with a view to achieving them have been described and analyzed.},
DOI = {10.3390/s21062062}
}



@Article{asi4010023,
AUTHOR = {Naseem, Usman and Khushi, Matloob and Khan, Shah Khalid and Shaukat, Kamran and Moni, Mohammad Ali},
TITLE = {A Comparative Analysis of Active Learning for Biomedical Text Mining},
JOURNAL = {Applied System Innovation},
VOLUME = {4},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2571-5577/4/1/23},
ISSN = {2571-5577},
ABSTRACT = {An enormous amount of clinical free-text information, such as pathology reports, progress reports, clinical notes and discharge summaries have been collected at hospitals and medical care clinics. These data provide an opportunity of developing many useful machine learning applications if the data could be transferred into a learn-able structure with appropriate labels for supervised learning. The annotation of this data has to be performed by qualified clinical experts, hence, limiting the use of this data due to the high cost of annotation. An underutilised technique of machine learning that can label new data called active learning (AL) is a promising candidate to address the high cost of the label the data. AL has been successfully applied to labelling speech recognition and text classification, however, there is a lack of literature investigating its use for clinical purposes. We performed a comparative investigation of various AL techniques using ML and deep learning (DL)-based strategies on three unique biomedical datasets. We investigated random sampling (RS), least confidence (LC), informative diversity and density (IDD), margin and maximum representativeness-diversity (MRD) AL query strategies. Our experiments show that AL has the potential to significantly reducing the cost of manual labelling. Furthermore, pre-labelling performed using AL expediates the labelling process by reducing the time required for labelling.},
DOI = {10.3390/asi4010023}
}



@Article{ani11030829,
AUTHOR = {Herlin, Anders and Brunberg, Emma and Hultgren, Jan and Högberg, Niclas and Rydberg, Anna and Skarin, Anna},
TITLE = {Animal Welfare Implications of Digital Tools for Monitoring and Management of Cattle and Sheep on Pasture},
JOURNAL = {Animals},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {829},
URL = {https://www.mdpi.com/2076-2615/11/3/829},
PubMedID = {33804235},
ISSN = {2076-2615},
ABSTRACT = {The opportunities for natural animal behaviours in pastures imply animal welfare benefits. Nevertheless, monitoring the animals can be challenging. The use of sensors, cameras, positioning equipment and unmanned aerial vehicles in large pastures has the potential to improve animal welfare surveillance. Directly or indirectly, sensors measure environmental factors together with the behaviour and physiological state of the animal, and deviations can trigger alarms for, e.g., disease, heat stress and imminent calving. Electronic positioning includes Radio Frequency Identification (RFID) for the recording of animals at fixed points. Positioning units (GPS) mounted on collars can determine animal movements over large areas, determine their habitat and, somewhat, health and welfare. In combination with other sensors, such units can give information that helps to evaluate the welfare of free-ranging animals. Drones equipped with cameras can also locate and count the animals, as well as herd them. Digitally defined virtual fences can keep animals within a predefined area without the use of physical barriers, relying on acoustic signals and weak electric shocks. Due to individual variations in learning ability, some individuals may be exposed to numerous electric shocks, which might compromise their welfare. More research and development are required, especially regarding the use of drones and virtual fences.},
DOI = {10.3390/ani11030829}
}



@Article{rs13061117,
AUTHOR = {Li, Jing and Xie, Yuguang and Li, Congcong and Dai, Yanran and Ma, Jiaxin and Dong, Zheng and Yang, Tao},
TITLE = {UAV-Assisted Wide Area Multi-Camera Space Alignment Based on Spatiotemporal Feature Map},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1117},
URL = {https://www.mdpi.com/2072-4292/13/6/1117},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we investigate the problem of aligning multiple deployed camera into one united coordinate system for cross-camera information sharing and intercommunication. However, the difficulty is greatly increased when faced with large-scale scene under chaotic camera deployment. To address this problem, we propose a UAV-assisted wide area multi-camera space alignment approach based on spatiotemporal feature map. It employs the great global perception of Unmanned Aerial Vehicles (UAVs) to meet the challenge from wide-range environment. Concretely, we first present a novel spatiotemporal feature map construction approach to represent the input aerial and ground monitoring data. In this way, the motion consistency across view is well mined to overcome the great perspective gap between the UAV and ground cameras. To obtain the corresponding relationship between their pixels, we propose a cross-view spatiotemporal matching strategy. Through solving relative relationship with the above air-to-ground point correspondences, all ground cameras can be aligned into one surveillance space. The proposed approach was evaluated in both simulation and real environments qualitatively and quantitatively. Extensive experimental results demonstrate that our system can successfully align all ground cameras with very small pixel error. Additionally, the comparisons with other works on different test situations also verify its superior performance.},
DOI = {10.3390/rs13061117}
}



@Article{su13063279,
AUTHOR = {Emin, Mirzat and Anwar, Erpan and Liu, Suhong and Emin, Bilal and Mamut, Maryam and Abdukeram, Abduwali and Liu, Ting},
TITLE = {Target Detection-Based Tree Recognition in a Spruce Forest Area with a High Tree Density—Implications for Estimating Tree Numbers},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {3279},
URL = {https://www.mdpi.com/2071-1050/13/6/3279},
ISSN = {2071-1050},
ABSTRACT = {Here, unmanned aerial vehicle (UAV) remote sensing and machine vision were used to automatically, accurately, and efficiently count Tianshan spruce and improve the efficiency of scientific forest management, focusing on a typical Tianshan spruce forest on Tianshan Mountain, middle Asia. First, the UAV in the sampling area was cropped from the image, and a target-labeling tool was used. The Tianshan spruce trees were annotated to construct a data set, and four models were used to identify and verify them in three different areas (low, medium, and high canopy closures). Finally, the combined number of trees was calculated. The average accuracy of the detection frame, mean accuracy and precision (mAP), was used to determine the target detection accuracy. The Faster Region Convolutional Neural Network (Faster-RCNN) model achieved the highest accuracies (96.36%, 96.32%, and 95.54% under low, medium, and high canopy closures, respectively) and the highest mAP (85%). Canopy closure affected the detection and recognition accuracy; YOLOv3, YOLOv4, and Faster-RCNN all showed varying spruce recognition accuracies at different densities. The accuracy of the Faster-RCNN model decreased by at least 0.82%. Combining UAV remote sensing with target detection networks can identify and quantify statistics regarding Tianshan spruce. This solves the shortcomings of traditional monitoring methods and is significant for understanding and monitoring forest ecosystems.},
DOI = {10.3390/su13063279}
}



@Article{agriengineering3010008,
AUTHOR = {Hardy, Tom and Kooistra, Lammert and Domingues Franceschini, Marston and Richter, Sebastiaan and Vonk, Erwin and van den Eertwegh, Gé and van Deijl, Dion},
TITLE = {Sen2Grass: A Cloud-Based Solution to Generate Field-Specific Grassland Information Derived from Sentinel-2 Imagery},
JOURNAL = {AgriEngineering},
VOLUME = {3},
YEAR = {2021},
NUMBER = {1},
PAGES = {118--137},
URL = {https://www.mdpi.com/2624-7402/3/1/8},
ISSN = {2624-7402},
ABSTRACT = {Grasslands are important for their ecological values and for agricultural activities such as livestock production worldwide. Efficient grassland management is vital to these values and activities, and remote sensing technologies are increasingly being used to characterize the spatiotemporal variation of grasslands to support those management practices. For this study, Sentinel-2 satellite imagery was used as an input to develop an open-source and automated monitoring system (Sen2Grass) to gain field-specific grassland information on the national and regional level for any given time range as of January 2016. This system was implemented in a cloud-computing platform (StellaSpark Nexus) designed to process large geospatial data streams from a variety of sources and was tested for a number of parcels from the Haus Riswick experimental farm in Germany. Despite outliers due to fluctuating weather conditions, vegetation index time series suggested four distinct growing cycles per growing season. Established relationships between vegetation indices and grassland yield showed poor to moderate positive trends, implying that vegetation indices could be a potential predictor for grassland biomass and chlorophyll content. However, the inclusion of larger and additional datasets such as Sentinel-1 imagery could be beneficial to developing more robust prediction models and for automatic detection of mowing events for grasslands.},
DOI = {10.3390/agriengineering3010008}
}



@Article{rs13061143,
AUTHOR = {Quan, Yinghui and Tong, Yingping and Feng, Wei and Dauphin, Gabriel and Huang, Wenjiang and Zhu, Wentao and Xing, Mengdao},
TITLE = {Relative Total Variation Structure Analysis-Based Fusion Method for Hyperspectral and LiDAR Data Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1143},
URL = {https://www.mdpi.com/2072-4292/13/6/1143},
ISSN = {2072-4292},
ABSTRACT = {The fusion of the hyperspectral image (HSI) and the light detecting and ranging (LiDAR) data has a wide range of applications. This paper proposes a novel feature fusion method for urban area classification, namely the relative total variation structure analysis (RTVSA), to combine various features derived from HSI and LiDAR data. In the feature extraction stage, a variety of high-performance methods including the extended multi-attribute profile, Gabor filter, and local binary pattern are used to extract the features of the input data. The relative total variation is then applied to remove useless texture information of the processed data. Finally, nonparametric weighted feature extraction is adopted to reduce the dimensions. Random forest and convolutional neural networks are utilized to evaluate the fusion images. Experiments conducted on two urban Houston University datasets (including Houston 2012 and the training portion of Houston 2017) demonstrate that the proposed method can extract the structural correlation from heterogeneous data, withstand a noise well, and improve the land cover classification accuracy.},
DOI = {10.3390/rs13061143}
}



@Article{agronomy11030575,
AUTHOR = {Sabzi, Sajad and Pourdarbani, Razieh and Rohban, Mohammad Hossein and García-Mateos, Ginés and Paliwal, Jitendra and Molina-Martínez, José Miguel},
TITLE = {Early Detection of Excess Nitrogen Consumption in Cucumber Plants Using Hyperspectral Imaging Based on Hybrid Neural Networks and the Imperialist Competitive Algorithm},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {575},
URL = {https://www.mdpi.com/2073-4395/11/3/575},
ISSN = {2073-4395},
ABSTRACT = {To achieve healthy and optimal yields of agricultural products, the principles of nutrition must be observed and appropriate fertilizers must be applied. Nutritional deficiencies or overabundance reduce the quality and yield of the products. Thus, their early detection prevents physiological disorders and associated diseases. Most research efforts have focused on spectroscopy, which extracts only spectral data from a single point of the product. The present study aims to detect early excess nitrogen in cucumber plants by using a new hyperspectral imaging technique based on a hybrid of artificial neural networks and the imperialist competitive algorithm (ANN-ICA), which can provide spectral and spatial information on the leaves at the same time. First, cucumber seeds were planted in 18 pots. The same inputs were applied to all the pots until the plants grew; after that, 30% excess nitrogen was applied to nine pots with irrigation water, while it remained constant in the other nine pots. Each day, six leaves were collected from each pot, and their images were captured using a hyperspectral camera (in the range of 400–1100 nm). The wavelengths of 715, 783 and 821 nm were determined as the most effective for early detection of excess nitrogen using a hybrid of artificial neural networks and the artificial bee colony algorithm (ANN-ABC). The parameter of days of treatment was classified using ANN-ICA. The performance of the classifier was evaluated using different criteria, namely recall, accuracy, specificity, precision and the F-measure. The results indicate that the differences between different days were statistically significant. This means that the hyperspectral imaging technique was able to detect plants with excess nitrogen in the near-infrared range (NIR), with a correct classification rate of 96.11%.},
DOI = {10.3390/agronomy11030575}
}



@Article{s21062141,
AUTHOR = {Nafea, Ohoud and Abdul, Wadood and Muhammad, Ghulam and Alsulaiman, Mansour},
TITLE = {Sensor-Based Human Activity Recognition with Spatio-Temporal Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2141},
URL = {https://www.mdpi.com/1424-8220/21/6/2141},
PubMedID = {33803891},
ISSN = {1424-8220},
ABSTRACT = {Human activity recognition (HAR) remains a challenging yet crucial problem to address in computer vision. HAR is primarily intended to be used with other technologies, such as the Internet of Things, to assist in healthcare and eldercare. With the development of deep learning, automatic high-level feature extraction has become a possibility and has been used to optimize HAR performance. Furthermore, deep-learning techniques have been applied in various fields for sensor-based HAR. This study introduces a new methodology using convolution neural networks (CNN) with varying kernel dimensions along with bi-directional long short-term memory (BiLSTM) to capture features at various resolutions. The novelty of this research lies in the effective selection of the optimal video representation and in the effective extraction of spatial and temporal features from sensor data using traditional CNN and BiLSTM. Wireless sensor data mining (WISDM) and UCI datasets are used for this proposed methodology in which data are collected through diverse methods, including accelerometers, sensors, and gyroscopes. The results indicate that the proposed scheme is efficient in improving HAR. It was thus found that unlike other available methods, the proposed method improved accuracy, attaining a higher score in the WISDM dataset compared to the UCI dataset (98.53% vs. 97.05%).},
DOI = {10.3390/s21062141}
}



@Article{s21062143,
AUTHOR = {Paiva, Sara and Ahad, Mohd Abdul and Tripathi, Gautami and Feroz, Noushaba and Casalino, Gabriella},
TITLE = {Enabling Technologies for Urban Smart Mobility: Recent Trends, Opportunities and Challenges},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2143},
URL = {https://www.mdpi.com/1424-8220/21/6/2143},
PubMedID = {33803903},
ISSN = {1424-8220},
ABSTRACT = {The increasing population across the globe makes it essential to link smart and sustainable city planning with the logistics of transporting people and goods, which will significantly contribute to how societies will face mobility in the coming years. The concept of smart mobility emerged with the popularity of smart cities and is aligned with the sustainable development goals defined by the United Nations. A reduction in traffic congestion and new route optimizations with reduced ecological footprint are some of the essential factors of smart mobility; however, other aspects must also be taken into account, such as the promotion of active mobility and inclusive mobility, encouraging the use of other types of environmentally friendly fuels and engagement with citizens. The Internet of Things (IoT), Artificial Intelligence (AI), Blockchain and Big Data technology will serve as the main entry points and fundamental pillars to promote the rise of new innovative solutions that will change the current paradigm for cities and their citizens. Mobility-as-a-service, traffic flow optimization, the optimization of logistics and autonomous vehicles are some of the services and applications that will encompass several changes in the coming years with the transition of existing cities into smart cities. This paper provides an extensive review of the current trends and solutions presented in the scope of smart mobility and enabling technologies that support it. An overview of how smart mobility fits into smart cities is provided by characterizing its main attributes and the key benefits of using smart mobility in a smart city ecosystem. Further, this paper highlights other various opportunities and challenges related to smart mobility. Lastly, the major services and applications that are expected to arise in the coming years within smart mobility are explored with the prospective future trends and scope.},
DOI = {10.3390/s21062143}
}



@Article{electronics10060724,
AUTHOR = {Yavariabdi, Amir and Kusetogullari, Huseyin and Celik, Turgay and Cicek, Hasan},
TITLE = {FastUAV-NET: A Multi-UAV Detection Algorithm for Embedded Platforms},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {724},
URL = {https://www.mdpi.com/2079-9292/10/6/724},
ISSN = {2079-9292},
ABSTRACT = {In this paper, a real-time deep learning-based framework for detecting and tracking Unmanned Aerial Vehicles (UAVs) in video streams captured by a fixed-wing UAV is proposed. The proposed framework consists of two steps, namely intra-frame multi-UAV detection and the inter-frame multi-UAV tracking. In the detection step, a new multi-scale UAV detection Convolutional Neural Network (CNN) architecture based on a shallow version of You Only Look Once version 3 (YOLOv3-tiny) widened by Inception blocks is designed to extract local and global features from input video streams. Here, the widened multi-UAV detection network architecture is termed as FastUAV-NET and aims to improve UAV detection accuracy while preserving computing time of one-step deep detection algorithms in the context of UAV-UAV tracking. To detect UAVs, the FastUAV-NET architecture uses five inception units and adopts a feature pyramid network to detect UAVs. To obtain a high frame rate, the proposed method is applied to every nth frame and then the detected UAVs are tracked in intermediate frames using scalable Kernel Correlation Filter algorithm. The results on the generated UAV-UAV dataset illustrate that the proposed framework obtains 0.7916 average precision with 29 FPS performance on Jetson-TX2. The results imply that the widening of CNN network is a much more effective way than increasing the depth of CNN and leading to a good trade-off between accurate detection and real-time performance. The FastUAV-NET model will be publicly available to the research community to further advance multi-UAV-UAV detection algorithms.},
DOI = {10.3390/electronics10060724}
}



@Article{s21062153,
AUTHOR = {Hou, Yuewu and Liu, Zhaoying and Zhang, Ting and Li, Yujian},
TITLE = {C-UNet: Complement UNet for Remote Sensing Road Extraction},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2153},
URL = {https://www.mdpi.com/1424-8220/21/6/2153},
PubMedID = {33808588},
ISSN = {1424-8220},
ABSTRACT = {Roads are important mode of transportation, which are very convenient for people’s daily work and life. However, it is challenging to accuratly extract road information from a high-resolution remote sensing image. This paper presents a road extraction method for remote sensing images with a complement UNet (C-UNet). C-UNet contains four modules. Firstly, the standard UNet is used to roughly extract road information from remote sensing images, getting the first segmentation result; secondly, a fixed threshold is utilized to erase partial extracted information; thirdly, a multi-scale dense dilated convolution UNet (MD-UNet) is introduced to discover the complement road areas in the erased masks, obtaining the second segmentation result; and, finally, we fuse the extraction results of the first and the third modules, getting the final segmentation results. Experimental results on the Massachusetts Road dataset indicate that our C-UNet gets the higher results than the state-of-the-art methods, demonstrating its effectiveness.},
DOI = {10.3390/s21062153}
}



@Article{rs13061172,
AUTHOR = {Chen, De-Yue and Peng, Ling and Li, Wei-Chao and Wang, Yin-Da},
TITLE = {Building Extraction and Number Statistics in WUI Areas Based on UNet Structure and Ensemble Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1172},
URL = {https://www.mdpi.com/2072-4292/13/6/1172},
ISSN = {2072-4292},
ABSTRACT = {Following the advancement and progression of urbanization, management problems of the wildland&ndash;urban interface (WUI) have become increasingly serious. WUI regional governance issues involve many factors including climate, humanities, etc., and have attracted attention and research from all walks of life. Building research plays a vital part in the WUI area. Building location is closely related with the planning and management of the WUI area, and the number of buildings is related to the rescue arrangement. There are two major methods to obtain this building information: one is to obtain them from relevant agencies, which is slow and lacks timeliness, while the other approach is to extract them from high-resolution remote sensing images, which is relatively inexpensive and offers improved timeliness. Inspired by the recent successful application of deep learning, in this paper, we propose a method for extracting building information from high-resolution remote sensing images based on deep learning, which is combined with ensemble learning to extract the building location. Further, we use the idea of image anomaly detection to estimate the number of buildings. After verification on two datasets, we obtain superior semantic segmentation results and achieve better building contour extraction and number estimation.},
DOI = {10.3390/rs13061172}
}



@Article{rs13061176,
AUTHOR = {Zhang, Cheng and Jiang, Wanshou and Zhao, Qing},
TITLE = {Semantic Segmentation of Aerial Imagery via Split-Attention Networks with Disentangled Nonlocal and Edge Supervision},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1176},
URL = {https://www.mdpi.com/2072-4292/13/6/1176},
ISSN = {2072-4292},
ABSTRACT = {In this work, we propose a new deep convolution neural network (DCNN) architecture for semantic segmentation of aerial imagery. Taking advantage of recent research, we use split-attention networks (ResNeSt) as the backbone for high-quality feature expression. Additionally, a disentangled nonlocal (DNL) block is integrated into our pipeline to express the inter-pixel long-distance dependence and highlight the edge pixels simultaneously. Moreover, the depth-wise separable convolution and atrous spatial pyramid pooling (ASPP) modules are combined to extract and fuse multiscale contextual features. Finally, an auxiliary edge detection task is designed to provide edge constraints for semantic segmentation. Evaluation of algorithms is conducted on two benchmarks provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). Extensive experiments demonstrate the effectiveness of each module of our architecture. Precision evaluation based on the Potsdam benchmark shows that the proposed DCNN achieves competitive performance over the state-of-the-art methods.},
DOI = {10.3390/rs13061176}
}



@Article{rs13061184,
AUTHOR = {Geng, Xiaomeng and Shi, Lei and Yang, Jie and Li, Pingxiang and Zhao, Lingli and Sun, Weidong and Zhao, Jinqi},
TITLE = {Ship Detection and Feature Visualization Analysis Based on Lightweight CNN in VH and VV Polarization Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1184},
URL = {https://www.mdpi.com/2072-4292/13/6/1184},
ISSN = {2072-4292},
ABSTRACT = {Synthetic aperture radar (SAR) is a significant application in maritime monitoring, which can provide SAR data throughout the day and in all weather conditions. With the development of artificial intelligence and big data technologies, the data-driven convolutional neural network (CNN) has become widely used in ship detection. However, the accuracy, feature visualization, and analysis of ship detection need to be improved further, when the CNN method is used. In this letter, we propose a two-stage ship detection for land-contained sea area without a traditional sea-land segmentation process. First, to decrease the possibly existing false alarms from the island, an island filter is used as the first step, and then threshold segmentation is used to quickly perform candidate detection. Second, a two-layer lightweight CNN model-based classifier is built to separate false alarms from the ship object. Finally, we discuss the CNN interpretation and visualize in detail when the ship is predicted in vertical–horizontal (VH) and vertical–vertical (VV) polarization. Experiments demonstrate that the proposed method can reach an accuracy of 99.4% and an F1 score of 0.99 based on the Sentinel-1 images for a ship with a size of less than 32 × 32.},
DOI = {10.3390/rs13061184}
}



@Article{land10030321,
AUTHOR = {Akumu, Clement E. and Amadi, Eze O. and Dennis, Samuel},
TITLE = {Application of Drone and WorldView-4 Satellite Data in Mapping and Monitoring Grazing Land Cover and Pasture Quality: Pre- and Post-Flooding},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {321},
URL = {https://www.mdpi.com/2073-445X/10/3/321},
ISSN = {2073-445X},
ABSTRACT = {Frequent flooding worldwide, especially in grazing environments, requires mapping and monitoring grazing land cover and pasture quality to support land management. Although drones, satellite, and machine learning technologies can be used to map land cover and pasture quality, there have been limited applications in grazing land environments, especially monitoring land cover change and pasture quality pre- and post-flood events. The use of high spatial resolution drone and satellite data such as WorldView-4 can provide effective mapping and monitoring in grazing land environments. The aim of this study was to utilize high spatial resolution drone and WorldView-4 satellite data to map and monitor grazing land cover change and pasture quality pre-and post-flooding. The grazing land cover was mapped pre-flooding using WorldView-4 satellite data and post-flooding using real-time drone data. The machine learning Random Forest classification algorithm was used to delineate land cover types and the normalized difference vegetation index (NDVI) was used to monitor pasture quality. This study found a seven percent (7%) increase in pasture cover and a one hundred percent (100%) increase in pasture quality post-flooding. The drone and WorldView-4 satellite data were useful to detect grazing land cover change at a finer scale.},
DOI = {10.3390/land10030321}
}



@Article{s21062180,
AUTHOR = {Liu, Chang and Szirányi, Tamás},
TITLE = {Real-Time Human Detection and Gesture Recognition for On-Board UAV Rescue},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2180},
URL = {https://www.mdpi.com/1424-8220/21/6/2180},
PubMedID = {33804718},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play an important role in numerous technical and scientific fields, especially in wilderness rescue. This paper carries out work on real-time UAV human detection and recognition of body and hand rescue gestures. We use body-featuring solutions to establish biometric communications, like yolo3-tiny for human detection. When the presence of a person is detected, the system will enter the gesture recognition phase, where the user and the drone can communicate briefly and effectively, avoiding the drawbacks of speech communication. A data-set of ten body rescue gestures (i.e., Kick, Punch, Squat, Stand, Attention, Cancel, Walk, Sit, Direction, and PhoneCall) has been created by a UAV on-board camera. The two most important gestures are the novel dynamic Attention and Cancel which represent the set and reset functions respectively. When the rescue gesture of the human body is recognized as Attention, the drone will gradually approach the user with a larger resolution for hand gesture recognition. The system achieves 99.80% accuracy on testing data in body gesture data-set and 94.71% accuracy on testing data in hand gesture data-set by using the deep learning method. Experiments conducted on real-time UAV cameras confirm our solution can achieve our expected UAV rescue purpose.},
DOI = {10.3390/s21062180}
}



@Article{rs13061198,
AUTHOR = {Liu, Bi-Yuan and Chen, Huai-Xin and Huang, Zhou and Liu, Xing and Yang, Yun-Zhi},
TITLE = {ZoomInNet: A Novel Small Object Detector in Drone Images with Cross-Scale Knowledge Distillation},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1198},
URL = {https://www.mdpi.com/2072-4292/13/6/1198},
ISSN = {2072-4292},
ABSTRACT = {Drone-based object detection has been widely applied in ground object surveillance, urban patrol, and some other fields. However, the dramatic scale changes and complex backgrounds of drone images usually result in weak feature representation of small objects, which makes it challenging to achieve high-precision object detection. Aiming to improve small objects detection, this paper proposes a novel cross-scale knowledge distillation (CSKD) method, which enhances the features of small objects in a manner similar to image enlargement, so it is termed as ZoomInNet. First, based on an efficient feature pyramid network structure, the teacher and student network are trained with images in different scales to introduce the cross-scale feature. Then, the proposed layer adaption (LA) and feature level alignment (FA) mechanisms are applied to align the feature size of the two models. After that, the adaptive key distillation point (AKDP) algorithm is used to get the crucial positions in feature maps that need knowledge distillation. Finally, the position-aware L2 loss is used to measure the difference between feature maps from cross-scale models, realizing the cross-scale information compression in a single model. Experiments on the challenging Visdrone2018 dataset show that the proposed method draws on the advantages of the image pyramid methods, while avoids the large calculation of them and significantly improves the detection accuracy of small objects. Simultaneously, the comparison with mainstream methods proves that our method has the best performance in small object detection.},
DOI = {10.3390/rs13061198}
}



@Article{app11062797,
AUTHOR = {Muñoz, Filiberto and Cervantes-Rojas, Jorge S. and Valdovinos, Jose M. and Sandre-Hernández, Omar and Salazar, Sergio and Romero, Hugo},
TITLE = {Dynamic Neural Network-Based Adaptive Tracking Control for an Autonomous Underwater Vehicle Subject to Modeling and Parametric Uncertainties},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2797},
URL = {https://www.mdpi.com/2076-3417/11/6/2797},
ISSN = {2076-3417},
ABSTRACT = {This research presents a way to improve the autonomous maneuvering capability of a four-degrees-of-freedom (4DOF) autonomous underwater vehicle (AUV) to perform trajectory tracking tasks in a disturbed underwater environment. This study considers four second-order input-affine nonlinear equations for the translational (x,y,z) and rotational (heading) dynamics of a real AUV subject to hydrodynamic parameter uncertainties (added mass and damping coefficients), unknown damping dynamics, and external disturbances. We proposed an identification-control scheme for each dynamic named Dynamic Neural Control System (DNCS) as a combination of an adaptive neural controller based on nonparametric identification of the effect of unknown dynamics and external disturbances, and on parametric estimation of the added mass dependent input gain. Several numerical simulations validate the satisfactory performance of the proposed DNCS tracking reference trajectories in comparison with a conventional feedback controller with no adaptive compensation. Some graphics showing dynamic approximation of the lumped disturbance as well as estimation of the parametric uncertainty are depicted, validating effective operation of the proposed DNCS when the system is almost completely unknown.},
DOI = {10.3390/app11062797}
}



@Article{s21062208,
AUTHOR = {Park, Kyung Ho and Park, Eunji and Kim, Huy Kang},
TITLE = {Unsupervised Fault Detection on Unmanned Aerial Vehicles: Encoding and Thresholding Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2208},
URL = {https://www.mdpi.com/1424-8220/21/6/2208},
PubMedID = {33809830},
ISSN = {1424-8220},
ABSTRACT = {Unmanned Aerial Vehicles are expected to create enormous benefits to society, but there are safety concerns in recognizing faults at the vehicle’s control component. Prior studies proposed various fault detection approaches leveraging heuristics-based rules and supervised learning-based models, but there were several drawbacks. The rule-based approaches required an engineer to update the rules on every type of fault, and the supervised learning-based approaches necessitated the acquisition of a finely-labeled training dataset. Moreover, both prior approaches commonly include a limit that the detection model can identify the trained type of faults only, but fail to recognize the unseen type of faults. In pursuit of resolving the aforementioned drawbacks, we proposed a fault detection model utilizing a stacked autoencoder that lies under unsupervised learning. The autoencoder was trained with data from safe UAV states, and its reconstruction loss was examined to distinguish the safe states and faulty states. The key contributions of our study are, as follows. First, we presented a series of analyses to extract essential features from raw UAV flight logs. Second, we designed a fault detection model consisting of the stacked autoencoder and the classifier. Lastly, we validated our approach’s fault detection performance with two datasets consisting of different types of UAV faults.},
DOI = {10.3390/s21062208}
}



@Article{electronics10060747,
AUTHOR = {Passafiume, Marco and Rojhani, Neda and Collodi, Giovanni and Cidronali, Alessandro},
TITLE = {Modeling Small UAV Micro-Doppler Signature Using Millimeter-Wave FMCW Radar},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {747},
URL = {https://www.mdpi.com/2079-9292/10/6/747},
ISSN = {2079-9292},
ABSTRACT = {With the increase in small unmanned aerial vehicle (UAV) applications in several technology areas, detection and small UAVs classification have become of interest. To cope with small radar cross-sections (RCSs), slow-flying speeds, and low flying altitudes, the micro-Doppler signature provides some of the most distinctive information to identify and classify targets in many radar systems. In this paper, we introduce an effective model for the micro-Doppler effect that is suitable for frequency-modulated continuous-wave (FMCW) radar applications, and exploit it to investigate UAV signatures. The latter depends on the number of UAV motors, which are considered vibrational sources, and their rotation speed. To demonstrate the reliability of the proposed model, it is used to build simulated FMCW radar images, which are compared with experimental data acquired by a 77 GHz FMCW multiple-input multiple-output (MIMO) cost-effective automotive radar platform. The experimental results confirm the model’s ability to estimate the class of the UAV, namely its number of motors, in different operative scenarios. In addition, the experimental results show that the motors rotation speed does not imprint a significant signature on the classification of the UAV; thus, the estimation of the number of motors represents the only viable parameter for small UAV classification using the micro-Doppler effect.},
DOI = {10.3390/electronics10060747}
}



@Article{s21062212,
AUTHOR = {Brunete, Alberto and Gambao, Ernesto and Hernando, Miguel and Cedazo, Raquel},
TITLE = {Smart Assistive Architecture for the Integration of IoT Devices, Robotic Systems, and Multimodal Interfaces in Healthcare Environments},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2212},
URL = {https://www.mdpi.com/1424-8220/21/6/2212},
PubMedID = {33809884},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a new architecture that integrates Internet of Things (IoT) devices, service robots, and users in a smart assistive environment. A new intuitive and multimodal interaction system supporting people with disabilities and bedbound patients is presented. This interaction system allows the user to control service robots and devices inside the room in five different ways: touch control, eye control, gesture control, voice control, and augmented reality control. The interaction system is comprised of an assistive robotic arm holding a tablet PC. The robotic arm can place the tablet PC in front of the user. A demonstration of the developed technology, a prototype of a smart room equipped with home automation devices, and the robotic assistive arm are presented. The results obtained from the use of the various interfaces and technologies are presented in the article. The results include user preference with regard to eye-base control (performing clicks, and using winks or gaze) and the use of mobile phones over augmented reality glasses, among others.},
DOI = {10.3390/s21062212}
}



@Article{rs13061205,
AUTHOR = {Zhao, Caidan and Luo, Gege and Wang, Yilin and Chen, Caiyun and Wu, Zhiqiang},
TITLE = {UAV Recognition Based on Micro-Doppler Dynamic Attribute-Guided Augmentation Algorithm},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1205},
URL = {https://www.mdpi.com/2072-4292/13/6/1205},
ISSN = {2072-4292},
ABSTRACT = {A micro-Doppler signature (m-DS) based on the rotation of drone blades is an effective way to detect and identify small drones. Deep-learning-based recognition algorithms can achieve higher recognition performance, but they needs a large amount of sample data to train models. In addition to the hovering state, the signal samples of small unmanned aerial vehicles (UAVs) should also include flight dynamics, such as vertical, pitch, forward and backward, roll, lateral, and yaw. However, it is difficult to collect all dynamic UAV signal samples under actual flight conditions, and these dynamic flight characteristics will lead to the deviation of the original features, thus affecting the performance of the recognizer. In this paper, we propose a small UAV m-DS recognition algorithm based on dynamic feature enhancement. We extract the combined principal component analysis and discrete wavelet transform (PCA-DWT) time–frequency characteristics and texture features of the UAV’s micro-Doppler signal and use a dynamic attribute-guided augmentation (DAGA) algorithm to expand the feature domain for model training to achieve an adaptive, accurate, and efficient multiclass recognition model in complex environments. After the training model is stable, the average recognition accuracy rate can reach 98% during dynamic flight.},
DOI = {10.3390/rs13061205}
}



@Article{rs13061211,
AUTHOR = {Fan, Pan and Lang, Guodong and Yan, Bin and Lei, Xiaoyan and Guo, Pengju and Liu, Zhijie and Yang, Fuzeng},
TITLE = {A Method of Segmenting Apples Based on Gray-Centered RGB Color Space},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1211},
URL = {https://www.mdpi.com/2072-4292/13/6/1211},
ISSN = {2072-4292},
ABSTRACT = {In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. The rapid and accurate identification of apple targets in an illuminated and unstructured natural orchard is still a key challenge for the picking robot’s vision system. In this paper, by combining local image features and color information, we propose a pixel patch segmentation method based on gray-centered red–green–blue (RGB) color space to address this issue. Different from the existing methods, this method presents a novel color feature selection method that accounts for the influence of illumination and shadow in apple images. By exploring both color features and local variation in apple images, the proposed method could effectively distinguish the apple fruit pixels from other pixels. Compared with the classical segmentation methods and conventional clustering algorithms as well as the popular deep-learning segmentation algorithms, the proposed method can segment apple images more accurately and effectively. The proposed method was tested on 180 apple images. It offered an average accuracy rate of 99.26%, recall rate of 98.69%, false positive rate of 0.06%, and false negative rate of 1.44%. Experimental results demonstrate the outstanding performance of the proposed method.},
DOI = {10.3390/rs13061211}
}



@Article{s21062233,
AUTHOR = {Li, Ke and Zhang, Kun and Zhang, Zhenchong and Liu, Zekun and Hua, Shuai and He, Jianliang},
TITLE = {A UAV Maneuver Decision-Making Algorithm for Autonomous Airdrop Based on Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2233},
URL = {https://www.mdpi.com/1424-8220/21/6/2233},
PubMedID = {33806886},
ISSN = {1424-8220},
ABSTRACT = {How to operate an unmanned aerial vehicle (UAV) safely and efficiently in an interactive environment is challenging. A large amount of research has been devoted to improve the intelligence of a UAV while performing a mission, where finding an optimal maneuver decision-making policy of the UAV has become one of the key issues when we attempt to enable the UAV autonomy. In this paper, we propose a maneuver decision-making algorithm based on deep reinforcement learning, which generates efficient maneuvers for a UAV agent to execute the airdrop mission autonomously in an interactive environment. Particularly, the training set of the learning algorithm by the Prioritized Experience Replay is constructed, that can accelerate the convergence speed of decision network training in the algorithm. It is shown that a desirable and effective maneuver decision-making policy can be found by extensive experimental results.},
DOI = {10.3390/s21062233}
}



@Article{w13060875,
AUTHOR = {Acosta-Morel, Montserrat and McNulty, Valerie Pietsch and Lummen, Natainia and Schill, Steven R. and Beck, Michael W.},
TITLE = {Shoreline Solutions: Guiding Efficient Data Selection for Coastal Risk Modeling and the Design of Adaptation Interventions},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {875},
URL = {https://www.mdpi.com/2073-4441/13/6/875},
ISSN = {2073-4441},
ABSTRACT = {The Caribbean is affected by climate change due to an increase in the variability, frequency, and intensity of extreme weather events. When coupled with sea level rise (SLR), poor urban development design, and loss of habitats, severe flooding often impacts the coastal zone. In order to protect citizens and adapt to a changing climate, national and local governments need to investigate their coastal vulnerability and climate change risks. To assess flood and inundation risk, some of the critical data are topography, bathymetry, and socio-economic. We review the datasets available for these parameters in Jamaica (and specifically Old Harbour Bay) and assess their pros and cons in terms of resolution and costs. We then examine how their use can affect the evaluation of the number of people and the value of infrastructure flooded in a typical sea level rise/flooding assessment. We find that there can be more than a three-fold difference in the estimate of people and property flooded under 3m SLR. We present an inventory of available environmental and economic datasets for modeling storm surge/SLR impacts and ecosystem-based coastal protection benefits at varying scales. We emphasize the importance of the careful selection of the appropriately scaled data for use in models that will inform climate adaptation planning, especially when considering sea level rise, in the coastal zone. Without a proper understanding of data needs and limitations, project developers and decision-makers overvalue investments in adaptation science which do not necessarily translate into effective adaptation implementation. Applying these datasets to estimate sea level rise and storm surge in an adaptation project in Jamaica, we found that less costly and lower resolution data and models provide up to three times lower coastal risk estimates than more expensive data and models, indicating that investments in better resolution digital elevation mapping (DEM) data are needed for targeted local-level decisions. However, we also identify that, with this general rule of thumb in mind, cost-effective, national data can be used by planners in the absence of high-resolution data to support adaptation action planning, possibly saving critical climate adaptation budgets for project implementation.},
DOI = {10.3390/w13060875}
}



@Article{rs13061224,
AUTHOR = {Azpiroz, Izar and Oses, Noelia and Quartulli, Marco and Olaizola, Igor G. and Guidotti, Diego and Marchi, Susanna},
TITLE = {Comparison of Climate Reanalysis and Remote-Sensing Data for Predicting Olive Phenology through Machine-Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {1224},
URL = {https://www.mdpi.com/2072-4292/13/6/1224},
ISSN = {2072-4292},
ABSTRACT = {Machine-learning algorithms used for modelling olive-tree phenology generally and largely rely on temperature data. In this study, we developed a prediction model on the basis of climate data and geophysical information. Remote measurements of weather conditions, terrain slope, and surface spectral reflectance were considered for this purpose. The accuracy of the temperature data worsened when replacing weather-station measurements with remote-sensing records, though the addition of more complete environmental data resulted in an efficient prediction model of olive-tree phenology. Filtering and embedded feature-selection techniques were employed to analyze the impact of variables on olive-tree phenology prediction, facilitating the inclusion of measurable information in decision support frameworks for the sustainable management of olive-tree systems.},
DOI = {10.3390/rs13061224}
}



@Article{rs13071238,
AUTHOR = {Kaivosoja, Jere and Hautsalo, Juho and Heikkinen, Jaakko and Hiltunen, Lea and Ruuttunen, Pentti and Näsi, Roope and Niemeläinen, Oiva and Lemsalu, Madis and Honkavaara, Eija and Salonen, Jukka},
TITLE = {Reference Measurements in Developing UAV Systems for Detecting Pests, Weeds, and Diseases},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1238},
URL = {https://www.mdpi.com/2072-4292/13/7/1238},
ISSN = {2072-4292},
ABSTRACT = {The development of UAV (unmanned aerial vehicle) imaging technologies for precision farming applications is rapid, and new studies are published frequently. In cases where measurements are based on aerial imaging, there is the need to have ground truth or reference data in order to develop reliable applications. However, in several precision farming use cases such as pests, weeds, and diseases detection, the reference data can be subjective or relatively difficult to capture. Furthermore, the collection of reference data is usually laborious and time consuming. It also appears that it is difficult to develop generalisable solutions for these areas. This review studies previous research related to pests, weeds, and diseases detection and mapping using UAV imaging in the precision farming context, underpinning the applied reference measurement techniques. The majority of the reviewed studies utilised subjective visual observations of UAV images, and only a few applied in situ measurements. The conclusion of the review is that there is a lack of quantitative and repeatable reference data measurement solutions in the areas of mapping pests, weeds, and diseases. In addition, the results that the studies present should be reflected in the applied references. An option in the future approach could be the use of synthetic data as reference.},
DOI = {10.3390/rs13071238}
}



@Article{electronics10070767,
AUTHOR = {Lee, Taekgyu and Kang, Yeonsik},
TITLE = {Performance Analysis of Deep Neural Network Controller for Autonomous Driving Learning from a Nonlinear Model Predictive Control Method},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {767},
URL = {https://www.mdpi.com/2079-9292/10/7/767},
ISSN = {2079-9292},
ABSTRACT = {Nonlinear model predictive control (NMPC) is based on a numerical optimization method considering the target system dynamics as constraints. This optimization process requires large amount of computation power and the computation time is often unpredictable which may cause the control update rate to overrun. Therefore, the performance must be carefully balanced against the computational time. To solve the computation problem, we propose a data-based control technique based on a deep neural network (DNN). The DNN is trained with closed-loop driving data of an NMPC. The proposed "DNN control technique based on NMPC driving data" achieves control characteristics comparable to those of a well-tuned NMPC within a reasonable computation period, which is verified with an experimental scaled-car platform and realistic numerical simulations.},
DOI = {10.3390/electronics10070767}
}



@Article{robotics10020052,
AUTHOR = {Oliveira, Luiz F. P. and Moreira, António P. and Silva, Manuel F.},
TITLE = {Advances in Agriculture Robotics: A State-of-the-Art Review and Challenges Ahead},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2218-6581/10/2/52},
ISSN = {2218-6581},
ABSTRACT = {The constant advances in agricultural robotics aim to overcome the challenges imposed by population growth, accelerated urbanization, high competitiveness of high-quality products, environmental preservation and a lack of qualified labor. In this sense, this review paper surveys the main existing applications of agricultural robotic systems for the execution of land preparation before planting, sowing, planting, plant treatment, harvesting, yield estimation and phenotyping. In general, all robots were evaluated according to the following criteria: its locomotion system, what is the final application, if it has sensors, robotic arm and/or computer vision algorithm, what is its development stage and which country and continent they belong. After evaluating all similar characteristics, to expose the research trends, common pitfalls and the characteristics that hinder commercial development, and discover which countries are investing into Research and Development (R&amp;D) in these technologies for the future, four major areas that need future research work for enhancing the state of the art in smart agriculture were highlighted: locomotion systems, sensors, computer vision algorithms and communication technologies. The results of this research suggest that the investment in agricultural robotic systems allows to achieve short—harvest monitoring—and long-term objectives—yield estimation.},
DOI = {10.3390/robotics10020052}
}



@Article{robotics10020053,
AUTHOR = {Oliveira, Luiz F. P. and Moreira, António P. and Silva, Manuel F.},
TITLE = {Advances in Forest Robotics: A State-of-the-Art Survey},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {53},
URL = {https://www.mdpi.com/2218-6581/10/2/53},
ISSN = {2218-6581},
ABSTRACT = {The development of robotic systems to operate in forest environments is of great relevance for the public and private sectors. In this sense, this article reviews several scientific papers, research projects and commercial products related to robotic applications for environmental preservation, monitoring, wildfire firefighting, inventory operations, planting, pruning and harvesting. After conducting critical analysis, the main characteristics observed were: (a) the locomotion system is directly affected by the type of environmental monitoring to be performed; (b) different reasons for pruning result in different locomotion and cutting systems; (c) each type of forest, in each season and each type of soil can directly interfere with the navigation technique used; and (d) the integration of the concept of swarm of robots with robots of different types of locomotion systems (land, air or sea) can compensate for the time of executing tasks in unstructured environments. Two major areas are proposed for future research works: Internet of Things (IoT)-based smart forest and navigation systems. It is expected that, with the various characteristics exposed in this paper, the current robotic forest systems will be improved, so that forest exploitation becomes more efficient and sustainable.},
DOI = {10.3390/robotics10020053}
}



@Article{electronics10070771,
AUTHOR = {Liu, Chuanyang and Wu, Yiquan and Liu, Jingjing and Sun, Zuo},
TITLE = {Improved YOLOv3 Network for Insulator Detection in Aerial Images with Diverse Background Interference},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {771},
URL = {https://www.mdpi.com/2079-9292/10/7/771},
ISSN = {2079-9292},
ABSTRACT = {Automatic inspection of insulators from high-voltage transmission lines is of paramount importance to the safety and reliable operation of the power grid. Due to different size insulators and the complex background of aerial images, it is a difficult task to recognize insulators in aerial views. Most of the traditional image processing methods and machine learning methods cannot achieve sufficient performance for insulator detection when diverse background interference is present. In this study, a deep learning method—based on You Only Look Once (YOLO)—will be proposed, capable of detecting insulators from aerial images with complex backgrounds. Firstly, aerial images with common aerial scenes were collected by Unmanned Aerial Vehicle (UAV), and a novel insulator dataset was constructed. Secondly, to enhance feature reuse and propagation, on the basis of YOLOv3 and Dense-Blocks, the YOLOv3-dense network was utilized for insulator detection. To improve detection accuracy for different sized insulators, a structure of multiscale feature fusion was adapted to the YOLOv3-dense network. To obtain abundant semantic information of upper and lower layers, multilevel feature mapping modules were employed across the YOLOv3-dense network. Finally, the YOLOv3-dense network and compared networks were trained and tested on the testing set. The average precision of YOLOv3-dense, YOLOv3, and YOLOv2 were 94.47%, 90.31%, and 83.43%, respectively. Experimental results and analysis validate the claim that the proposed YOLOv3-dense network achieves good performance in the detection of different size insulators amid diverse background interference.},
DOI = {10.3390/electronics10070771}
}



@Article{geomatics1020010,
AUTHOR = {Khedr, Maan and El-Sheimy, Naser},
TITLE = {S-PDR: SBAUPT-Based Pedestrian Dead Reckoning Algorithm for Free-Moving Handheld Devices},
JOURNAL = {Geomatics},
VOLUME = {1},
YEAR = {2021},
NUMBER = {2},
PAGES = {148--176},
URL = {https://www.mdpi.com/2673-7418/1/2/10},
ISSN = {2673-7418},
ABSTRACT = {Mobile location-based services (MLBS) are attracting attention for their potential public and personal use for a variety of applications such as location-based advertisement, smart shopping, smart cities, health applications, emergency response, and even gaming. Many of these applications rely on Inertial Navigation Systems (INS) due to the degraded GNSS services indoors. INS-based MLBS using smartphones is hindered by the quality of the MEMS sensors provided in smartphones which suffer from high noise and errors resulting in high drift in the navigation solution rapidly. Pedestrian dead reckoning (PDR) is an INS-based navigation technique that exploits human motion to reduce navigation solution errors, but the errors cannot be eliminated without aid from other techniques. The purpose of this study is to enhance and extend the short-term reliability of PDR systems for smartphones as a standalone system through an enhanced step detection algorithm, a periodic attitude correction technique, and a novel PCA-based motion direction estimation technique. Testing shows that the developed system (S-PDR) provides a reliable short-term navigation solution with a final positioning error that is up to 6 m after 3 min runtime. These results were compared to a PDR solution using an Xsens IMU which is known to be a high grade MEMS IMU and was found to be worse than S-PDR. The findings show that S-PDR can be used to aid GNSS in challenging environments and can be a viable option for short-term indoor navigation until aiding is provided by alternative means. Furthermore, the extended reliable solution of S-PDR can help reduce the operational complexity of aiding navigation systems such as RF-based indoor navigation and magnetic map matching as it reduces the frequency by which these aiding techniques are required and applied.},
DOI = {10.3390/geomatics1020010}
}



@Article{rs13071248,
AUTHOR = {Xu, Hao and Yao, Wei and Cheng, Li and Li, Bo},
TITLE = {Multiple Spectral Resolution 3D Convolutional Neural Network for Hyperspectral Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1248},
URL = {https://www.mdpi.com/2072-4292/13/7/1248},
ISSN = {2072-4292},
ABSTRACT = {In recent years, benefiting from the rapid development of deep learning technology in the field of computer vision, the study of hyperspectral image (HSI) classification has also made great progress. However, compared with ordinary RGB images, HSIs are more like 3D cubes; therefore, it is necessary and beneficial to explore classification methods suitable for the very special data structure of HSIs. In this paper, we propose Multiple Spectral Resolution 3D Convolutional Neural Network (MSR-3DCNN) for HSI classification tasks. In MSR-3DCNN, we expand the idea of multi-scale feature fusion and dilated convolution from the spatial dimension to the spectral dimension, and combine 3D convolution and residual connection; therefore, it can better adapt to the 3D cubic form of hyperspectral data and make efficient use of spectral information in different bands. Experimental results on four benchmark datasets show the effectiveness of the proposed approach and its superiority as compared with some state-of-the-art (SOTA) HSI classification methods.},
DOI = {10.3390/rs13071248}
}



@Article{agronomy11040621,
AUTHOR = {López-Andreu, Francisco Javier and Erena, Manuel and Dominguez-Gómez, Jose Antonio and López-Morales, Juan Antonio},
TITLE = {Sentinel-2 Images and Machine Learning as Tool for Monitoring of the Common Agricultural Policy: Calasparra Rice as a Case Study},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {621},
URL = {https://www.mdpi.com/2073-4395/11/4/621},
ISSN = {2073-4395},
ABSTRACT = {The European Commission introduces the Control by Monitoring through new technologies to manage Common Agricultural Policy funds through the Regulation 2018/746. The advances in remote sensing have been considered one of these new technologies, mainly since the European Space Agency designed the Copernicus Programme. The Sentinel-1 (radar range) and Sentinel-2 (optical range) satellites have been designed for monitoring agricultural problems based on the characteristics they provide. The data provided by the Sentinel 2 missions, together with the emergence of different scientific disciplines in artificial intelligence —especially machine learning— offer the perfect basis for identifying and classifying any crop and its phenological state. Our research is based on developing and evaluating a pixel-based supervised classification scheme to produce accurate rice crop mapping in a smallholder agricultural zone in Calasparra, Murcia, Spain. Several models are considered to obtain the most suitable model for each element of the time series used; pixel-based classification is performed and finished with a statistical treatment. The highly accurate results obtained, especially across the most significant vegetative development dates, indicate the benefits of using Sentinel-2 data combined with Machine Learning techniques to identify rice crops. It should be noted that it was possible to locate rice crop areas with an overall accuracy of 94% and standard deviation of 1%, which could be increased to 96% (±1%) if we focus on the months of the crop’s highest development state. Thanks to the proposed methodology, the on-site inspections carried out, 5% of the files, have been replaced by remote sensing evaluations of 100% of the analyzed season files. Besides, by adjusting the model input data, it is possible to detect unproductive or abandoned plots.},
DOI = {10.3390/agronomy11040621}
}



@Article{rs13071266,
AUTHOR = {Rudge, Mitchel L. M. and Levick, Shaun R. and Bartolo, Renee E. and Erskine, Peter D.},
TITLE = {Modelling the Diameter Distribution of Savanna Trees with Drone-Based LiDAR},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1266},
URL = {https://www.mdpi.com/2072-4292/13/7/1266},
ISSN = {2072-4292},
ABSTRACT = {The diameter distribution of savanna tree populations is a valuable indicator of savanna health because changes in the number and size of trees can signal a shift from savanna to grassland or forest. Savanna diameter distributions have traditionally been monitored with forestry techniques, where stem diameter at breast height (DBH) is measured in the field within defined sub-hectare plots. However, because the spatial scale of these plots is often misaligned with the scale of variability in tree populations, there is a need for techniques that can scale-up diameter distribution surveys. Dense point clouds collected from uncrewed aerial vehicle laser scanners (UAV-LS), also known as drone-based LiDAR (Light Detection and Ranging), can be segmented into individual tree crowns then related to stem diameter with the application of allometric scaling equations. Here, we sought to test the potential of UAV-LS tree segmentation and allometric scaling to model the diameter distributions of savanna trees. We collected both UAV-LS and field-survey data from five one-hectare savanna woodland plots in northern Australia, which were divided into two calibration and three validation plots. Within the two calibration plots, allometric scaling equations were developed by linking field-surveyed DBH to the tree metrics of manually delineated tree crowns, where the best performing model had a bias of 1.8% and the relatively high RMSE of 39.2%. A segmentation algorithm was then applied to segment individual tree crowns from UAV-LS derived point clouds, and individual tree level segmentation accuracy was assessed against the manually delineated crowns. 47% of crowns were accurately segmented within the calibration plots and 68% within the validation plots. Using the site-specific allometry, DBH was modelled from crown metrics within all five plots, and these modelled results were compared to field-surveyed diameter distributions. In all plots, there were significant differences between field-surveyed and UAV-LS modelled diameter distributions, which became similar at two of the plots when smaller trees (&lt;10 cm DBH) were excluded. Although the modelled diameter distributions followed the overall trend of field surveys, the non-significant result demonstrates a need for the adoption of remotely detectable proxies of tree size which could replace DBH, as well as more accurate tree detection and segmentation methods for savanna ecosystems.},
DOI = {10.3390/rs13071266}
}



@Article{rs13071277,
AUTHOR = {Khosravi, Vahid and Ardejani, Faramarz Doulati and Gholizadeh, Asa and Saberioon, Mohammadmehdi},
TITLE = {Satellite Imagery for Monitoring and Mapping Soil Chromium Pollution in a Mine Waste Dump},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1277},
URL = {https://www.mdpi.com/2072-4292/13/7/1277},
ISSN = {2072-4292},
ABSTRACT = {Weathering and oxidation of sulphide minerals in mine wastes release toxic elements in surrounding environments. As an alternative to traditional sampling and chemical analysis methods, the capability of proximal and remote sensing techniques was investigated in this study to predict Chromium (Cr) concentration in 120 soil samples collected from a dumpsite in Sarcheshmeh copper mine, Iran. The samples’ mineralogy and Cr concentration were determined and were then subjected to laboratory reflectance spectroscopy in the range of Visible–Near Infrared–Shortwave Infrared (VNIR–SWIR: 350–2500 nm). The raw spectra were pre-processed using Savitzky-Golay First-Derivative (SG-FD) and Savitzky-Golay Second-Derivative (SG-SD) algorithms. The important wavelengths were determined using Partial Least Squares Regression (PLSR) coefficients and Genetic Algorithm (GA). Artificial Neural Networks (ANN), Stepwise Multiple Linear Regression (SMLR) and PLSR data mining methods were applied to the selected spectral variables to assess Cr concentration. The developed models were then applied to the selected bands of Aster, Hyperion, Sentinel-2A, and Landsat 8-OLI satellite images of the area. Afterwards, rasters obtained from the best prediction model were segmented using a binary fitness function. According to the outputs of the laboratory reflectance spectroscopy, the highest prediction accuracy was obtained using ANN applied to the SD pre-processed spectra with R2 = 0.91, RMSE = 8.73 mg/kg and RPD = 2.76. SD-ANN also showed an acceptable performance on mapping the spatial distribution of Cr using the ordinary kriging technique. Using satellite images, SD-SMLR provided the best prediction models with R2 values of 0.61 and 0.53 for Hyperion and Sentinel-2A, respectively. This led to the higher visual similarity of the segmented Hyperion and Sentinel-2A images with the Cr distribution map. This study’s findings indicated that applying the best prediction models obtained by spectroscopy to the selected wavebands of Hyperion and Sentinel-2A satellite imagery could be considered a promising technique for rapid, cost-effective and eco-friendly assessment of Cr concentration in highly heterogeneous mining areas.},
DOI = {10.3390/rs13071277}
}



@Article{app11073002,
AUTHOR = {Henninger, Helen and Biggs, James and von Ellenrieder, Karl},
TITLE = {Safety-Aware Optimal Attitude Pointing for Low-Thrust Satellites},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3002},
URL = {https://www.mdpi.com/2076-3417/11/7/3002},
ISSN = {2076-3417},
ABSTRACT = {In geostationary orbit, long eclipses and the seasonal variations in the direction and intensity of the solar input can cause damage to sensitive equipment during attitude maneuvers, which may inadvertently point the equipment towards the Sun. The requirement that transmitting and receiving antennae remain pointed towards the Earth creates further restrictions to pointing directions. The aim of the study is to construct a novel geometric and reinforcement-learning-based method to determine attitude guidance maneuvers that maintain the equipment in safe and operational orientations throughout an attitude maneuver. The attitude trajectory is computed numerically using the geometric framing of Pontryagin’s maximum principle applied to the vehicle kinematics using the global matrix Lie group representation on SO(3), and the angular velocities are shaped using free parameters. The values of these free parameters are determined by a reinforcement learning algorithm to avoid the forbidden areas while maintaining the pointing in operational areas (modeled as subsets of the two-sphere of all possible pointing directions of a particular axis). The method is applied to a model geosynchronous satellite and demonstrated in a simulation.},
DOI = {10.3390/app11073002}
}



@Article{electronics10070798,
AUTHOR = {Sánchez, José David Vega and Urquiza-Aguiar, Luis and Paredes Paredes, Martha Cecilia},
TITLE = {Fading Channel Models for mm-Wave Communications},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {798},
URL = {https://www.mdpi.com/2079-9292/10/7/798},
ISSN = {2079-9292},
ABSTRACT = {A realistic performance assessment of any wireless communication system requires the use of a fading channel model that reflects its main characteristics. The traditional Rayleigh and Nakagami-m models have been (and still are) the basis of most theoretical research on wireless technologies today, even for emerging technologies, such as millimeter-wave communications (mm-Wave). In this article, we show that the fluctuating multiple-ray (FMR) and κ-μ shadowed models had a better fit (i.e., lowest mean square error statistical test) to field measurements in outdoor environments at 28 GHz than the conventional channel models. Therefore, these generalized models are feasible alternatives that can be used as a benchmark when evaluating communication performance in mm-Wave scenarios.},
DOI = {10.3390/electronics10070798}
}



@Article{s21072385,
AUTHOR = {Jasinski, Tomasz and Brooker, Graham and Antipov, Irina},
TITLE = {W-Band Multi-Aspect High Resolution Range Profile Radar Target Classification Using Support Vector Machines},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2385},
URL = {https://www.mdpi.com/1424-8220/21/7/2385},
PubMedID = {33808183},
ISSN = {1424-8220},
ABSTRACT = {Millimeter-wave (W-band) radar measurements were taken for two maritime targets instrumented with attitude and heading reference systems (AHRSs) in a littoral environment with the aim of developing a multiaspect classifier. The focus was on resource-limited implementations such as short-range, tactical, unmanned aircraft systems (UASs) and dealing with limited and imbalanced datasets. Radar imaging and preprocessing consisted of recording high-resolution range profiles (HRRPs) and performing range alignment using peak detection and fast Fourier transforms (FFTs). HRRPs were used because of their simplicity, reliability, and speed. The features used were fixed-length, frequency domain range profiles. Two linear support vector machine (SVM)-based classifiers were developed which both yielded excellent results in their general forms and were simple to implement. The first approach utilized the positive predictive value (PPV) and negative predictive value (NPV) statistics of the SVM directly to generate target probabilities and consequently determine the optimal aspect transitions for classification. The second approach used the Kolmogorov–Smirnov test for dimensionality reduction, followed by concatenating feature vectors across several aspects. The latter approach is particularly well-suited to resource-constrained scenarios, potentially allowing for retraining and updating in the field.},
DOI = {10.3390/s21072385}
}



@Article{rs13071311,
AUTHOR = {Xu, Danqing and Wu, Yiquan},
TITLE = {FE-YOLO: A Feature Enhancement Network for Remote Sensing Target Detection},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1311},
URL = {https://www.mdpi.com/2072-4292/13/7/1311},
ISSN = {2072-4292},
ABSTRACT = {In the past few decades, target detection from remote sensing images gained from aircraft or satellites has become one of the hottest topics. However, the existing algorithms are still limited by the detection of small remote sensing targets. Benefiting from the great development of computing power, deep learning has also made great breakthroughs. Due to a large number of small targets and complexity of background, the task of remote sensing target detection is still a challenge. In this work, we establish a series of feature enhancement modules for the network based on YOLO (You Only Look Once) -V3 to improve the performance of feature extraction. Therefore, we term our proposed network as FE-YOLO. In addition, to realize fast detection, the original Darknet-53 was simplified. Experimental results on remote sensing datasets show that our proposed FE-YOLO performs better than other state-of-the-art target detection models.},
DOI = {10.3390/rs13071311}
}



@Article{electronics10070820,
AUTHOR = {Ammar, Adel and Koubaa, Anis and Ahmed, Mohanned and Saad, Abdulrahman and Benjdira, Bilel},
TITLE = {Vehicle Detection from Aerial Images Using Deep Learning: A Comparative Study},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {820},
URL = {https://www.mdpi.com/2079-9292/10/7/820},
ISSN = {2079-9292},
ABSTRACT = {This paper addresses the problem of car detection from aerial images using Convolutional Neural Networks (CNNs). This problem presents additional challenges as compared to car (or any object) detection from ground images because the features of vehicles from aerial images are more difficult to discern. To investigate this issue, we assess the performance of three state-of-the-art CNN algorithms, namely Faster R-CNN, which is the most popular region-based algorithm, as well as YOLOv3 and YOLOv4, which are known to be the fastest detection algorithms. We analyze two datasets with different characteristics to check the impact of various factors, such as the UAV’s (unmanned aerial vehicle) altitude, camera resolution, and object size. A total of 52 training experiments were conducted to account for the effect of different hyperparameter values. The objective of this work is to conduct the most robust and exhaustive comparison between these three cutting-edge algorithms on the specific domain of aerial images. By using a variety of metrics, we show that the difference between YOLOv4 and YOLOv3 on the two datasets is statistically insignificant in terms of Average Precision (AP) (contrary to what was obtained on the COCO dataset). However, both of them yield markedly better performance than Faster R-CNN in most configurations. The only exception is that both of them exhibit a lower recall when object sizes and scales in the testing dataset differ largely from those in the training dataset.},
DOI = {10.3390/electronics10070820}
}



@Article{rs13071321,
AUTHOR = {Gong, Yiping and Zhang, Fan and Jia, Xiangyang and Huang, Xianfeng and Li, Deren and Mao, Zhu},
TITLE = {Deep Neural Networks for Quantitative Damage Evaluation of Building Losses Using Aerial Oblique Images: Case Study on the Great Wall (China)},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1321},
URL = {https://www.mdpi.com/2072-4292/13/7/1321},
ISSN = {2072-4292},
ABSTRACT = {Automated damage evaluation is of great importance in the maintenance and preservation of heritage structures. Damage investigation of large cultural buildings is time-consuming and labor-intensive, meaning that many buildings are not repaired in a timely manner. Additionally, some buildings in harsh environments are impossible to reach, increasing the difficulty of damage investigation. Oblique images facilitate damage detection in large buildings, yet quantitative damage information, such as area or volume, is difficult to generate. In this paper, we propose a method for quantitative damage evaluation of large heritage buildings in wild areas with repetitive structures based on drone images. Unlike existing methods that focus on building surfaces, we study the damage of building components and extract hidden linear symmetry information, which is useful for localizing missing parts in architectural restoration. First, we reconstruct a 3D mesh model based on the photogrammetric method using high-resolution oblique images captured by drone. Second, we extract 3D objects by applying advanced deep learning methods to the images and projecting the 2D object segmentation results to 3D mesh models. For accurate 2D object extraction, we propose an edge-enhanced method to improve the segmentation accuracy of object edges. 3D object fragments from multiple views are integrated to build complete individual objects according to the geometric features. Third, the damage condition of objects is estimated in 3D space by calculating the volume reduction. To obtain the damage condition of an entire building, we define the damage degree in three levels: no or slight damage, moderate damage and severe damage, and then collect statistics on the number of damaged objects at each level. Finally, through an analysis of the building structure, we extract the linear symmetry surface from the remaining damaged objects and use the symmetry surface to localize the positions of missing objects. This procedure was tested and validated in a case study (the Jiankou Great Wall in China). The experimental results show that in terms of segmentation accuracy, our method obtains results of 93.23% mAP and 84.21% mIoU on oblique images and 72.45% mIoU on the 3D mesh model. Moreover, the proposed method shows effectiveness in performing damage assessment of objects and missing part localization.},
DOI = {10.3390/rs13071321}
}



@Article{s21072401,
AUTHOR = {Mehmood, Yasir and Aslam, Jawad and Ullah, Nasim and Chowdhury, Md. Shahariar and Techato, Kuaanan and Alzaed, Ali Nasser},
TITLE = {Adaptive Robust Trajectory Tracking Control of Multiple Quad-Rotor UAVs with Parametric Uncertainties and Disturbances},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2401},
URL = {https://www.mdpi.com/1424-8220/21/7/2401},
PubMedID = {33807144},
ISSN = {1424-8220},
ABSTRACT = {Recently, formation flying of multiple unmanned aerial vehicles (UAVs) found numerous applications in various areas such as surveillance, industrial automation and disaster management. The accuracy and reliability for performing group tasks by multiple UAVs is highly dependent on the applied control strategy. The formation and trajectories of multiple UAVs are governed by two separate controllers, namely formation and trajectory tracking controllers respectively. In presence of environmental effects, disturbances due to wind and parametric uncertainties, the controller design process is a challenging task. This article proposes a robust adaptive formation and trajectory tacking control of multiple quad-rotor UAVs using super twisting sliding mode control method. In the proposed design, Lyapunov function-based adaptive disturbance estimators are used to compensate for the effects of external disturbances and parametric uncertainties. The stability of the proposed controllers is guaranteed using Lyapunov theorems. Two variants of the control schemes, namely fixed gain super twisting SMC (STSMC) and adaptive super twisting SMC (ASTSMC) are tested using numerical simulations performed in MATLAB/Simulink. From the results presented, it is verified that in presence of disturbances, the proposed ASTSMC controller exhibits enhanced robustness as compared to the fixed gain STSMC.},
DOI = {10.3390/s21072401}
}



@Article{rs13071327,
AUTHOR = {Tian, Ling and Cao, Yu and He, Bokun and Zhang, Yifan and He, Chu and Li, Deshi},
TITLE = {Image Enhancement Driven by Object Characteristics and Dense Feature Reuse Network for Ship Target Detection in Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1327},
URL = {https://www.mdpi.com/2072-4292/13/7/1327},
ISSN = {2072-4292},
ABSTRACT = {As the application scenarios of remote sensing imagery (RSI) become richer, the task of ship detection from an overhead perspective is of great significance. Compared with traditional methods, the use of deep learning ideas has more prospects. However, the Convolutional Neural Network (CNN) has poor resistance to sample differences in detection tasks, and the huge differences in the image environment, background, and quality of RSIs affect the performance for target detection tasks; on the other hand, upsampling or pooling operations result in the loss of detailed information in the features, and the CNN with outstanding results are often accompanied by a high computation and a large amount of memory storage. Considering the characteristics of ship targets in RSIs, this study proposes a detection framework combining an image enhancement module with a dense feature reuse module: (1) drawing on the ideas of the generative adversarial network (GAN), we designed an image enhancement module driven by object characteristics, which improves the quality of the ship target in the images while augmenting the training set; (2) the intensive feature extraction module was designed to integrate low-level location information and high-level semantic information of different resolutions while minimizing the computation, which can improve the efficiency of feature reuse in the network; (3) we introduced the receptive field expansion module to obtain a wider range of deep semantic information and enhance the ability to extract features of targets were at different sizes. Experiments were carried out on two types of ship datasets, optical RSI and Synthetic Aperture Radar (SAR) images. The proposed framework was implemented on classic detection networks such as You Only Look Once (YOLO) and Mask-RCNN. The experimental results verify the effectiveness of the proposed method.},
DOI = {10.3390/rs13071327}
}



@Article{electronics10070831,
AUTHOR = {Al-Darraji, Izzat and Piromalis, Dimitrios and Kakei, Ayad A. and Khan, Fazal Qudus and Stojmenovic, Milos and Tsaramirsis, Georgios and Papageorgas, Panagiotis G.},
TITLE = {Adaptive Robust Controller Design-Based RBF Neural Network for Aerial Robot Arm Model},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {831},
URL = {https://www.mdpi.com/2079-9292/10/7/831},
ISSN = {2079-9292},
ABSTRACT = {Aerial Robot Arms (ARAs) enable aerial drones to interact and influence objects in various environments. Traditional ARA controllers need the availability of a high-precision model to avoid high control chattering. Furthermore, in practical applications of aerial object manipulation, the payloads that ARAs can handle vary, depending on the nature of the task. The high uncertainties due to modeling errors and an unknown payload are inversely proportional to the stability of ARAs. To address the issue of stability, a new adaptive robust controller, based on the Radial Basis Function (RBF) neural network, is proposed. A three-tier approach is also followed. Firstly, a detailed new model for the ARA is derived using the Lagrange–d’Alembert principle. Secondly, an adaptive robust controller, based on a sliding mode, is designed to manipulate the problem of uncertainties, including modeling errors. Last, a higher stability controller, based on the RBF neural network, is implemented with the adaptive robust controller to stabilize the ARAs, avoiding modeling errors and unknown payload issues. The novelty of the proposed design is that it takes into account high nonlinearities, coupling control loops, high modeling errors, and disturbances due to payloads and environmental conditions. The model was evaluated by the simulation of a case study that includes the two proposed controllers and ARA trajectory tracking. The simulation results show the validation and notability of the presented control algorithm.},
DOI = {10.3390/electronics10070831}
}



@Article{electronics10070833,
AUTHOR = {Kang, Mi-Seon and Kim, Pyong-Kun and Lim, Kil-Taek and Cho, You-Ze},
TITLE = {Method for Obtaining Better Traffic Survey Data},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {833},
URL = {https://www.mdpi.com/2079-9292/10/7/833},
ISSN = {2079-9292},
ABSTRACT = {Road traffic surveys determine the number and type of vehicles passing by a specific point over a certain period of time. The manual estimation of the number and type of vehicles from images captured by a camera is the most commonly used method. However, this method has the disadvantage of requiring high amounts of manpower and cost. Recently, methods of automating traffic volume surveys using sensors or deep learning have been widely attempted, but there is the disadvantage that a person must finally manually verify the data in order to ensure that they are reliable. In order to address these shortcomings, we propose a method for efficiently conducting road traffic volume surveys and obtaining highly reliable data. The proposed method detects vehicles on the road from CCTV (Closed-circuit television) images and classifies vehicle types using deep learning or a similar method. After that, it automatically informs the user of candidates with a high probability of error and provides a method for efficient verification. The performance of the proposed method was tested using a data set collected by an actual road traffic survey company. As a result, we proved that our method shows better accuracy than the previous method. The proposed method can reduce the labor and cost in road traffic volume surveys, and increase the reliability of the data due to more accurate results.},
DOI = {10.3390/electronics10070833}
}



@Article{mi12040375,
AUTHOR = {Tokunaga, Shinya and Premachandra, Chinthaka and Premachandra, H. Waruna H. and Kawanaka, Hiroharu and Sumathipala, Sagara and Sudantha, B. S.},
TITLE = {Autonomous Spiral Motion by a Small-Type Robot on an Obstacle-Available Surface},
JOURNAL = {Micromachines},
VOLUME = {12},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {375},
URL = {https://www.mdpi.com/2072-666X/12/4/375},
ISSN = {2072-666X},
ABSTRACT = {Several robot-related studies have been conducted in recent years; however, studies on the autonomous travel of small mobile robots in small spaces are lacking. In this study, we investigate the development of autonomous travel for small robots that need to travel and cover the entire smooth surface, such as those employed for cleaning tables or solar panels. We consider an obstacle-available surface and target this travel on it by proposing a spiral motion method. To achieve the spiral motion, we focus on developing autonomous avoidance of obstacles, return to original path, and fall prevention when robots traverse a surface. The development of regular travel by a robot without an encoder is an important feature of this study. The traveled distance was measured using the traveling time. We achieved spiral motion by analyzing the data from multiple small sensors installed on the robot by introducing a new attitude-control method, and we ensured that the robot returned to the original spiral path autonomously after avoiding obstacles and without falling over the edge of the surface.},
DOI = {10.3390/mi12040375}
}



@Article{aerospace8040093,
AUTHOR = {Ribeiro, Marta and Ellerbroek, Joost and Hoekstra, Jacco},
TITLE = {Velocity Obstacle Based Conflict Avoidance in Urban Environment with Variable Speed Limit},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {93},
URL = {https://www.mdpi.com/2226-4310/8/4/93},
ISSN = {2226-4310},
ABSTRACT = {Current investigations into urban aerial mobility, as well as the continuing growth of global air transportation, have renewed interest in conflict detection and resolution (CD&amp;R) methods. The use of drones for applications such as package delivery, would result in traffic densities that are orders of magnitude higher than those currently observed in manned aviation. Such densities do not only make automated conflict detection and resolution a necessity, but will also force a re-evaluation of aspects such as coordination vs. priority, or state vs. intent. This paper looks into enabling a safe introduction of drones into urban airspace by setting travelling rules in the operating airspace which benefit tactical conflict resolution. First, conflicts resulting from changes of direction are added to conflict resolution with intent trajectory propagation. Second, the likelihood of aircraft with opposing headings meeting in conflict is reduced by separating traffic into different layers per heading–altitude rules. Guidelines are set in place to make sure aircraft respect the heading ranges allowed at every crossed layer. Finally, we use a reinforcement learning agent to implement variable speed limits towards creating a more homogeneous traffic situation between cruising and climbing/descending aircraft. The effects of all of these variables were tested through fast-time simulations on an open source airspace simulation platform. Results showed that we were able to improve the operational safety of several scenarios.},
DOI = {10.3390/aerospace8040093}
}



@Article{s21072416,
AUTHOR = {Wang, Fei and Liu, Zhendong and Zhu, Hongchun and Wu, Pengda and Li, Chengming},
TITLE = {An Improved Method for Stable Feature Points Selection in Structure-from-Motion Considering Image Semantic and Structural Characteristics},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2416},
URL = {https://www.mdpi.com/1424-8220/21/7/2416},
PubMedID = {33915845},
ISSN = {1424-8220},
ABSTRACT = {Feature matching plays a crucial role in the process of 3D reconstruction based on the structure from motion (SfM) technique. For a large collection of oblique images, feature matching is one of the most time-consuming steps, and the matching result directly affects the accuracy of subsequent tasks. Therefore, how to extract the reasonable feature points robustly and efficiently to improve the matching speed and quality has received extensive attention from scholars worldwide. Most studies perform quantitative feature point selection based on image Difference-of-Gaussian (DoG) pyramids in practice. However, the stability and spatial distribution of feature points are not considered enough, resulting in selected feature points that may not adequately reflect the scene structures and cannot guarantee the matching rate and the aerial triangulation accuracy. To address these issues, an improved method for stable feature point selection in SfM considering image semantic and structural characteristics is proposed. First, the visible-band difference vegetation index is used to identify the vegetation areas from oblique images, and the line feature in the image is extracted by the optimized line segment detector algorithm. Second, the feature point two-tuple classification model is established, in which the vegetation area recognition result is used as the semantic constraint, the line feature extraction result is used as the structural constraint, and the feature points are divided into three types. Finally, a progressive selection algorithm for feature points is proposed, in which feature points in the DoG pyramid are selected by classes and levels until the number of feature points is satisfied. Oblique images of a 40-km2 area in Dongying city, China, were used for validation. The experimental results show that compared to the state-of-the-art method, the method proposed in this paper not only effectively reduces the number of feature points but also better reflects the scene structure. At the same time, the average reprojection error of the aerial triangulation decrease by 20%, the feature point matching rate increase by 3%, the selected feature points are more stable and reasonable.},
DOI = {10.3390/s21072416}
}



@Article{app11073127,
AUTHOR = {Lerro, Angelo and Battipede, Manuela},
TITLE = {Safety Analysis of a Certifiable Air Data System Based on Synthetic Sensors for Flow Angle Estimation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3127},
URL = {https://www.mdpi.com/2076-3417/11/7/3127},
ISSN = {2076-3417},
ABSTRACT = {This work deals with the safety analysis of an air data system (ADS) partially based on synthetic sensors. The ADS is designed for the small aircraft transportation (SAT) community and is suitable for future unmanned aerial vehicles and urban air mobility applications. The ADS’s main innovation is based on estimation of the flow angles (angle-of-attack and angle-of-sideslip) using synthetic sensors instead of classical vanes (or sensors), whereas pressure and temperature are directly measured with Pitot and temperature probes. As the air data system is a safety-critical system, safety analyses are performed and the results are compared with the safety objectives required by the aircraft integrator. The present paper introduces the common aeronautical procedures for system safety assessment applied to a safety critical system partially based on synthetic sensors. The mean time between failures of ADS’s sub-parts are estimated on a statistical basis in order to evaluate the failure rate of the ADS’s functions. The proposed safety analysis is also useful in identifying the most critical air data system parts and sub-parts. Possible technological gaps to be filled to achieve the airworthiness safety objectives with nonredundant architectures are also identified.},
DOI = {10.3390/app11073127}
}



@Article{agronomy11040667,
AUTHOR = {Araújo, Sara Oleiro and Peres, Ricardo Silva and Barata, José and Lidon, Fernando and Ramalho, José Cochicho},
TITLE = {Characterising the Agriculture 4.0 Landscape—Emerging Trends, Challenges and Opportunities},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {667},
URL = {https://www.mdpi.com/2073-4395/11/4/667},
ISSN = {2073-4395},
ABSTRACT = {Investment in technological research is imperative to stimulate the development of sustainable solutions for the agricultural sector. Advances in Internet of Things, sensors and sensor networks, robotics, artificial intelligence, big data, cloud computing, etc. foster the transition towards the Agriculture 4.0 era. This fourth revolution is currently seen as a possible solution for improving agricultural growth, ensuring the future needs of the global population in a fair, resilient and sustainable way. In this context, this article aims at characterising the current Agriculture 4.0 landscape. Emerging trends were compiled using a semi-automated process by analysing relevant scientific publications published in the past ten years. Subsequently, a literature review focusing these trends was conducted, with a particular emphasis on their applications in real environments. From the results of the study, some challenges are discussed, as well as opportunities for future research. Finally, a high-level cloud-based IoT architecture is presented, serving as foundation for designing future smart agricultural systems. It is expected that this work will positively impact the research around Agriculture 4.0 systems, providing a clear characterisation of the concept along with guidelines to assist the actors in a successful transition towards the digitalisation of the sector.},
DOI = {10.3390/agronomy11040667}
}



@Article{rs13071341,
AUTHOR = {Appeltans, Simon and Pieters, Jan G. and Mouazen, Abdul M.},
TITLE = {Detection of Leek Rust Disease under Field Conditions Using Hyperspectral Proximal Sensing and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1341},
URL = {https://www.mdpi.com/2072-4292/13/7/1341},
ISSN = {2072-4292},
ABSTRACT = {Rust disease is an important problem for leek cultivation worldwide. It reduces market value and in extreme cases destroys the entire harvest. Farmers have to resort to periodical full-field fungicide applications to prevent the spread of disease, once every 1 to 5 weeks, depending on the cultivar and weather conditions. This implies an economic cost for the farmer and an environmental cost for society. Hyperspectral sensors have been extensively used to address this issue in research, but their application in the field has been limited to a relatively low number of crops, excluding leek, due to the high investment costs and complex data gathering and analysis associated with these sensors. To fill this gap, a methodology was developed for detecting leek rust disease using hyperspectral proximal sensing data combined with supervised machine learning. First, a hyperspectral library was constructed containing 43,416 spectra with a waveband range of 400–1000 nm, measured under field conditions. Then, an extensive evaluation of 11 common classifiers was performed using the scikit-learn machine learning library in Python, combined with a variety of wavelength selection techniques and preprocessing strategies. The best performing model was a (linear) logistic regression model that was able to correctly classify rust disease with an accuracy of 98.14%, using reflectance values at 556 and 661 nm, combined with the value of the first derivative at 511 nm. This model was used to classify unlabelled hyperspectral images, confirming that the model was able to accurately classify leek rust disease symptoms. It can be concluded that the results in this work are an important step towards the mapping of leek rust disease, and that future research is needed to overcome certain challenges before variable rate fungicide applications can be adopted against leek rust disease.},
DOI = {10.3390/rs13071341}
}



@Article{rs13071359,
AUTHOR = {Vélez-Nicolás, Mercedes and García-López, Santiago and Barbero, Luis and Ruiz-Ortiz, Verónica and Sánchez-Bellón, Ángel},
TITLE = {Applications of Unmanned Aerial Systems (UASs) in Hydrology: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1359},
URL = {https://www.mdpi.com/2072-4292/13/7/1359},
ISSN = {2072-4292},
ABSTRACT = {In less than two decades, UASs (unmanned aerial systems) have revolutionized the field of hydrology, bridging the gap between traditional satellite observations and ground-based measurements and allowing the limitations of manned aircraft to be overcome. With unparalleled spatial and temporal resolutions and product-tailoring possibilities, UAS are contributing to the acquisition of large volumes of data on water bodies, submerged parameters and their interactions in different hydrological contexts and in inaccessible or hazardous locations. This paper provides a comprehensive review of 122 works on the applications of UASs in surface water and groundwater research with a purpose-oriented approach. Concretely, the review addresses: (i) the current applications of UAS in surface and groundwater studies, (ii) the type of platforms and sensors mainly used in these tasks, (iii) types of products generated from UAS-borne data, (iv) the associated advantages and limitations, and (v) knowledge gaps and future prospects of UASs application in hydrology. The first aim of this review is to serve as a reference or introductory document for all researchers and water managers who are interested in embracing this novel technology. The second aim is to unify in a single document all the possibilities, potential approaches and results obtained by different authors through the implementation of UASs.},
DOI = {10.3390/rs13071359}
}



@Article{s21072447,
AUTHOR = {Park, Jonghyuk and Park, Jonghun and Shin, Dongmin and Choi, Yerim},
TITLE = {A BCI Based Alerting System for Attention Recovery of UAV Operators},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2447},
URL = {https://www.mdpi.com/1424-8220/21/7/2447},
PubMedID = {33918116},
ISSN = {1424-8220},
ABSTRACT = {As unmanned aerial vehicles have become popular, the number of accidents caused by an operator’s inattention have increased. To prevent such accidents, the operator should maintain an attention status. However, limited research has been conducted on the brain-computer interface (BCI)-based system with an alerting module for the operator’s attention recovery of unmanned aerial vehicles. Therefore, we introduce a detection and alerting system that prevents an unmanned aerial vehicle operator from falling into inattention status by using the operator’s electroencephalogram signal. The proposed system consists of the following three components: a signal processing module, which collects and preprocesses an electroencephalogram signal of an operator, an inattention detection module, which determines whether an inattention status occurred based on the preprocessed signal, and, lastly, an alert providing module that presents stimulus to an operator when inattention is detected. As a result of evaluating the performance with a real-world dataset, it was shown that the proposed system successfully contributed to the recovery of operator attention in the evaluating dataset, although statistical significance could not be established due to the small number of subjects.},
DOI = {10.3390/s21072447}
}



@Article{app11073179,
AUTHOR = {Coquet, Charles and Arnold, Andreas and Bouvet, Pierre-Jean},
TITLE = {Control of a Robotic Swarm Formation to Track a Dynamic Target with Communication Constraints: Analysis and Simulation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3179},
URL = {https://www.mdpi.com/2076-3417/11/7/3179},
ISSN = {2076-3417},
ABSTRACT = {We describe and analyze the Local Charged Particle Swarm Optimization (LCPSO) algorithm, that we designed to solve the problem of tracking a moving target releasing scalar information in a constrained environment using a swarm of agents. This method is inspired by flocking algorithms and the Particle Swarm Optimization (PSO) algorithm for function optimization. Four parameters drive LCPSO—the number of agents; the inertia weight; the attraction/repulsion weight; and the inter-agent distance. Using APF (Artificial Potential Field), we provide a mathematical analysis of the LCPSO algorithm under some simplifying assumptions. First, the swarm will aggregate and attain a stable formation, whatever the initial conditions. Second, the swarm moves thanks to an attractor in the swarm, which serves as a guide for the other agents to head for the target. By focusing on a simple application of target tracking with communication constraints, we then remove those assumptions one by one. We show the algorithm is resilient to constraints on the communication range and the behavior of the target. Results on simulation confirm our theoretical analysis. This provides useful guidelines to understand and control the LCPSO algorithm as a function of swarm characteristics as well as the nature of the target.},
DOI = {10.3390/app11073179}
}



@Article{rs13071371,
AUTHOR = {Wang, Junshu and Yang, Yue and Chen, Yuan and Han, Yuxing},
TITLE = {LighterGAN: An Illumination Enhancement Method for Urban UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1371},
URL = {https://www.mdpi.com/2072-4292/13/7/1371},
ISSN = {2072-4292},
ABSTRACT = {In unmanned aerial vehicle based urban observation and monitoring, the performance of computer vision algorithms is inevitably limited by the low illumination and light pollution caused degradation, therefore, the application image enhancement is a considerable prerequisite for the performance of subsequent image processing algorithms. Therefore, we proposed a deep learning and generative adversarial network based model for UAV low illumination image enhancement, named LighterGAN. The design of LighterGAN refers to the CycleGAN model with two improvements—attention mechanism and semantic consistency loss—having been proposed to the original structure. Additionally, an unpaired dataset that was captured by urban UAV aerial photography has been used to train this unsupervised learning model. Furthermore, in order to explore the advantages of the improvements, both the performance in the illumination enhancement task and the generalization ability improvement of LighterGAN were proven in the comparative experiments combining subjective and objective evaluations. In the experiments with five cutting edge image enhancement algorithms, in the test set, LighterGAN achieved the best results in both visual perception and PIQE (perception based image quality evaluator, a MATLAB build-in function, the lower the score, the higher the image quality) score of enhanced images, scores were 4.91 and 11.75 respectively, better than EnlightenGAN the state-of-the-art. In the enhancement of low illumination sub-dataset Y (containing 2000 images), LighterGAN also achieved the lowest PIQE score of 12.37, 2.85 points lower than second place. Moreover, compared with the CycleGAN, the improvement of generalization ability was also demonstrated. In the test set generated images, LighterGAN was 6.66 percent higher than CycleGAN in subjective authenticity assessment and 3.84 lower in PIQE score, meanwhile, in the whole dataset generated images, the PIQE score of LighterGAN is 11.67, 4.86 lower than CycleGAN.},
DOI = {10.3390/rs13071371}
}



@Article{app11073185,
AUTHOR = {Del Pozo, Susana and Sáez Blázquez, Cristina and Martín Nieto, Ignacio and Lagüela, Susana},
TITLE = {Integrated Approach for Detecting Convection Effects in Geothermal Environments Based on TIR Camera Measurements},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {3185},
URL = {https://www.mdpi.com/2076-3417/11/7/3185},
ISSN = {2076-3417},
ABSTRACT = {Thermal characterization of soils is essential for many applications, including design of geothermal systems. Traditional devices focus on the computation of thermal conductivity, omitting the analysis of the convection effect, which is important for horizontal geothermal systems. In this paper, a procedure based on the monitoring of the surface of the soil with a thermal infrared (TIR) camera is developed for the evaluation of the global thermal imbalance on the surface and in-depth. This procedure allows for the computation of thermal conductivity and global convection heat rate, consequently constituting a complete thermal characterization of the geothermal system. The validation of the results is performed through the evaluation of the radiometric calibration of the thermal infrared camera used for the monitoring and the comparison of the thermal conductivity values obtained in-depth, with traditional methods, and for the surface of the system.},
DOI = {10.3390/app11073185}
}



@Article{s21072505,
AUTHOR = {Wu, Rouwan and Xu, Zhiyong and Zhang, Jianlin and Zhang, Lihong},
TITLE = {Robust Global Motion Estimation for Video Stabilization Based on Improved K-Means Clustering and Superpixel},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2505},
URL = {https://www.mdpi.com/1424-8220/21/7/2505},
PubMedID = {33916773},
ISSN = {1424-8220},
ABSTRACT = {Obtaining accurate global motion is a crucial step for video stabilization. This paper proposes a robust and simple method to implement global motion estimation. We don’t extend the framework of 2D video stabilization but add a “plug and play” module to motion estimation based on feature points. Firstly, simple linear iterative clustering (SLIC) pre-segmentation is used to obtain superpixels of the video frame, clustering is performed according to the superpixel centroid motion vector and cluster center with large value is eliminated. Secondly, in order to obtain accurate global motion estimation, an improved K-means clustering is proposed. We match the feature points of the remaining superpixels between two adjacent frames, establish a feature points’ motion vector space, and use improved K-means clustering for clustering. Finally, the richest cluster is being retained, and the global motion is obtained by homography transformation. Our proposed method has been verified on different types of videos and has efficient performance than traditional approaches. The stabilization video has an average improvement of 0.24 in the structural similarity index than the original video and 0.1 higher than the traditional method.},
DOI = {10.3390/s21072505}
}



@Article{rs13071391,
AUTHOR = {Fernandez-Beltran, Ruben and Baidar, Tina and Kang, Jian and Pla, Filiberto},
TITLE = {Rice-Yield Prediction with Multi-Temporal Sentinel-2 Data and 3D CNN: A Case Study in Nepal},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {1391},
URL = {https://www.mdpi.com/2072-4292/13/7/1391},
ISSN = {2072-4292},
ABSTRACT = {Crop yield estimation is a major issue of crop monitoring which remains particularly challenging in developing countries due to the problem of timely and adequate data availability. Whereas traditional agricultural systems mainly rely on scarce ground-survey data, freely available multi-temporal and multi-spectral remote sensing images are excellent tools to support these vulnerable systems by accurately monitoring and estimating crop yields before harvest. In this context, we introduce the use of Sentinel-2 (S2) imagery, with a medium spatial, spectral and temporal resolutions, to estimate rice crop yields in Nepal as a case study. Firstly, we build a new large-scale rice crop database (RicePAL) composed by multi-temporal S2 and climate/soil data from the Terai districts of Nepal. Secondly, we propose a novel 3D Convolutional Neural Network (CNN) adapted to these intrinsic data constraints for the accurate rice crop yield estimation. Thirdly, we study the effect of considering different temporal, climate and soil data configurations in terms of the performance achieved by the proposed approach and several state-of-the-art regression and CNN-based yield estimation methods. The extensive experiments conducted in this work demonstrate the suitability of the proposed CNN-based framework for rice crop yield estimation in the developing country of Nepal using S2 data.},
DOI = {10.3390/rs13071391}
}



@Article{s21072534,
AUTHOR = {Doukhi, Oualid and Lee, Deok-Jin},
TITLE = {Deep Reinforcement Learning for End-to-End Local Motion Planning of Autonomous Aerial Robots in Unknown Outdoor Environments: Real-Time Flight Experiments},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {2534},
URL = {https://www.mdpi.com/1424-8220/21/7/2534},
PubMedID = {33916624},
ISSN = {1424-8220},
ABSTRACT = {Autonomous navigation and collision avoidance missions represent a significant challenge for robotics systems as they generally operate in dynamic environments that require a high level of autonomy and flexible decision-making capabilities. This challenge becomes more applicable in micro aerial vehicles (MAVs) due to their limited size and computational power. This paper presents a novel approach for enabling a micro aerial vehicle system equipped with a laser range finder to autonomously navigate among obstacles and achieve a user-specified goal location in a GPS-denied environment, without the need for mapping or path planning. The proposed system uses an actor–critic-based reinforcement learning technique to train the aerial robot in a Gazebo simulator to perform a point-goal navigation task by directly mapping the noisy MAV’s state and laser scan measurements to continuous motion control. The obtained policy can perform collision-free flight in the real world while being trained entirely on a 3D simulator. Intensive simulations and real-time experiments were conducted and compared with a nonlinear model predictive control technique to show the generalization capabilities to new unseen environments, and robustness against localization noise. The obtained results demonstrate our system’s effectiveness in flying safely and reaching the desired points by planning smooth forward linear velocity and heading rates.},
DOI = {10.3390/s21072534}
}



@Article{cli9040058,
AUTHOR = {Ghaffarian, Saman and Emtehani, Sobhan},
TITLE = {Monitoring Urban Deprived Areas with Remote Sensing and Machine Learning in Case of Disaster Recovery},
JOURNAL = {Climate},
VOLUME = {9},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {58},
URL = {https://www.mdpi.com/2225-1154/9/4/58},
ISSN = {2225-1154},
ABSTRACT = {Rapid urbanization and increasing population in cities with a large portion of them settled in deprived neighborhoods, mostly defined as slum areas, have escalated inequality and vulnerability to natural disasters. As a result, monitoring such areas is essential to provide information and support decision-makers and urban planners, especially in case of disaster recovery. Here, we developed an approach to monitor the urban deprived areas over a four-year period after super Typhoon Haiyan, which struck Tacloban city, in the Philippines, in 2013, using high-resolution satellite images and machine learning methods. A Support Vector Machine classification method supported by a local binary patterns feature extraction model was initially performed to detect slum areas in the pre-disaster, just after/event, and post-disaster images. Afterward, a dense conditional random fields model was employed to produce the final slum areas maps. The developed method detected slum areas with accuracies over 83%. We produced the damage and recovery maps based on change analysis over the detected slum areas. The results revealed that most of the slum areas were reconstructed 4 years after Typhoon Haiyan, and thus, the city returned to the pre-existing vulnerability level.},
DOI = {10.3390/cli9040058}
}



@Article{electricity2020007,
AUTHOR = {Speranza, Nicholas A. and Rave, Christopher J. and Pei, Yong},
TITLE = {Energy-Efficient On-Platform Target Classification for Electric Air Transportation Systems},
JOURNAL = {Electricity},
VOLUME = {2},
YEAR = {2021},
NUMBER = {2},
PAGES = {110--123},
URL = {https://www.mdpi.com/2673-4826/2/2/7},
ISSN = {2673-4826},
ABSTRACT = {Due to the predicted rise of Unmanned Aircraft Systems (UAS) in commercial, civil, and military operations, there is a desire to make UASs more energy efficient so they can proliferate with ease of deployment and maximal life per charge. To address current limitations, a three-tiered approach is investigated to mitigate Unmanned Aerial Vehicle (UAV) hover time, reduce network datalink transmission to a ground station, and provide a real-time framework for Sense-and-Avoidance (SAA) target classification. An energy-efficient UAS architecture framework is presented, and a corresponding SAA prototype is developed using commercial hardware to validate the proposed architecture using an experimental methodology. The proposed architecture utilizes classical computer vision methods within the Detection Subsystem coupled with deeply learned Convolutional Neural Networks (CNN) within the Classification Subsystem. Real-time operations of three frames per second are realized enabling UAV hover time and associated energy consumption during SAA processing to be effectively eliminated. Additional energy improvements are not addressed in the scope of this work. Inference accuracy is improved by 19% over baseline COTS models and current non-adaptive, single-stage SAA architectures. Overall, by pushing SAA processing to the edge of the sensors, network offload transmissions and reductions in processing time and energy consumption are feasible and realistic in future battery-powered electric air transportation systems.},
DOI = {10.3390/electricity2020007}
}



@Article{electronics10070868,
AUTHOR = {Martínez, Anselmo and Belmonte, Lidia M. and García, Arturo S. and Fernández-Caballero, Antonio and Morales, Rafael},
TITLE = {Facial Emotion Recognition from an Unmanned Flying Social Robot for Home Care of Dependent People},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {868},
URL = {https://www.mdpi.com/2079-9292/10/7/868},
ISSN = {2079-9292},
ABSTRACT = {This work is part of an ongoing research project to develop an unmanned flying social robot to monitor dependants at home in order to detect the person’s state and bring the necessary assistance. In this sense, this paper focuses on the description of a virtual reality (VR) simulation platform for the monitoring process of an avatar in a virtual home by a rotatory-wing autonomous unmanned aerial vehicle (UAV). This platform is based on a distributed architecture composed of three modules communicated through the message queue telemetry transport (MQTT) protocol: the UAV Simulator implemented in MATLAB/Simulink, the VR Visualiser developed in Unity, and the new emotion recognition (ER) system developed in Python. Using a face detection algorithm and a convolutional neural network (CNN), the ER System is able to detect the person’s face in the image captured by the UAV’s on-board camera and classify the emotion among seven possible ones (surprise; fear; happiness; sadness; disgust; anger; or neutral expression). The experimental results demonstrate the correct integration of this new computer vision module within the VR platform, as well as the good performance of the designed CNN, with around 85% in the F1-score, a mean of the precision and recall of the model. The developed emotion detection system can be used in the future implementation of the assistance UAV that monitors dependent people in a real environment, since the methodology used is valid for images of real people.},
DOI = {10.3390/electronics10070868}
}



@Article{electronics10070872,
AUTHOR = {Sun, Yixin and Luo, Yusen and Chai, Xiaoyu and Zhang, Pengpeng and Zhang, Qian and Xu, Lizhang and Wei, Lele},
TITLE = {Double-Threshold Segmentation of Panicle and Clustering Adaptive Density Estimation for Mature Rice Plants Based on 3D Point Cloud},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {872},
URL = {https://www.mdpi.com/2079-9292/10/7/872},
ISSN = {2079-9292},
ABSTRACT = {Crop density estimation ahead of the combine harvester provides a valuable reference for operators to keep the feeding amount stable in agriculture production, and, as a consequence, guaranteeing the working stability and improving the operation efficiency. For the current method depending on LiDAR, it is difficult to extract individual plants for mature rice plants with luxuriant branches and leaves, as well as bent and intersected panicles. Therefore, this paper proposes a clustering adaptive density estimation method based on the constructed LiDAR measurement system and double-threshold segmentation. The Otsu algorithm is adopted to construct a double-threshold according to elevation and inflection intensity in different parts of the rice plant, after reducing noise through the statistical outlier removal (SOR) algorithm. For adaptively parameter adjustment of supervoxel clustering and mean-shift clustering during density estimation, the calculation relationship between influencing factors (including seed-point size and kernel-bandwidth size) and number of points are, respectively, deduced by analysis. The experiment result of density estimation proved the two clustering methods effective, with a Root Mean Square Error (RMSE) of 9.968 and 5.877, and a Mean Absolute Percent Error (MAPE) of 5.67% and 3.37%, and the average accuracy was more than 90% and 95%, respectively. This estimation method is of positive significance for crop density measurement and could lay the foundation for intelligent harvest.},
DOI = {10.3390/electronics10070872}
}



@Article{rs13081416,
AUTHOR = {Bao, Min and Chala Urgessa, Guyo and Xing, Mengdao and Han, Liang and Chen, Rui},
TITLE = {Toward More Robust and Real-Time Unmanned Aerial Vehicle Detection and Tracking via Cross-Scale Feature Aggregation Based on the Center Keypoint},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1416},
URL = {https://www.mdpi.com/2072-4292/13/8/1416},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play an essential role in various applications, such as transportation and intelligent environmental sensing. However, due to camera motion and complex environments, it can be difficult to recognize the UAV from its surroundings thus, traditional methods often miss detection of UAVs and generate false alarms. To address these issues, we propose a novel method for detecting and tracking UAVs. First, a cross-scale feature aggregation CenterNet (CFACN) is constructed to recognize the UAVs. CFACN is a free anchor-based center point estimation method that can effectively decrease the false alarm rate, the misdetection of small targets, and computational complexity. Secondly, the region of interest-scale-crop-resize (RSCR) method is utilized to merge CFACN and region-of-interest (ROI) CFACN (ROI-CFACN) further, in order to improve the accuracy at a lower computational cost. Finally, the Kalman filter is adopted to track the UAV. The effectiveness of our method is validated using a collected UAV dataset. The experimental results demonstrate that our methods can achieve higher accuracy with lower computational cost, being superior to BiFPN, CenterNet, YoLo, and their variants on the same dataset.},
DOI = {10.3390/rs13081416}
}



@Article{rs13081420,
AUTHOR = {Tang, Mingliang and Esmaeili, Kamran},
TITLE = {Heap Leach Pad Surface Moisture Monitoring Using Drone-Based Aerial Images and Convolutional Neural Networks: A Case Study at the El Gallo Mine, Mexico},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1420},
URL = {https://www.mdpi.com/2072-4292/13/8/1420},
ISSN = {2072-4292},
ABSTRACT = {An efficient metal recovery in heap leach operations relies on uniform distribution of leaching reagent solution over the heap leach pad surface. However, the current practices for heap leach pad (HLP) surface moisture monitoring often rely on manual inspection, which is labor-intensive, time-consuming, discontinuous, and intermittent. In order to complement the manual monitoring process and reduce the frequency of exposing technical manpower to the hazardous leaching reagent (e.g., dilute cyanide solution in gold leaching), this manuscript describes a case study of implementing an HLP surface moisture monitoring method based on drone-based aerial images and convolutional neural networks (CNNs). Field data collection was conducted on a gold HLP at the El Gallo mine, Mexico. A commercially available hexa-copter drone was equipped with one visible-light (RGB) camera and one thermal infrared sensor to acquire RGB and thermal images from the HLP surface. The collected data had high spatial and temporal resolutions. The high-quality aerial images were used to generate surface moisture maps of the HLP based on two CNN approaches. The generated maps provide direct visualization of the different moisture zones across the HLP surface, and such information can be used to detect potential operational issues related to distribution of reagent solution and to facilitate timely decision making in heap leach operations.},
DOI = {10.3390/rs13081420}
}



@Article{su13084115,
AUTHOR = {Budiman, Jaka and Bahrawi, Jarbou and Hidayatulloh, Asep and Almazroui, Mansour and Elhag, Mohamed},
TITLE = {Volumetric Quantification of Flash Flood Using Microwave Data on a Watershed Scale in Arid Environments, Saudi Arabia},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {4115},
URL = {https://www.mdpi.com/2071-1050/13/8/4115},
ISSN = {2071-1050},
ABSTRACT = {Actual flood mapping and quantification in an area provide valuable information for the stakeholder to prevent future losses. This study presents the actual flash flood quantification in Al-Lith Watershed, Saudi Arabia. The study is divided into two steps: first is actual flood mapping using remote sensing data, and the second is the flood volume calculation. Two Sentinel-1 images are processed to map the actual flood, i.e., image from 25 May 2018 (dry condition), and 24 November 2018 (peak flood condition). SNAP software is used for the flood mapping step. During SNAP processing, selecting the backscatter data representing the actual flood in an arid region is challenging. The dB range value from 7.23–14.22 is believed to represent the flood. In GIS software, the flood map result is converted into polygon to define the flood boundary. The flood boundary that is overlaid with Digital Elevation Map (DEM) is filled with the same elevation value. The Focal Statistics neighborhood method with three iterations is used to generate the flood surface elevation inside the flood boundary. The raster contains depth information is derived by subtraction of the flood surface elevation with DEM. Several steps are carried out to minimize the overcalculation outside the flood boundary. The flood volume can be derived by the multiplication of flood depth points with each cell size area. The flash flood volume in Al-Lith Watershed on 24 November 2018 is 155,507,439 m3. Validity checks are performed by comparing it with other studies, and the result shows that the number is reliable.},
DOI = {10.3390/su13084115}
}



@Article{rs13081424,
AUTHOR = {Terres de Lima, Lucas and Fernández-Fernández, Sandra and Gonçalves, João Francisco and Magalhães Filho, Luiz and Bernardes, Cristina},
TITLE = {Development of Tools for Coastal Management in Google Earth Engine: Uncertainty Bathtub Model and Bruun Rule},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1424},
URL = {https://www.mdpi.com/2072-4292/13/8/1424},
ISSN = {2072-4292},
ABSTRACT = {Sea-level rise is a problem increasingly affecting coastal areas worldwide. The existence of free and open-source models to estimate the sea-level impact can contribute to improve coastal management. This study aims to develop and validate two different models to predict the sea-level rise impact supported by Google Earth Engine (GEE)—a cloud-based platform for planetary-scale environmental data analysis. The first model is a Bathtub Model based on the uncertainty of projections of the sea-level rise impact module of TerrSet—Geospatial Monitoring and Modeling System software. The validation process performed in the Rio Grande do Sul coastal plain (S Brazil) resulted in correlations from 0.75 to 1.00. The second model uses the Bruun rule formula implemented in GEE and can determine the coastline retreat of a profile by creatting a simple vector line from topo-bathymetric data. The model shows a very high correlation (0.97) with a classical Bruun rule study performed in the Aveiro coast (NW Portugal). Therefore, the achieved results disclose that the GEE platform is suitable to perform these analysis. The models developed have been openly shared, enabling the continuous improvement of the code by the scientific community.},
DOI = {10.3390/rs13081424}
}



@Article{e23040435,
AUTHOR = {Zhang, Xixin and Yang, Yuhang and Li, Zhiyong and Ning, Xin and Qin, Yilang and Cai, Weiwei},
TITLE = {An Improved Encoder-Decoder Network Based on Strip Pool Method Applied to Segmentation of Farmland Vacancy Field},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {435},
URL = {https://www.mdpi.com/1099-4300/23/4/435},
PubMedID = {33917753},
ISSN = {1099-4300},
ABSTRACT = {In the research of green vegetation coverage in the field of remote sensing image segmentation, crop planting area is often obtained by semantic segmentation of images taken from high altitude. This method can be used to obtain the rate of cultivated land in a region (such as a country), but it does not reflect the real situation of a particular farmland. Therefore, this paper takes low-altitude images of farmland to build a dataset. After comparing several mainstream semantic segmentation algorithms, a new method that is more suitable for farmland vacancy segmentation is proposed. Additionally, the Strip Pooling module (SPM) and the Mixed Pooling module (MPM), with strip pooling as their core, are designed and fused into the semantic segmentation network structure to better extract the vacancy features. Considering the high cost of manual data annotation, this paper uses an improved ResNet network as the backbone of signal transmission, and meanwhile uses data augmentation to improve the performance and robustness of the model. As a result, the accuracy of the proposed method in the test set is 95.6%, mIoU is 77.6%, and the error rate is 7%. Compared to the existing model, the mIoU value is improved by nearly 4%, reaching the level of practical application.},
DOI = {10.3390/e23040435}
}



@Article{a14040119,
AUTHOR = {Miao, Qing and Wei, Juhui and Wang, Jiongqi and Chen, Yuyun},
TITLE = {Fault Diagnosis Algorithm Based on Adjustable Nonlinear PI State Observer and Its Application in UAV Fault Diagnosis},
JOURNAL = {Algorithms},
VOLUME = {14},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {119},
URL = {https://www.mdpi.com/1999-4893/14/4/119},
ISSN = {1999-4893},
ABSTRACT = {Aiming at the problem of fault diagnosis in continuous time systems, a kind of fault diagnosis algorithm based on adaptive nonlinear proportional integral (PI) observer, which can realize the effective fault identification, is studied in this paper. Firstly, the stability and stability conditions of fault diagnosis method based on the PI observer are analyzed, and the upper bound of the fault estimation error is given. Secondly, the fault diagnosis algorithm based on adjustable nonlinear PI observer is designed and constructed, it is analyzed and we proved that the upper bound of fault estimation under this algorithm is better than that of the traditional method. Finally, the L-1011 unmanned aerial vehicle (UAV) is taken as the experimental object for numerical simulation, and the fault diagnosis method based on adaptive observer factor achieves faster response speed and more accurate fault identification results.},
DOI = {10.3390/a14040119}
}



@Article{en14082059,
AUTHOR = {Basso, Mattia and Cravero, Carlo and Marsano, Davide},
TITLE = {Aerodynamic Effect of the Gurney Flap on the Front Wing of a F1 Car and Flow Interactions with Car Components},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2059},
URL = {https://www.mdpi.com/1996-1073/14/8/2059},
ISSN = {1996-1073},
ABSTRACT = {The design of a racing car needs several aerodynamic design steps in order to achieve high performance. Each component has an aerodynamic interaction with the others and high performance requires a good match between them. The front wing is undoubtedly one of the main components to determine car performance with a strong interaction with the downstream components. The Gurney Flap (GF) is a small appendix perpendicular to the pressure side of the front wing at the trailing edge that can dramatically improve the front wing performance. In the literature, the performance of a GF on a single profile is well documented, while in this paper the GF mounted on the front wing of a racing car has been investigated and the interactions through the 3D flow structures are discussed. The global drag and downforce performance on the main components of the vehicle have been examined by comparing the cases with and without a GF. The GF increases the downforce by about 24% compared to a limited increase in the drag force. A fluid dynamic analysis has been carried out to understand the physical mechanisms of the flow interaction induced to the other components. The GF, in fact, enhances the ground effect, by redistributing the flow that interacts differently with the other components i.e., the wheel zone.},
DOI = {10.3390/en14082059}
}



@Article{ijgi10040251,
AUTHOR = {Ludwig, Christina and Hecht, Robert and Lautenbach, Sven and Schorcht, Martin and Zipf, Alexander},
TITLE = {Mapping Public Urban Green Spaces Based on OpenStreetMap and Sentinel-2 Imagery Using Belief Functions},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {251},
URL = {https://www.mdpi.com/2220-9964/10/4/251},
ISSN = {2220-9964},
ABSTRACT = {Public urban green spaces are important for the urban quality of life. Still, comprehensive open data sets on urban green spaces are not available for most cities. As open and globally available data sets, the potential of Sentinel-2 satellite imagery and OpenStreetMap (OSM) data for urban green space mapping is high but limited due to their respective uncertainties. Sentinel-2 imagery cannot distinguish public from private green spaces and its spatial resolution of 10 m fails to capture fine-grained urban structures, while in OSM green spaces are not mapped consistently and with the same level of completeness everywhere. To address these limitations, we propose to fuse these data sets under explicit consideration of their uncertainties. The Sentinel-2 derived Normalized Difference Vegetation Index was fused with OSM data using the Dempster–Shafer theory to enhance the detection of small vegetated areas. The distinction between public and private green spaces was achieved using a Bayesian hierarchical model and OSM data. The analysis was performed based on land use parcels derived from OSM data and tested for the city of Dresden, Germany. The overall accuracy of the final map of public urban green spaces was 95% and was mainly influenced by the uncertainty of the public accessibility model.},
DOI = {10.3390/ijgi10040251}
}



@Article{s21082650,
AUTHOR = {Choi, Daegyun and Bell, William and Kim, Donghoon and Kim, Jichul},
TITLE = {UAV-Driven Structural Crack Detection and Location Determination Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2650},
URL = {https://www.mdpi.com/1424-8220/21/8/2650},
PubMedID = {33918951},
ISSN = {1424-8220},
ABSTRACT = {Structural cracks are a vital feature in evaluating the health of aging structures. Inspectors regularly monitor structures’ health using visual information because early detection of cracks on highly trafficked structures is critical for maintaining the public’s safety. In this work, a framework for detecting cracks along with their locations is proposed. Image data provided by an unmanned aerial vehicle (UAV) is stitched using image processing techniques to overcome limitations in the resolution of cameras. This stitched image is analyzed to identify cracks using a deep learning model that makes judgements regarding the presence of cracks in the image. Moreover, cracks’ locations are determined using data from UAV sensors. To validate the system, cracks forming on an actual building are captured by a UAV, and these images are analyzed to detect and locate cracks. The proposed framework is proven as an effective way to detect cracks and to represent the cracks’ locations.},
DOI = {10.3390/s21082650}
}



@Article{rs13081464,
AUTHOR = {Liang, Zhu and Wang, Changming and Duan, Zhijie and Liu, Hailiang and Liu, Xiaoyang and Ullah Jan Khan, Kaleem},
TITLE = {A Hybrid Model Consisting of Supervised and Unsupervised Learning for Landslide Susceptibility Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1464},
URL = {https://www.mdpi.com/2072-4292/13/8/1464},
ISSN = {2072-4292},
ABSTRACT = {Landslides cause huge damage to social economy and human beings every year. Landslide susceptibility mapping (LSM) occupies an important position in land use and risk management. This study is to investigate a hybrid model which makes full use of the advantage of supervised learning model (SLM) and unsupervised learning model (ULM). Firstly, ten continuous variables were used to develop a ULM which consisted of factor analysis (FA) and k-means cluster for a preliminary landslide susceptibility map. Secondly, 351 landslides with “1” label were collected and the same number of non-landslide samples with “0” label were selected from the very low susceptibility area in the preliminary map, constituting a new priori condition for a SLM, and thirteen factors were used for the modeling of gradient boosting decision tree (GBDT) which represented for SLM. Finally, the performance of different models was verified using related indexes. The results showed that the performance of the pretreated GBDT model was improved with sensitivity, specificity, accuracy and the area under the curve (AUC) values of 88.60%, 92.59%, 90.60% and 0.976, respectively. It can be concluded that a pretreated model with strong robustness can be constructed by increasing the purity of samples.},
DOI = {10.3390/rs13081464}
}



@Article{electronics10080905,
AUTHOR = {Rodríguez-García, Miguel Ángel and García-Sánchez, Francisco and Valencia-García, Rafael},
TITLE = {Knowledge-Based System for Crop Pests and Diseases Recognition},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {905},
URL = {https://www.mdpi.com/2079-9292/10/8/905},
ISSN = {2079-9292},
ABSTRACT = {With the rapid increase in the world’s population, there is an ever-growing need for a sustainable food supply. Agriculture is one of the pillars for worldwide food provisioning, with fruits and vegetables being essential for a healthy diet. However, in the last few years the worldwide dispersion of virulent plant pests and diseases has caused significant decreases in the yield and quality of crops, in particular fruit, cereal and vegetables. Climate change and the intensification of global trade flows further accentuate the issue. Integrated Pest Management (IPM) is an approach to pest control that aims at maintaining pest insects at tolerable levels, keeping pest populations below an economic injury level. Under these circumstances, the early identification of pests and diseases becomes crucial. In this work, we present the first step towards a fully fledged, semantically enhanced decision support system for IPM. The ultimate goal is to build a complete agricultural knowledge base by gathering data from multiple, heterogeneous sources and to develop a system to assist farmers in decision making concerning the control of pests and diseases. The pest classifier framework has been evaluated in a simulated environment, obtaining an aggregated accuracy of 98.8%.},
DOI = {10.3390/electronics10080905}
}



@Article{s21082679,
AUTHOR = {Ye, Zhoujing and Yan, Guannan and Wei, Ya and Zhou, Bin and Li, Ning and Shen, Shihui and Wang, Linbing},
TITLE = {Real-Time and Efficient Traffic Information Acquisition via Pavement Vibration IoT Monitoring System},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2679},
URL = {https://www.mdpi.com/1424-8220/21/8/2679},
PubMedID = {33920249},
ISSN = {1424-8220},
ABSTRACT = {Traditional road-embedded monitoring systems for traffic monitoring have the disadvantages of a short life, high energy consumption and data redundancy, resulting in insufficient durability and high cost. In order to improve the durability and efficiency of the road-embedded monitoring system, a pavement vibration monitoring system is developed based on the Internet of things (IoT). The system includes multi-acceleration sensing nodes, a gateway, and a cloud platform. The key design principles and technologies of each part of the system are proposed, which provides valuable experience for the application of IoT monitoring technology in road infrastructures. Characterized by low power consumption, distributed computing, and high extensibility properties, the pavement vibration IoT monitoring system can realize the monitoring, transmission, and analysis of pavement vibration signal, and acquires the real-time traffic information. This road-embedded system improves the intellectual capacity of road infrastructure and is conducive to the construction of a new generation of smart roads.},
DOI = {10.3390/s21082679}
}



@Article{app11083417,
AUTHOR = {Ahmed, Nafis and Pawase, Chaitali J. and Chang, KyungHi},
TITLE = {Distributed 3-D Path Planning for Multi-UAVs with Full Area Surveillance Based on Particle Swarm Optimization},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3417},
URL = {https://www.mdpi.com/2076-3417/11/8/3417},
ISSN = {2076-3417},
ABSTRACT = {Collision-free distributed path planning for the swarm of unmanned aerial vehicles (UAVs) in a stochastic and dynamic environment is an emerging and challenging subject for research in the field of a communication system. Monitoring the methods and approaches for multi-UAVs with full area surveillance is needed in both military and civilian applications, in order to protect human beings and infrastructure, as well as their social security. To perform the path planning for multiple unmanned aerial vehicles, we propose a trajectory planner based on Particle Swarm Optimization (PSO) algorithm to derive a distributed full coverage optimal path planning, and a trajectory planner is developed using a dynamic fitness function. In this paper, to obtain dynamic fitness, we implemented the PSO algorithm independently in each UAV, by maximizing the fitness function and minimizing the cost function. Simulation results show that the proposed distributed path planning algorithm generates feasible optimal trajectories and update maps for the swarm of UAVs to surveil the entire area of interest.},
DOI = {10.3390/app11083417}
}



@Article{rs13081471,
AUTHOR = {Marin, Diego Bedin and Ferraz, Gabriel Araújo e Silva and Guimarães, Paulo Henrique Sales and Schwerz, Felipe and Santana, Lucas Santos and Barbosa, Brenon Dienevam Souza and Barata, Rafael Alexandre Pena and Faria, Rafael de Oliveira and Dias, Jessica Ellen Lima and Conti, Leonardo and Rossi, Giuseppe},
TITLE = {Remotely Piloted Aircraft and Random Forest in the Evaluation of the Spatial Variability of Foliar Nitrogen in Coffee Crop},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1471},
URL = {https://www.mdpi.com/2072-4292/13/8/1471},
ISSN = {2072-4292},
ABSTRACT = {The development of approaches to determine the spatial variability of nitrogen (N) into coffee leaves is essential to increase productivity and reduce production costs and environmental impacts associated with excessive N applications. Thus, this study aimed to assess the potential of the Random Forest (RF) machine learning method applied to vegetation indices (VI) obtained from Remotely Piloted Aircraft (RPA) images to measure the N content in coffee plants. A total of 10 VI were obtained from multispectral images by a camera attached to a rotary-wing RPA. The RGB orthomosaic was used to determine sampling points at the crop area, which were ranked by N levels in the plants as deficient, critical, or sufficient. The chemical analysis of N content in the coffee leaves, as well as the VI values in sample points, were used as input parameters for the image training and its classification by the RF. The suggested model has shown global accuracy and a kappa coefficient of up to 0.91 and 0.86, respectively. The best results were achieved using the Green Normalized Difference Vegetation (GNDVI) and Green Optimized Soil Adjusted Vegetation Index (GOSAVI). In addition, the model enabled the evaluation of the spatial distribution of N in the coffee trees, as well as quantification of N deficiency in the crop for the whole area. The GNDVI and GOSAVI allowed the verification that 22% of the entire crop area had plants with N deficiency symptoms, which would result in a reduction of 78% in the amount of N applied by the producer.},
DOI = {10.3390/rs13081471}
}



@Article{app11083435,
AUTHOR = {Kim, Jeonghwan and Lee, Soomin and Seo, Jongwon and Lee, Dong-Eun and Choi, Hee Seon},
TITLE = {The Integration of Earthwork Design Review and Planning Using UAV-Based Point Cloud and BIM},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3435},
URL = {https://www.mdpi.com/2076-3417/11/8/3435},
ISSN = {2076-3417},
ABSTRACT = {Earthwork is seemingly guesswork, but it requires a high level of accuracy and precise planning. Differences between earthwork design and finishing levels cause project delays and cost overrun due to the time-consuming nature of earthwork re-work. Therefore, error-free earthwork planning and design review is a key to the success of earthwork projects. This study utilized an integrated approach of an unmanned aerial vehicle (UAV)-based point cloud and BIM (Building Information Modeling) to verify the design and to operate the earthwork planning. The integrated approach was proposed and applied to a 420 square meters housing construction project to review an original earthwork design and create an earthwork plan for excavator work. As a result, errors in earthwork design that caused by inaccurate initial DEM was revealed, thus the earthwork design was revised with a UAV-based point cloud map. Additionally, the integrated approach was able to generate an explicit task sequence for an excavator.},
DOI = {10.3390/app11083435}
}



@Article{app11083454,
AUTHOR = {Gaspar, Pedro D. and Fernandez, Carlos M. and Soares, Vasco N. G. J. and Caldeira, João M. L. P. and Silva, Hélio},
TITLE = {Development of Technological Capabilities through the Internet of Things (IoT): Survey of Opportunities and Barriers for IoT Implementation in Portugal’s Agro-Industry},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3454},
URL = {https://www.mdpi.com/2076-3417/11/8/3454},
ISSN = {2076-3417},
ABSTRACT = {The agro-industrial sector consumes a significant amount of natural resources for farming and meat production. By 2050, population growth is expected, generating more demand and, consequently, more consumption of scarce resources. This challenging scenario is a concern of the European Commission, revealed in the Green Deal commitment and by the United Nations’ 12th goal of sustainable development. Thus, organizations must increase productivity and be more sustainable as soon as possible. Internet of Things (IoT) is introduced as a solution to facilitate agro-food companies to be more eco-efficient, mainly facing difficulties on farms, such as food loss and waste, best efficiency in management of resources, and production. The deployment of this technology depends on the stage of maturity and potential of implementation. To assess and characterize companies, with respect of IoT implementation, a survey was applied in 21 micro, small and medium agro-food companies, belonging to milk, honey, olive oil, jams, fruticulture, bakery and pastry, meat, coffee, and wine sectors, in the central region of Portugal. As results, this paper reveals the stage of maturity, level of sophistication, potential, opportunities, solutions, and barriers for implementation of IoT. Additionally, suggestions and recommendations to improve practices are discussed.},
DOI = {10.3390/app11083454}
}



@Article{agronomy11040749,
AUTHOR = {Torres-Sánchez, Jorge and Mesas-Carrascosa, Francisco Javier and Jiménez-Brenes, Francisco M. and de Castro, Ana I. and López-Granados, Francisca},
TITLE = {Early Detection of Broad-Leaved and Grass Weeds in Wide Row Crops Using Artificial Neural Networks and UAV Imagery},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {749},
URL = {https://www.mdpi.com/2073-4395/11/4/749},
ISSN = {2073-4395},
ABSTRACT = {Significant advances in weed mapping from unmanned aerial platforms have been achieved in recent years. The detection of weed location has made possible the generation of site specific weed treatments to reduce the use of herbicides according to weed cover maps. However, the characterization of weed infestations should not be limited to the location of weed stands, but should also be able to distinguish the types of weeds to allow the best possible choice of herbicide treatment to be applied. A first step in this direction should be the discrimination between broad-leaved (dicotyledonous) and grass (monocotyledonous) weeds. Considering the advances in weed detection based on images acquired by unmanned aerial vehicles, and the ability of neural networks to solve hard classification problems in remote sensing, these technologies have been merged in this study with the aim of exploring their potential for broadleaf and grass weed detection in wide-row herbaceous crops such as sunflower and cotton. Overall accuracies of around 80% were obtained in both crops, with user accuracy for broad-leaved and grass weeds around 75% and 65%, respectively. These results confirm the potential of the presented combination of technologies for improving the characterization of different weed infestations, which would allow the generation of timely and adequate herbicide treatment maps according to groups of weeds.},
DOI = {10.3390/agronomy11040749}
}



@Article{s21082748,
AUTHOR = {Leon-Medina, Jersson X. and Anaya, Maribel and Parés, Núria and Tibaduiza, Diego A. and Pozo, Francesc},
TITLE = {Structural Damage Classification in a Jacket-Type Wind-Turbine Foundation Using Principal Component Analysis and Extreme Gradient Boosting},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2748},
URL = {https://www.mdpi.com/1424-8220/21/8/2748},
PubMedID = {33924654},
ISSN = {1424-8220},
ABSTRACT = {Damage classification is an important topic in the development of structural health monitoring systems. When applied to wind-turbine foundations, it provides information about the state of the structure, helps in maintenance, and prevents catastrophic failures. A data-driven pattern-recognition methodology for structural damage classification was developed in this study. The proposed methodology involves several stages: (1) data acquisition, (2) data arrangement, (3) data normalization through the mean-centered unitary group-scaling method, (4) linear feature extraction, (5) classification using the extreme gradient boosting machine learning classifier, and (6) validation applying a 5-fold cross-validation technique. The linear feature extraction capabilities of principal component analysis are employed; the original data of 58,008 features is reduced to only 21 features. The methodology is validated with an experimental test performed in a small-scale wind-turbine foundation structure that simulates the perturbation effects caused by wind and marine waves by applying an unknown white noise signal excitation to the structure. A vibration-response methodology is selected for collecting accelerometer data from both the healthy structure and the structure subjected to four different damage scenarios. The datasets are satisfactorily classified, with performance measures over 99.9% after using the proposed damage classification methodology.},
DOI = {10.3390/s21082748}
}



@Article{sym13040680,
AUTHOR = {Wang, Qiuzhen and Zhang, Hai},
TITLE = {A Self-Organizing Area Coverage Method for Swarm Robots Based on Gradient and Grouping},
JOURNAL = {Symmetry},
VOLUME = {13},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {680},
URL = {https://www.mdpi.com/2073-8994/13/4/680},
ISSN = {2073-8994},
ABSTRACT = {The openness of the environment brings great challenges to the swarm robotic system to cover the task area quickly and effectively. In this paper, a coverage method based on gradient and grouping (GGC) is proposed. What is novel about our proposed solution is that it is suitable for extremely simple robots that lack computing or storage power. Through the change of the robot gradient, the swarm robot system with very simple functions can effectively self-organize to cover the unknown task area. By grouping the swarm robots, each group can cover the task area in parallel, which greatly improves the coverage speed. We verified our proposed method through experimental simulation and found that the gradient and grouping-based method in this paper was superior to other methods in terms of coverage, coverage completion time, and other aspects. Simultaneously, the robustness of the proposed method is analyzed and admirable experimental results are obtained. Because the applicable robot is very simple, the method in this paper can be applied to the submillimeter swarm robot system, which will lay the foundation for micro medicine.},
DOI = {10.3390/sym13040680}
}



@Article{rs13081508,
AUTHOR = {Kang, Yeseong and Nam, Jinwoo and Kim, Younggwang and Lee, Seongtae and Seong, Deokgyeong and Jang, Sihyeong and Ryu, Chanseok},
TITLE = {Assessment of Regression Models for Predicting Rice Yield and Protein Content Using Unmanned Aerial Vehicle-Based Multispectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1508},
URL = {https://www.mdpi.com/2072-4292/13/8/1508},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle-based multispectral imagery including five spectral bands (blue, green, red, red-edge, and near-infrared) for a rice field in the ripening stage was used to develop regression models for predicting the rice yield and protein content and to select the most suitable regression analysis method for the year-invariant model: partial least squares regression, ridge regression, and artificial neural network (ANN). The regression models developed with six vegetation indices (green normalization difference vegetation index (GNDVI), normalization difference red-edge index (NDRE), chlorophyll index red edge (CIrededge), difference NIR/Green green difference vegetation index (GDVI), green-red NDVI (GRNDVI), and medium resolution imaging spectrometer terrestrial chlorophyll index (MTCI)), calculated from the spectral bands, were applied to single years (2018, 2019, and 2020) and multiple years (2018 + 2019, 2018 + 2020, 2019 + 2020, and all years). The regression models were cross-validated through mutual prediction against the vegetation indices in nonoverlapping years, and the prediction errors were evaluated via root mean squared error of prediction (RMSEP). The ANN model was reproducible, with low and sustained prediction errors of 24.2 kg/1000 m2 ≤ RMSEP ≤ 59.1 kg/1000 m2 in rice yield and 0.14% ≤ RMSEP ≤ 0.28% in rice-protein content in all single-year and multiple-year analyses. When the importance of each vegetation index of the regression models was evaluated, only the ANN model showed the same ranking in the vegetation index of the first (MTCI in both rice yield and protein content) and second importance (CIrededge in rice yield and GRNDVI in rice-protein content). Overall, this means that the ANN model has the highest potential for developing a year-invariant model with stable RMSEP and consistent variable ranking.},
DOI = {10.3390/rs13081508}
}



@Article{rs13081509,
AUTHOR = {Hu, Xikun and Ban, Yifang and Nascetti, Andrea},
TITLE = {Uni-Temporal Multispectral Imagery for Burned Area Mapping with Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1509},
URL = {https://www.mdpi.com/2072-4292/13/8/1509},
ISSN = {2072-4292},
ABSTRACT = {Accurate burned area information is needed to assess the impacts of wildfires on people, communities, and natural ecosystems. Various burned area detection methods have been developed using satellite remote sensing measurements with wide coverage and frequent revisits. Our study aims to expound on the capability of deep learning (DL) models for automatically mapping burned areas from uni-temporal multispectral imagery. Specifically, several semantic segmentation network architectures, i.e., U-Net, HRNet, Fast-SCNN, and DeepLabv3+, and machine learning (ML) algorithms were applied to Sentinel-2 imagery and Landsat-8 imagery in three wildfire sites in two different local climate zones. The validation results show that the DL algorithms outperform the ML methods in two of the three cases with the compact burned scars, while ML methods seem to be more suitable for mapping dispersed burn in boreal forests. Using Sentinel-2 images, U-Net and HRNet exhibit comparatively identical performance with higher kappa (around 0.9) in one heterogeneous Mediterranean fire site in Greece; Fast-SCNN performs better than others with kappa over 0.79 in one compact boreal forest fire with various burn severity in Sweden. Furthermore, directly transferring the trained models to corresponding Landsat-8 data, HRNet dominates in the three test sites among DL models and can preserve the high accuracy. The results demonstrated that DL models can make full use of contextual information and capture spatial details in multiple scales from fire-sensitive spectral bands to map burned areas. Using only a post-fire image, the DL methods not only provide automatic, accurate, and bias-free large-scale mapping option with cross-sensor applicability, but also have potential to be used for onboard processing in the next Earth observation satellites.},
DOI = {10.3390/rs13081509}
}



@Article{s21082765,
AUTHOR = {Taghvaee, Hamidreza and Jain, Akshay and Timoneda, Xavier and Liaskos, Christos and Abadal, Sergi and Alarcón, Eduard and Cabellos-Aparicio, Albert},
TITLE = {Radiation Pattern Prediction for Metasurfaces: A Neural Network-Based Approach},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2765},
URL = {https://www.mdpi.com/1424-8220/21/8/2765},
PubMedID = {33919861},
ISSN = {1424-8220},
ABSTRACT = {As the current standardization for the 5G networks nears completion, work towards understanding the potential technologies for the 6G wireless networks is already underway. One of these potential technologies for the 6G networks is reconfigurable intelligent surfaces. They offer unprecedented degrees of freedom towards engineering the wireless channel, i.e., the ability to modify the characteristics of the channel whenever and however required. Nevertheless, such properties demand that the response of the associated metasurface is well understood under all possible operational conditions. While an understanding of the radiation pattern characteristics can be obtained through either analytical models or full-wave simulations, they suffer from inaccuracy and extremely high computational complexity, respectively. Hence, in this paper, we propose a neural network-based approach that enables a fast and accurate characterization of the metasurface response. We analyze multiple scenarios and demonstrate the capabilities and utility of the proposed methodology. Concretely, we show that this method can learn and predict the parameters governing the reflected wave radiation pattern with an accuracy of a full-wave simulation (98.8–99.8%) and the time and computational complexity of an analytical model. The aforementioned result and methodology will be of specific importance for the design, fault tolerance, and maintenance of the thousands of reconfigurable intelligent surfaces that will be deployed in the 6G network environment.},
DOI = {10.3390/s21082765}
}



@Article{rs13081523,
AUTHOR = {Shao, Yang and Cooner, Austin J. and Walsh, Stephen J.},
TITLE = {Assessing Deep Convolutional Neural Networks and Assisted Machine Perception for Urban Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1523},
URL = {https://www.mdpi.com/2072-4292/13/8/1523},
ISSN = {2072-4292},
ABSTRACT = {High-spatial-resolution satellite imagery has been widely applied for detailed urban mapping. Recently, deep convolutional neural networks (DCNNs) have shown promise in certain remote sensing applications, but they are still relatively new techniques for general urban mapping. This study examines the use of two DCNNs (U-Net and VGG16) to provide an automatic schema to support high-resolution mapping of buildings, road/open built-up, and vegetation cover. Using WorldView-2 imagery as input, we first applied an established OBIA method to characterize major urban land cover classes. An OBIA-derived urban map was then divided into a training and testing region to evaluate the DCNNs’ performance. For U-Net mapping, we were particularly interested in how sample size or the number of image tiles affect mapping accuracy. U-Net generated cross-validation accuracies ranging from 40.5 to 95.2% for training sample sizes from 32 to 4096 image tiles (each tile was 256 by 256 pixels). A per-pixel accuracy assessment led to 87.8 percent overall accuracy for the testing region, suggesting U-Net’s good generalization capabilities. For the VGG16 mapping, we proposed an object-based framing paradigm that retains spatial information and assists machine perception through Gaussian blurring. Gaussian blurring was used as a pre-processing step to enhance the contrast between objects of interest and background (contextual) information. Combined with the pre-trained VGG16 and transfer learning, this analytical approach generated a 77.3 percent overall accuracy for per-object assessment. The mapping accuracy could be further improved given more robust segmentation algorithms and better quantity/quality of training samples. Our study shows significant promise for DCNN implementation for urban mapping and our approach can transfer to a number of other remote sensing applications.},
DOI = {10.3390/rs13081523}
}



@Article{app11083547,
AUTHOR = {Hou, Xiaoyu and Zhang, Kunlin and Xu, Jihui and Huang, Wei and Yu, Xinmiao and Xu, Huaiyu},
TITLE = {Object Detection in Drone Imagery via Sample Balance Strategies and Local Feature Enhancement},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3547},
URL = {https://www.mdpi.com/2076-3417/11/8/3547},
ISSN = {2076-3417},
ABSTRACT = {With the advent of drones, new potential applications have emerged for the unconstrained analysis of images and videos from aerial view cameras. Despite the tremendous success of the generic object detection methods developed using ground-based photos, a considerable performance drop is observed when these same methods are directly applied to images captured by Unmanned Aerial Vehicles (UAVs). Usually, most of the work goes into improving the performance of the detector in aspects such as design loss, training sample selection, feature enhancement, and so forth. This paper proposes a detection framework based on an anchor-free detector with several modules, including a sample balance strategies module and super-resolved generated feature module, to improve performance. We proposed the sample balance strategies module to optimize the imbalance among training samples, especially the imbalance between positive and negative, and easy and hard samples. Due to the high frequencies and noisy representation of the small objects in images captured by drones, the detection task is extraordinarily challenging. However, when compared with other algorithms of this kind, our method achieves better results. We also propose a super-resolved generated GAN (Generative Adversarial Network) module with center-ness weights to effectively enhance the local feature map. Finally, we demonstrate our method’s effectiveness with the proposed modules by carrying out a state-of-the-art performance on Visdrone2020 benchmarks.},
DOI = {10.3390/app11083547}
}



@Article{s21082803,
AUTHOR = {Jaffari, Rabeea and Hashmani, Manzoor Ahmed and Reyes-Aldasoro, Constantino Carlos},
TITLE = {A Novel Focal Phi Loss for Power Line Segmentation with Auxiliary Classifier U-Net},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2803},
URL = {https://www.mdpi.com/1424-8220/21/8/2803},
PubMedID = {33923472},
ISSN = {1424-8220},
ABSTRACT = {The segmentation of power lines (PLs) from aerial images is a crucial task for the safe navigation of unmanned aerial vehicles (UAVs) operating at low altitudes. Despite the advances in deep learning-based approaches for PL segmentation, these models are still vulnerable to the class imbalance present in the data. The PLs occupy only a minimal portion (1–5%) of the aerial images as compared to the background region (95–99%). Generally, this class imbalance problem is addressed via the use of PL-specific detectors in conjunction with the popular class balanced cross entropy (BBCE) loss function. However, these PL-specific detectors do not work outside their application areas and a BBCE loss requires hyperparameter tuning for class-wise weights, which is not trivial. Moreover, the BBCE loss results in low dice scores and precision values and thus, fails to achieve an optimal trade-off between dice scores, model accuracy, and precision–recall values. In this work, we propose a generalized focal loss function based on the Matthews correlation coefficient (MCC) or the Phi coefficient to address the class imbalance problem in PL segmentation while utilizing a generic deep segmentation architecture. We evaluate our loss function by improving the vanilla U-Net model with an additional convolutional auxiliary classifier head (ACU-Net) for better learning and faster model convergence. The evaluation of two PL datasets, namely the Mendeley Power Line Dataset and the Power Line Dataset of Urban Scenes (PLDU), where PLs occupy around 1% and 2% of the aerial images area, respectively, reveal that our proposed loss function outperforms the popular BBCE loss by 16% in PL dice scores on both the datasets, 19% in precision and false detection rate (FDR) values for the Mendeley PL dataset and 15% in precision and FDR values for the PLDU with a minor degradation in the accuracy and recall values. Moreover, our proposed ACU-Net outperforms the baseline vanilla U-Net for the characteristic evaluation parameters in the range of 1–10% for both the PL datasets. Thus, our proposed loss function with ACU-Net achieves an optimal trade-off for the characteristic evaluation parameters without any bells and whistles. Our code is available at Github.},
DOI = {10.3390/s21082803}
}



@Article{app11083586,
AUTHOR = {Xu, Gaofei and Guo, Wei and Zhao, Yang and Zhou, Yue and Zhang, Yinlong and Liu, Xinyu and Xu, Gaopeng and Li, Guangwei},
TITLE = {Online Learning Based Underwater Robotic Thruster Fault Detection},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {3586},
URL = {https://www.mdpi.com/2076-3417/11/8/3586},
ISSN = {2076-3417},
ABSTRACT = {This paper presents a novel online learning-based fault detection designed for underwater robotic thruster health monitoring. In the fault detection algorithm, we build a mathematical model between the control variable and the propeller speed by fitting collected online work status data to the model. To improve the accuracy of online modeling, a multi-center PSO algorithm with memory ability is utilized to optimize the modeling parameters. Additionally, a model online update mechanism is designed to accommodate the model to the change of thruster work status and sea environment. During the operation, propeller speed of the underwater robot is predicted through the online learning-based model, and the model residuals are used for thruster health monitoring. To avoid false alarm, an adaptive fault detection strategy is established based on model online update mechanism. The proposed method has been extensively evaluated using different underwater robotics, through a sea trial data simulation, a pool test fault detection experiment and a sea trial fault detection experiment. Compared with fixed model-based method, speed prediction MAE of the online learning model is at least 37.9% lower than that of the fixed model. The online learning-based method show no misdiagnosis in experiments, while the fixed model-based method is misdiagnosed. Experimental results show that the proposed method is competitive in terms of accuracy, adaptability, and robustness.},
DOI = {10.3390/app11083586}
}



@Article{s21082824,
AUTHOR = {Coluccia, Angelo and Fascista, Alessio and Schumann, Arne and Sommer, Lars and Dimou, Anastasios and Zarpalas, Dimitrios and Méndez, Miguel and de la Iglesia, David and González, Iago and Mercier, Jean-Philippe and Gagné, Guillaume and Mitra, Arka and Rajashekar, Shobha},
TITLE = {Drone vs. Bird Detection: Deep Learning Algorithms and Results from a Grand Challenge},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2824},
URL = {https://www.mdpi.com/1424-8220/21/8/2824},
PubMedID = {33923829},
ISSN = {1424-8220},
ABSTRACT = {Adopting effective techniques to automatically detect and identify small drones is a very compelling need for a number of different stakeholders in both the public and private sectors. This work presents three different original approaches that competed in a grand challenge on the “Drone vs. Bird” detection problem. The goal is to detect one or more drones appearing at some time point in video sequences where birds and other distractor objects may be also present, together with motion in background or foreground. Algorithms should raise an alarm and provide a position estimate only when a drone is present, while not issuing alarms on birds, nor being confused by the rest of the scene. In particular, three original approaches based on different deep learning strategies are proposed and compared on a real-world dataset provided by a consortium of universities and research centers, under the 2020 edition of the Drone vs. Bird Detection Challenge. Results show that there is a range in difficulty among different test sequences, depending on the size and the shape visibility of the drone in the sequence, while sequences recorded by a moving camera and very distant drones are the most challenging ones. The performance comparison reveals that the different approaches perform somewhat complementary, in terms of correct detection rate, false alarm rate, and average precision.},
DOI = {10.3390/s21082824}
}



@Article{drones5020028,
AUTHOR = {Li, Joan Y. Q. and Duce, Stephanie and Joyce, Karen E. and Xiang, Wei},
TITLE = {SeeCucumbers: Using Deep Learning and Drone Imagery to Detect Sea Cucumbers on Coral Reef Flats},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {28},
URL = {https://www.mdpi.com/2504-446X/5/2/28},
ISSN = {2504-446X},
ABSTRACT = {Sea cucumbers (Holothuroidea or holothurians) are a valuable fishery and are also crucial nutrient recyclers, bioturbation agents, and hosts for many biotic associates. Their ecological impacts could be substantial given their high abundance in some reef locations and thus monitoring their populations and spatial distribution is of research interest. Traditional in situ surveys are laborious and only cover small areas but drones offer an opportunity to scale observations more broadly, especially if the holothurians can be automatically detected in drone imagery using deep learning algorithms. We adapted the object detection algorithm YOLOv3 to detect holothurians from drone imagery at Hideaway Bay, Queensland, Australia. We successfully detected 11,462 of 12,956 individuals over 2.7ha with an average density of 0.5 individual/m2. We tested a range of hyperparameters to determine the optimal detector performance and achieved 0.855 mAP, 0.82 precision, 0.83 recall, and 0.82 F1 score. We found as few as ten labelled drone images was sufficient to train an acceptable detection model (0.799 mAP). Our results illustrate the potential of using small, affordable drones with direct implementation of open-source object detection models to survey holothurians and other shallow water sessile species.},
DOI = {10.3390/drones5020028}
}



@Article{rs13081557,
AUTHOR = {Balsi, Marco and Moroni, Monica and Chiarabini, Valter and Tanda, Giovanni},
TITLE = {High-Resolution Aerial Detection of Marine Plastic Litter by Hyperspectral Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1557},
URL = {https://www.mdpi.com/2072-4292/13/8/1557},
ISSN = {2072-4292},
ABSTRACT = {An automatic custom-made procedure is developed to identify macroplastic debris loads in coastal and marine environment, through hyperspectral imaging from unmanned aerial vehicles (UAVs). Results obtained during a remote-sensing field campaign carried out in the seashore of Sassari (Sardinia, Italy) are presented. A push-broom-sensor-based spectral device, carried onboard a DJI Matrice 600 drone, was employed for the acquisition of spectral data in the range 900−1700 nm. The hyperspectral platform was realized by assembling commercial devices, whereas algorithms for mosaicking, post-flight georeferencing, and orthorectification of the acquired images were developed in-house. Generation of the hyperspectral cube was based on mosaicking visible-spectrum images acquired synchronously with the hyperspectral lines, by performing correlation-based registration and applying the same translations, rotations, and scale changes to the hyperspectral data. Plastics detection was based on statistically relevant feature selection and Linear Discriminant Analysis, trained on a manually labeled sample. The results obtained from the inspection of either the beach site or the sea water facing the beach clearly show the successful separate identification of polyethylene (PE) and polyethylene terephthalate (PET) objects through the post-processing data treatment based on the developed classifier algorithm. As a further implementation of the procedure described, direct real-time processing, by an embedded computer carried onboard the drone, permitted the immediate plastics identification (and visual inspection in synchronized images) during the UAV survey, as documented by short video sequences provided in this research paper.},
DOI = {10.3390/rs13081557}
}



@Article{s21082834,
AUTHOR = {Kazaz, Billur and Poddar, Subhadipto and Arabi, Saeed and Perez, Michael A. and Sharma, Anuj and Whitman, J. Blake},
TITLE = {Deep Learning-Based Object Detection for Unmanned Aerial Systems (UASs)-Based Inspections of Construction Stormwater Practices},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2834},
URL = {https://www.mdpi.com/1424-8220/21/8/2834},
PubMedID = {33920610},
ISSN = {1424-8220},
ABSTRACT = {Construction activities typically create large amounts of ground disturbance, which can lead to increased rates of soil erosion. Construction stormwater practices are used on active jobsites to protect downstream waterbodies from offsite sediment transport. Federal and state regulations require routine pollution prevention inspections to ensure that temporary stormwater practices are in place and performing as intended. This study addresses the existing challenges and limitations in the construction stormwater inspections and presents a unique approach for performing unmanned aerial system (UAS)-based inspections. Deep learning-based object detection principles were applied to identify and locate practices installed on active construction sites. The system integrates a post-processing stage by clustering results. The developed framework consists of data preparation with aerial inspections, model training, validation of the model, and testing for accuracy. The developed model was created from 800 aerial images and was used to detect four different types of construction stormwater practices at 100% accuracy on the Mean Average Precision (MAP) with minimal false positive detections. Results indicate that object detection could be implemented on UAS-acquired imagery as a novel approach to construction stormwater inspections and provide accurate results for site plan comparisons by rapidly detecting the quantity and location of field-installed stormwater practices.},
DOI = {10.3390/s21082834}
}



@Article{rs13081562,
AUTHOR = {Ge, Xiangyu and Ding, Jianli and Jin, Xiuliang and Wang, Jingzhe and Chen, Xiangyue and Li, Xiaohang and Liu, Jie and Xie, Boqiang},
TITLE = {Estimating Agricultural Soil Moisture Content through UAV-Based Hyperspectral Images in the Arid Region},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1562},
URL = {https://www.mdpi.com/2072-4292/13/8/1562},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV)-based hyperspectral remote sensing is an important monitoring technology for the soil moisture content (SMC) of agroecological systems in arid regions. This technology develops precision farming and agricultural informatization. However, hyperspectral data are generally used in data mining. In this study, UAV-based hyperspectral imaging data with a resolution o 4 cm and totaling 70 soil samples (0–10 cm) were collected from farmland (2.5 × 104 m2) near Fukang City, Xinjiang Uygur Autonomous Region, China. Four estimation strategies were tested: the original image (strategy I), first- and second-order derivative methods (strategy II), the fractional-order derivative (FOD) technique (strategy III), and the optimal fractional order combined with the optimal multiband indices (strategy IV). These strategies were based on the eXtreme Gradient Boost (XGBoost) algorithm, with the aim of building the best estimation model for agricultural SMC in arid regions. The results demonstrated that FOD technology could effectively mine information (with an absolute maximum correlation coefficient of 0.768). By comparison, strategy IV yielded the best estimates out of the methods tested (R2val = 0.921, RMSEP = 1.943, and RPD = 2.736) for the SMC. The model derived from the order of 0.4 within strategy IV worked relatively well among the different derivative methods (strategy I, II, and III). In conclusion, the combination of FOD technology and the optimal multiband indices generated a highly accurate model within the XGBoost algorithm for SMC estimation. This research provided a promising data mining approach for UAV-based hyperspectral imaging data.},
DOI = {10.3390/rs13081562}
}



@Article{s21082835,
AUTHOR = {Hashima, Sherief and Hatano, Kohei and Kasban, Hany and Mahmoud Mohamed, Ehab},
TITLE = {Wi-Fi Assisted Contextual Multi-Armed Bandit for Neighbor Discovery and Selection in Millimeter Wave Device to Device Communications},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2835},
URL = {https://www.mdpi.com/1424-8220/21/8/2835},
PubMedID = {33920717},
ISSN = {1424-8220},
ABSTRACT = {The unique features of millimeter waves (mmWaves) motivate its leveraging to future, beyond-fifth-generation/sixth-generation (B5G/6G)-based device-to-device (D2D) communications. However, the neighborhood discovery and selection (NDS) problem still needs intelligent solutions due to the trade-off of investigating adjacent devices for the optimum device choice against the crucial beamform training (BT) overhead. In this paper, by making use of multiband (μW/mmWave) standard devices, the mmWave NDS problem is addressed using machine-learning-based contextual multi-armed bandit (CMAB) algorithms. This is done by leveraging the context information of Wi-Fi signal characteristics, i.e., received signal strength (RSS), mean, and variance, to further improve the NDS method. In this setup, the transmitting device acts as the player, the arms are the candidate mmWave D2D links between that device and its neighbors, while the reward is the average throughput. We examine the NDS’s primary trade-off and the impacts of the contextual information on the total performance. Furthermore, modified energy-aware linear upper confidence bound (EA-LinUCB) and contextual Thomson sampling (EA-CTS) algorithms are proposed to handle the problem through reflecting the nearby devices’ withstanding battery levels, which simulate real scenarios. Simulation results ensure the superior efficiency of the proposed algorithms over the single band (mmWave) energy-aware noncontextual MAB algorithms (EA-UCB and EA-TS) and traditional schemes regarding energy efficiency and average throughput with a reasonable convergence rate.},
DOI = {10.3390/s21082835}
}



@Article{su13084511,
AUTHOR = {Haque, Amlan and Islam, Nahina and Samrat, Nahidul Hoque and Dey, Shuvashis and Ray, Biplob},
TITLE = {Smart Farming through Responsible Leadership in Bangladesh: Possibilities, Opportunities, and Beyond},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {4511},
URL = {https://www.mdpi.com/2071-1050/13/8/4511},
ISSN = {2071-1050},
ABSTRACT = {Smart farming has the potential to overcome the challenge of 2050 to feed 10 billion people. Both artificial intelligence (AI) and the internet of things (IoT) have become critical prerequisites to smart farming due to their high interoperability, sensors, and cutting-edge technologies. Extending the role of responsible leadership, this paper proposes an AI and IoT based smart farming system in Bangladesh. With a comprehensive literature review, this paper counsels the need to go beyond the simple application of traditional farming and irrigation practices and recommends implementing smart farming enabling responsible leadership to uphold sustainable agriculture. It contributes to the current literature of smart farming in several ways. First, this paper helps to understand the prospect and challenges of both AI and IoT and the requirement of smart farming in a nonwestern context. Second, it clarifies the interventions of responsible leadership into Bangladesh’s agriculture sector and justifies the demand for sustainable smart farming. Third, this paper is a step forward to explore future empirical studies for the effective and efficient use of AI and IoT to adopt smart farming. Finally, this paper will help policymakers to take responsible initiatives to plan and apply smart farming in a developing economy like Bangladesh.},
DOI = {10.3390/su13084511}
}



@Article{s21082862,
AUTHOR = {Yanes Luis, Samuel and Gutiérrez-Reina, Daniel and Toral Marín, Sergio},
TITLE = {A Dimensional Comparison between Evolutionary Algorithm and Deep Reinforcement Learning Methodologies for Autonomous Surface Vehicles with Water Quality Sensors},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2862},
URL = {https://www.mdpi.com/1424-8220/21/8/2862},
PubMedID = {33921649},
ISSN = {1424-8220},
ABSTRACT = {The monitoring of water resources using Autonomous Surface Vehicles with water-quality sensors has been a recent approach due to the advances in unmanned transportation technology. The Ypacaraí Lake, the biggest water resource in Paraguay, suffers from a major contamination problem because of cyanobacteria blooms. In order to supervise the blooms using these on-board sensor modules, a Non-Homogeneous Patrolling Problem (a NP-hard problem) must be solved in a feasible amount of time. A dimensionality study is addressed to compare the most common methodologies, Evolutionary Algorithm and Deep Reinforcement Learning, in different map scales and fleet sizes with changes in the environmental conditions. The results determined that Deep Q-Learning overcomes the evolutionary method in terms of sample-efficiency by 50–70% in higher resolutions. Furthermore, it reacts better than the Evolutionary Algorithm in high space-state actions. In contrast, the evolutionary approach shows a better efficiency in lower resolutions and needs fewer parameters to synthesize robust solutions. This study reveals that Deep Q-learning approaches exceed in efficiency for the Non-Homogeneous Patrolling Problem but with many hyper-parameters involved in the stability and convergence.},
DOI = {10.3390/s21082862}
}



@Article{pr9040717,
AUTHOR = {Zhao, Yifan and Zhao, Xingdong and Dai, Jiajia and Yu, Wenlong},
TITLE = {Analysis of the Surface Subsidence Induced by Mining Near-Surface Thick Lead-Zinc Deposit Based on Numerical Simulation},
JOURNAL = {Processes},
VOLUME = {9},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {717},
URL = {https://www.mdpi.com/2227-9717/9/4/717},
ISSN = {2227-9717},
ABSTRACT = {This paper describes a case study of surface subsidence in the Hongling Lead-Zinc Mine. Hongling Lead-Zinc Mine is located in Inner Mongolia, China, about 240 km away from the border between China and Mongolia. There is a batch of outcrops of the near-surface thick steep-dip metamorphic orebody. The large-scale surface subsidence induced by underground excavation has brought some impact on the safety of herdsmen and their daily husbandry activities nearby. The requirements of reclamation for subsidence areas in the relevant laws and regulations, raise enormous pressure and risk on safe and economic operation. In this paper, a 3D numerical model of this mine was built by 3DMine and FLAC3D to analyse the excavation procedure and mechanism. The results of the simulation were in good agreement with the field subsidence data collected by satellites and unmanned aerial vehicles from 2009 to 2019. The analysis showed that the current mining method—an integrated underground method of stoping and caving—accelerated the surface subsidence, and some measures of monitoring, controlling and management were expected to take in order to improve economic and ecological benefits.},
DOI = {10.3390/pr9040717}
}



@Article{s21082864,
AUTHOR = {Zhang, Yuanping and Huang, Xiumei and Yang, Ming},
TITLE = {A Hybrid Visual Tracking Algorithm Based on SOM Network and Correlation Filter},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {2864},
URL = {https://www.mdpi.com/1424-8220/21/8/2864},
PubMedID = {33921720},
ISSN = {1424-8220},
ABSTRACT = {To meet the challenge of video target tracking, based on a self-organization mapping network (SOM) and correlation filter, a long-term visual tracking algorithm is proposed. Objects in different videos or images often have completely different appearance, therefore, the self-organization mapping neural network with the characteristics of signal processing mechanism of human brain neurons is used to perform adaptive and unsupervised features learning. A reliable method of robust target tracking is proposed, based on multiple adaptive correlation filters with a memory function of target appearance at the same time. Filters in our method have different updating strategies and can carry out long-term tracking cooperatively. The first is the displacement filter, a kernelized correlation filter that combines contextual characteristics to precisely locate and track targets. Secondly, the scale filters are used to predict the changing scale of a target. Finally, the memory filter is used to maintain the appearance of the target in long-term memory and judge whether the target has failed to track. If the tracking fails, the incremental learning detector is used to recover the target tracking in the way of sliding window. Several experiments show that our method can effectively solve the tracking problems such as severe occlusion, target loss and scale change, and is superior to the state-of-the-art methods in the aspects of efficiency, accuracy and robustness.},
DOI = {10.3390/s21082864}
}



@Article{rs13081599,
AUTHOR = {Seitsonen, Oula and Ikäheimo, Janne},
TITLE = {Detecting Archaeological Features with Airborne Laser Scanning in the Alpine Tundra of Sápmi, Northern Finland},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1599},
URL = {https://www.mdpi.com/2072-4292/13/8/1599},
ISSN = {2072-4292},
ABSTRACT = {Open access airborne laser scanning (ALS) data have been available in Finland for over a decade and have been actively applied by the Finnish archaeologists in that time. The low resolution of this laser scanning 2008–2019 dataset (0.5 points/m2), however, has hindered its usability for archaeological prospection. In the summer of 2020, the situation changed markedly, when the Finnish National Land Survey started a new countrywide ALS survey with a higher resolution of 5 points/m2. In this paper we present the first results of applying this newly available ALS material for archaeological studies. Finnish LIDARK consortium has initiated the development of semi-automated approaches for visualizing, detecting, and analyzing archaeological features with this new dataset. Our first case studies are situated in the Alpine tundra environment of Sápmi in northern Finland, and the assessed archaeological features range from prehistoric sites to indigenous Sámi reindeer herding features and Second Word War-era German military structures. Already the initial analyses of the new ALS-5p data show their huge potential for locating, mapping, and assessing archaeological material. These results also suggest an imminent burst in the number of known archaeological sites, especially in the poorly accessible and little studied northern wilderness areas, when more data become available.},
DOI = {10.3390/rs13081599}
}



@Article{rs13081602,
AUTHOR = {Sun, Qiaoqiao and Liu, Xuefeng and Bourennane, Salah},
TITLE = {Unsupervised Multi-Level Feature Extraction for Improvement of Hyperspectral Classification},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {8},
ARTICLE-NUMBER = {1602},
URL = {https://www.mdpi.com/2072-4292/13/8/1602},
ISSN = {2072-4292},
ABSTRACT = {Deep learning models have strong abilities in learning features and they have been successfully applied in hyperspectral images (HSIs). However, the training of most deep learning models requires labeled samples and the collection of labeled samples are labor-consuming in HSI. In addition, single-level features from a single layer are usually considered, which may result in the loss of some important information. Using multiple networks to obtain multi-level features is a solution, but at the cost of longer training time and computational complexity. To solve these problems, a novel unsupervised multi-level feature extraction framework that is based on a three dimensional convolutional autoencoder (3D-CAE) is proposed in this paper. The designed 3D-CAE is stacked by fully 3D convolutional layers and 3D deconvolutional layers, which allows for the spectral-spatial information of targets to be mined simultaneously. Besides, the 3D-CAE can be trained in an unsupervised way without involving labeled samples. Moreover, the multi-level features are directly obtained from the encoded layers with different scales and resolutions, which is more efficient than using multiple networks to get them. The effectiveness of the proposed multi-level features is verified on two hyperspectral data sets. The results demonstrate that the proposed method has great promise in unsupervised feature learning and can help us to further improve the hyperspectral classification when compared with single-level features.},
DOI = {10.3390/rs13081602}
}



