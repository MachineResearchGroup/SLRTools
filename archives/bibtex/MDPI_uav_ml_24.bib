
@Article{rs11091025,
AUTHOR = {Li, Weijia and He, Conghui and Fu, Haohuan and Zheng, Juepeng and Dong, Runmin and Xia, Maocai and Yu, Le and Luk, Wayne},
TITLE = {A Real-Time Tree Crown Detection Approach for Large-Scale Remote Sensing Images on FPGAs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1025},
URL = {https://www.mdpi.com/2072-4292/11/9/1025},
ISSN = {2072-4292},
ABSTRACT = {The on-board real-time tree crown detection from high-resolution remote sensing images is beneficial for avoiding the delay between data acquisition and processing, reducing the quantity of data transmission from the satellite to the ground, monitoring the growing condition of individual trees, and discovering the damage of trees as early as possible, etc. Existing high performance platform based tree crown detection studies either focus on processing images in a small size or suffer from high power consumption or slow processing speed. In this paper, we propose the first FPGA-based real-time tree crown detection approach for large-scale satellite images. A pipelined-friendly and resource-economic tree crown detection algorithm (PF-TCD) is designed through reconstructing and modifying the workflow of the original algorithm into three computational kernels on FPGAs. Compared with the well-optimized software implementation of the original algorithm on an Intel 12-core CPU, our proposed PF-TCD obtains the speedup of 18.75 times for a satellite image with a size of 12,188 &times; 12,576 pixels without reducing the detection accuracy. The image processing time for the large-scale remote sensing image is only 0.33 s, which satisfies the requirements of the on-board real-time data processing on satellites.},
DOI = {10.3390/rs11091025}
}



@Article{beverages5020033,
AUTHOR = {Gonzalez Viejo, Claudia and Torrico, Damir D. and Dunshea, Frank R. and Fuentes, Sigfredo},
TITLE = {Development of Artificial Neural Network Models to Assess Beer Acceptability Based on Sensory Properties Using a Robotic Pourer: A Comparative Model Approach to Achieve an Artificial Intelligence System},
JOURNAL = {Beverages},
VOLUME = {5},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {33},
URL = {https://www.mdpi.com/2306-5710/5/2/33},
ISSN = {2306-5710},
ABSTRACT = {Artificial neural networks (ANN) have become popular for optimization and prediction of parameters in foods, beverages, agriculture and medicine. For brewing, they have been explored to develop rapid methods to assess product quality and acceptability. Different beers (N = 17) were analyzed in triplicates using a robotic pourer, RoboBEER (University of Melbourne, Melbourne, Australia), to assess 15 color and foam-related parameters using computer-vision. Those samples were tested using sensory analysis for acceptability of carbonation mouthfeel, bitterness, flavor and overall liking with 30 consumers using a 9-point hedonic scale. ANN models were developed using 17 different training algorithms with 15 color and foam-related parameters as inputs and liking of four descriptors obtained from consumers as targets. Each algorithm was tested using five, seven and ten neurons and compared to select the best model based on correlation coefficients, slope and performance (mean squared error (MSE). Bayesian Regularization algorithm with seven neurons presented the best correlation (R = 0.98) and highest performance (MSE = 0.03) with no overfitting. These models may be used as a cost-effective method for fast-screening of beers during processing to assess acceptability more efficiently. The use of RoboBEER, computer-vision algorithms and ANN will allow the implementation of an artificial intelligence system for the brewing industry to assess its effectiveness.},
DOI = {10.3390/beverages5020033}
}



@Article{rs11091040,
AUTHOR = {He, Haiqing and Zhou, Junchao and Chen, Min and Chen, Ting and Li, Dajun and Cheng, Penggen},
TITLE = {Building Extraction from UAV Images Jointly Using 6D-SLIC and Multiscale Siamese Convolutional Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1040},
URL = {https://www.mdpi.com/2072-4292/11/9/1040},
ISSN = {2072-4292},
ABSTRACT = {Automatic building extraction using a single data type, either 2D remotely-sensed images or light detection and ranging 3D point clouds, remains insufficient to accurately delineate building outlines for automatic mapping, despite active research in this area and the significant progress which has been achieved in the past decade. This paper presents an effective approach to extracting buildings from Unmanned Aerial Vehicle (UAV) images through the incorporation of superpixel segmentation and semantic recognition. A framework for building extraction is constructed by jointly using an improved Simple Linear Iterative Clustering (SLIC) algorithm and Multiscale Siamese Convolutional Networks (MSCNs). The SLIC algorithm, improved by additionally imposing a digital surface model for superpixel segmentation, namely 6D-SLIC, is suited for building boundary detection under building and image backgrounds with similar radiometric signatures. The proposed MSCNs, including a feature learning network and a binary decision network, are used to automatically learn a multiscale hierarchical feature representation and detect building objects under various complex backgrounds. In addition, a gamma-transform green leaf index is proposed to truncate vegetation superpixels for further processing to improve the robustness and efficiency of building detection, the Douglas&ndash;Peucker algorithm and iterative optimization are used to eliminate jagged details generated from small structures as a result of superpixel segmentation. In the experiments, the UAV datasets, including many buildings in urban and rural areas with irregular shapes and different heights and that are obscured by trees, are collected to evaluate the proposed method. The experimental results based on the qualitative and quantitative measures confirm the effectiveness and high accuracy of the proposed framework relative to the digitized results. The proposed framework performs better than state-of-the-art building extraction methods, given its higher values of recall, precision, and intersection over Union (IoU).},
DOI = {10.3390/rs11091040}
}



@Article{sym11050617,
AUTHOR = {Li, Zhiwei and Lu, Yu and Shi, Yun and Wang, Zengguang and Qiao, Wenxin and Liu, Yicen},
TITLE = {A Dyna-Q-Based Solution for UAV Networks Against Smart Jamming Attacks},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {617},
URL = {https://www.mdpi.com/2073-8994/11/5/617},
ISSN = {2073-8994},
ABSTRACT = {Unmanned aerial vehicle (UAV) networks have a wide range of applications, such as in the Internet of Things (IoT), 5G communications, and so forth. However, the communications between UAVs and UAVs to ground control stations mainly use radio channels, and therefore these communications are vulnerable to cyberattacks. With the advent of software-defined radio (SDR), smart attacks that can flexibly select attack strategies according to the defender&rsquo;s state information are gradually attracting the attention of researchers and potential attackers of UAV networks. The smart attack can even induce the defender to take a specific defense strategy, causing even greater damage. Inspired by symmetrical thinking, a solution using a software-defined network (SDN) to combat software-defined radio was proposed. We propose a network architecture which uses dual controllers, including a UAV flight controller and SDN controller, to achieve collaborative decision-making. Built on the top of the SDN, the state information of the whole network converges quickly and is fitted to an environment model used to develop an improved Dyna-Q-based reinforcement learning algorithm. The improved algorithm integrates the power allocation and track planning of UAVs into a unified action space. The simulation data showed that the proposed communication solution can effectively avoid smart jamming attacks and has faster learning efficiency and higher convergence performance than the compared algorithms.},
DOI = {10.3390/sym11050617}
}



@Article{s19092130,
AUTHOR = {Zemmour, Elie and Kurtser, Polina and Edan, Yael},
TITLE = {Automatic Parameter Tuning for Adaptive Thresholding in Fruit Detection},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {2130},
URL = {https://www.mdpi.com/1424-8220/19/9/2130},
ISSN = {1424-8220},
ABSTRACT = {This paper presents an automatic parameter tuning procedure specially developed for a dynamic adaptive thresholding algorithm for fruit detection. One of the major algorithm strengths is its high detection performances using a small set of training images. The algorithm enables robust detection in highly-variable lighting conditions. The image is dynamically split into variably-sized regions, where each region has approximately homogeneous lighting conditions. Nine thresholds were selected to accommodate three different illumination levels for three different dimensions in four color spaces: RGB, HSI, LAB, and NDI. Each color space uses a different method to represent a pixel in an image: RGB (Red, Green, Blue), HSI (Hue, Saturation, Intensity), LAB (Lightness, Green to Red and Blue to Yellow) and NDI (Normalized Difference Index, which represents the normal difference between the RGB color dimensions). The thresholds were selected by quantifying the required relation between the true positive rate and false positive rate. A tuning process was developed to determine the best fit values of the algorithm parameters to enable easy adaption to different kinds of fruits (shapes, colors) and environments (illumination conditions). Extensive analyses were conducted on three different databases acquired in natural growing conditions: red apples (nine images with 113 apples), green grape clusters (129 images with 1078 grape clusters), and yellow peppers (30 images with 73 peppers). These databases are provided as part of this paper for future developments. The algorithm was evaluated using cross-validation with 70% images for training and 30% images for testing. The algorithm successfully detected apples and peppers in variable lighting conditions resulting with an F-score of 93.17% and 99.31% respectively. Results show the importance of the tuning process for the generalization of the algorithm to different kinds of fruits and environments. In addition, this research revealed the importance of evaluating different color spaces since for each kind of fruit, a different color space might be superior over the others. The LAB color space is most robust to noise. The algorithm is robust to changes in the threshold learned by the training process and to noise effects in images.},
DOI = {10.3390/s19092130}
}



@Article{rs11091128,
AUTHOR = {Rahnemoonfar, Maryam and Dobbs, Dugan and Yari, Masoud and Starek, Michael J.},
TITLE = {DisCountNet: Discriminating and Counting Network for Real-Time Counting and Localization of Sparse Objects in High-Resolution UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1128},
URL = {https://www.mdpi.com/2072-4292/11/9/1128},
ISSN = {2072-4292},
ABSTRACT = {Recent deep-learning counting techniques revolve around two distinct features of data&mdash;sparse data, which favors detection networks, or dense data where density map networks are used. Both techniques fail to address a third scenario, where dense objects are sparsely located. Raw aerial images represent sparse distributions of data in most situations. To address this issue, we propose a novel and exceedingly portable end-to-end model, DisCountNet, and an example dataset to test it on. DisCountNet is a two-stage network that uses theories from both detection and heat-map networks to provide a simple yet powerful design. The first stage, DiscNet, operates on the theory of coarse detection, but does so by converting a rich and high-resolution image into a sparse representation where only important information is encoded. Following this, CountNet operates on the dense regions of the sparse matrix to generate a density map, which provides fine locations and count predictions on densities of objects. Comparing the proposed network to current state-of-the-art networks, we find that we can maintain competitive performance while using a fraction of the computational complexity, resulting in a real-time solution.},
DOI = {10.3390/rs11091128}
}



@Article{app9091952,
AUTHOR = {Petrellis, Nikos},
TITLE = {Plant Disease Diagnosis for Smart Phone Applications with Extensible Set of Diseases},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1952},
URL = {https://www.mdpi.com/2076-3417/9/9/1952},
ISSN = {2076-3417},
ABSTRACT = {A plant disease diagnosis method that can be implemented with the resources of a mobile phone application, that does not have to be connected to a remote server, is presented and evaluated on citrus diseases. It can be used both by amateur gardeners and by professional agriculturists for early detection of diseases. The features used are extracted from photographs of plant parts like leaves or fruits and include the color, the relative area and the number of the lesion spots. These classification features, along with additional information like weather metadata, form disease signatures that can be easily defined by the end user (e.g., an agronomist). These signatures are based on the statistical processing of a small number of representative training photographs. The extracted features of a test photograph are compared against the disease signatures in order to select the most likely disease. An important advantage of the proposed approach is that the diagnosis does not depend on the orientation, the scale or the resolution of the photograph. The experiments have been conducted under several light exposure conditions. The accuracy was experimentally measured between 70% and 99%. An acceptable accuracy higher than 90% can be achieved in most of the cases since the lesion spots can recognized interactively with high precision.},
DOI = {10.3390/app9091952}
}



@Article{rs11101153,
AUTHOR = {Bejiga, Mesay Belete and Melgani, Farid and Beraldini, Pietro},
TITLE = {Domain Adversarial Neural Networks for Large-Scale Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1153},
URL = {https://www.mdpi.com/2072-4292/11/10/1153},
ISSN = {2072-4292},
ABSTRACT = {Learning classification models require sufficiently labeled training samples, however, collecting labeled samples for every new problem is time-consuming and costly. An alternative approach is to transfer knowledge from one problem to another, which is called transfer learning. Domain adaptation (DA) is a type of transfer learning that aims to find a new latent space where the domain discrepancy between the source and the target domain is negligible. In this work, we propose an unsupervised DA technique called domain adversarial neural networks (DANNs), composed of a feature extractor, a class predictor, and domain classifier blocks, for large-scale land cover classification. Contrary to the traditional methods that perform representation and classifier learning in separate stages, DANNs combine them into a single stage, thereby learning a new representation of the input data that is both domain-invariant and discriminative. Once trained, the classifier of a DANN can be used to predict both source and target domain labels. Additionally, we also modify the domain classifier of a DANN to evaluate its suitability for multi-target domain adaptation problems. Experimental results obtained for both single and multiple target DA problems show that the proposed method provides a performance gain of up to 40%.},
DOI = {10.3390/rs11101153}
}



@Article{rs11101157,
AUTHOR = {Fuentes-Pacheco, Jorge and Torres-Olivares, Juan and Roman-Rangel, Edgar and Cervantes, Salvador and Juarez-Lopez, Porfirio and Hermosillo-Valadez, Jorge and Rendón-Mancha, Juan Manuel},
TITLE = {Fig Plant Segmentation from Aerial Images Using a Deep Convolutional Encoder-Decoder Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1157},
URL = {https://www.mdpi.com/2072-4292/11/10/1157},
ISSN = {2072-4292},
ABSTRACT = {Crop segmentation is an important task in Precision Agriculture, where the use of aerial robots with an on-board camera has contributed to the development of new solution alternatives. We address the problem of fig plant segmentation in top-view RGB (Red-Green-Blue) images of a crop grown under open-field difficult circumstances of complex lighting conditions and non-ideal crop maintenance practices defined by local farmers. We present a Convolutional Neural Network (CNN) with an encoder-decoder architecture that classifies each pixel as crop or non-crop using only raw colour images as input. Our approach achieves a mean accuracy of 93.85% despite the complexity of the background and a highly variable visual appearance of the leaves. We make available our CNN code to the research community, as well as the aerial image data set and a hand-made ground truth segmentation with pixel precision to facilitate the comparison among different algorithms.},
DOI = {10.3390/rs11101157}
}



@Article{s19102251,
AUTHOR = {Shan, Zeyong and Li, Ruijian and Schwertfeger, Sören},
TITLE = {RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2251},
URL = {https://www.mdpi.com/1424-8220/19/10/2251},
ISSN = {1424-8220},
ABSTRACT = {Using camera sensors for ground robot Simultaneous Localization and Mapping (SLAM) has many benefits over laser-based approaches, such as the low cost and higher robustness. RGBD sensors promise the best of both worlds: dense data from cameras with depth information. This paper proposes to fuse RGBD and IMU data for a visual SLAM system, called VINS-RGBD, that is built upon the open source VINS-Mono software. The paper analyses the VINS approach and highlights the observability problems. Then, we extend the VINS-Mono system to make use of the depth data during the initialization process as well as during the VIO (Visual Inertial Odometry) phase. Furthermore, we integrate a mapping system based on subsampled depth data and octree filtering to achieve real-time mapping, including loop closing. We provide the software as well as datasets for evaluation. Our extensive experiments are performed with hand-held, wheeled and tracked robots in different environments. We show that ORB-SLAM2 fails for our application and see that our VINS-RGBD approach is superior to VINS-Mono.},
DOI = {10.3390/s19102251}
}



@Article{app9101997,
AUTHOR = {Muñoz–Bañón, Miguel Á. and del Pino, Iván and Candelas, Francisco A. and Torres, Fernando},
TITLE = {Framework for Fast Experimental Testing of Autonomous Navigation Algorithms},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1997},
URL = {https://www.mdpi.com/2076-3417/9/10/1997},
ISSN = {2076-3417},
ABSTRACT = {Research in mobile robotics requires fully operative autonomous systems to test and compare algorithms in real-world conditions. However, the implementation of such systems remains to be a highly time-consuming process. In this work, we present an robot operating system (ROS)-based navigation framework that allows the generation of new autonomous navigation applications in a fast and simple way. Our framework provides a powerful basic structure based on abstraction levels that ease the implementation of minimal solutions with all the functionalities required to implement a whole autonomous system. This approach helps to keep the focus in any sub-problem of interest (i.g. localization or control) while permitting to carry out experimental tests in the context of a complete application. To show the validity of the proposed framework we implement an autonomous navigation system for a ground robot using a localization module that fuses global navigation satellite system (GNSS) positioning and Monte Carlo localization by means of a Kalman filter. Experimental tests are performed in two different outdoor environments, over more than twenty kilometers. All the developed software is available in a GitHub repository.},
DOI = {10.3390/app9101997}
}



@Article{app9102009,
AUTHOR = {Han, Jiaming and Yang, Zhong and Zhang, Qiuyan and Chen, Cong and Li, Hongchen and Lai, Shangxiang and Hu, Guoxiong and Xu, Changliang and Xu, Hao and Wang, Di and Chen, Rui},
TITLE = {A Method of Insulator Faults Detection in Aerial Images for High-Voltage Transmission Lines Inspection},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2009},
URL = {https://www.mdpi.com/2076-3417/9/10/2009},
ISSN = {2076-3417},
ABSTRACT = {Insulator faults detection is an important task for high-voltage transmission line inspection. However, current methods often suffer from the lack of accuracy and robustness. Moreover, these methods can only detect one fault in the insulator string, but cannot detect a multi-fault. In this paper, a novel method is proposed for insulator one fault and multi-fault detection in UAV-based aerial images, the backgrounds of which usually contain much complex interference. The shapes of the insulators also vary obviously due to the changes in filming angle and distance. To reduce the impact of complex interference on insulator faults detection, we make full use of the deep neural network to distinguish between insulators and background interference. First of all, plenty of insulator aerial images with manually labelled ground-truth are collected to construct a standard insulator detection dataset &lsquo;InST_detection&rsquo;. Secondly, a new convolutional network is proposed to obtain accurate insulator string positions in the aerial image. Finally, a novel fault detection method is proposed that can detect both insulator one fault and multi-fault in aerial images. Experimental results on a large number of aerial images show that our proposed method is more effective and efficient than the state-of-the-art insulator fault detection methods.},
DOI = {10.3390/app9102009}
}



@Article{s19102271,
AUTHOR = {Bi, Fukun and Hou, Jinyuan and Chen, Liang and Yang, Zhihua and Wang, Yanping},
TITLE = {Ship Detection for Optical Remote Sensing Images Based on Visual Attention Enhanced Network},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2271},
URL = {https://www.mdpi.com/1424-8220/19/10/2271},
ISSN = {1424-8220},
ABSTRACT = {Ship detection plays a significant role in military and civil fields. Although some state-of-the-art detection methods, based on convolutional neural networks (CNN) have certain advantages, they still cannot solve the challenge well, including the large size of images, complex scene structure, a large amount of false alarm interference, and inshore ships. This paper proposes a ship detection method from optical remote sensing images, based on visual attention enhanced network. To effectively reduce false alarm in non-ship area and improve the detection efficiency from remote sensing images, we developed a light-weight local candidate scene network(     L 2     CSN) to extract the local candidate scenes with ships. Then, for the selected local candidate scenes, we propose a ship detection method, based on the visual attention DSOD(VA-DSOD). Here, to enhance the detection performance and positioning accuracy of inshore ships, we both extract semantic features, based on DSOD and embed a visual attention enhanced network in DSOD to extract the visual features. We test the detection method on a large number of typical remote sensing datasets, which consist of Google Earth images and GaoFen-2 images. We regard the state-of-the-art method [sliding window DSOD (SW+DSOD)] as a baseline, which achieves the average precision (AP) of 82.33%. The AP of the proposed method increases by 7.53%. The detection and location performance of our proposed method outperforms the baseline in complex remote sensing scenes.},
DOI = {10.3390/s19102271}
}



@Article{sym11050678,
AUTHOR = {Wang, Bodi and Liu, Guixiong and Wu, Junfang},
TITLE = {Blind Deblurring of Saturated Images Based on Optimization and Deep Learning for Dynamic Visual Inspection on the Assembly Line},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {678},
URL = {https://www.mdpi.com/2073-8994/11/5/678},
ISSN = {2073-8994},
ABSTRACT = {Image deblurring can improve visual quality and mitigates motion blur for dynamic visual inspection. We propose a method to deblur saturated images for dynamic visual inspection by applying blur kernel estimation and deconvolution modeling. The blur kernel is estimated in a transform domain, whereas the deconvolution model is decoupled into deblurring and denoising stages via variable splitting. Deblurring predicts the mask specifying saturated pixels, which are then discarded, and denoising is learned via the fast and flexible denoising network (FFDNet) convolutional neural network (CNN) at a wide range of noise levels. Hence, the proposed deconvolution model provides the benefits of both model optimization and deep learning. Experiments demonstrate that the proposed method suitably restores visual quality and outperforms existing approaches with good score improvements.},
DOI = {10.3390/sym11050678}
}



@Article{rs11101174,
AUTHOR = {Sheykhmousa, Mohammadreza and Kerle, Norman and Kuffer, Monika and Ghaffarian, Saman},
TITLE = {Post-Disaster Recovery Assessment with Machine Learning-Derived Land Cover and Land Use Information},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1174},
URL = {https://www.mdpi.com/2072-4292/11/10/1174},
ISSN = {2072-4292},
ABSTRACT = {Post-disaster recovery (PDR) is a complex, long-lasting, resource intensive, and poorly understood process. PDR goes beyond physical reconstruction (physical recovery) and includes relevant processes such as economic and social (functional recovery) processes. Knowing the size and location of the places that positively or negatively recovered is important to effectively support policymakers to help readjust planning and resource allocation to rebuild better. Disasters and the subsequent recovery are mainly expressed through unique land cover and land use changes (LCLUCs). Although LCLUCs have been widely studied in remote sensing, their value for recovery assessment has not yet been explored, which is the focus of this paper. An RS-based methodology was created for PDR assessment based on multi-temporal, very high-resolution satellite images. Different trajectories of change were analyzed and evaluated, i.e., transition patterns (TPs) that signal positive or negative recovery. Experimental analysis was carried out on three WorldView-2 images acquired over Tacloban city, Philippines, which was heavily affected by Typhoon Haiyan in 2013. Support vector machine, a robust machine learning algorithm, was employed with texture features extracted from the grey level co-occurrence matrix and local binary patterns. Although classification results for the images before and four years after the typhoon show high accuracy, substantial uncertainties mark the results for the immediate post-event image. All land cover (LC) and land use (LU) classified maps were stacked, and only changes related to TPs were extracted. The final products are LC and LU recovery maps that quantify the PDR process at the pixel level. It was found that physical and functional recovery can be mainly explained through the LCLUC information. In addition, LC and LU-based recovery maps support a general and a detailed recovery understanding, respectively. It is therefore suggested to use the LC and LU-based recovery maps to monitor and support the short and the long-term recovery, respectively.},
DOI = {10.3390/rs11101174}
}



@Article{rs11101180,
AUTHOR = {Buters, Todd M. and Bateman, Philip W. and Robinson, Todd and Belton, David and Dixon, Kingsley W. and Cross, Adam T.},
TITLE = {Methodological Ambiguity and Inconsistency Constrain Unmanned Aerial Vehicles as A Silver Bullet for Monitoring Ecological Restoration},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1180},
URL = {https://www.mdpi.com/2072-4292/11/10/1180},
ISSN = {2072-4292},
ABSTRACT = {The last decade has seen an exponential increase in the application of unmanned aerial vehicles (UAVs) to ecological monitoring research, though with little standardisation or comparability in methodological approaches and research aims. We reviewed the international peer-reviewed literature in order to explore the potential limitations on the feasibility of UAV-use in the monitoring of ecological restoration, and examined how they might be mitigated to maximise the quality, reliability and comparability of UAV-generated data. We found little evidence of translational research applying UAV-based approaches to ecological restoration, with less than 7% of 2133 published UAV monitoring studies centred around ecological restoration. Of the 48 studies, &gt; 65% had been published in the three years preceding this study. Where studies utilised UAVs for rehabilitation or restoration applications, there was a strong propensity for single-sensor monitoring using commercially available RPAs fitted with the modest-resolution RGB sensors available. There was a strong positive correlation between the use of complex and expensive sensors (e.g., LiDAR, thermal cameras, hyperspectral sensors) and the complexity of chosen image classification techniques (e.g., machine learning), suggesting that cost remains a primary constraint to the wide application of multiple or complex sensors in UAV-based research. We propose that if UAV-acquired data are to represent the future of ecological monitoring, research requires a) consistency in the proven application of different platforms and sensors to the monitoring of target landforms, organisms and ecosystems, underpinned by clearly articulated monitoring goals and outcomes; b) optimization of data analysis techniques and the manner in which data are reported, undertaken in cross-disciplinary partnership with fields such as bioinformatics and machine learning; and c) the development of sound, reasonable and multi-laterally homogenous regulatory and policy framework supporting the application of UAVs to the large-scale and potentially trans-disciplinary ecological applications of the future.},
DOI = {10.3390/rs11101180}
}



@Article{app9102066,
AUTHOR = {Abioye, Ayodeji Opeyemi and Prior, Stephen D. and Saddington, Peter and Ramchurn, Sarvapali D.},
TITLE = {Effects of Varying Noise Levels and Lighting Levels on Multimodal Speech and Visual Gesture Interaction with Aerobots},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2066},
URL = {https://www.mdpi.com/2076-3417/9/10/2066},
ISSN = {2076-3417},
ABSTRACT = {This paper investigated the effects of varying noise levels and varying lighting levels on speech and gesture control command interfaces for aerobots. The aim was to determine the practical suitability of the multimodal combination of speech and visual gesture in human aerobotic interaction, by investigating the limits and feasibility of use of the individual components. In order to determine this, a custom multimodal speech and visual gesture interface was developed using CMU (Carnegie Mellon University) sphinx and OpenCV (Open source Computer Vision) libraries, respectively. An experiment study was designed to measure the individual effects of each of the two main components of speech and gesture, and 37 participants were recruited to participate in the experiment. The ambient noise level was varied from 55 dB to 85 dB. The ambient lighting level was varied from 10 Lux to 1400 Lux, under different lighting colour temperature mixtures of yellow (3500 K) and white (5500 K), and different background for capturing the finger gestures. The results of the experiment, which consisted of around 3108 speech utterance and 999 gesture quality observations, were presented and discussed. It was observed that speech recognition accuracy/success rate falls as noise levels rise, with 75 dB noise level being the aerobot&rsquo;s practical application limit, as the speech control interaction becomes very unreliable due to poor recognition beyond this. It was concluded that multi-word speech commands were considered more reliable and effective than single-word speech commands. In addition, some speech command words (e.g., land) were more noise resistant than others (e.g., hover) at higher noise levels, due to their articulation. From the results of the gesture-lighting experiment, the effects of both lighting conditions and the environment background on the quality of gesture recognition, was almost insignificant, less than 0.5%. The implication of this is that other factors such as the gesture capture system design and technology (camera and computer hardware), type of gesture being captured (upper body, whole body, hand, fingers, or facial gestures), and the image processing technique (gesture classification algorithms), are more important in developing a successful gesture recognition system. Some further works were suggested based on the conclusions drawn from this findings which included using alternative ASR (Automatic Speech Recognition) speech models and developing more robust gesture recognition algorithm.},
DOI = {10.3390/app9102066}
}



@Article{ijgi8050240,
AUTHOR = {Kim, Nari and Ha, Kyung-Ja and Park, No-Wook and Cho, Jaeil and Hong, Sungwook and Lee, Yang-Won},
TITLE = {A Comparison Between Major Artificial Intelligence Models for Crop Yield Prediction: Case Study of the Midwestern United States, 2006–2015},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {240},
URL = {https://www.mdpi.com/2220-9964/8/5/240},
ISSN = {2220-9964},
ABSTRACT = {This paper compares different artificial intelligence (AI) models in order to develop the best crop yield prediction model for the Midwestern United States (US). Through experiments to examine the effects of phenology using three different periods, we selected the July&ndash;August (JA) database as the best months to predict corn and soybean yields. Six different AI models for crop yield prediction are tested in this research. Then, a comprehensive and objective comparison is conducted between the AI models. Particularly for the deep neural network (DNN) model, we performed an optimization process to ensure the best configurations for the layer structure, cost function, optimizer, activation function, and drop-out ratio. In terms of mean absolute error (MAE), our DNN model with the JA database was approximately 21&ndash;33% and 17&ndash;22% more accurate for corn and soybean yields, respectively, than the other five AI models. This indicates that corn and soybean yields for a given year can be forecasted in advance, at the beginning of September, approximately a month or more ahead of harvesting time. A combination of the optimized DNN model and spatial statistical methods should be investigated in future work, to mitigate partly clustered errors in some regions.},
DOI = {10.3390/ijgi8050240}
}



@Article{agronomy9050258,
AUTHOR = {Chawade, Aakash and van Ham, Joost and Blomquist, Hanna and Bagge, Oscar and Alexandersson, Erik and Ortiz, Rodomiro},
TITLE = {High-Throughput Field-Phenotyping Tools for Plant Breeding and Precision Agriculture},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {258},
URL = {https://www.mdpi.com/2073-4395/9/5/258},
ISSN = {2073-4395},
ABSTRACT = {High-throughput field phenotyping has garnered major attention in recent years leading to the development of several new protocols for recording various plant traits of interest. Phenotyping of plants for breeding and for precision agriculture have different requirements due to different sizes of the plots and fields, differing purposes and the urgency of the action required after phenotyping. While in plant breeding phenotyping is done on several thousand small plots mainly to evaluate them for various traits, in plant cultivation, phenotyping is done in large fields to detect the occurrence of plant stresses and weeds at an early stage. The aim of this review is to highlight how various high-throughput phenotyping methods are used for plant breeding and farming and the key differences in the applications of such methods. Thus, various techniques for plant phenotyping are presented together with applications of these techniques for breeding and cultivation. Several examples from the literature using these techniques are summarized and the key technical aspects are highlighted.},
DOI = {10.3390/agronomy9050258}
}



@Article{rs11101241,
AUTHOR = {Li, Jing and Chen, Shuo and Zhang, Fangbing and Li, Erkang and Yang, Tao and Lu, Zhaoyang},
TITLE = {An Adaptive Framework for Multi-Vehicle Ground Speed Estimation in Airborne Videos},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1241},
URL = {https://www.mdpi.com/2072-4292/11/10/1241},
ISSN = {2072-4292},
ABSTRACT = {With the rapid development of unmanned aerial vehicles (UAVs), UAV-based intelligent airborne surveillance systems represented by real-time ground vehicle speed estimation have attracted wide attention from researchers. However, there are still many challenges in extracting speed information from UAV videos, including the dynamic moving background, small target size, complicated environment, and diverse scenes. In this paper, we propose a novel adaptive framework for multi-vehicle ground speed estimation in airborne videos. Firstly, we build a traffic dataset based on UAV. Then, we use the deep learning detection algorithm to detect the vehicle in the UAV field of view and obtain the trajectory in the image through the tracking-by-detection algorithm. Thereafter, we present a motion compensation method based on homography. This method obtains matching feature points by an optical flow method and eliminates the influence of the detected target to accurately calculate the homography matrix to determine the real motion trajectory in the current frame. Finally, vehicle speed is estimated based on the mapping relationship between the pixel distance and the actual distance. The method regards the actual size of the car as prior information and adaptively recovers the pixel scale by estimating the vehicle size in the image; it then calculates the vehicle speed. In order to evaluate the performance of the proposed system, we carry out a large number of experiments on the AirSim Simulation platform as well as real UAV aerial surveillance experiments. Through quantitative and qualitative analysis of the simulation results and real experiments, we verify that the proposed system has a unique ability to detect, track, and estimate the speed of ground vehicles simultaneously even with a single downward-looking camera. Additionally, the system can obtain effective and accurate speed estimation results, even in various complex scenes.},
DOI = {10.3390/rs11101241}
}



@Article{rs11101243,
AUTHOR = {Ge, Xuming and Wu, Bo and Li, Yuan and Hu, Han},
TITLE = {A Multi-Primitive-Based Hierarchical Optimal Approach for Semantic Labeling of ALS Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1243},
URL = {https://www.mdpi.com/2072-4292/11/10/1243},
ISSN = {2072-4292},
ABSTRACT = {There are normally three main steps to carrying out the labeling of airborne laser scanning (ALS) point clouds. The first step is to use appropriate primitives to represent the scanning scenes, the second is to calculate the discriminative features of each primitive, and the third is to introduce a classifier to label the point clouds. This paper investigates multiple primitives to effectively represent scenes and exploit their geometric relationships. Relationships are graded according to the properties of related primitives. Then, based on initial labeling results, a novel, hierarchical, and optimal strategy is developed to optimize semantic labeling results. The proposed approach was tested using two sets of representative ALS point clouds, namely the Vaihingen datasets and Hong Kong&rsquo;s Central District dataset. The results were compared with those generated by other typical methods in previous work. Quantitative assessments for the two experimental datasets showed that the performance of the proposed approach was superior to reference methods in both datasets. The scores for correctness attained over 98% in all cases of the Vaihingen datasets and up to 96% in the Hong Kong dataset. The results reveal that our approach of labeling different classes in terms of ALS point clouds is robust and bears significance for future applications, such as 3D modeling and change detection from point clouds.},
DOI = {10.3390/rs11101243}
}



@Article{electronics8050576,
AUTHOR = {You, Shixun and Diao, Ming and Gao, Lipeng},
TITLE = {Completing Explorer Games with a Deep Reinforcement Learning Framework Based on Behavior Angle Navigation},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {5},
ARTICLE-NUMBER = {576},
URL = {https://www.mdpi.com/2079-9292/8/5/576},
ISSN = {2079-9292},
ABSTRACT = {In cognitive electronic warfare, when a typical combat vehicle, such as an unmanned combat air vehicle (UCAV), uses radar sensors to explore an unknown space, the target-searching fails due to an inefficient servoing/tracking system. Thus, to solve this problem, we developed an autonomous reasoning search method that can generate efficient decision-making actions and guide the UCAV as early as possible to the target area. For high-dimensional continuous action space, the UCAV&rsquo;s maneuvering strategies are subject to certain physical constraints. We first record the path histories of the UCAV as a sample set of supervised experiments and then construct a grid cell network using long short-term memory (LSTM) to generate a new displacement prediction to replace the target location estimation. Finally, we enable a variety of continuous-control-based deep reinforcement learning algorithms to output optimal/sub-optimal decision-making actions. All these tasks are performed in a three-dimensional target-searching simulator, i.e., the Explorer game. Please note that we use the behavior angle (BHA) for the first time as the main factor of the reward-shaping of the deep reinforcement learning framework and successfully make the trained UCAV achieve a 99.96% target destruction rate, i.e., the game win rate, in a 0.1 s operating cycle.},
DOI = {10.3390/electronics8050576}
}



@Article{s19102396,
AUTHOR = {Lin, Shijie and Wang, Jinwang and Peng, Rui and Yang, Wen},
TITLE = {Development of an Autonomous Unmanned Aerial Manipulator Based on a Real-Time Oriented-Object Detection Method},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2396},
URL = {https://www.mdpi.com/1424-8220/19/10/2396},
ISSN = {1424-8220},
ABSTRACT = {Autonomous Unmanned Aerial Manipulators (UAMs) have shown promising potential in mobile 3-dimensional grasping applications, but they still suffer from some difficulties impeding their board applications, such as target detection and indoor positioning. For the autonomous grasping mission, the UAMs need ability to recognize the objects and grasp them. Considering the efficiency and precision, we present a novel oriented-object detection method called Rotation-SqueezeDet. This method can run on embedded-platforms in near real-time. Besides, this method can give the oriented bounding box of an object in images to enable a rotation-aware grasping. Based on this method, a UAM platform was designed and built. We have given the formulation, positioning, control, and planning of the whole UAM system. All the mechanical designs are fully provided as open-source hardware for reuse by the community. Finally, the effectiveness of the proposed scheme was validated in multiple experimental trials, highlighting its applicability of autonomous aerial rotational grasping in Global Positioning System (GPS) denied environments. We believe this system can be deployed to many potential workplaces which need UAM to accomplish difficult manipulation tasks.},
DOI = {10.3390/s19102396}
}



@Article{molecules24102025,
AUTHOR = {Tan, Jin Yeong and Ker, Pin Jern and Lau, K. Y. and Hannan, M. A. and Tang, Shirley Gee Hoon},
TITLE = {Applications of Photonics in Agriculture Sector: A Review},
JOURNAL = {Molecules},
VOLUME = {24},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2025},
URL = {https://www.mdpi.com/1420-3049/24/10/2025},
PubMedID = {31137897},
ISSN = {1420-3049},
ABSTRACT = {The agricultural industry has made a tremendous contribution to the foundations of civilization. Basic essentials such as food, beverages, clothes and domestic materials are enriched by the agricultural industry. However, the traditional method in agriculture cultivation is labor-intensive and inadequate to meet the accelerating nature of human demands. This scenario raises the need to explore state-of-the-art crop cultivation and harvesting technologies. In this regard, optics and photonics technologies have proven to be effective solutions. This paper aims to present a comprehensive review of three photonic techniques, namely imaging, spectroscopy and spectral imaging, in a comparative manner for agriculture applications. Essentially, the spectral imaging technique is a robust solution which combines the benefits of both imaging and spectroscopy but faces the risk of underutilization. This review also comprehends the practicality of all three techniques by presenting existing examples in agricultural applications. Furthermore, the potential of these techniques is reviewed and critiqued by looking into agricultural activities involving palm oil, rubber, and agro-food crops. All the possible issues and challenges in implementing the photonic techniques in agriculture are given prominence with a few selective recommendations. The highlighted insights in this review will hopefully lead to an increased effort in the development of photonics applications for the future agricultural industry.},
DOI = {10.3390/molecules24102025}
}



@Article{rs11111269,
AUTHOR = {Brabant, Charlotte and Alvarez-Vanhard, Emilien and Laribi, Achour and Morin, Gwénaël and Thanh Nguyen, Kim and Thomas, Alban and Houet, Thomas},
TITLE = {Comparison of Hyperspectral Techniques for Urban Tree Diversity Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1269},
URL = {https://www.mdpi.com/2072-4292/11/11/1269},
ISSN = {2072-4292},
ABSTRACT = {This research aims to assess the capabilities of Very High Spatial Resolution (VHSR) hyperspectral satellite data in order to discriminate urban tree diversity. Four dimension reduction methods and two classifiers are tested, using two learning methods and applied with four in situ sample datasets. An airborne HySpex image (408 bands/2 m) was acquired in July 2015 from which prototypal spaceborne hyperspectral images (named HYPXIM) at 4 m and 8 m and a multispectral Sentinel2 image at 10 m have been simulated for the purpose of this study. A comparison is made using these methods and datasets. The influence of dimension reduction methods is assessed on hyperspectral (HySpex and HYPXIM) and Sentinel2 datasets. The influence of conventional classifiers (Support Vector Machine &ndash;SVM&ndash; and Random Forest &ndash;RF&ndash;) and learning methods is evaluated on all image datasets (reduced and non-reduced hyperspectral and Sentinel2 datasets). Results show that HYPXIM 4 m and HySpex 2 m reduced by Minimum Noise Fraction (MNF) provide the greatest classification of 14 species using the SVM with an overall accuracy of 78.4% (&plusmn;1.5) and a kappa index of agreement of 0.7. More generally, the learning methods have a stronger influence than classifiers, or even than dimensional reduction methods, on urban tree diversity classification. Prototypal HYPXIM images appear to present a great compromise (192 spectral bands/4 m resolution) for urban vegetation applications compared to HySpex or Sentinel2 images.},
DOI = {10.3390/rs11111269}
}



@Article{s19112448,
AUTHOR = {Xiong, Xin and Zhang, Jingjin and Guo, Doudou and Chang, Liying and Huang, Danfeng},
TITLE = {Non-Invasive Sensing of Nitrogen in Plant Using Digital Images and Machine Learning for Brassica Campestris ssp. Chinensis L.},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2448},
URL = {https://www.mdpi.com/1424-8220/19/11/2448},
ISSN = {1424-8220},
ABSTRACT = {Monitoring plant nitrogen (N) in a timely way and accurately is critical for precision fertilization. The imaging technology based on visible light is relatively inexpensive and ubiquitous, and open-source analysis tools have proliferated. In this study, texture- and geometry-related phenotyping combined with color properties were investigated for their potential use in evaluating N in pakchoi (Brassica campestris ssp. chinensis L.). Potted pakchoi treated with four levels of N were cultivated in a greenhouse. Their top-view images were acquired using a camera at six growth stages. The corresponding plant N concentration was determined destructively. The quantitative relationships between the nitrogen nutrition index (NNI) and the image-based phenotyping features were established using the following algorithms: random forest (RF), support vector regression (SVR), and neural network (NN). The results showed the full model based on the color, texture, and geometry-related features outperforms the model based on only the color-related feature in predicting the NNI. The RF full model exhibited the most robust performance in both the seedling and harvest stages, reaching prediction accuracies of 0.823 and 0.943, respectively. The high prediction accuracy of the model allows for a low-cost, non-destructive monitoring of N in the field of precision crop management.},
DOI = {10.3390/s19112448}
}



@Article{cancers11060756,
AUTHOR = {Halicek, Martin and Fabelo, Himar and Ortega, Samuel and Callico, Gustavo M. and Fei, Baowei},
TITLE = {In-Vivo and Ex-Vivo Tissue Analysis through Hyperspectral Imaging Techniques: Revealing the Invisible Features of Cancer},
JOURNAL = {Cancers},
VOLUME = {11},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {756},
URL = {https://www.mdpi.com/2072-6694/11/6/756},
ISSN = {2072-6694},
ABSTRACT = {In contrast to conventional optical imaging modalities, hyperspectral imaging (HSI) is able to capture much more information from a certain scene, both within and beyond the visual spectral range (from 400 to 700 nm). This imaging modality is based on the principle that each material provides different responses to light reflection, absorption, and scattering across the electromagnetic spectrum. Due to these properties, it is possible to differentiate and identify the different materials/substances presented in a certain scene by their spectral signature. Over the last two decades, HSI has demonstrated potential to become a powerful tool to study and identify several diseases in the medical field, being a non-contact, non-ionizing, and a label-free imaging modality. In this review, the use of HSI as an imaging tool for the analysis and detection of cancer is presented. The basic concepts related to this technology are detailed. The most relevant, state-of-the-art studies that can be found in the literature using HSI for cancer analysis are presented and summarized, both in-vivo and ex-vivo. Lastly, we discuss the current limitations of this technology in the field of cancer detection, together with some insights into possible future steps in the improvement of this technology.},
DOI = {10.3390/cancers11060756}
}



@Article{electronics8060615,
AUTHOR = {Wong, Ching-Chang and Liu, Chih-Cheng and Xiao, Sheng-Ru and Yang, Hao-Yu and Lau, Meng-Cheng},
TITLE = {Q-Learning of Straightforward Gait Pattern for Humanoid Robot Based on Automatic Training Platform},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {615},
URL = {https://www.mdpi.com/2079-9292/8/6/615},
ISSN = {2079-9292},
ABSTRACT = {In this paper, an oscillator-based gait pattern with sinusoidal functions is designed and implemented on a field-programmable gate array (FPGA) chip to generate a trajectory plan and achieve bipedal locomotion for a small-sized humanoid robot. In order to let the robot can walk straight, the turning direction is viewed as a parameter of the gait pattern and Q-learning is used to obtain a straightforward gait pattern. Moreover, an automatic training platform is designed so that the learning process is automated. In this way, the turning direction can be adjusted flexibly and efficiently under the supervision of the automatic training platform. The experimental results show that the proposed learning framework allows the humanoid robot to gradually walk straight in the automated learning process.},
DOI = {10.3390/electronics8060615}
}



@Article{f10060478,
AUTHOR = {Zhou, Xisheng and Li, Long and Chen, Longqian and Liu, Yunqiang and Cui, Yifan and Zhang, Yu and Zhang, Ting},
TITLE = {Discriminating Urban Forest Types from Sentinel-2A Image Data through Linear Spectral Mixture Analysis: A Case Study of Xuzhou, East China},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {478},
URL = {https://www.mdpi.com/1999-4907/10/6/478},
ISSN = {1999-4907},
ABSTRACT = {Urban forests are an important component of the urban ecosystem. Urban forest types are a key piece of information required for monitoring the condition of an urban ecosystem. In this study, we propose an urban forest type discrimination method based on linear spectral mixture analysis (LSMA) and a support vector machine (SVM) in the case study of Xuzhou, east China. From 10-m Sentinel-2A imagery data, three different vegetation endmembers, namely broadleaved forest, coniferous forest, and low vegetation, and their abundances were extracted through LSMA. Using a combination of image spectra, topography, texture, and vegetation abundances, four SVM classification models were performed and compared to investigate the impact of these features on classification accuracy. With a particular interest in the role that vegetation abundances play in classification, we also compared SVM and other classifiers, i.e., random forest (RF), artificial neural network (ANN), and quick unbiased efficient statistical tree (QUEST). Results indicate that (1) the LSMA method can derive accurate vegetation abundances from Sentinel-2A image data, and the root-mean-square error (RMSE) was 0.019; (2) the classification accuracies of the four SVM models were improved after adding topographic features, textural features, and vegetation abundances one after the other; (3) the SVM produced higher classification accuracies than the other three classifiers when identical classification features were used; and (4) vegetation endmember abundances improved classification accuracy regardless of which classifier was used. It is concluded that Sentinel-2A image data has a strong capability to discriminate urban forest types in spectrally heterogeneous urban areas, and that vegetation abundances derived from LSMA can enhance such discrimination.},
DOI = {10.3390/f10060478}
}



@Article{rs11111308,
AUTHOR = {Wang, Dongliang and Shao, Quanqin and Yue, Huanyin},
TITLE = {Surveying Wild Animals from Satellites, Manned Aircraft and Unmanned Aerial Systems (UASs): A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1308},
URL = {https://www.mdpi.com/2072-4292/11/11/1308},
ISSN = {2072-4292},
ABSTRACT = {This article reviews studies regarding wild animal surveys based on multiple platforms, including satellites, manned aircraft, and unmanned aircraft systems (UASs), and focuses on the data used, animal detection methods, and their accuracies. We also discuss the advantages and limitations of each type of remote sensing data and highlight some new research opportunities and challenges. Submeter very-high-resolution (VHR) spaceborne imagery has potential in modeling the population dynamics of large (&gt;0.6 m) wild animals at large spatial and temporal scales, but has difficulty discerning small (&lt;0.6 m) animals at the species level, although high-resolution commercial satellites, such as WorldView-3 and -4, have been able to collect images with a ground resolution of up to 0.31 m in panchromatic mode. This situation will not change unless the satellite image resolution is greatly improved in the future. Manned aerial surveys have long been employed to capture the centimeter-scale images required for animal censuses over large areas. However, such aerial surveys are costly to implement in small areas and can cause significant disturbances to wild animals because of their noise. In contrast, UAS surveys are seen as a safe, convenient and less expensive alternative to ground-based and conventional manned aerial surveys, but most UASs can cover only small areas. The proposed use of UAS imagery in combination with VHR satellite imagery would produce critical population data for large wild animal species and colonies over large areas. The development of software systems for automatically producing image mosaics and recognizing wild animals will further improve survey efficiency.},
DOI = {10.3390/rs11111308}
}



