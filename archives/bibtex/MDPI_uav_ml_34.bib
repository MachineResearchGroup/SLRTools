
@Article{rs12193122,
AUTHOR = {Zhao, Yuan and Chen, Xiaoqiu and Smallman, Thomas Luke and Flack-Prain, Sophie and Milodowski, David T. and Williams, Mathew},
TITLE = {Characterizing the Error and Bias of Remotely Sensed LAI Products: An Example for Tropical and Subtropical Evergreen Forests in South China},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3122},
URL = {https://www.mdpi.com/2072-4292/12/19/3122},
ISSN = {2072-4292},
ABSTRACT = {Leaf area is a key parameter underpinning ecosystem carbon, water and energy exchanges via photosynthesis, transpiration and absorption of radiation, from local to global scales. Satellite-based Earth Observation (EO) can provide estimates of leaf area index (LAI) with global coverage and high temporal frequency. However, the error and bias contained within these EO products and their variation in time and across spatial resolutions remain poorly understood. Here, we used nearly 8000 in situ measurements of LAI from six forest environments in southern China to evaluate the magnitude, uncertainty, and dynamics of three widely used EO LAI products. The finer spatial resolution GEOV3 PROBA-V 300 m LAI product best estimates the observed LAI from a multi-site dataset (R2 = 0.45, bias = &minus;0.54 m2 m&minus;2, RMSE = 1.21 m2 m&minus;2) and importantly captures canopy dynamics well, including the amplitude and phase. The GEOV2 PROBA-V 1 km LAI product performed the next best (R2 = 0.36, bias = &minus;2.04 m2 m&minus;2, RMSE = 2.32 m2 m&minus;2) followed by MODIS 500 m LAI (R2 = 0.20, bias = &minus;1.47 m2 m&minus;2, RMSE = 2.29 m2 m&minus;2). The MODIS 500 m product did not capture the temporal dynamics observed in situ across southern China. The uncertainties estimated by each of the EO products are substantially smaller (3&ndash;5 times) than the observed bias for EO products against in situ measurements. Thus, reported product uncertainties are substantially underestimated and do not fully account for their total uncertainty. Overall, our analysis indicates that both the retrieval algorithm and spatial resolution play an important role in accurately estimating LAI for the dense canopy forests in Southern China. When constraining models of the carbon cycle and other ecosystem processes are run, studies should assume that current EO product LAI uncertainty estimates underestimate their true uncertainty value.},
DOI = {10.3390/rs12193122}
}



@Article{drones4040064,
AUTHOR = {Raoult, Vincent and Colefax, Andrew P and Allan, Blake M. and Cagnazzi, Daniele and Castelblanco-Mart√≠nez, Nataly and Ierodiaconou, Daniel and Johnston, David W. and Landeo-Yauri, Sarah and Lyons, Mitchell and Pirotta, Vanessa and Schofield, Gail and Butcher, Paul A},
TITLE = {Operational Protocols for the Use of Drones in Marine Animal Research},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {64},
URL = {https://www.mdpi.com/2504-446X/4/4/64},
ISSN = {2504-446X},
ABSTRACT = {The use of drones to study marine animals shows promise for the examination of numerous aspects of their ecology, behaviour, health and movement patterns. However, the responses of some marine phyla to the presence of drones varies broadly, as do the general operational protocols used to study them. Inconsistent methodological approaches could lead to difficulties comparing studies and can call into question the repeatability of research. This review draws on current literature and researchers with a wealth of practical experience to outline the idiosyncrasies of studying various marine taxa with drones. We also outline current best practice for drone operation in marine environments based on the literature and our practical experience in the field. The protocols outlined herein will be of use to researchers interested in incorporating drones as a tool into their research on marine animals and will help form consistent approaches for drone-based studies in the future.},
DOI = {10.3390/drones4040064}
}



@Article{drones4040063,
AUTHOR = {Steup, Christoph and Parlow, Simon and Mai, Sebastian and Mostaghim, Sanaz},
TITLE = {Generic Component-Based Mission-Centric Energy Model for Micro-Scale Unmanned Aerial Vehicles},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {63},
URL = {https://www.mdpi.com/2504-446X/4/4/63},
ISSN = {2504-446X},
ABSTRACT = {The trend towards the usage of battery-electric unmanned aerial vehicles needs new strategies in mission planning and in the design of the systems themselves. To create an optimal mission plan and take appropriate decisions during the mission, a reliable, accurate and adaptive energy model is of utmost importance. However, most existing approaches either use very generic models or ones that are especially tailored towards a specific UAV. We present a generic energy model that is based on decomposing a robotic system into multiple observable components. The generic model is applied to a swarm of quadcopters and evaluated in multiple flights with different manoeuvres. We additionally use the data from practical experiments to learn and generate a mission-agnostic energy model which can match the typical behaviour of our quadcopters such as hovering; movement in x, y and z directions; landing; communication; and illumination. The learned energy model concurs with the overall energy consumption with an accuracy over 95% compared to the training flights for the indoor use case. An extended model reduces the error to less than 1.4%. Consequently, the proposed model enables an estimation of the energy used in flight and on the ground, which can be easily incorporated in autonomous systems and enhance decision-making with reliable input. The used learning mechanism allows to deploy the approach with minimal effort to new platforms needing only some representative test missions, which was shown using additional outdoor validation flights with a different quadcopter of the same build and the originally trained models. This set-up increased the prediction error of our model to 4.46%.},
DOI = {10.3390/drones4040063}
}



@Article{s20195495,
AUTHOR = {El Boudani, Brahim and Kanaris, Loizos and Kokkinis, Akis and Kyriacou, Michalis and Chrysoulas, Christos and Stavrou, Stavros and Dagiuklas, Tasos},
TITLE = {Implementing Deep Learning Techniques in 5G IoT Networks for 3D Indoor Positioning: DELTA (DeEp Learning-Based Co-operaTive Architecture)},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5495},
URL = {https://www.mdpi.com/1424-8220/20/19/5495},
ISSN = {1424-8220},
ABSTRACT = {In the near future, the fifth-generation wireless technology is expected to be rolled out, offering low latency, high bandwidth and multiple antennas deployed in a single access point. This ecosystem will help further enhance various location-based scenarios such as assets tracking in smart factories, precise smart management of hydroponic indoor vertical farms and indoor way-finding in smart hospitals. Such a system will also integrate existing technologies like the Internet of Things (IoT), WiFi and other network infrastructures. In this respect, 5G precise indoor localization using heterogeneous IoT technologies (Zigbee, Raspberry Pi, Arduino, BLE, etc.) is a challenging research area. In this work, an experimental 5G testbed has been designed integrating C-RAN and IoT networks. This testbed is used to improve both vertical and horizontal localization (3D Localization) in a 5G IoT environment. To achieve this, we propose the DEep Learning-based co-operaTive Architecture (DELTA) machine learning model implemented on a 3D multi-layered fingerprint radiomap. The DELTA begins by estimating the 2D location. Then, the output is recursively used to predict the 3D location of a mobile station. This approach is going to benefit use cases such as 3D indoor navigation in multi-floor smart factories or in large complex buildings. Finally, we have observed that the proposed model has outperformed traditional algorithms such as Support Vector Machine (SVM) and K-Nearest Neighbor (KNN).},
DOI = {10.3390/s20195495}
}



@Article{s20195496,
AUTHOR = {Sabra, Adham and Fung, Wai-Keung},
TITLE = {A Fuzzy Cooperative Localisation Framework for Underwater Robotic Swarms},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5496},
URL = {https://www.mdpi.com/1424-8220/20/19/5496},
ISSN = {1424-8220},
ABSTRACT = {This article proposes a holistic localisation framework for underwater robotic swarms to dynamically fuse multiple position estimates of an autonomous underwater vehicle while using fuzzy decision support system. A number of underwater localisation methods have been proposed in the literature for wireless sensor networks. The proposed navigation framework harnesses the established localisation methods in order to provide navigation aids in the absence of acoustic exteroceptive sensors navigation aid (i.e., ultra-short base line) and it can be extended to accommodate newly developed localisation methods by expanding the fuzzy rule base. Simplicity, flexibility, and scalability are the main three advantages that are inherent in the proposed localisation framework when compared to other traditional and commonly adopted underwater localisation methods, such as the Extended Kalman Filter. A physics-based simulation platform that considers environment&rsquo;s hydrodynamics, industrial grade inertial measurement unit, and underwater acoustic communications characteristics is implemented in order to validate the proposed localisation framework on a swarm size of 150 autonomous underwater vehicles. The proposed fuzzy-based localisation algorithm improves the entire swarm mean localisation error and standard deviation by 16.53% and 35.17%, respectively, when compared to the Extended Kalman Filter based localisation with round-robin scheduling.},
DOI = {10.3390/s20195496}
}



@Article{agriculture10100434,
AUTHOR = {Filip, Martin and Zoubek, Tomas and Bumbalek, Roman and Cerny, Pavel and Batista, Carlos E. and Olsan, Pavel and Bartos, Petr and Kriz, Pavel and Xiao, Maohua and Dolan, Antonin and Findura, Pavol},
TITLE = {Advanced Computational Methods for Agriculture Machinery Movement Optimization with Applications in Sugarcane Production},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {434},
URL = {https://www.mdpi.com/2077-0472/10/10/434},
ISSN = {2077-0472},
ABSTRACT = {This paper considers the evolution of processes applied in agriculture for field operations developed from non-organized handmade activities into very specialized and organized production processes. A set of new approaches based on the application of metaheuristic optimization methods and smart automatization known as Agriculture 4.0 has enabled a rapid increase in in-field operations&rsquo; productivity and offered unprecedented economic benefits. The aim of this paper is to review modern approaches to agriculture machinery movement optimization with applications in sugarcane production. Approaches based on algorithms for the division of spatial configuration, route planning or path planning, as well as approaches using cost parameters, e.g., energy, fuel and time consumption, are presented. The combination of algorithmic and economic methodologies including evaluation of the savings and investments and their cost/benefit relation is discussed.},
DOI = {10.3390/agriculture10100434}
}



@Article{rs12193164,
AUTHOR = {Banerjee, Bikram Pratap and Spangenberg, German and Kant, Surya},
TITLE = {Fusion of Spectral and Structural Information from Aerial Images for Improved Biomass Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3164},
URL = {https://www.mdpi.com/2072-4292/12/19/3164},
ISSN = {2072-4292},
ABSTRACT = {Efficient, precise and timely measurement of plant traits is important in the assessment of a breeding population. Estimating crop biomass in breeding trials using high-throughput technologies is difficult, as reproductive and senescence stages do not relate to reflectance spectra, and multiple growth stages occur concurrently in diverse genotypes. Additionally, vegetation indices (VIs) saturate at high canopy coverage, and vertical growth profiles are difficult to capture using VIs. A novel approach was implemented involving a fusion of complementary spectral and structural information, to calculate intermediate metrics such as crop height model (CHM), crop coverage (CC) and crop volume (CV), which were finally used to calculate dry (DW) and fresh (FW) weight of above-ground biomass in wheat. The intermediate metrics, CHM (R2 = 0.81, SEE = 4.19 cm) and CC (OA = 99.2%, &Kappa; = 0.98) were found to be accurate against equivalent ground truth measurements. The metrics CV and CV&times;VIs were used to develop an effective and accurate linear regression model relationship with DW (R2 = 0.96 and SEE = 69.2 g/m2) and FW (R2 = 0.89 and SEE = 333.54 g/m2). The implemented approach outperformed commonly used VIs for estimation of biomass at all growth stages in wheat. The achieved results strongly support the applicability of the proposed approach for high-throughput phenotyping of germplasm in wheat and other crop species.},
DOI = {10.3390/rs12193164}
}



@Article{agriculture10100436,
AUTHOR = {Niazian, Mohsen and Niedba≈Ça, Gniewko},
TITLE = {Machine Learning for Plant Breeding and Biotechnology},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {436},
URL = {https://www.mdpi.com/2077-0472/10/10/436},
ISSN = {2077-0472},
ABSTRACT = {Classical univariate and multivariate statistics are the most common methods used for data analysis in plant breeding and biotechnology studies. Evaluation of genetic diversity, classification of plant genotypes, analysis of yield components, yield stability analysis, assessment of biotic and abiotic stresses, prediction of parental combinations in hybrid breeding programs, and analysis of in vitro-based biotechnological experiments are mainly performed by classical statistical methods. Despite successful applications, these classical statistical methods have low efficiency in analyzing data obtained from plant studies, as the genotype, environment, and their interaction (G &times; E) result in nondeterministic and nonlinear nature of plant characteristics. Large-scale data flow, including phenomics, metabolomics, genomics, and big data, must be analyzed for efficient interpretation of results affected by G &times; E. Nonlinear nonparametric machine learning techniques are more efficient than classical statistical models in handling large amounts of complex and nondeterministic information with &ldquo;multiple-independent variables versus multiple-dependent variables&rdquo; nature. Neural networks, partial least square regression, random forest, and support vector machines are some of the most fascinating machine learning models that have been widely applied to analyze nonlinear and complex data in both classical plant breeding and in vitro-based biotechnological studies. High interpretive power of machine learning algorithms has made them popular in the analysis of plant complex multifactorial characteristics. The classification of different plant genotypes with morphological and molecular markers, modeling and predicting important quantitative characteristics of plants, the interpretation of complex and nonlinear relationships of plant characteristics, and predicting and optimizing of in vitro breeding methods are the examples of applications of machine learning in conventional plant breeding and in vitro-based biotechnological studies. Precision agriculture is possible through accurate measurement of plant characteristics using imaging techniques and then efficient analysis of reliable extracted data using machine learning algorithms. Perfect interpretation of high-throughput phenotyping data is applicable through coupled machine learning-image processing. Some applied and potentially applicable capabilities of machine learning techniques in conventional and in vitro-based plant breeding studies have been discussed in this overview. Discussions are of great value for future studies and could inspire researchers to apply machine learning in new layers of plant breeding.},
DOI = {10.3390/agriculture10100436}
}



@Article{s20195538,
AUTHOR = {Zhang, Yunsheng and Zhu, Yaochen and Li, Haifeng and Chen, Siyang and Peng, Jian and Zhao, Ling},
TITLE = {Automatic Changes Detection between Outdated Building Maps and New VHR Images Based on Pre-Trained Fully Convolutional Feature Maps},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5538},
URL = {https://www.mdpi.com/1424-8220/20/19/5538},
ISSN = {1424-8220},
ABSTRACT = {Detecting changes between the existing building basemaps and newly acquired high spatial resolution remotely sensed (HRS) images is a time-consuming task. This is mainly because of the data labeling and poor performance of hand-crafted features. In this paper, for efficient feature extraction, we propose a fully convolutional feature extractor that is reconstructed from the deep convolutional neural network (DCNN) and pre-trained on the Pascal VOC dataset. Our proposed method extract pixel-wise features, and choose salient features based on a random forest (RF) algorithm using the existing basemaps. A data cleaning method through cross-validation and label-uncertainty estimation is also proposed to select potential correct labels and use them for training an RF classifier to extract the building from new HRS images. The pixel-wise initial classification results are refined based on a superpixel-based graph cuts algorithm and compared to the existing building basemaps to obtain the change map. Experiments with two simulated and three real datasets confirm the effectiveness of our proposed method and indicate high accuracy and low false alarm rate.},
DOI = {10.3390/s20195538}
}



@Article{s20195539,
AUTHOR = {Pop, MƒÉdƒÉlin-Dorin and Pro»ôtean, Octavian and David, Tudor-Mihai and Pro»ôtean, Gabriela},
TITLE = {Hybrid Solution Combining Kalman Filtering with Takagi‚ÄìSugeno Fuzzy Inference System for Online Car-Following Model Calibration},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5539},
URL = {https://www.mdpi.com/1424-8220/20/19/5539},
ISSN = {1424-8220},
ABSTRACT = {Nowadays, the intelligent transportation concept has become one of the most important research fields. All of us depend on mobility, even when we talk about people, provide services, or move goods. Researchers have tried to create and test different transportation models that can optimize traffic flow through road networks and, implicitly, reduce travel times. To validate these new models, the necessity of having a calibration process defined has emerged. Calibration is mandatory in the modeling process because it ensures the achievement of a model closer to the real system. The purpose of this paper is to propose a new multidisciplinary approach combining microscopic traffic modeling theory with intelligent control systems concepts like fuzzy inference in the traffic model calibration. The chosen Takagi&ndash;Sugeno fuzzy inference system proves its adaptive capacity for real-time systems. This concept will be applied to the specific microscopic car-following model parameters in combination with a Kalman filter. The results will demonstrate how the microscopic traffic model parameters can adapt based on real data to prove the model validity.},
DOI = {10.3390/s20195539}
}



@Article{rs12193171,
AUTHOR = {Park, Jinseok and Jang, Seongju and Hong, Rokgi and Suh, Kyo and Song, Inhong},
TITLE = {Development of Land Cover Classification Model Using AI Based FusionNet Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3171},
URL = {https://www.mdpi.com/2072-4292/12/19/3171},
ISSN = {2072-4292},
ABSTRACT = {Prompt updates of land cover maps are important, as spatial information of land cover is widely used in many areas. However, current manual digitizing methods are time consuming and labor intensive, hindering rapid updates of land cover maps. The objective of this study was to develop an artificial intelligence (AI) based land cover classification model that allows for rapid land cover classification from high-resolution remote sensing (HRRS) images. The model comprises of three modules: pre-processing, land cover classification, and post-processing modules. The pre-processing module separates the HRRS image into multiple aspects by overlapping 75% using the sliding window algorithm. The land cover classification module was developed using the convolutional neural network (CNN) concept, based the FusionNet network and used to assign a land cover type to the separated HRRS images. Post-processing module determines ultimate land cover types by summing up the separated land cover result from the land cover classification module. Model training and validation were conducted to evaluate the performance of the developed model. The land cover maps and orthographic images of 547.29 km2 in area from the Jeonnam province in Korea were used to train the model. For model validation, two spatial and temporal different sites, one from Subuk-myeon of Jeonnam province in 2018 and the other from Daseo-myeon of Chungbuk province in 2016, were randomly chosen. The model performed reasonably well, demonstrating overall accuracies of 0.81 and 0.71, and kappa coefficients of 0.75 and 0.64, for the respective validation sites. The model performance was better when only considering the agricultural area by showing overall accuracy of 0.83 and kappa coefficients of 0.73. It was concluded that the developed model may assist rapid land cover update especially for agricultural areas and incorporation field boundary lineation is suggested as future study to further improve the model accuracy.},
DOI = {10.3390/rs12193171}
}



@Article{rs12193175,
AUTHOR = {Geng, Kai and Sun, Xian and Yan, Zhiyuan and Diao, Wenhui and Gao, Xin},
TITLE = {Topological Space Knowledge Distillation for Compact Road Extraction in Optical Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3175},
URL = {https://www.mdpi.com/2072-4292/12/19/3175},
ISSN = {2072-4292},
ABSTRACT = {Road extraction from optical remote sensing images has drawn much attention in recent decades and has a wide range of applications. Most of the previous studies rarely take into account the unique topological characteristics of the road. It is the most apparent feature of linear structure that describes the variety of connection relationships of the road. However, designing a particular topological feature extraction network usually results in a model that is too heavy and impractical. To address the problems mentioned above, in this paper, we propose a lightweight topological space network for road extraction based on knowledge distillation (TSKD-Road). Specifically, (1) narrow and short roads easily influence topological features extracted directly in optical remote sensing images. Therefore, we propose a denser teacher network for extracting road structures; (2) to enhance the weight of topological features, we propose a topological space loss calculation model with multiple widths and depths; (3) based on the above innovations, a topological space knowledge distillation framework is proposed, which aims to transfer different kinds of knowledge acquired in a heavy net to a lightweight net, while significantly improving the lightweight net‚Äôs accuracy. Experiments were conducted on two publicly available benchmark datasets, which show the obvious superiority and effectiveness of our network.},
DOI = {10.3390/rs12193175}
}



@Article{rs12193184,
AUTHOR = {Camarretta, Nicol√≤ and A. Harrison, Peter and Lucieer, Arko and M. Potts, Brad and Davidson, Neil and Hunt, Mark},
TITLE = {From Drones to Phenotype: Using UAV-LiDAR to Detect Species and Provenance Variation in Tree Productivity and Structure},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3184},
URL = {https://www.mdpi.com/2072-4292/12/19/3184},
ISSN = {2072-4292},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) for remote sensing of natural environments has increased over the last decade. However, applications of this technology for high-throughput individual tree phenotyping in a quantitative genetic framework are rare. We here demonstrate a two-phased analytical pipeline that rapidly phenotypes and filters for genetic signals in traditional and novel tree productivity and architectural traits derived from ultra-dense light detection and ranging (LiDAR) point clouds. The goal of this study was rapidly phenotype individual trees to understand the genetic basis of ecologically and economically significant traits important for guiding the management of natural resources. Individual tree point clouds were acquired using UAV-LiDAR captured over a multi-provenance common-garden restoration field trial located in Tasmania, Australia, established using two eucalypt species (Eucalyptus pauciflora and Eucalyptus tenuiramis). Twenty-five tree productivity and architectural traits were calculated for each individual tree point cloud. The first phase of the analytical pipeline found significant species differences in 13 of the 25 derived traits, revealing key structural differences in productivity and crown architecture between species. The second phase investigated the within species variation in the same 25 structural traits. Significant provenance variation was detected for 20 structural traits in E. pauciflora and 10 in E. tenuiramis, with signals of divergent selection found for 11 and 7 traits, respectively, putatively driven by the home-site environment shaping the observed variation. Our results highlight the genetic-based diversity within and between species for traits important for forest structure, such as crown density and structural complexity. As species and provenances are being increasingly translocated across the landscape to mitigate the effects of rapid climate change, our results that were achieved through rapid phenotyping using UAV-LiDAR, raise the need to understand the functional value of productivity and architectural traits reflecting species and provenance differences in crown structure and the interplay they have on the dependent biotic communities.},
DOI = {10.3390/rs12193184}
}



@Article{app10196881,
AUTHOR = {Ciaburro, Giuseppe and Iannace, Gino and Puyana-Romero, Virginia and Trematerra, Amelia},
TITLE = {A Comparison between Numerical Simulation Models for the Prediction of Acoustic Behavior of Giant Reeds Shredded},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {6881},
URL = {https://www.mdpi.com/2076-3417/10/19/6881},
ISSN = {2076-3417},
ABSTRACT = {Giant reeds represent a natural fiber widely available in some areas of the world. Its use can be particularly useful as the uncontrolled growth of giant reeds can be a problem because large areas are invaded by them and the crops are damaged. In this study, two models of numerical simulation of the acoustic behavior of giant reeds were put in comparison: the Hamet model and a model based on artificial neural networks. First, the characteristics of the reeds were examined and the procedures for the preparation of the samples to be analyzed were described. Then air flow resistance, porosity and sound absorption coefficient were measured and analyzed in detail. Finally, the results of the numerical modeling of the acoustic coefficient were compared. The neural network-based model showed high Pearson correlation coefficient value, indicating a large number of correct predictions.},
DOI = {10.3390/app10196881}
}



@Article{agriculture10100451,
AUTHOR = {L√≥pez-Calder√≥n, Magali J. and Estrada-√Åvalos, Juan and Rodr√≠guez-Moreno, V√≠ctor M. and Mauricio-Ruvalcaba, Jorge E. and Mart√≠nez-Sifuentes, Aldo R. and Delgado-Ram√≠rez, Gerardo and Miguel-Valle, Enrique},
TITLE = {Estimation of Total Nitrogen Content in Forage Maize (Zea mays L.) Using Spectral Indices: Analysis by Random Forest},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {451},
URL = {https://www.mdpi.com/2077-0472/10/10/451},
ISSN = {2077-0472},
ABSTRACT = {Knowing the total Nitrogen content (Nt) of forage maize (Zea mays) is important so that decisions can be made quickly and efficiently to adjust the timing and amount of both irrigation and fertilizer. In 2017 and 2018 during three growing cycles in two study plots, leaf samples were collected and the Dumas method was used to estimate Nt. During the same growing seasons and on the same sampling plots, a Parrot Sequoia camera mounted on an unmanned aerial vehicle (UAV) was used to collect high resolution images of forage maize study plots. Thirteen multispectral indices were generated and, from these, a Random Forest (RF) algorithm was used to estimate Nt. RF is a machine-learning technique and is designed to work with extremely large datasets. Overall analysis showed five of the 13 indices as the most important. One of these five, the Transformed Chlorophyll Absorption in Reflectance Index/Optimized Soil-Adjusted Vegetation Index, was found to be the most important for estimation of Nt in forage maize (R2 = 0.76). RF handled the complex dataset in a time-efficient manner and Nt did not differ significantly when compared between traditional methods of evaluating Nt at the canopy level and using UAVs and RF to estimate Nt in forage maize. This result is an opportunity to explore many new research options in precision farming and digital agriculture.},
DOI = {10.3390/agriculture10100451}
}



@Article{rs12193206,
AUTHOR = {Esfandiari, Morteza and Abdi, Ghasem and Jabari, Shabnam and McGrath, Heather and Coleman, David},
TITLE = {Flood Hazard Risk Mapping Using a Pseudo Supervised Random Forest},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3206},
URL = {https://www.mdpi.com/2072-4292/12/19/3206},
ISSN = {2072-4292},
ABSTRACT = {Devastating floods occur regularly around the world. Recently, machine learning models have been used for flood susceptibility mapping. However, even when these algorithms are provided with adequate ground truth training samples, they can fail to predict flood extends reliably. On the other hand, the height above nearest drainage (HAND) model can produce flood prediction maps with limited accuracy. The objective of this research is to produce an accurate and dynamic flood modeling technique to produce flood maps as a function of water level by combining the HAND model and machine learning. In this paper, the HAND model was utilized to generate a preliminary flood map; then, the predictions of the HAND model were used to produce pseudo training samples for a R.F. model. To improve the R.F. training stage, five of the most effective flood mapping conditioning factors are used, namely, Altitude, Slope, Aspect, Distance from River and Land use/cover map. In this approach, the R.F. model is trained to dynamically estimate the flood extent with the pseudo training points acquired from the HAND model. However, due to the limited accuracy of the HAND model, a random sample consensus (RANSAC) method was used to detect outliers. The accuracy of the proposed model for flood extent prediction, was tested on different flood events in the city of Fredericton, NB, Canada in 2014, 2016, 2018, 2019. Furthermore, to ensure that the proposed model can produce accurate flood maps in other areas as well, it was also tested on the 2019 flood in Gatineau, QC, Canada. Accuracy assessment metrics, such as overall accuracy, Cohen&rsquo;s kappa coefficient, Matthews correlation coefficient, true positive rate (TPR), true negative rate (TNR), false positive rate (FPR) and false negative rate (FNR), were used to compare the predicted flood extent of the study areas, to the extent estimated by the HAND model and the extent imaged by Sentinel-2 and Landsat satellites. The results confirm that the proposed model can improve the flood extent prediction of the HAND model without using any ground truth training data.},
DOI = {10.3390/rs12193206}
}



@Article{land9100368,
AUTHOR = {Esmali Ouri, Abazar and Golshan, Mohammad and Janizadeh, Saeid and Cerd√†, Artemi and Melesse, Assefa M.},
TITLE = {Soil Erosion Susceptibility Mapping in Kozetopraghi Catchment, Iran: A Mixed Approach Using Rainfall Simulator and Data Mining Techniques},
JOURNAL = {Land},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {368},
URL = {https://www.mdpi.com/2073-445X/9/10/368},
ISSN = {2073-445X},
ABSTRACT = {Soil erosion determines landforms, soil formation and distribution, soil fertility, and land degradation processes. In arid and semiarid ecosystems, soil erosion is a key process to understand, foresee, and prevent desertification. Addressing soil erosion throughout watersheds scales requires basic information to develop soil erosion control strategies and to reduce land degradation. To assess and remediate the non-sustainable soil erosion rates, restoration programs benefit from the knowledge of the spatial distribution of the soil losses to develop maps of soil erosion. This study presents Support Vector Machine (SVM), Random Forest (RF), and adaptive boosting (AdaBoost) data mining models to map soil erosion susceptibility in Kozetopraghi watershed, Iran. A soil erosion inventory map was prepared from field rainfall simulation experiments on 174 randomly selected points along the Kozetopraghi watershed. In previous studies, this map has been prepared using indirect methods such as the Universal Soil Loss Equation to assess soil erosion. Direct field measurements for mapping soil erosion susceptibility have so far not been carried out in our study site in the past. The soil erosion rate data generated by simulated rainfall in 1 m2 plots at rainfall rate of 40 mmh&minus;1 was used to develop the soil erosion map. Of the available data, 70% and 30% were randomly classified to calibrate and validate the models, respectively. As a result, the RF model with the highest area under the curve (AUC) value in a receiver operating characteristics (ROC) curve (0.91), and the lowest mean square error (MSE) value (0.09), has the most concordance and spatial differentiation. Sensitivity analysis by Jackknife and IncNodePurity methods indicates that the slope angle is the most important factor within the soil erosion susceptibility map. The RF susceptibility map showed that the areas located in the center and near the watershed outlet have the most susceptibility to soil erosion. This information can be used to support the development of sustainable restoration plans with more accuracy. Our methodology has been evaluated and can be also applied in other regions.},
DOI = {10.3390/land9100368}
}



@Article{rs12193216,
AUTHOR = {Maimaitiyiming, Matthew and Sagan, Vasit and Sidike, Paheding and Maimaitijiang, Maitiniyazi and Miller, Allison J. and Kwasniewski, Misha},
TITLE = {Leveraging Very-High Spatial Resolution Hyperspectral and Thermal UAV Imageries for Characterizing Diurnal Indicators of Grapevine Physiology},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3216},
URL = {https://www.mdpi.com/2072-4292/12/19/3216},
ISSN = {2072-4292},
ABSTRACT = {Efficient and accurate methods to monitor crop physiological responses help growers better understand crop physiology and improve crop productivity. In recent years, developments in unmanned aerial vehicles (UAV) and sensor technology have enabled image acquisition at very-high spectral, spatial, and temporal resolutions. However, potential applications and limitations of very-high-resolution (VHR) hyperspectral and thermal UAV imaging for characterization of plant diurnal physiology remain largely unknown, due to issues related to shadow and canopy heterogeneity. In this study, we propose a canopy zone-weighting (CZW) method to leverage the potential of VHR (&le;9 cm) hyperspectral and thermal UAV imageries in estimating physiological indicators, such as stomatal conductance (Gs) and steady-state fluorescence (Fs). Diurnal flights and concurrent in-situ measurements were conducted during grapevine growing seasons in 2017 and 2018 in a vineyard in Missouri, USA. We used neural net classifier and the Canny edge detection method to extract pure vine canopy from the hyperspectral and thermal images, respectively. Then, the vine canopy was segmented into three canopy zones (sunlit, nadir, and shaded) using K-means clustering based on the canopy shadow fraction and canopy temperature. Common reflectance-based spectral indices, sun-induced chlorophyll fluorescence (SIF), and simplified canopy water stress index (siCWSI) were computed as image retrievals. Using the coefficient of determination (R2) established between the image retrievals from three canopy zones and the in-situ measurements as a weight factor, weighted image retrievals were calculated and their correlation with in-situ measurements was explored. The results showed that the most frequent and the highest correlations were found for Gs and Fs, with CZW-based Photochemical reflectance index (PRI), SIF, and siCWSI (PRICZW, SIFCZW, and siCWSICZW), respectively. When all flights combined for the given field campaign date, PRICZW, SIFCZW, and siCWSICZW significantly improved the relationship with Gs and Fs. The proposed approach takes full advantage of VHR hyperspectral and thermal UAV imageries, and suggests that the CZW method is simple yet effective in estimating Gs and Fs.},
DOI = {10.3390/rs12193216}
}



@Article{rs12193228,
AUTHOR = {Qiu, Zhengchao and Xiang, Haitao and Ma, Fei and Du, Changwen},
TITLE = {Qualifications of Rice Growth Indicators Optimized at Different Growth Stages Using Unmanned Aerial Vehicle Digital Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3228},
URL = {https://www.mdpi.com/2072-4292/12/19/3228},
ISSN = {2072-4292},
ABSTRACT = {The accurate estimation of the key growth indicators of rice is conducive to rice production, and the rapid monitoring of these indicators can be achieved through remote sensing using the commercial RGB cameras of unmanned aerial vehicles (UAVs). However, the method of using UAV RGB images lacks an optimized model to achieve accurate qualifications of rice growth indicators. In this study, we established a correlation between the multi-stage vegetation indices (VIs) extracted from UAV imagery and the leaf dry biomass, leaf area index, and leaf total nitrogen for each growth stage of rice. Then, we used the optimal VI (OVI) method and object-oriented segmentation (OS) method to remove the noncanopy area of the image to improve the estimation accuracy. We selected the OVI and the models with the best correlation for each growth stage to establish a simple estimation model database. The results showed that the OVI and OS methods to remove the noncanopy area can improve the correlation between the key growth indicators and VI of rice. At the tillering stage and early jointing stage, the correlations between leaf dry biomass (LDB) and the Green Leaf Index (GLI) and Red Green Ratio Index (RGRI) were 0.829 and 0.881, respectively; at the early jointing stage and late jointing stage, the coefficient of determination (R2) between the Leaf Area Index (LAI) and Modified Green Red Vegetation Index (MGRVI) was 0.803 and 0.875, respectively; at the early stage and the filling stage, the correlations between the leaf total nitrogen (LTN) and UAV vegetation index and the Excess Red Vegetation Index (ExR) were 0.861 and 0.931, respectively. By using the simple estimation model database established using the UAV-based VI and the measured indicators at different growth stages, the rice growth indicators can be estimated for each stage. The proposed estimation model database for monitoring rice at the different growth stages is helpful for improving the estimation accuracy of the key rice growth indicators and accurately managing rice production.},
DOI = {10.3390/rs12193228}
}



@Article{rs12193233,
AUTHOR = {Meng, Ran and Lv, Zhengang and Yan, Jianbing and Chen, Gengshen and Zhao, Feng and Zeng, Linglin and Xu, Binyuan},
TITLE = {Development of Spectral Disease Indices for Southern Corn Rust Detection and Severity Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3233},
URL = {https://www.mdpi.com/2072-4292/12/19/3233},
ISSN = {2072-4292},
ABSTRACT = {Southern Corn Rust (SCR) is one of the most destructive diseases in corn production, significantly affecting corn quality and yields globally. Field-based fast, nondestructive diagnosis of SCR is critical for smart agriculture applications to reduce pesticide use and ensure food safety. The development of spectral disease indices (SDIs), based on in situ leaf reflectance spectra, has proven to be an effective method in detecting plant diseases in the field. However, little is known about leaf spectral signatures that can assist in the accurate diagnosis of SCR, and no SDIs-based model has been reported for the field-based SCR monitoring. Here, to address those issues, we developed SDIs-based monitoring models to detect SCR-infected leaves and classify SCR damage severity. In detail, we first collected in situ leaf reflectance spectra (350&ndash;2500 nm) of healthy and infected corn plants with three severity levels (light, medium, and severe) using a portable spectrometer. Then, the RELIEF-F algorithm was performed to select the most discriminative features (wavelengths) and two band normalized differences for developing SDIs (i.e., health index and severity index) in SCR detection and severity classification, respectively. The leaf reflectance spectra, most sensitive to SCR detection and severity classification, were found in the 572 nm, 766 nm, and 1445 nm wavelength and 575 nm, 640 nm, and 1670 nm wavelength, respectively. These spectral features were associated with leaf pigment and leaf water content. Finally, by employing a support vector machine (SVM), the performances of developed SCR-SDIs were assessed and compared with 38 stress-related vegetation indices (VIs) identified in the literature. The SDIs-based models developed in this study achieved an overall accuracy of 87% and 70% in SCR detection and severity classification, 1.1% and 8.3% higher than the other best VIs-based model under study, respectively. Our results thus suggest that the SCR-SDIs is a promising tool for fast, nondestructive diagnosis of SCR in the field over large areas. To our knowledge, this study represents one of the first few efforts to provide a theoretical basis for remote sensing of SCR at field and larger scales. With the increasing use of unmanned aerial vehicles (UAVs) with hyperspectral measurement capability, more studies should be conducted to expand our developed SCR-SDIs for SCR monitoring at different study sites and growing stages in the future.},
DOI = {10.3390/rs12193233}
}



@Article{electronics9101640,
AUTHOR = {Khan, Usman Ali and Lee, Sang Sun},
TITLE = {Distance-Based Resource Allocation for Vehicle-to-Pedestrian Safety Communication},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1640},
URL = {https://www.mdpi.com/2079-9292/9/10/1640},
ISSN = {2079-9292},
ABSTRACT = {Cellular Vehicle to Everything (V2X) has redefined the vehicular communication architecture as something that needs an ultra-reliable link, high capacity, and fast message delivery in vehicular networks. The V2X scenarios are broadly categorized as Vehicle to Vehicle (V2V), Vehicle to Infrastructure (V2I), Vehicle to Pedestrians (V2P), and Vehicle to Network (V2N). Vulnerable pedestrians belong to the V2P category and hence require an ultra-reliable link and a fast message delivery in case the moving vehicle is in the close proximity of the pedestrian. However, congestion in the network calls for an optimized resource allocation that would allow a fast and secure connection between a vehicle and the pedestrian. In this paper, we have proposed a distance-based resource allocation that classifies the pedestrians in different categories, performs a one-to-many weighted bipartite matching, and finally a reinforcement learning based power allocation.},
DOI = {10.3390/electronics9101640}
}



@Article{s20195676,
AUTHOR = {Kim, Jisung and Jeong, Youngdo and Lee, Hyojin and Yun, Hongsik},
TITLE = {Marker-Based Structural Displacement Measurement Models with Camera Movement Error Correction Using Image Matching and Anomaly Detection},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {5676},
URL = {https://www.mdpi.com/1424-8220/20/19/5676},
ISSN = {1424-8220},
ABSTRACT = {To prevent collapse accidents at construction sites, the marker-based displacement measurement method was developed. However, it has difficulty in obtaining accurate measurements at long distances (&gt;50 m) in an outdoor environment because of camera movements. To overcome this problem, marker-based structural displacement measurement models using image matching and anomaly detection were designed in this study. Then, the performance of each model in terms of camera movement error correction was verified through comparison with that of a conventional model. The results show that the systematic errors due to camera movements (&lt;1.7&deg;) were corrected. The detection rate of markers with displacement reached 95%, and the probability that the error size would be less than 10 mm was &ge; 95% with a 95% confidence interval at a distance of more than 100 m. Moreover, the normalized mean square error was less than 0.1. The models developed in this study can measure the pure displacement of an object without the systematic errors caused by camera movements. Furthermore, these models can be used to measure the displacements of distant structures using closed-circuit television cameras and markers in an outdoor environment with high accuracy.},
DOI = {10.3390/s20195676}
}



@Article{rs12193237,
AUTHOR = {Osco, Lucas Prado and Junior, Jos√© Marcato and Ramos, Ana Paula Marques and Furuya, Danielle Elis Garcia and Santana, Dthenifer Cordeiro and Teodoro, Larissa Pereira Ribeiro and Gon√ßalves, Wesley Nunes and Baio, F√°bio Henrique Rojo and Pistori, Hemerson and Junior, Carlos Antonio da Silva and Teodoro, Paulo Eduardo},
TITLE = {Leaf Nitrogen Concentration and Plant Height Prediction for Maize Using UAV-Based Multispectral Imagery and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3237},
URL = {https://www.mdpi.com/2072-4292/12/19/3237},
ISSN = {2072-4292},
ABSTRACT = {Under ideal conditions of nitrogen (N), maize (Zea mays L.) can grow to its full potential, reaching maximum plant height (PH). As a rapid and nondestructive approach, the analysis of unmanned aerial vehicles (UAV)-based imagery may be of assistance to estimate N and height. The main objective of this study is to present an approach to predict leaf nitrogen concentration (LNC, g kg&minus;1) and PH (m) with machine learning techniques and UAV-based multispectral imagery in maize plants. An experiment with 11 maize cultivars under two rates of N fertilization was carried during the 2017/2018 and 2018/2019 crop seasons. The spectral vegetation indices (VI) normalized difference vegetation index (NDVI), normalized difference red-edge index (NDRE), green normalized difference vegetation (GNDVI), and the soil adjusted vegetation index (SAVI) were extracted from the images and, in a computational system, used alongside the spectral bands as input parameters for different machine learning models. A randomized 10-fold cross-validation strategy, with a total of 100 replicates, was used to evaluate the performance of 9 supervised machine learning (ML) models using the Pearson&rsquo;s correlation coefficient (r), mean absolute error (MAE), coefficient of regression (R&sup2;), and root mean square error (RMSE) metrics. The results indicated that the random forest (RF) algorithm performed better, with r and RMSE, respectively, of 0.91 and 1.9 g.kg&minus;&sup1; for LNC, and 0.86 and 0.17 m for PH. It was also demonstrated that VIs contributed more to the algorithm&rsquo;s performances than individual spectral bands. This study concludes that the RF model is appropriate to predict both agronomic variables in maize and may help farmers to monitor their plants based upon their LNC and PH diagnosis and use this knowledge to improve their production rates in the subsequent seasons.},
DOI = {10.3390/rs12193237}
}



@Article{rs12193265,
AUTHOR = {Sonobe, Rei and Yamashita, Hiroto and Mihara, Harumi and Morita, Akio and Ikka, Takashi},
TITLE = {Estimation of Leaf Chlorophyll a, b and Carotenoid Contents and Their Ratios Using Hyperspectral Reflectance},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {19},
ARTICLE-NUMBER = {3265},
URL = {https://www.mdpi.com/2072-4292/12/19/3265},
ISSN = {2072-4292},
ABSTRACT = {Japanese horseradish (wasabi) grows in very specific conditions, and recent environmental climate changes have damaged wasabi production. In addition, the optimal culture methods are not well known, and it is becoming increasingly difficult for incipient farmers to cultivate it. Chlorophyll a, b and carotenoid contents, as well as their allocation, could be an adequate indicator in evaluating its production and environmental stress; thus, developing an in situ method to monitor photosynthetic pigments based on reflectance could be useful for agricultural management. Besides original reflectance (OR), five pre-processing techniques, namely, first derivative reflectance (FDR), continuum-removed (CR), de-trending (DT), multiplicative scatter correction (MSC), and standard normal variate transformation (SNV), were compared to assess the accuracy of the estimation. Furthermore, five machine learning algorithms&mdash;random forest (RF), support vector machine (SVM), kernel-based extreme learning machine (KELM), Cubist, and Stochastic Gradient Boosting (SGB)&mdash;were considered. To classify the samples under different pH or sulphur ion concentration conditions, the end of the red edge bands was effective for OR, FDR, DT, MSC, and SNV, while a green-peak band was effective for CR. Overall, KELM and Cubist showed high performance and incorporating pre-processing techniques was effective for obtaining estimated values with high accuracy. The best combinations were found to be DT&ndash;KELM for chl a (RPD = 1.511&ndash;5.17, RMSE = 1.23&ndash;3.62 &mu;g cm&minus;2) and chl a:b (RPD = 0.73&ndash;3.17, RMSE = 0.13&ndash;0.60); CR&ndash;KELM for chl b (RPD = 1.92&ndash;5.06, RMSE = 0.41&ndash;1.03 &mu;g cm&minus;2) and chl a:car (RPD = 1.31&ndash;3.23, RMSE = 0.26&ndash;0.50); SNV&ndash;Cubist for car (RPD = 1.63&ndash;3.32, RMSE = 0.31&ndash;1.89 &mu;g cm&minus;2); and DT&ndash;Cubist for chl:car (RPD = 1.53&ndash;3.96, RMSE = 0.27&ndash;0.74).},
DOI = {10.3390/rs12193265}
}



@Article{s20205762,
AUTHOR = {Santos, Andr√© A. and Rocha, Filipe A. S. and Reis, Agnaldo J. da R. and Guimar√£es, Frederico G.},
TITLE = {Automatic System for Visual Detection of Dirt Buildup on Conveyor Belts Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5762},
URL = {https://www.mdpi.com/1424-8220/20/20/5762},
ISSN = {1424-8220},
ABSTRACT = {Conveyor belts are the most widespread means of transportation for large quantities of materials in the mining sector. Therefore, autonomous methods that can help human beings to perform the inspection of the belt conveyor system is a major concern for companies. In this context, we present in this work a novel and automatic visual detector that recognizes dirt buildup on the structures of conveyor belts, which is one of the tasks of the maintenance inspectors. This visual detector can be embedded as sensors in autonomous robots for the inspection activity. The proposed system involves training a convolutional neural network from RGB images. The use of the transfer learning technique, i.e., retraining consolidated networks for image classification with our collected images has shown very effective. Two different approaches for transfer learning have been analyzed. The best one presented an average accuracy of 0.8975 with an F-1 Score of 0.8773 for the dirt recognition. A field validation experiment served to evaluate the performance of the proposed system in a real time classification task.},
DOI = {10.3390/s20205762}
}



@Article{rs12203318,
AUTHOR = {Na, Jiaming and Xue, Kaikai and Xiong, Liyang and Tang, Guoan and Ding, Hu and Strobl, Josef and Pfeifer, Norbert},
TITLE = {UAV-Based Terrain Modeling under Vegetation in the Chinese Loess Plateau: A Deep Learning and Terrain Correction Ensemble Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3318},
URL = {https://www.mdpi.com/2072-4292/12/20/3318},
ISSN = {2072-4292},
ABSTRACT = {Accurate topographic mapping is a critical task for various environmental applications because elevation affects hydrodynamics and vegetation distributions. UAV photogrammetry is popular in terrain modelling because of its lower cost compared to laser scanning. However, this method is restricted in vegetation area with a complex terrain, due to reduced ground visibility and lack of robust and automatic filtering algorithms. To solve this problem, this work proposed an ensemble method of deep learning and terrain correction. First, image matching point cloud was generated by UAV photogrammetry. Second, vegetation points were identified based on U-net deep learning network. After that, ground elevation was corrected by estimating vegetation height to generate the digital terrain model (DTM). Two scenarios, namely, discrete and continuous vegetation areas were considered. The vegetation points in the discrete area were directly removed and then interpolated, and terrain correction was applied for the points in the continuous areas. Case studies were conducted in three different landforms in the loess plateau of China, and accuracy assessment indicated that the overall accuracy of vegetation detection was 95.0%, and the MSE (Mean Square Error) of final DTM (Digital Terrain Model) was 0.024 m.},
DOI = {10.3390/rs12203318}
}



@Article{rs12203328,
AUTHOR = {Imangholiloo, Mohammad and Saarinen, Ninni and Holopainen, Markus and Yu, Xiaowei and Hyypp√§, Juha and Vastaranta, Mikko},
TITLE = {Using Leaf-Off and Leaf-On Multispectral Airborne Laser Scanning Data to Characterize Seedling Stands},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3328},
URL = {https://www.mdpi.com/2072-4292/12/20/3328},
ISSN = {2072-4292},
ABSTRACT = {Information from seedling stands in time and space is essential for sustainable forest management. To fulfil these informational needs with limited resources, remote sensing is seen as an intriguing alternative for forest inventorying. The structure and tree species composition in seedling stands have created challenges for capturing this information using sensors providing sparse point densities that do not have the ability to penetrate canopy gaps or provide spectral information. Therefore, multispectral airborne laser scanning (mALS) systems providing dense point clouds coupled with multispectral intensity data theoretically offer advantages for the characterization of seedling stands. The aim of this study was to investigate the capability of Optech Titan mALS data to characterize seedling stands in leaf-off and leaf-on conditions, as well as to retrieve the most important forest inventory attributes, such as distinguishing deciduous from coniferous trees, and estimating tree density and height. First, single-tree detection approaches were used to derive crown boundaries and tree heights from which forest structural attributes were aggregated for sample plots. To predict tree species, a random forests classifier was trained using features from two single-channel intensities (SCIs) with wavelengths of 1550 (SCI-Ch1) and 1064 nm (SCI-Ch2), and multichannel intensity (MCI) data composed of three mALS channels. The most important and uncorrelated features were analyzed and selected from 208 features. The highest overall accuracies in classification of Norway spruce, birch, and nontree class in leaf-off and leaf-on conditions obtained using SCI-Ch1 and SCI-Ch2 were 87.36% and 69.47%, respectively. The use of MCI data improved classification by up to 96.55% and 92.54% in leaf-off and leaf-on conditions, respectively. Overall, leaf-off data were favorable for distinguishing deciduous from coniferous trees and tree density estimation with a relative root mean square error (RMSE) of 37.9%, whereas leaf-on data provided more accurate height estimations, with a relative RMSE of 10.76%. Determining the canopy threshold for separating ground returns from vegetation returns was found to be critical, as mapped trees might have a height below one meter. The results showed that mALS data provided benefits for characterizing seedling stands compared to single-channel ALS systems.},
DOI = {10.3390/rs12203328}
}



@Article{app10207120,
AUTHOR = {Mohammed, Thaha and Albeshri, Aiiad and Katib, Iyad and Mehmood, Rashid},
TITLE = {UbiPriSEQ‚ÄîDeep Reinforcement Learning to Manage Privacy, Security, Energy, and QoS in 5G IoT HetNets},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7120},
URL = {https://www.mdpi.com/2076-3417/10/20/7120},
ISSN = {2076-3417},
ABSTRACT = {5G networks and Internet of Things (IoT) offer a powerful platform for ubiquitous environments with their ubiquitous sensing, high speeds and other benefits. The data, analytics, and other computations need to be optimally moved and placed in these environments, dynamically, such that energy-efficiency and QoS demands are best satisfied. A particular challenge in this context is to preserve privacy and security while delivering quality of service (QoS) and energy-efficiency. Many works have tried to address these challenges but without a focus on optimizing all of them and assuming fixed models of environments and security threats. This paper proposes the UbiPriSEQ framework that uses Deep Reinforcement Learning (DRL) to adaptively, dynamically, and holistically optimize QoS, energy-efficiency, security, and privacy. UbiPriSEQ is built on a three-layered model and comprises two modules. UbiPriSEQ devises policies and makes decisions related to important parameters including local processing and offloading rates for data and computations, radio channel states, transmit power, task priority, and selection of fog nodes for offloading, data migration, and so forth. UbiPriSEQ is implemented in Python over the TensorFlow platform and is evaluated using a real-life application in terms of SINR, privacy metric, latency, and utility function, manifesting great promise.},
DOI = {10.3390/app10207120}
}



@Article{s20205805,
AUTHOR = {Ai, Tianfu and Xu, Bin and Xiang, Changle and Fan, Wei and Zhang, Yibo},
TITLE = {Modeling of a Novel Coaxial Ducted Fan Aerial Robot Combined with Corner Environment by Using Artificial Neural Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5805},
URL = {https://www.mdpi.com/1424-8220/20/20/5805},
ISSN = {1424-8220},
ABSTRACT = {A novel coaxial ducted fan aerial robot with a manipulator is proposed which can achieve some hover operation tasks in a corner environment, such as switching on and off a wall-attached button on the corner. In order to study the aerodynamic interference between the prototype and the environment when the aerial robot is hovering in the corner environment, a method for the comprehensive modeling of the prototype and corner environment based on the artificial neural network is presented. By using the CFD simulation software, the flow field of the prototype at different positions with the corner effect is analyzed. After determining the input, output and structure of the neural network model, the Adam and gradient descent algorithms are selected as the neural network training algorithms, respectively. In addition, to optimize the initial weights and biases of the neural network model, the genetic algorithm is precisely used. The three-dimensional prediction surfaces generated by the three methods of the neural network, kriging surface and the polynomial fitting are compared. The results show that the neural network has high prediction accuracy, and can be applied to the comprehensive modeling of the prototype and the corner environment.},
DOI = {10.3390/s20205805}
}



@Article{sym12101682,
AUTHOR = {Wang, Qiuzhen and Mao, Xinjun},
TITLE = {Dynamic Task Allocation Method of Swarm Robots Based on Optimal Mass Transport Theory},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1682},
URL = {https://www.mdpi.com/2073-8994/12/10/1682},
ISSN = {2073-8994},
ABSTRACT = {It is difficult for swarm robots to allocate tasks efficiently by self-organization in a dynamic unknown environment. The computational cost of swarm robots will be significantly increased for large-scale tasks, and the unbalanced task allocation of robots will also lead to a decrease in system efficiency. To address these issues, we propose a dynamic task allocation method of swarm robots based on optimal mass transport theory. The problem of large-scale tasks is solved by grouping swarm robots to complete regional tasks. The task reallocation mechanism realizes the balanced task allocation of individual robots. This paper solves the symmetric assignment between robot and task and between the robot groups and the regional tasks. Our simulation and experimental results demonstrate that the proposed method can make the swarm robots self-organize to allocate large-scale dynamic tasks effectively. The tasks can also be balanced allocated to each robot in the swarm of robots.},
DOI = {10.3390/sym12101682}
}



@Article{agriculture10100475,
AUTHOR = {Abraham, Emerson Rodolfo and Mendes dos Reis, Jo√£o Gilberto and Vendrametto, Oduvaldo and Oliveira Costa Neto, Pedro Luiz de and Carlo Toloi, Rodrigo and Souza, Aguinaldo Eduardo de and Oliveira Morais, Marcos de},
TITLE = {Time Series Prediction with Artificial Neural Networks: An Analysis Using Brazilian Soybean Production},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {475},
URL = {https://www.mdpi.com/2077-0472/10/10/475},
ISSN = {2077-0472},
ABSTRACT = {Food production to meet human demand has been a challenge to society. Nowadays, one of the main sources of feeding is soybean. Considering agriculture food crops, soybean is sixth by production volume and the fourth by both production area and economic value. The grain can be used directly to human consumption, but it is highly used as a source of protein for animal production that corresponds 75% of the total, or as oil and derived food products. Brazil and the US are the most important players responsible for more than 70% of world production. Therefore, a reliable forecasting is essential for decision-makers to plan adequate policies to this important commodity and to establish the necessary logistical resources. In this sense, this study aims to predict soybean harvest area, yield, and production using Artificial Neural Networks (ANN) and compare with classical methods of Time Series Analysis. To this end, we collected data from a time series (1961&ndash;2016) regarding soybean production in Brazil. The results reveal that ANN is the best approach to predict soybean harvest area and production while classical linear function remains more effective to predict soybean yield. Moreover, ANN presents as a reliable model to predict time series and can help the stakeholders to anticipate the world soybean offer.},
DOI = {10.3390/agriculture10100475}
}



@Article{rs12203364,
AUTHOR = {Collins, Adam M. and Brodie, Katherine L. and Bak, Andrew Spicer and Hesser, Tyler J. and Farthing, Matthew W. and Lee, Jonghyun and Long, Joseph W.},
TITLE = {Bathymetric Inversion and Uncertainty Estimation from Synthetic Surf-Zone Imagery with Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3364},
URL = {https://www.mdpi.com/2072-4292/12/20/3364},
ISSN = {2072-4292},
ABSTRACT = {Resolving surf-zone bathymetry from high-resolution imagery typically involves measuring wave speeds and performing a physics-based inversion process using linear wave theory, or data assimilation techniques which combine multiple remotely sensed parameters with numerical models. In this work, we explored what types of coastal imagery can be best utilized in a 2-dimensional fully convolutional neural network to directly estimate nearshore bathymetry from optical expressions of wave kinematics. Specifically, we explored utilizing time-averaged images (timex) of the surf-zone, which can be used as a proxy for wave dissipation, as well as including a single-frame image input, which has visible patterns of wave refraction and instantaneous expressions of wave breaking. Our results show both types of imagery can be used to estimate nearshore bathymetry. However, the single-frame imagery provides more complete information across the domain, decreasing the error over the test set by approximately 10% relative to using timex imagery alone. A network incorporating both inputs had the best performance, with an overall root-mean-squared-error of 0.39 m. Activation maps demonstrate the additional information provided by the single-frame imagery in non-breaking wave areas which aid in prediction. Uncertainty in model predictions is explored through three techniques (Monte Carlo (MC) dropout, infer-transformation, and infer-noise) to provide additional actionable information about the spatial reliability of each bathymetric prediction.},
DOI = {10.3390/rs12203364}
}



@Article{computers9040084,
AUTHOR = {Doumbia, Mamadou and Cheng, Xu},
TITLE = {State Estimation and Localization Based on Sensor Fusion for Autonomous Robots in Indoor Environment},
JOURNAL = {Computers},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {84},
URL = {https://www.mdpi.com/2073-431X/9/4/84},
ISSN = {2073-431X},
ABSTRACT = {Currently, almost all robot state estimation and localization systems are based on the Kalman filter (KF) and its derived methods, in particular the unscented Kalman filter (UKF). When applying the UKF alone, the estimate of the state is not sufficiently precise. In this paper, a new hierarchical infrared navigational algorithm hybridization (HIRNAH) system is developed to provide better state estimation and localization for mobile robots. Two navigation subsystems (inertial navigation system (INS) and, using a novel infrared navigation algorithm (NIRNA), Odom-NIRNA) and an RPLIDAR-A3 scanner cooperation to build HIRNAH. The robot pose (position and orientation) errors are estimated by a system filtering module (SFM) and used to smooth the robot&rsquo;s final poses. A prototype (two rotary encoders, one smartphone-based robot sensing model and one RPLIDAR-A3 scanner) has been built and mounted on a four-wheeled mobile robot (4-WMR). Simulation results have motivated real-life experiments, and obtained results are compared to some existent research (hardware and control technology navigation (HCTNav), rapid exploring random tree (RRT) and in stand-alone mode (INS)) for performance measurements. The experimental results confirm that HIRNAH presents a more accurate estimation and a lower mean square error (MSE) of the robot&rsquo;s state than those calculated by the previously cited HCTNav, RRT and INS.},
DOI = {10.3390/computers9040084}
}



@Article{rs12203392,
AUTHOR = {Dong, Xiancong and Li, Xiaojie and Zheng, Xingming and Jiang, Tao and Li, Xiaofeng},
TITLE = {Effect of Saline Soil Cracks on Satellite Spectral Inversion Electrical Conductivity},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3392},
URL = {https://www.mdpi.com/2072-4292/12/20/3392},
ISSN = {2072-4292},
ABSTRACT = {The dehydration cracking of saline soil is a kind of common natural phenomenon, and the cracks of saline soil will affect the satellite spectrum, and then affect the accuracy of satellite spectral inversion of electrical conductivity (EC). This study introduces the concept of crack rate (CR) to describe the crack information of saline soil, and quantifies the influence of saline soil crack on the EC of satellite spectral inversion. In 2014 and 2020, the satellite-ground synchronous observation experiments of soda-type inland saline soil and coastal chlorinated-type saline soil were carried out, and the CR of surface cracked saline soil was extracted by an image processing algorithm. For the saline soil spectrum data, the correlation analysis method is used to establish the best band combination that characterizes the relationship between the different saline soil spectrum data and salinity, and the EC inversion model is established using the BP neural network method. The results show that: after the CR is introduced, the determination coefficient (R2) for the EC of soda-type saline soil satellite spectral inversion increased from 0.59 to 0.67, with an increase of 14.42%, and the mean square error (MSE) reduced from 0.20 to 0.16, with a decrease of 19.49%. The R2 for the EC of coastal chlorinated-type saline soil satellite spectral inversion increased from 0.64 to 0.75, an increase of 17.73%, and the MSE decreased from 0.16 to 0.12, a decrease of 25.15%. The study proved the influence of the cracks in the saline soil on the satellite spectrum and provided a new way to improve the accuracy of the satellite spectrum inversion of the EC of the cracked saline soil.},
DOI = {10.3390/rs12203392}
}



@Article{s20205857,
AUTHOR = {Johnson, Brandy J. and Malanoski, Anthony P. and Erickson, Jeffrey S.},
TITLE = {Development of a Colorimetric Sensor for Autonomous, Networked, Real-Time Application},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5857},
URL = {https://www.mdpi.com/1424-8220/20/20/5857},
ISSN = {1424-8220},
ABSTRACT = {This review describes an ongoing effort intended to develop wireless sensor networks for real-time monitoring of airborne targets across a broad area. The goal is to apply the spectrophotometric characteristics of porphyrins and metalloporphyrins in a colorimetric array for detection and discrimination of changes in the chemical composition of environmental air samples. The work includes hardware, software, and firmware design as well as development of algorithms for identification of event occurrence and discrimination of targets. Here, we describe the prototype devices and algorithms related to this effort as well as work directed at selection of indicator arrays for use with the system. Finally, we review the field trials completed with the prototype devices and discuss the outlook for further development.},
DOI = {10.3390/s20205857}
}



@Article{rs12203396,
AUTHOR = {Colorado, Julian D. and Cera-Bornacelli, Natalia and Caldas, Juan S. and Petro, Eliel and Rebolledo, Maria C. and Cuellar, David and Calderon, Francisco and Mondragon, Ivan F. and Jaramillo-Botero, Andres},
TITLE = {Estimation of Nitrogen in Rice Crops from UAV-Captured Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3396},
URL = {https://www.mdpi.com/2072-4292/12/20/3396},
ISSN = {2072-4292},
ABSTRACT = {Leaf nitrogen (N) directly correlates to chlorophyll production, affecting crop growth and yield. Farmers use soil plant analysis development (SPAD) devices to calculate the amount of chlorophyll present in plants. However, monitoring large-scale crops using SPAD is prohibitively time-consuming and demanding. This paper presents an unmanned aerial vehicle (UAV) solution for estimating leaf N content in rice crops, from multispectral imagery. Our contribution is twofold: (i) a novel trajectory control strategy to reduce the angular wind-induced perturbations that affect image sampling accuracy during UAV flight, and (ii) machine learning models to estimate the canopy N via vegetation indices (VIs) obtained from the aerial imagery. This approach integrates an image processing algorithm using the GrabCut segmentation method with a guided filtering refinement process, to calculate the VIs according to the plots of interest. Three machine learning methods based on multivariable linear regressions (MLR), support vector machines (SVM), and neural networks (NN), were applied and compared through the entire phonological cycle of the crop: vegetative (V), reproductive (R), and ripening (Ri). Correlations were obtained by comparing our methods against an assembled ground-truth of SPAD measurements. The higher N correlations were achieved with NN: 0.98 (V), 0.94 (R), and 0.89 (Ri). We claim that the proposed UAV stabilization control algorithm significantly improves on the N-to-SPAD correlations by minimizing wind perturbations in real-time and reducing the need for offline image corrections.},
DOI = {10.3390/rs12203396}
}



@Article{app10207263,
AUTHOR = {Lee, Yong-Hyeok and Jang, Dong-Won and Kim, Jae-Bin and Park, Rae-Hong and Park, Hyung-Min},
TITLE = {Audio‚ÄìVisual Speech Recognition Based on Dual Cross-Modality Attentions with the Transformer Model},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {7263},
URL = {https://www.mdpi.com/2076-3417/10/20/7263},
ISSN = {2076-3417},
ABSTRACT = {Since attention mechanism was introduced in neural machine translation, attention has been combined with the long short-term memory (LSTM) or replaced the LSTM in a transformer model to overcome the sequence-to-sequence (seq2seq) problems with the LSTM. In contrast to the neural machine translation, audio&ndash;visual speech recognition (AVSR) may provide improved performance by learning the correlation between audio and visual modalities. As a result that the audio has richer information than the video related to lips, AVSR is hard to train attentions with balanced modalities. In order to increase the role of visual modality to a level of audio modality by fully exploiting input information in learning attentions, we propose a dual cross-modality (DCM) attention scheme that utilizes both an audio context vector using video query and a video context vector using audio query. Furthermore, we introduce a connectionist-temporal-classification (CTC) loss in combination with our attention-based model to force monotonic alignments required in AVSR. Recognition experiments on LRS2-BBC and LRS3-TED datasets showed that the proposed model with the DCM attention scheme and the hybrid CTC/attention architecture achieved at least a relative improvement of 7.3% on average in the word error rate (WER) compared to competing methods based on the transformer model.},
DOI = {10.3390/app10207263}
}



@Article{rs12203416,
AUTHOR = {Temitope Yekeen, Shamsudeen and Balogun, Abdul-Lateef},
TITLE = {Advances in Remote Sensing Technology, Machine Learning and Deep Learning for Marine Oil Spill Detection, Prediction and Vulnerability Assessment},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3416},
URL = {https://www.mdpi.com/2072-4292/12/20/3416},
ISSN = {2072-4292},
ABSTRACT = {Although advancements in remote sensing technology have facilitated quick capture and identification of the source and location of oil spills in water bodies, the presence of other biogenic elements (lookalikes) with similar visual attributes hinder rapid detection and prompt decision making for emergency response. To date, different methods have been applied to distinguish oil spills from lookalikes with limited success. In addition, accurately modeling the trajectory of oil spills remains a challenge. Thus, we aim to provide further insights on the multi-faceted problem by undertaking a holistic review of past and current approaches to marine oil spill disaster reduction as well as explore the potentials of emerging digital trends in minimizing oil spill hazards. The scope of previous reviews is extended by covering the inter-related dimensions of detection, discrimination, and trajectory prediction of oil spills for vulnerability assessment. Findings show that both optical and microwave airborne and satellite remote sensors are used for oil spill monitoring with microwave sensors being more widely used due to their ability to operate under any weather condition. However, the accuracy of both sensors is affected by the presence of biogenic elements, leading to false positive depiction of oil spills. Statistical image segmentation has been widely used to discriminate lookalikes from oil spills with varying levels of accuracy but the emergence of digitalization technologies in the fourth industrial revolution (IR 4.0) is enabling the use of Machine learning (ML) and deep learning (DL) models, which are more promising than the statistical methods. The Support Vector Machine (SVM) and Artificial Neural Network (ANN) are the most used machine learning algorithms for oil spill detection, although the restriction of ML models to feed forward image classification without support for the end-to-end trainable framework limits its accuracy. On the other hand, deep learning models&rsquo; strong feature extraction and autonomous learning capability enhance their detection accuracy. Also, mathematical models based on lagrangian method have improved oil spill trajectory prediction with higher real time accuracy than the conventional worst case, average and survey-based approaches. However, these newer models are unable to quantify oil droplets and uncertainty in vulnerability prediction. Considering that there is yet no single best remote sensing technique for unambiguous detection and discrimination of oil spills and lookalikes, it is imperative to advance research in the field in order to improve existing technology and develop specialized sensors for accurate oil spill detection and enhanced classification, leveraging emerging geospatial computer vision initiatives.},
DOI = {10.3390/rs12203416}
}



@Article{electronics9101714,
AUTHOR = {Park, JiWoong and Nam, SungChan and Choi, HongBeom and Ko, YoungEun and Ko, Young-Bae},
TITLE = {Improving Deep Learning-Based UWB LOS/NLOS Identification with Transfer Learning: An Empirical Approach},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1714},
URL = {https://www.mdpi.com/2079-9292/9/10/1714},
ISSN = {2079-9292},
ABSTRACT = {This paper presents an improved ultra-wideband (UWB) line of sight (LOS)/non-line of sight (NLOS) identification scheme based on a hybrid method of deep learning and transfer learning. Previous studies have limitations, in that the classification accuracy significantly decreases in an unknown place. To solve this problem, we propose a transfer learning-based NLOS identification method for classifying the NLOS conditions of the UWB signal in an unmeasured environment. Both the multilayer perceptron and convolutional neural network (CNN) are introduced as classifiers for NLOS conditions. We evaluate the proposed scheme by conducting experiments in both measured and unmeasured environments. Channel data were measured using a Decawave EVK1000 in two similar indoor office environments. In the unmeasured environment, the existing CNN method showed an accuracy of approximately 44%, but when the proposed scheme was applied to the CNN, it showed an accuracy of up to 98%. The training time of the proposed scheme was measured to be approximately 48 times faster than that of the existing CNN. When comparing the proposed scheme with learning a new CNN in an unmeasured environment, the proposed scheme demonstrated an approximately 10% higher accuracy and approximately five times faster training time.},
DOI = {10.3390/electronics9101714}
}



@Article{s20205904,
AUTHOR = {Digulescu, Angela and Despina-Stoian, Cristina and StƒÉnescu, Denis and Popescu, Florin and Enache, Florin and Ioana, Cornel and RƒÉdoi, Emanuel and R√Æncu, Iulian and »òerbƒÉnescu, Alexandru},
TITLE = {New Approach of UAV Movement Detection and Characterization Using Advanced Signal Processing Methods Based on UWB Sensing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5904},
URL = {https://www.mdpi.com/1424-8220/20/20/5904},
ISSN = {1424-8220},
ABSTRACT = {In the last years, the commercial drone/unmanned aerial vehicles market has grown due to their technological performances (provided by the multiple onboard available sensors), low price, and ease of use. Being very attractive for an increasing number of applications, their presence represents a major issue for public or classified areas with a special status, because of the rising number of incidents. Our paper proposes a new approach for the drone movement detection and characterization based on the ultra-wide band (UWB) sensing system and advanced signal processing methods. This approach characterizes the movement of the drone using classical methods such as correlation, envelope detection, time-scale analysis, but also a new method, the recurrence plot analysis. The obtained results are compared in terms of movement map accuracy and required computation time in order to offer a future starting point for the drone intrusion detection.},
DOI = {10.3390/s20205904}
}



@Article{electronics9101735,
AUTHOR = {Rodr√≠guez-Abreo, Omar and Garcia-Guendulain, Juan Manuel and Hern√°ndez-Alvarado, Rodrigo and Flores Rangel, Alejandro and Fuentes-Silva, Carlos},
TITLE = {Genetic Algorithm-Based Tuning of Backstepping Controller for a Quadrotor-Type Unmanned Aerial Vehicle},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1735},
URL = {https://www.mdpi.com/2079-9292/9/10/1735},
ISSN = {2079-9292},
ABSTRACT = {Backstepping is a control technique based on Lyapunov&rsquo;s theory that has been successfully implemented in the control of motors and robots by several nonlinear methods. However, there are no standardized methods for tuning control gains (unlike the PIDs). This paper shows the tuning gains of the backstepping controller, using Genetic Algorithms (GA), for an Unmanned Aerial Vehicle (UAV), quadrotor type, designed for autonomous trajectory tracking. First, a dynamic model of the vehicle is obtained through the Newton‚ÄíEuler methodology. Then, the control law is obtained, and self-tuning is performed, through which we can obtain suitable values of the gains in order to achieve the design requirements. In this work, the establishment time and maximum impulse are considered as such. The tuning and simulations of the system response were performed using the MATLAB-Simulink environment, obtaining as a result the compliance of the design parameters and the correct tracking of different trajectories. The results show that self-tuning by means of genetic algorithms satisfactorily adjusts for the gains of a backstepping controller applied to a quadrotor and allows for the implementation of a control system that responds appropriately to errors of different magnitude.},
DOI = {10.3390/electronics9101735}
}



@Article{s20205940,
AUTHOR = {Klaer, Peter and Huang, Andi and S√©vigny, Pascale and Rajan, Sreeraman and Pant, Shashank and Patnaik, Prakash and Balaji, Bhashyam},
TITLE = {An Investigation of Rotary Drone HERM Line Spectrum under Manoeuvering Conditions},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {5940},
URL = {https://www.mdpi.com/1424-8220/20/20/5940},
ISSN = {1424-8220},
ABSTRACT = {Detecting and identifying drones is of great interest due to the proliferation of highly manoeuverable drones with on-board sensors of increasing sensing capabilities. In this paper, we investigate the use of radars for tackling this problem. In particular, we focus on the problem of detecting rotary drones and distinguishing between single-propeller and multi-propeller drones using a micro-Doppler analysis. Two different radars were used, an ultra wideband (UWB) continuous wave (CW) C-band radar and an automotive frequency modulated continuous wave (FMCW) W-band radar, to collect micro-Doppler signatures of the drones. By taking a closer look at HElicopter Rotor Modulation (HERM) lines, the spool and chopping lines are identified for the first time in the context of drones to determine the number of propeller blades. Furthermore, a new multi-frequency analysis method using HERM lines is developed, which allows the detection of propeller rotation rates (spool and chopping frequencies) of single and multi-propeller drones. Therefore, the presented method is a promising technique to aid in the classification of drones.},
DOI = {10.3390/s20205940}
}



@Article{rs12203460,
AUTHOR = {Liu, Jia and Chen, Jianjun and Qin, Qiaoting and You, Haotian and Han, Xiaowen and Zhou, Guoqing},
TITLE = {Patch Pattern and Ecological Risk Assessment of Alpine Grassland in the Source Region of the Yellow River},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {20},
ARTICLE-NUMBER = {3460},
URL = {https://www.mdpi.com/2072-4292/12/20/3460},
ISSN = {2072-4292},
ABSTRACT = {The source region of the Yellow River (SRYR) is an important water conservation and animal husbandry resource in China. It is of great significance to understand the patch pattern and ecological risk of alpine grassland in the SRYR for ecological environment management. This study first used 12 unmanned aerial vehicle (UAV) aerial images and eight moderate resolution imaging spectroradiometer (MODIS) vegetation index product MOD13Q1 images from July to August in 2019 to extract alpine grassland patch patterns in the SRYR, then constructed an ecological risk model based on the landscape vulnerability index and landscape disturbance index, and finally combined spatial self-reliance correlation and semi-variance analysis methods to explore the spatial distribution of ecological risks. The results showed that the patch fragmentation degree (Pi), area weighted shape index (AWMSI), and separation degree (Si) of the four grassland types in the SRYR are ordered as follows: alpine steppe &gt; degraded meadow &gt; alpine meadow &gt; swamp meadow. Moreover, the greater the fractional vegetation cover (FVC), the greater the landscape dominance index (DOi), and the better the ecosystem stability. The spatial difference of ecological risk in the SRYR shows a situation of low risk in the east (ERImin=1.5355) and high risk in the west (ERImax = 70.6429). High FVC was found in low and mild low risk areas where the vegetation types are mainly swamp meadow and shrub, while low FVC was found in high and mild high-risk areas where the vegetation types are mainly alpine steppe and degraded meadow. The spatial distribution of ecological risk of the SRYR has obvious positive spatial correlation (Moran's I = 0.863), the spatial aggregation distribution is distinct, and the local space has significant high-high aggregation and low&ndash;low aggregation phenomena. The results of this study reveal that patch characteristics have good indicative significance for alpine grassland ecological protection and should be considered in future studies. In addition, the ecological risk in the SRYR is relatively high, especially in the western region, which should be taken seriously in future ecological management and governance.},
DOI = {10.3390/rs12203460}
}



@Article{agronomy10111624,
AUTHOR = {Husin, Nur A. and Khairunniza-Bejo, Siti and Abdullah, Ahmad F. and Kassim, Muhamad S. M. and Ahmad, Desa and Aziz, Mohd H. A.},
TITLE = {Classification of Basal Stem Rot Disease in Oil Palm Plantations Using Terrestrial Laser Scanning Data and Machine Learning},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1624},
URL = {https://www.mdpi.com/2073-4395/10/11/1624},
ISSN = {2073-4395},
ABSTRACT = {The oil palm industry is vital for the Malaysian economy. However, it is threatened by the Ganoderma boninense fungus, which causes basal stem rot (BSR) disease. Foliar symptoms of the disease include the appearance of several unopened spears, flat crowns, and small crown size. The effect of this disease depends on the severity of the infection. Currently, the disease can be detected manually by analyzing the oil palm tree&rsquo;s physical structure. Terrestrial laser scanning (TLS) is an active ranging method that uses laser light, which can directly represent the tree&rsquo;s external structure. This study aimed to classify the healthiness levels of the BSR disease using a machine learning (ML) approach. A total of 80 oil palm trees with four different healthiness levels were pre-determined by the experts during data collection with 40 each for training and testing. The four healthiness levels are T0 (healthy), T1 (mildly infected), T2 (moderately infected), and T3 (severely infected), with 10 trees in each level. A terrestrial scanner was mounted at a height of 1 m, and each oil palm was scanned at four positions at a distance of 1.5 m around the tree. Five tree features were extracted from the TLS data: C200 (crown slice at 200 cm from the top), C850 (crown slice at 850 cm from the top), crown area (number of pixels inside the crown), frond angle, and frond number. C200 and C850 were obtained using the crown stratification method, while the other three features were obtained from the top-down image. The obtained features were then analyzed by principal component analysis (PCA) to reduce the dimensionality of the dataset and increase its interpretability while at the same time minimizing information loss. The results showed that the kernel na&iuml;ve Bayes (KNB) model developed using the input parameters of the principal components (PCs) 1 and 2 had the best performance among 90 other models with a multiple level accuracy of 85% and a Kappa coefficient of 0.80. Furthermore, the combination of the two highest PC variance with the most weighted to frond number, frond angle, crown area, and C200 significantly contributed to the classification success. The model also could classify healthy and mildly infected trees with 100% accuracy. Therefore, it can be concluded that the ML approach using TLS data can be used to predict early BSR infection with high accuracy.},
DOI = {10.3390/agronomy10111624}
}



@Article{rs12213474,
AUTHOR = {Wang, Chunsheng and Chang, Lili and Zhao, Lingran and Niu, Ruiqing},
TITLE = {Automatic Identification and Dynamic Monitoring of Open-Pit Mines Based on Improved Mask R-CNN and Transfer Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3474},
URL = {https://www.mdpi.com/2072-4292/12/21/3474},
ISSN = {2072-4292},
ABSTRACT = {As the ecological problems caused by mine development become increasingly prominent, the conflict between mining activity and environmental protection is gradually intensifying. There is an urgent problem regarding how to effectively monitor mineral exploitation activities. In order to automatic identify and dynamically monitor open-pit mines of Hubei Province, an open-pit mine extraction model based on Improved Mask R-CNN (Region Convolutional Neural Network) and Transfer learning (IMRT) is proposed, a set of multi-source open-pit mine sample databases consisting of Gaofen-1, Gaofen-2 and Google Earth satellite images with a resolution of two meters is constructed, and an automatic batch production process of open-pit mine targets is designed. In this paper, pixel-based evaluation indexes and object-based evaluation indexes are used to compare the recognition effect of IMRT, faster R-CNN, Maximum Likelihood (MLE) and Support Vector Machine (SVM). The IMRT model has the best performance in Pixel Accuracy (PA), Kappa and MissingAlarm, with values of 0.9718, 0.8251 and 0.0862, respectively, which shows that the IMRT model has a better effect on open-pit mine automatic identification, and the results are also used as evaluation units of the environmental damages of the mines. The evaluation results show that level ‚Ö† (serious) land occupation and destruction of key mining areas account for 34.62%, and 36.2% of topographical landscape damage approached level I. This study has great practical significance in terms of realizing the coordinated development of mines and ecological environments.},
DOI = {10.3390/rs12213474}
}



@Article{app10217482,
AUTHOR = {Urrea, Claudio and Kern, John and Alvarado, Johanna},
TITLE = {Design and Evaluation of a New Fuzzy Control Algorithm Applied to a Manipulator Robot},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7482},
URL = {https://www.mdpi.com/2076-3417/10/21/7482},
ISSN = {2076-3417},
ABSTRACT = {In this article, we propose a new scheme for a fuzzy logic controller, which includes acceleration as one of its linguistic variables, as opposed to other techniques and approaches that have been developed and reported in the literature. This method is used for controlling the tracking of the trajectory followed by the joints of a 2-DoF manipulator robot. To this end, a complete simulation environment is developed through the MatLab/Simulink&reg; software. The dynamic model of the manipulator robot includes a vector that consists of the estimate of the friction forces present in the joints. Then, a controller based on fuzzy logic is designed and implemented for each joint. Finally, the performance of the developed system is assessed and then compared to the performance of a classic PID controller. The incorporation of the fuzzy variable acceleration significantly improved the system&rsquo;s response.},
DOI = {10.3390/app10217482}
}



@Article{agronomy10111648,
AUTHOR = {Demestichas, Konstantinos and Daskalakis, Emmanouil},
TITLE = {Data Lifecycle Management in Precision Agriculture Supported by Information and Communication Technology},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1648},
URL = {https://www.mdpi.com/2073-4395/10/11/1648},
ISSN = {2073-4395},
ABSTRACT = {The role of agriculture in environmental degradation and climate change has been at the center of a long-lasting and controversial debate. This situation combined with the expected growth in crop demand and the increasing prices of fertilizers and pesticides has made the need for a more resource-efficient and environmentally sustainable agriculture more evident than ever. Precision agriculture (PA), as a relatively new farming management concept, aims to improve crop performance as well as to reduce the environmental footprint by utilizing information about the temporal and the spatial variability of crops. Information and communication technology (ICT) systems have influenced and shaped every part of modern life, and PA is no exception. The current paper conducts a literature review of prominent ICT solutions, focusing on their role in supporting different phases of the lifecycle of PA-related data. In addition to this, a data lifecycle model was developed as part of a novel categorization approach for the analyzed solutions.},
DOI = {10.3390/agronomy10111648}
}



@Article{rs12213511,
AUTHOR = {Eskandari, Roghieh and Mahdianpari, Masoud and Mohammadimanesh, Fariba and Salehi, Bahram and Brisco, Brian and Homayouni, Saeid},
TITLE = {Meta-analysis of Unmanned Aerial Vehicle (UAV) Imagery for Agro-environmental Monitoring Using Machine Learning and Statistical Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3511},
URL = {https://www.mdpi.com/2072-4292/12/21/3511},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) imaging systems have recently gained significant attention from researchers and practitioners as a cost-effective means for agro-environmental applications. In particular, machine learning algorithms have been applied to UAV-based remote sensing data for enhancing the UAV capabilities of various applications. This systematic review was performed on studies through a statistical meta-analysis of UAV applications along with machine learning algorithms in agro-environmental monitoring. For this purpose, a total number of 163 peer-reviewed articles published in 13 high-impact remote sensing journals over the past 20 years were reviewed focusing on several features, including study area, application, sensor type, platform type, and spatial resolution. The meta-analysis revealed that 62% and 38% of the studies applied regression and classification models, respectively. Visible sensor technology was the most frequently used sensor with the highest overall accuracy among classification articles. Regarding regression models, linear regression and random forest were the most frequently applied models in UAV remote sensing imagery processing. Finally, the results of this study confirm that applying machine learning approaches on UAV imagery produces fast and reliable results. Agriculture, forestry, and grassland mapping were found as the top three UAV applications in this review, in 42%, 22%, and 8% of the studies, respectively.},
DOI = {10.3390/rs12213511}
}



@Article{s20216097,
AUTHOR = {Gardner, Marcus and Mancero Castillo, C. Sebastian and Wilson, Samuel and Farina, Dario and Burdet, Etienne and Khoo, Boo Cheong and Atashzar, S. Farokh and Vaidyanathan, Ravi},
TITLE = {A Multimodal Intention Detection Sensor Suite for Shared Autonomy of Upper-Limb Robotic Prostheses},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6097},
URL = {https://www.mdpi.com/1424-8220/20/21/6097},
ISSN = {1424-8220},
ABSTRACT = {Neurorobotic augmentation (e.g., robotic assist) is now in regular use to support individuals suffering from impaired motor functions. A major unresolved challenge, however, is the excessive cognitive load necessary for the human&ndash;machine interface (HMI). Grasp control remains one of the most challenging HMI tasks, demanding simultaneous, agile, and precise control of multiple degrees-of-freedom (DoFs) while following a specific timing pattern in the joint and human&ndash;robot task spaces. Most commercially available systems use either an indirect mode-switching configuration or a limited sequential control strategy, limiting activation to one DoF at a time. To address this challenge, we introduce a shared autonomy framework centred around a low-cost multi-modal sensor suite fusing: (a) mechanomyography (MMG) to estimate the intended muscle activation, (b) camera-based visual information for integrated autonomous object recognition, and (c) inertial measurement to enhance intention prediction based on the grasping trajectory. The complete system predicts user intent for grasp based on measured dynamical features during natural motions. A total of 84 motion features were extracted from the sensor suite, and tests were conducted on 10 able-bodied and 1 amputee participants for grasping common household objects with a robotic hand. Real-time grasp classification accuracy using visual and motion features obtained 100%, 82.5%, and 88.9% across all participants for detecting and executing grasping actions for a bottle, lid, and box, respectively. The proposed multimodal sensor suite is a novel approach for predicting different grasp strategies and automating task performance using a commercial upper-limb prosthetic device. The system also shows potential to improve the usability of modern neurorobotic systems due to the intuitive control design.},
DOI = {10.3390/s20216097}
}



@Article{w12113010,
AUTHOR = {Wang, Ruimeng and Xia, Haoming and Qin, Yaochen and Niu, Wenhui and Pan, Li and Li, Rumeng and Zhao, Xiaoyang and Bian, Xiqing and Fu, Pinde},
TITLE = {Dynamic Monitoring of Surface Water Area during 1989‚Äì2019 in the Hetao Plain Using Landsat Data in Google Earth Engine},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3010},
URL = {https://www.mdpi.com/2073-4441/12/11/3010},
ISSN = {2073-4441},
ABSTRACT = {The spatio-temporal change of the surface water is very important to agricultural, economic, and social development in the Hetao Plain, as well as the structure and function of the ecosystem. To understand the long-term changes of the surface water area in the Hetao Plain, we used all available Landsat images (7534 scenes) and adopted the modified Normalized Difference Water Index (mNDWI), Enhanced Vegetation Index (EVI), and Normalized Difference Vegetation Index (NDVI) to map the open-surface water from 1989 to 2019 in the Google Earth Engine (GEE) cloud platform. We further analyzed precipitation, temperature, and irrigated area, revealing the impact of climate change and human activities on long-term surface water changes. The results show the following. (1) In the last 31 years, the maximum, seasonal, and annual average water body area values in the Hetao Plain have exhibited a downward trend. Meanwhile, the number of maximum, seasonal, and permanent water bodies displayed a significant upward trend. (2) The variation of the surface water area in the Hetao Plain is mainly affected by the maximum water body area, while the variation of the water body number is mainly affected by the number of minimum water bodies. (3) Precipitation has statistically significant positive effects on the water body area and water body number, which has statistically significant negative effects with temperature and irrigation. The findings of this study can be used to help the policy-makers and farmers understand changing water resources and its driving mechanism and provide a reference for water resources management, agricultural irrigation, and ecological protection.},
DOI = {10.3390/w12113010}
}



@Article{rs12213533,
AUTHOR = {Pedro, D√°rio and Matos-Carvalho, Jo√£o P. and Azevedo, F√°bio and Sacoto-Martins, Ricardo and Bernardo, Lu√≠s and Campos, Lu√≠s and Fonseca, Jos√© M. and Mora, Andr√©},
TITLE = {FFAU‚ÄîFramework for Fully Autonomous UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3533},
URL = {https://www.mdpi.com/2072-4292/12/21/3533},
ISSN = {2072-4292},
ABSTRACT = {Unmanned Aerial Vehicles (UAVs), although hardly a new technology, have recently gained a prominent role in many industries being widely used not only among enthusiastic consumers, but also in high demanding professional situations, and will have a massive societal impact over the coming years. However, the operation of UAVs is fraught with serious safety risks, such as collisions with dynamic obstacles (birds, other UAVs, or randomly thrown objects). These collision scenarios are complex to analyze in real-time, sometimes being computationally impossible to solve with existing State of the Art (SoA) algorithms, making the use of UAVs an operational hazard and therefore significantly reducing their commercial applicability in urban environments. In this work, a conceptual framework for both stand-alone and swarm (networked) UAVs is introduced, with a focus on the architectural requirements of the collision avoidance subsystem to achieve acceptable levels of safety and reliability. The SoA principles for collision avoidance against stationary objects are reviewed and a novel approach is described, using deep learning techniques to solve the computational intensive problem of real-time collision avoidance with dynamic objects. The proposed framework includes a web-interface allowing the full control of UAVs as remote clients with a supervisor cloud-based platform. The feasibility of the proposed approach was demonstrated through experimental tests using a UAV, developed from scratch using the proposed framework. Test flight results are presented for an autonomous UAV monitored from multiple countries across the world.},
DOI = {10.3390/rs12213533}
}



@Article{app10217622,
AUTHOR = {Vujasinoviƒá, St√©phane and Becker, Stefan and Breuer, Timo and Bullinger, Sebastian and Scherer-Negenborn, Norbert and Arens, Michael},
TITLE = {Integration of the 3D Environment for UAV Onboard Visual Object Tracking},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7622},
URL = {https://www.mdpi.com/2076-3417/10/21/7622},
ISSN = {2076-3417},
ABSTRACT = {Single visual object tracking from an unmanned aerial vehicle (UAV) poses fundamental challenges such as object occlusion, small-scale objects, background clutter, and abrupt camera motion. To tackle these difficulties, we propose to integrate the 3D structure of the observed scene into a detection-by-tracking algorithm. We introduce a pipeline that combines a model-free visual object tracker, a sparse 3D reconstruction, and a state estimator. The 3D reconstruction of the scene is computed with an image-based Structure-from-Motion (SfM) component that enables us to leverage a state estimator in the corresponding 3D scene during tracking. By representing the position of the target in 3D space rather than in image space, we stabilize the tracking during ego-motion and improve the handling of occlusions, background clutter, and small-scale objects. We evaluated our approach on prototypical image sequences, captured from a UAV with low-altitude oblique views. For this purpose, we adapted an existing dataset for visual object tracking and reconstructed the observed scene in 3D. The experimental results demonstrate that the proposed approach outperforms methods using plain visual cues as well as approaches leveraging image-space-based state estimations. We believe that our approach can be beneficial for trafficmonitoring, video surveillance, and navigation.},
DOI = {10.3390/app10217622}
}



@Article{s20216187,
AUTHOR = {F. Pinto, Milena and G. Melo, Aurelio and M. Hon√≥rio, Leonardo and L. M. Marcato, Andr√© and G. S. Concei√ß√£o, Andr√© and O. Timotheo, Amanda},
TITLE = {Deep Learning Applied to Vegetation Identification and Removal Using Multidimensional Aerial Data},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6187},
URL = {https://www.mdpi.com/1424-8220/20/21/6187},
ISSN = {1424-8220},
ABSTRACT = {When performing structural inspection, the generation of three-dimensional (3D) point clouds is a common resource. Those are usually generated from photogrammetry or through laser scan techniques. However, a significant drawback for complete inspection is the presence of covering vegetation, hiding possible structural problems, and making difficult the acquisition of proper object surfaces in order to provide a reliable diagnostic. Therefore, this research&rsquo;s main contribution is developing an effective vegetation removal methodology through the use of a deep learning structure that is capable of identifying and extracting covering vegetation in 3D point clouds. The proposed approach uses pre and post-processing filtering stages that take advantage of colored point clouds, if they are available, or operate independently. The results showed high classification accuracy and good effectiveness when compared with similar methods in the literature. After this step, if color is available, then a color filter is applied, enhancing the results obtained. Besides, the results are analyzed in light of real Structure From Motion (SFM) reconstruction data, which further validates the proposed method. This research also presented a colored point cloud library of bushes built for the work used by other studies in the field.},
DOI = {10.3390/s20216187}
}



@Article{rs12213552,
AUTHOR = {Yoo, Cheolhee and Lee, Yeonsu and Cho, Dongjin and Im, Jungho and Han, Daehyeon},
TITLE = {Improving Local Climate Zone Classification Using Incomplete Building Data and Sentinel 2 Images Based on Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3552},
URL = {https://www.mdpi.com/2072-4292/12/21/3552},
ISSN = {2072-4292},
ABSTRACT = {Recent studies have enhanced the mapping performance of the local climate zone (LCZ), a standard framework for evaluating urban form and function for urban heat island research, through remote sensing (RS) images and deep learning classifiers such as convolutional neural networks (CNNs). The accuracy in the urban-type LCZ (LCZ1-10), however, remains relatively low because RS data cannot provide vertical or horizontal building components in detail. Geographic information system (GIS)-based building datasets can be used as primary sources in LCZ classification, but there is a limit to using them as input data for CNN due to their incompleteness. This study proposes novel methods to classify LCZ using Sentinel 2 images and incomplete building data based on a CNN classifier. We designed three schemes (S1, S2, and a scheme fusion; SF) for mapping 50 m LCZs in two megacities: Berlin and Seoul. S1 used only RS images, and S2 used RS and building components such as area and height (or the number of stories). SF combined two schemes (S1 and S2) based on three conditions, mainly focusing on the confidence level of the CNN classifier. When compared to S1, the overall accuracies for all LCZ classes (OA) and the urban-type LCZ (OAurb) of SF increased by about 4% and 7&ndash;9%, respectively, for the two study areas. This study shows that SF can compensate for the imperfections in the building data, which causes misclassifications in S2. The suggested approach can be excellent guidance to produce a high accuracy LCZ map for cities where building databases can be obtained, even if they are incomplete.},
DOI = {10.3390/rs12213552}
}



@Article{s20216219,
AUTHOR = {Vega D√≠az, Jhon Jairo and Vlaminck, Michiel and Lefkaditis, Dionysios and Orjuela Vargas, Sergio Alejandro and Luong, Hiep},
TITLE = {Solar Panel Detection within Complex Backgrounds Using Thermal Images Acquired by UAVs},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6219},
URL = {https://www.mdpi.com/1424-8220/20/21/6219},
ISSN = {1424-8220},
ABSTRACT = {The installation of solar plants everywhere in the world increases year by year. Automated diagnostic methods are needed to inspect the solar plants and to identify anomalies within these photovoltaic panels. The inspection is usually carried out by unmanned aerial vehicles (UAVs) using thermal imaging sensors. The first step in the whole process is to detect the solar panels in those images. However, standard image processing techniques fail in case of low-contrast images or images with complex backgrounds. Moreover, the shades of power lines or structures similar to solar panels impede the automated detection process. In this research, two self-developed methods are compared for the detection of panels in this context, one based on classical techniques and another one based on deep learning, both with a common post-processing step. The first method is based on edge detection and classification, in contrast to the second method is based on training a region based convolutional neural networks to identify a panel. The first method corrects for the low contrast of the thermal image using several preprocessing techniques. Subsequently, edge detection, segmentation and segment classification are applied. The latter is done using a support vector machine trained with an optimized texture descriptor vector. The second method is based on deep learning trained with images that have been subjected to three different pre-processing operations. The postprocessing use the detected panels to infer the location of panels that were not detected. This step selects contours from detected panels based on the panel area and the angle of rotation. Then new panels are determined by the extrapolation of these contours. The panels in 100 random images taken from eleven UAV flights over three solar plants are labeled and used to evaluate the detection methods. The metrics for the new method based on classical techniques reaches a precision of 0.997, a recall of 0.970 and a F1 score of 0.983. The metrics for the method of deep learning reaches a precision of 0.996, a recall of 0.981 and a F1 score of 0.989. The two panel detection methods are highly effective in the presence of complex backgrounds.},
DOI = {10.3390/s20216219}
}



@Article{drones4040069,
AUTHOR = {Garzon-Lopez, Carol X. and Lasso, Eloisa},
TITLE = {Species Classification in a Tropical Alpine Ecosystem Using UAV-Borne RGB and Hyperspectral Imagery},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/2504-446X/4/4/69},
ISSN = {2504-446X},
ABSTRACT = {P&aacute;ramos host more than 3500 vascular plant species and are crucial water providers for millions of people in the northern Andes. Monitoring species distribution at large scales is an urgent conservation priority in the face of ongoing climatic changes and increasing anthropogenic pressure on this ecosystem. For the first time in this ecosystem, we explored the potential of unoccupied aerial vehicles (UAV)-borne red, green, and blue wavelengths (RGB) and hyperspectral imagery for p&aacute;ramo species classification by collecting both types of images in a 10-ha area, and ground vegetation cover data from 10 plots within this area. Five plots were used for calibration and the other five for validation. With the hyperspectral data, we tested our capacity to detect five representative p&aacute;ramo species with different growth forms using support vector machine (SVM) and random forest (RF) classifiers in combination with three feature selection methods and two class groups. Using RGB images, we could classify 21 species with an accuracy greater than 97%. From hyperspectral imaging, the highest accuracy (89%) was found using models built with RF or SVM classifiers combined with a binary grouping method and the sequential floating forward selection feature. Our results demonstrate that p&aacute;ramo species can be accurately mapped using both RGB and hyperspectral imagery.},
DOI = {10.3390/drones4040069}
}



@Article{rs12213587,
AUTHOR = {Masjedi, Ali and Crawford, Melba M. and Carpenter, Neal R. and Tuinstra, Mitchell R.},
TITLE = {Multi-Temporal Predictive Modelling of Sorghum Biomass Using UAV-Based Hyperspectral and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3587},
URL = {https://www.mdpi.com/2072-4292/12/21/3587},
ISSN = {2072-4292},
ABSTRACT = {High-throughput phenotyping using high spatial, spectral, and temporal resolution remote sensing (RS) data has become a critical part of the plant breeding chain focused on reducing the time and cost of the selection process for the &ldquo;best&rdquo; genotypes with respect to the trait(s) of interest. In this paper, the potential of accurate and reliable sorghum biomass prediction using visible and near infrared (VNIR) and short-wave infrared (SWIR) hyperspectral data as well as light detection and ranging (LiDAR) data acquired by sensors mounted on UAV platforms is investigated. Predictive models are developed using classical regression-based machine learning methods for nine experiments conducted during the 2017 and 2018 growing seasons at the Agronomy Center for Research and Education (ACRE) at Purdue University, Indiana, USA. The impact of the regression method, data source, timing of RS and field-based biomass reference data acquisition, and the number of samples on the prediction results are investigated. R2 values for end-of-season biomass ranged from 0.64 to 0.89 for different experiments when features from all the data sources were included. Geometry-based features derived from the LiDAR point cloud to characterize plant structure and chemistry-based features extracted from hyperspectral data provided the most accurate predictions. Evaluation of the impact of the time of data acquisition during the growing season on the prediction results indicated that although the most accurate and reliable predictions of final biomass were achieved using remotely sensed data from mid-season to end-of-season, predictions in mid-season provided adequate results to differentiate between promising varieties for selection. The analysis of variance (ANOVA) of the accuracies of the predictive models showed that both the data source and regression method are important factors for a reliable prediction; however, the data source was more important with 69% significance, versus 28% significance for the regression method.},
DOI = {10.3390/rs12213587}
}



@Article{s20216247,
AUTHOR = {Calvario, Gabriela and Alarc√≥n, Teresa E. and Dalmau, Oscar and Sierra, Basilio and Hernandez, Carmen},
TITLE = {An Agave Counting Methodology Based on Mathematical Morphology and Images Acquired through Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6247},
URL = {https://www.mdpi.com/1424-8220/20/21/6247},
ISSN = {1424-8220},
ABSTRACT = {Blue agave is an important commercial crop in Mexico, and it is the main source of the traditional mexican beverage known as tequila. The variety of blue agave crop known as Tequilana Weber is a crucial element for tequila agribusiness and the agricultural economy in Mexico. The number of agave plants in the field is one of the main parameters for estimating production of tequila. In this manuscript, we describe a mathematical morphology-based algorithm that addresses the agave automatic counting task. The proposed methodology was applied to a set of real images collected using an Unmanned Aerial Vehicle equipped with a digital Red-Green-Blue (RGB) camera. The number of plants automatically identified in the collected images was compared to the number of plants counted by hand. Accuracy of the proposed algorithm depended on the size heterogeneity of plants in the field and illumination. Accuracy ranged from 0.8309 to 0.9806, and performance of the proposed algorithm was satisfactory.},
DOI = {10.3390/s20216247}
}



@Article{informatics7040050,
AUTHOR = {Kazllarof, Vangjel and Karlos, Stamatis and Kotsiantis, Sotiris},
TITLE = {Investigation of Combining Logitboost(M5P) under Active Learning Classification Tasks},
JOURNAL = {Informatics},
VOLUME = {7},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2227-9709/7/4/50},
ISSN = {2227-9709},
ABSTRACT = {Active learning is the category of partially supervised algorithms that is differentiated by its strategy to combine both the predictive ability of a base learner and the human knowledge so as to exploit adequately the existence of unlabeled data. Its ambition is to compose powerful learning algorithms which otherwise would be based only on insufficient labelled samples. Since the latter kind of information could raise important monetization costs and time obstacles, the human contribution should be seriously restricted compared with the former. For this reason, we investigate the use of the Logitboost wrapper classifier, a popular variant of ensemble algorithms which adopts the technique of boosting along with a regression base learner based on Model trees into 3 different active learning query strategies. We study its efficiency against 10 separate learners under a well-described active learning framework over 91 datasets which have been split to binary and multi-class problems. We also included one typical Logitboost variant with a separate internal regressor for discriminating the benefits of adopting a more accurate regression tree than one-node trees, while we examined the efficacy of one hyperparameter of the proposed algorithm. Since the application of the boosting technique may provide overall less biased predictions, we assume that the proposed algorithm, named as Logitboost(M5P), could provide both accurate and robust decisions under active learning scenarios that would be beneficial on real-life weakly supervised classification tasks. Its smoother weighting stage over the misclassified cases during training as well as the accurate behavior of M5P are the main factors that lead towards this performance. Proper statistical comparisons over the metric of classification accuracy verify our assumptions, while adoption of M5P instead of weak decision trees was proven to be more competitive for the majority of the examined problems. We present our results through appropriate summarization approaches and explanatory visualizations, commenting our results per case.},
DOI = {10.3390/informatics7040050}
}



@Article{iot1020020,
AUTHOR = {Kontogiannis, Sotirios and Asiminidis, Christodoulos},
TITLE = {A Proposed Low-Cost Viticulture Stress Framework for Table Grape Varieties},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {337--359},
URL = {https://www.mdpi.com/2624-831X/1/2/20},
ISSN = {2624-831X},
ABSTRACT = {Climate change significantly affects viticulture by reducing the production yield and the quality characteristics of its final products. In some observed cases, the consequences of climate outages such as droughts, hail and floods are absolutely devastating for the farmers and the sustained local economies. Hence, it is essential to develop new in implementation monitoring solutions that offer remote real-time surveillance, alert triggering, minimum maintenance and automated generation of incident alerts with precision responses. This paper presents a new framework and a system for vine stress monitoring called Vity-stress. The Vity-stress framework combines field measurements with precise viticulture suggestions and stress avoidance planning. The key points of the proposed framework&rsquo;s system are that it is easy to develop, easy to maintain and cheap to implement applicability. Focusing on the Mediterranean cultivated table grape varieties that are strongly affected by climate change, we propose a new stress conditions monitoring system to support our framework. The proposition includes distributed field located sensors and a novel camera module implementing deep neural network algorithms to detect stress indicators. Additionally, a new wireless sensor network supported by the iBeacon protocol has been developed. The results of the sensory measurements&rsquo; data logging and imposed image detection process&rsquo;s evaluation shows that the proposed system can successfully detect different stress levels in vineyards, which in turn can allow producers to identify specific areas for irrigation, thereby saving water, energy and time.},
DOI = {10.3390/iot1020020}
}



@Article{rs12213617,
AUTHOR = {Trevisan, Rodrigo and P√©rez, Osvaldo and Schmitz, Nathan and Diers, Brian and Martin, Nicolas},
TITLE = {High-Throughput Phenotyping of Soybean Maturity Using Time Series UAV Imagery and Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3617},
URL = {https://www.mdpi.com/2072-4292/12/21/3617},
ISSN = {2072-4292},
ABSTRACT = {Soybean maturity is a trait of critical importance for the development of new soybean cultivars, nevertheless, its characterization based on visual ratings has many challenges. Unmanned aerial vehicles (UAVs) imagery-based high-throughput phenotyping methodologies have been proposed as an alternative to the traditional visual ratings of pod senescence. However, the lack of scalable and accurate methods to extract the desired information from the images remains a significant bottleneck in breeding programs. The objective of this study was to develop an image-based high-throughput phenotyping system for evaluating soybean maturity in breeding programs. Images were acquired twice a week, starting when the earlier lines began maturation until the latest ones were mature. Two complementary convolutional neural networks (CNN) were developed to predict the maturity date. The first using a single date and the second using the five best image dates identified by the first model. The proposed CNN architecture was validated using more than 15,000 ground truth observations from five trials, including data from three growing seasons and two countries. The trained model showed good generalization capability with a root mean squared error lower than two days in four out of five trials. Four methods of estimating prediction uncertainty showed potential at identifying different sources of errors in the maturity date predictions. The architecture developed solves limitations of previous research and can be used at scale in commercial breeding programs.},
DOI = {10.3390/rs12213617}
}



@Article{rs12213621,
AUTHOR = {Bi, Luning and Hu, Guiping and Raza, Muhammad Mohsin and Kandel, Yuba and Leandro, Leonor and Mueller, Daren},
TITLE = {A Gated Recurrent Units (GRU)-Based Model for Early Detection of Soybean Sudden Death Syndrome through Time-Series Satellite Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3621},
URL = {https://www.mdpi.com/2072-4292/12/21/3621},
ISSN = {2072-4292},
ABSTRACT = {In general, early detection and timely management of plant diseases are essential for reducing yield loss. Traditional manual inspection of fields is often time-consuming and laborious. Automated imaging techniques have recently been successfully applied to detect plant diseases. However, these methods mostly focus on the current state of the crop. This paper proposes a gated recurrent unit (GRU)-based model to predict soybean sudden death syndrome (SDS) disease development. To detect SDS at a quadrat level, the proposed method uses satellite images collected from PlanetScope as the training set. The pixel image data include the spectral bands of red, green, blue and near-infrared (NIR). Data collected during the 2016 and 2017 soybean-growing seasons were analyzed. Instead of using individual static imagery, the GRU-based model converts the original imagery into time-series data. SDS predictions were made on different data scenarios and the results were compared with fully connected deep neural network (FCDNN) and XGBoost methods. The overall test accuracy of classifying healthy and diseased quadrates in all methods was above 76%. The test accuracy of the FCDNN and XGBoost were 76.3&ndash;85.5% and 80.6&ndash;89.2%, respectively, while the test accuracy of the GRU-based model was 82.5&ndash;90.4%. The calculation results show that the proposed method can improve the detection accuracy by up to 7% with time-series imagery. Thus, the proposed method has the potential to predict SDS at a future time.},
DOI = {10.3390/rs12213621}
}



@Article{s20216299,
AUTHOR = {Bhowmick, Sutanu and Nagarajaiah, Satish and Veeraraghavan, Ashok},
TITLE = {Vision and Deep Learning-Based Algorithms to Detect and Quantify Cracks on Concrete Surfaces from UAV Videos},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {6299},
URL = {https://www.mdpi.com/1424-8220/20/21/6299},
ISSN = {1424-8220},
ABSTRACT = {Immediate assessment of structural integrity of important civil infrastructures, like bridges, hospitals, or dams, is of utmost importance after natural disasters. Currently, inspection is performed manually by engineers who look for local damages and their extent on significant locations of the structure to understand its implication on its global stability. However, the whole process is time-consuming and prone to human errors. Due to their size and extent, some regions of civil structures are hard to gain access for manual inspection. In such situations, a vision-based system of Unmanned Aerial Vehicles (UAVs) programmed with Artificial Intelligence algorithms may be an effective alternative to carry out a health assessment of civil infrastructures in a timely manner. This paper proposes a framework of achieving the above-mentioned goal using computer vision and deep learning algorithms for detection of cracks on the concrete surface from its image by carrying out image segmentation of pixels, i.e., classification of pixels in an image of the concrete surface and whether it belongs to cracks or not. The image segmentation or dense pixel level classification is carried out using a deep neural network architecture named U-Net. Further, morphological operations on the segmented images result in dense measurements of crack geometry, like length, width, area, and crack orientation for individual cracks present in the image. The efficacy and robustness of the proposed method as a viable real-life application was validated by carrying out a laboratory experiment of a four-point bending test on an 8-foot-long concrete beam of which the video is recorded using a camera mounted on a UAV-based, as well as a still ground-based, video camera. Detection, quantification, and localization of damage on a civil infrastructure using the proposed framework can directly be used in the prognosis of the structure&rsquo;s ability to withstand service loads.},
DOI = {10.3390/s20216299}
}



@Article{app10217930,
AUTHOR = {Vintimilla-Tapia, Pa√∫l and Bravo-Torres, Jack and L√≥pez-Nores, Mart√≠n and Gallegos-Segovia, Pablo and Ord√≥√±ez-Morales, Esteban and Ramos-Cabrer, Manuel},
TITLE = {VaNetChain: A Framework for Trustworthy Exchanges of Information in VANETs Based on Blockchain and a Virtualization Layer},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {7930},
URL = {https://www.mdpi.com/2076-3417/10/21/7930},
ISSN = {2076-3417},
ABSTRACT = {Vehicular ad hoc networks (VANETs) face challenges related to the reliability of the data exchanged and the unstability of the communication links. These shortcomings have hampered the development of the long-awaited applications that would turn roads into a smart environment. We present a framework to deploy such services, in which a virtualization layer ensures means to efficiently deliver messages between vehicles and roadside units (RSUs) and, on top of that, blockchain technology is used to enable features of data integrity, traceability, and reliability that cannot be furnished by existing consensus and reputation mechanisms. A simulation experiment is included to determine the optimal number of RSUs to be installed as supporting infrastructure in a city.},
DOI = {10.3390/app10217930}
}



@Article{rs12213666,
AUTHOR = {Lei, Tsu Chiang and Wan, Shiuan and Wu, Shih-Chieh and Wang, Hsin-Ping},
TITLE = {A New Approach of Ensemble Learning Technique to Resolve the Uncertainties of Paddy Area through Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3666},
URL = {https://www.mdpi.com/2072-4292/12/21/3666},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing technology has rendered lots of information in agriculture. It has usually been used to monitor paddy growing ecosystems in the past few decades. However, there are uncertainties in data fusion techniques which can be resolved in image classification on paddy rice. In this study, a series of learning concepts integrated by a probability progress Fuzzy Dempster-Shafer (FDS) analysis is presented to upgrade various models and different types of image data which is the goal of this study. More specifically, the study utilized the FDS to generate a series of probability models in the classification of the system. In addition, Logistic Regression (LR), Support Vector Machine (SVM), and Neural Network (NN) approaches are employed into the developed FDS system. Furthermore, two different image types are Satellite Image and Aerial Photo used as the analysis material. The overall classification accuracy has been improved to 97.27%, and the kappa value is 0.93. The overall accuracy of the paddy field image classification for a multi-period of mid-scale satellite images is between 85% and 90%. The overall accuracy of the classification using multi-spectral numerical aerial photos can be between 91% and 95%. The FDS improves the accuracy of the above image classification results.},
DOI = {10.3390/rs12213666}
}



@Article{rs12223698,
AUTHOR = {Pastucha, El≈ºbieta and Puniach, Edyta and ≈öcis≈Çowicz, Agnieszka and ƒÜwiƒÖka≈Ça, Pawe≈Ç and Niewiem, Witold and WiƒÖcek, Pawe≈Ç},
TITLE = {3D Reconstruction of Power Lines Using UAV Images to Monitor Corridor Clearance},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3698},
URL = {https://www.mdpi.com/2072-4292/12/22/3698},
ISSN = {2072-4292},
ABSTRACT = {Regular power line inspections are essential to ensure the reliability of electricity supply. The inspections of overground power submission lines include corridor clearance monitoring and fault identification. The power lines corridor is a three-dimensional space around power cables defined by a set distance. Any obstacles breaching this space should be detected, as they potentially threaten the safety of the infrastructure. Corridor clearance monitoring is usually performed either by a labor-intensive total station survey (TS), terrestrial laser scanning (TLS), or expensive airborne laser scanning (ALS) from a plane or a helicopter. This paper proposes a method that uses unmanned aerial vehicle (UAV) images to monitor corridor clearance. To maintain the adequate accuracy of the relative position of wires in regard to surrounding obstacles, the same data were used both to reconstruct a point cloud representation of a digital surface model (DSM) and a 3D power line. The proposed algorithm detects power lines in a series of images using decorrelation stretch for initial image processing, the modified Prewitt filter for edge enhancement, random sample consensus (RANSAC) with additional parameters for line fitting, and epipolar geometry for 3D reconstruction. DSM points intruding into the corridor are then detected by calculating the spatial distance between a reconstructed power line and the DSM point cloud representation. Problematic objects are localized by segmenting points into voxels and then subsequent clusterization. The processing results were compared to the results of two verification methods&mdash;TS and TLS. The comparison results show that the proposed method can be used to survey power lines with an accuracy consistent with that of classical measurements.},
DOI = {10.3390/rs12223698}
}



@Article{s20226439,
AUTHOR = {Xu, Wei and Bao, Xiangyu and Chen, Genglin and Neumann, Ingo},
TITLE = {Intelligent Calibration of Static FEA Computations Based on Terrestrial Laser Scanning Reference},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6439},
URL = {https://www.mdpi.com/1424-8220/20/22/6439},
ISSN = {1424-8220},
ABSTRACT = {The demand for efficient and accurate finite element analysis (FEA) is becoming more prevalent with the increase in advanced calibration technologies and sensor-based monitoring methods. The current research explores a deep learning-based methodology to calibrate FEA results. The utilization of monitoring reference results from measurements, e.g., terrestrial laser scanning, can help to capture the actual features in the static loading process. We learn the deviation sequence results between the standard FEA computations with the simplified geometry and refined reference values by the long short-term memory method. The complex changing principles in different deviations are trained and captured effectively in the training process of deep learning. Hence, we generate the FEA sequence results corresponding to next adjacent loading steps. The final FEA computations are calibrated by the threshold control. The calibration reduces the mean square errors of the FEA future sequence results significantly. This strengthens the calibration depth. Consequently, the calibration of FEA computations with deep learning can play a helpful role in the prediction and monitoring problems regarding the future structural behaviors.},
DOI = {10.3390/s20226439}
}



@Article{s20226442,
AUTHOR = {Barmpoutis, Panagiotis and Papaioannou, Periklis and Dimitropoulos, Kosmas and Grammalidis, Nikos},
TITLE = {A Review on Early Forest Fire Detection Systems Using Optical Remote Sensing},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6442},
URL = {https://www.mdpi.com/1424-8220/20/22/6442},
ISSN = {1424-8220},
ABSTRACT = {The environmental challenges the world faces nowadays have never been greater or more complex. Global areas covered by forests and urban woodlands are threatened by natural disasters that have increased dramatically during the last decades, in terms of both frequency and magnitude. Large-scale forest fires are one of the most harmful natural hazards affecting climate change and life around the world. Thus, to minimize their impacts on people and nature, the adoption of well-planned and closely coordinated effective prevention, early warning, and response approaches are necessary. This paper presents an overview of the optical remote sensing technologies used in early fire warning systems and provides an extensive survey on both flame and smoke detection algorithms employed by each technology. Three types of systems are identified, namely terrestrial, airborne, and spaceborne-based systems, while various models aiming to detect fire occurrences with high accuracy in challenging environments are studied. Finally, the strengths and weaknesses of fire detection systems based on optical remote sensing are discussed aiming to contribute to future research projects for the development of early warning fire systems.},
DOI = {10.3390/s20226442}
}



@Article{f11111190,
AUTHOR = {Lebedev, Vadim G. and Lebedeva, Tatyana N. and Chernodubov, Aleksey I. and Shestibratov, Konstantin A.},
TITLE = {Genomic Selection for Forest Tree Improvement: Methods, Achievements and Perspectives},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1190},
URL = {https://www.mdpi.com/1999-4907/11/11/1190},
ISSN = {1999-4907},
ABSTRACT = {The breeding of forest trees is only a few decades old, and is a much more complicated, longer, and expensive endeavor than the breeding of agricultural crops. One breeding cycle for forest trees can take 20&ndash;30 years. Recent advances in genomics and molecular biology have revolutionized traditional plant breeding based on visual phenotype assessment: the development of different types of molecular markers has made genotype selection possible. Marker-assisted breeding can significantly accelerate the breeding process, but this method has not been shown to be effective for selection of complex traits on forest trees. This new method of genomic selection is based on the analysis of all effects of quantitative trait loci (QTLs) using a large number of molecular markers distributed throughout the genome, which makes it possible to assess the genomic estimated breeding value (GEBV) of an individual. This approach is expected to be much more efficient for forest tree improvement than traditional breeding. Here, we review the current state of the art in the application of genomic selection in forest tree breeding and discuss different methods of genotyping and phenotyping. We also compare the accuracies of genomic prediction models and highlight the importance of a prior cost-benefit analysis before implementing genomic selection. Perspectives for the further development of this approach in forest breeding are also discussed: expanding the range of species and the list of valuable traits, the application of high-throughput phenotyping methods, and the possibility of using epigenetic variance to improve of forest trees.},
DOI = {10.3390/f11111190}
}



@Article{app10228005,
AUTHOR = {Giebas, Damian and Wojszczyk, Rafa≈Ç},
TITLE = {Atomicity Violation in Multithreaded Applications and Its Detection in Static Code Analysis Process},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8005},
URL = {https://www.mdpi.com/2076-3417/10/22/8005},
ISSN = {2076-3417},
ABSTRACT = {This paper is a contribution to the field of research dealing with the parallel computing, which is used in multithreaded applications. The paper discusses the characteristics of atomicity violation in multithreaded applications and develops a new definition of atomicity violation based on previously defined relationships between operations, that can be used to atomicity violation detection. A method of detection of conflicts causing atomicity violation was also developed using the source code model of multithreaded applications that predicts errors in the software.},
DOI = {10.3390/app10228005}
}



@Article{app10228008,
AUTHOR = {Kim, Byunghyun and Cho, Soojin},
TITLE = {Automated Multiple Concrete Damage Detection Using Instance Segmentation Deep Learning Model},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8008},
URL = {https://www.mdpi.com/2076-3417/10/22/8008},
ISSN = {2076-3417},
ABSTRACT = {In many developed countries with a long history of urbanization, there is an increasing need for automated computer vision (CV)-based inspection to replace conventional labor-intensive visual inspection. This paper proposes a technique for the automated detection of multiple concrete damage based on a state-of-the-art deep learning framework, Mask R-CNN, developed for instance segmentation. The structure of Mask R-CNN, which consists of three stages (region proposal, classification, and segmentation) is optimized for multiple concrete damage detection. The optimized Mask R-CNN is trained with 765 concrete images including cracks, efflorescence, rebar exposure, and spalling. The performance of the trained Mask R-CNN is evaluated with 25 actual test images containing damage as well as environmental objects. Two types of metrics are proposed to measure localization and segmentation performance. On average, 90.41% precision and 90.81% recall are achieved for localization and 87.24% precision and 87.58% recall for segmentation, which indicates the excellent field applicability of the trained Mask R-CNN. This paper also qualitatively discusses the test results by explaining that the architecture of Mask R-CNN that is optimized for general object detection purposes, can be modified to detect long and slender shapes of cracks, rebar exposure, and efflorescence in further research.},
DOI = {10.3390/app10228008}
}



@Article{rs12223708,
AUTHOR = {Feng, Ziyi and Huang, Guanhua and Chi, Daocai},
TITLE = {Classification of the Complex Agricultural Planting Structure with a Semi-Supervised Extreme Learning Machine Framework},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3708},
URL = {https://www.mdpi.com/2072-4292/12/22/3708},
ISSN = {2072-4292},
ABSTRACT = {Many approaches have been developed to analyze remote sensing images. However, for the classification of large-scale problems, most algorithms showed low computational efficiency and low accuracy. In this paper, the newly developed semi-supervised extreme learning machine (SS-ELM) framework with k-means clustering algorithm for image segmentation and co-training algorithm to enlarge the sample sets was used to classify the agricultural planting structure at large-scale areas. Data sets collected from a small-scale area within the Hetao Irrigation District (HID) at the upper reaches of the Yellow River basin were used to evaluate the SS-ELM framework. The results of the SS-ELM algorithm were compared with those of the random forest (RF), ELM, support vector machine (SVM) and semi-supervised support vector machine (S-SVM) algorithms. Then the SS-ELM algorithm was applied to analyze the complex planting structure of HID in 1986&ndash;2010 by comparing the remote sensing estimated results with the statistical data. In the small-scale case, the SS-ELM algorithm performed better than the RF, ELM, SVM, and S-SVM algorithms. For the SS-ELM algorithm, the average overall accuracy (OA) was in a range of 83.00&ndash;92.17%. On the contrary, for the other four algorithms, their average OA values ranged from 56.97% to 92.84%. Whereas, in the classification of planting structure in HID, the SS-ELM algorithm had an excellent performance in classification accuracy and computational efficiency for three major planting crops including maize, wheat, and sunflowers. The estimated areas by using the SS-ELM algorithm based on the remote sensing images were consistent with the statistical data, and their difference was within a range of 3&ndash;25%. This implied that the SS-ELM framework could be served as an effective method for the classification of complex planting structures with relatively fast training, good generalization, universal approximation capability, and reasonable learning accuracy.},
DOI = {10.3390/rs12223708}
}



@Article{agronomy10111762,
AUTHOR = {Zhao, Biquan and Li, Jiating and Baenziger, P. Stephen and Belamkar, Vikas and Ge, Yufeng and Zhang, Jian and Shi, Yeyin},
TITLE = {Automatic Wheat Lodging Detection and Mapping in Aerial Imagery to Support High-Throughput Phenotyping and In-Season Crop Management},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1762},
URL = {https://www.mdpi.com/2073-4395/10/11/1762},
ISSN = {2073-4395},
ABSTRACT = {Latest advances in unmanned aerial vehicle (UAV) technology and convolutional neural networks (CNNs) allow us to detect crop lodging in a more precise and accurate way. However, the performance and generalization of a model capable of detecting lodging when the plants may show different spectral and morphological signatures have not been investigated much. This study investigated and compared the performance of models trained using aerial imagery collected at two growth stages of winter wheat with different canopy phenotypes. Specifically, three CNN-based models were trained with aerial imagery collected at early grain filling stage only, at physiological maturity only, and at both stages. Results show that the multi-stage model trained by images from both growth stages outperformed the models trained by images from individual growth stages on all testing data. The mean accuracy of the multi-stage model was 89.23% for both growth stages, while the mean of the other two models were 52.32% and 84.9%, respectively. This study demonstrates the importance of diversity of training data in big data analytics, and the feasibility of developing a universal decision support system for wheat lodging detection and mapping multi-growth stages with high-resolution remote sensing imagery.},
DOI = {10.3390/agronomy10111762}
}



@Article{rs12223714,
AUTHOR = {Zeng, Qingjie and Qin, Hanlin and Yan, Xiang and Yang, Tingwu},
TITLE = {Fourier Domain Anomaly Detection and Spectral Fusion for Stripe Noise Removal of TIR Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3714},
URL = {https://www.mdpi.com/2072-4292/12/22/3714},
ISSN = {2072-4292},
ABSTRACT = {Stripe noise is a common and unwelcome noise pattern in various thermal infrared (TIR) image data including conventional TIR images and remote sensing TIR spectral images. Most existing stripe noise removal (destriping) methods are often difficult to keep a good and robust efficacy in dealing with the real-life complex noise cases. In this paper, based on the intrinsic spectral properties of TIR images and stripe noise, we propose a novel two-stage transform domain destriping method called Fourier domain anomaly detection and spectral fusion (ADSF). Considering the principal frequencies polluted by stripe noise as outliers in the statistical spectrum of TIR images, our naive idea is first to detect the potential anomalies and then correct them effectively in the Fourier domain to reconstruct a desired destriping result. More specifically, anomaly detection for stripe frequencies is achieved through a regional comparison between the original spectrum and the expected spectrum that statistically follows a generalized Laplacian regression model, and then an anomaly weight map is generated accordingly. In the correction stage, we propose a guidance-image-based spectrum fusion strategy, which integrates the original spectrum and the spectrum of a guidance image via the anomaly weight map. The final reconstruction result not only has no stripe noise but also maintains image structures and details well. Extensive real experiments are performed on conventional TIR images and remote sensing spectral images, respectively. The qualitative and quantitative assessment results demonstrate the superior effectiveness and strong robustness of the proposed method.},
DOI = {10.3390/rs12223714}
}



@Article{rs12223715,
AUTHOR = {Park, Minsoo and Tran, Dai Quoc and Jung, Daekyo and Park, Seunghee},
TITLE = {Wildfire-Detection Method Using DenseNet and CycleGAN Data Augmentation-Based Remote Camera Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3715},
URL = {https://www.mdpi.com/2072-4292/12/22/3715},
ISSN = {2072-4292},
ABSTRACT = {To minimize the damage caused by wildfires, a deep learning-based wildfire-detection technology that extracts features and patterns from surveillance camera images was developed. However, many studies related to wildfire-image classification based on deep learning have highlighted the problem of data imbalance between wildfire-image data and forest-image data. This data imbalance causes model performance degradation. In this study, wildfire images were generated using a cycle-consistent generative adversarial network (CycleGAN) to eliminate data imbalances. In addition, a densely-connected-convolutional-networks-based (DenseNet-based) framework was proposed and its performance was compared with pre-trained models. While training with a train set containing an image generated by a GAN in the proposed DenseNet-based model, the best performance result value was realized among the models with an accuracy of 98.27% and an F1 score of 98.16, obtained using the test dataset. Finally, this trained model was applied to high-quality drone images of wildfires. The experimental results showed that the proposed framework demonstrated high wildfire-detection accuracy.},
DOI = {10.3390/rs12223715}
}



@Article{s20226485,
AUTHOR = {Stuparu, Delia-Georgiana and Ciobanu, Radu-Ioan and Dobre, Ciprian},
TITLE = {Vehicle Detection in Overhead Satellite Images Using a One-Stage Object Detection Model},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6485},
URL = {https://www.mdpi.com/1424-8220/20/22/6485},
ISSN = {1424-8220},
ABSTRACT = {In order to improve the traffic in large cities and to avoid congestion, advanced methods of detecting and predicting vehicle behaviour are needed. Such methods require complex information regarding the number of vehicles on the roads, their positions, directions, etc. One way to obtain this information is by analyzing overhead images collected by satellites or drones, and extracting information from them through intelligent machine learning models. Thus, in this paper we propose and present a one-stage object detection model for finding vehicles in satellite images using the RetinaNet architecture and the Cars Overhead With Context dataset. By analyzing the results obtained by the proposed model, we show that it has a very good vehicle detection accuracy and a very low detection time, which shows that it can be employed to successfully extract data from real-time satellite or drone data.},
DOI = {10.3390/s20226485}
}



@Article{make2040030,
AUTHOR = {Combey, Th√©o and Loison, Ant√≥nio and Faucher, Maxime and Hajri, Hatem},
TITLE = {Probabilistic Jacobian-Based Saliency Maps Attacks},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {2},
YEAR = {2020},
NUMBER = {4},
PAGES = {558--578},
URL = {https://www.mdpi.com/2504-4990/2/4/30},
ISSN = {2504-4990},
ABSTRACT = {Neural network classifiers (NNCs) are known to be vulnerable to malicious adversarial perturbations of inputs including those modifying a small fraction of the input features named sparse or L0 attacks. Effective and fast L0 attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are practical to fool NNCs but also to improve their robustness. In this paper, we show that penalising saliency maps of JSMA by the output probabilities and the input features of the NNC leads to more powerful attack algorithms that better take into account each input&rsquo;s characteristics. This leads us to introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), and demonstrate through a variety of white-box and black-box experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they are both significantly faster and more efficient than the original targeted and non-targeted versions of JSMA. Experiments also demonstrate, in some cases, very competitive results of our attacks in comparison with the Carlini-Wagner (CW) L0 attack, while remaining, like JSMA, significantly faster (WJSMA and TJSMA are more than 50 times faster than CW L0 on CIFAR-10). Therefore, our new attacks provide good trade-offs between JSMA and CW for L0 real-time adversarial testing on datasets such as the ones previously cited.},
DOI = {10.3390/make2040030}
}



@Article{smartcities3040065,
AUTHOR = {Thakker, Dhavalkumar and Mishra, Bhupesh Kumar and Abdullatif, Amr and Mazumdar, Suvodeep and Simpson, Sydney},
TITLE = {Explainable Artificial Intelligence for Developing Smart Cities Solutions},
JOURNAL = {Smart Cities},
VOLUME = {3},
YEAR = {2020},
NUMBER = {4},
PAGES = {1353--1382},
URL = {https://www.mdpi.com/2624-6511/3/4/65},
ISSN = {2624-6511},
ABSTRACT = {Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions, Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative training datasets and features engineering and less on the available domain expertise. We argue that such an approach to solution development makes the outcome of solutions less explainable, i.e., it is often not possible to explain the results of the model. There is a growing concern among policymakers in cities with this lack of explainability of AI solutions, and this is considered a major hindrance in the wider acceptability and trust in such AI-based solutions. In this work, we survey the concept of &lsquo;explainable deep learning&rsquo; as a subset of the &lsquo;explainable AI&rsquo; problem and propose a new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring application in the context of a European Commission-funded project. Monitoring of gullies and drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc., and building a DL-based classifier to detect such objects and classify blockages based on the presence and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier, the DL component detects object presence and coverage level and semantic rules designed with close consultation with experts carry out the classification. By using the expert knowledge in the flooding context, our hybrid classifier provides the flexibility on categorising the image using objects and their coverage relationships. The experimental results demonstrated with a real-world use case showed that this hybrid approach of image classification has on average 11% improvement (F-Measure) in image classification performance compared to DL-only classifier. It also has the distinct advantage of integrating experts&rsquo; knowledge on defining the decision-making rules to represent the complex circumstances and using such knowledge to explain the results.},
DOI = {10.3390/smartcities3040065}
}



@Article{app10228105,
AUTHOR = {Kim, Jung Jin and Kim, Ah-Ram and Lee, Seong-Won},
TITLE = {Artificial Neural Network-Based Automated Crack Detection and Analysis for the Inspection of Concrete Structures},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8105},
URL = {https://www.mdpi.com/2076-3417/10/22/8105},
ISSN = {2076-3417},
ABSTRACT = {The damage investigation and inspection methods for infrastructures performed in small-scale (type III) facilities usually involve a visual examination by an inspector using surveying tools (e.g., cracking, crack microscope, etc.) in the field. These methods can interfere with the subjectivity of the inspector, which may reduce the objectivity and reliability of the record. Therefore, a new image analysis technique is needed to automatically detect cracks and analyze the characteristics of the cracks objectively. In this study, an image analysis technique using deep learning is developed to detect cracks and analyze characteristics (e.g., length, and width) in images for small-scale facilities. Three stages of image processing pipeline are proposed to obtain crack detection and its characteristics. In the first and second stages, two-dimensional convolutional neural networks are used for crack image detection (e.g., classification and segmentation). Based on convolution neural network for the detection, hierarchical feature learning architecture is applied into our deep learning network. After deep learning-based detection, in the third stage, thinning and tracking algorithms are applied to analyze length and width of crack in the image. The performance of the proposed method was tested using various crack images with label and the results showed good performance of crack detection and its measurement.},
DOI = {10.3390/app10228105}
}



@Article{rs12223764,
AUTHOR = {Zhang, Peng and Du, Peijun and Lin, Cong and Wang, Xin and Li, Erzhu and Xue, Zhaohui and Bai, Xuyu},
TITLE = {A Hybrid Attention-Aware Fusion Network (HAFNet) for Building Extraction from High-Resolution Imagery and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3764},
URL = {https://www.mdpi.com/2072-4292/12/22/3764},
ISSN = {2072-4292},
ABSTRACT = {Automated extraction of buildings from earth observation (EO) data has long been a fundamental but challenging research topic. Combining data from different modalities (e.g., high-resolution imagery (HRI) and light detection and ranging (LiDAR) data) has shown great potential in building extraction. Recent studies have examined the role that deep learning (DL) could play in both multimodal data fusion and urban object extraction. However, DL-based multimodal fusion networks may encounter the following limitations: (1) the individual modal and cross-modal features, which we consider both useful and important for final prediction, cannot be sufficiently learned and utilized and (2) the multimodal features are fused by a simple summation or concatenation, which appears ambiguous in selecting cross-modal complementary information. In this paper, we address these two limitations by proposing a hybrid attention-aware fusion network (HAFNet) for building extraction. It consists of RGB-specific, digital surface model (DSM)-specific, and cross-modal streams to sufficiently learn and utilize both individual modal and cross-modal features. Furthermore, an attention-aware multimodal fusion block (Att-MFBlock) was introduced to overcome the fusion problem by adaptively selecting and combining complementary features from each modality. Extensive experiments conducted on two publicly available datasets demonstrated the effectiveness of the proposed HAFNet for building extraction.},
DOI = {10.3390/rs12223764}
}



@Article{s20226554,
AUTHOR = {P√©rez, Javier and Guardiola, Jose-Luis and Perez, Alberto J. and Perez-Cortes, Juan-Carlos},
TITLE = {Probabilistic Evaluation of 3D Surfaces Using Statistical Shape Models (SSM)},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6554},
URL = {https://www.mdpi.com/1424-8220/20/22/6554},
ISSN = {1424-8220},
ABSTRACT = {Inspecting a 3D object which shape has elastic manufacturing tolerances in order to find defects is a challenging and time-consuming task. This task usually involves humans, either in the specification stage followed by some automatic measurements, or in other points along the process. Even when a detailed inspection is performed, the measurements are limited to a few dimensions instead of a complete examination of the object. In this work, a probabilistic method to evaluate 3D surfaces is presented. This algorithm relies on a training stage to learn the shape of the object building a statistical shape model. Making use of this model, any inspected object can be evaluated obtaining a probability that the whole object or any of its dimensions are compatible with the model, thus allowing to easily find defective objects. Results in simulated and real environments are presented and compared to two different alternatives.},
DOI = {10.3390/s20226554}
}



@Article{rs12223776,
AUTHOR = {Tassi, Andrea and Vizzari, Marco},
TITLE = {Object-Oriented LULC Classification in Google Earth Engine Combining SNIC, GLCM, and Machine Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3776},
URL = {https://www.mdpi.com/2072-4292/12/22/3776},
ISSN = {2072-4292},
ABSTRACT = {Google Earth Engine (GEE) is a versatile cloud platform in which pixel-based (PB) and object-oriented (OO) Land Use&ndash;Land Cover (LULC) classification approaches can be implemented, thanks to the availability of the many state-of-art functions comprising various Machine Learning (ML) algorithms. OO approaches, including both object segmentation and object textural analysis, are still not common in the GEE environment, probably due to the difficulties existing in concatenating the proper functions, and in tuning the various parameters to overcome the GEE computational limits. In this context, this work is aimed at developing and testing an OO classification approach combining the Simple Non-Iterative Clustering (SNIC) algorithm to identify spatial clusters, the Gray-Level Co-occurrence Matrix (GLCM) to calculate cluster textural indices, and two ML algorithms (Random Forest (RF) or Support Vector Machine (SVM)) to perform the final classification. A Principal Components Analysis (PCA) is applied to the main seven GLCM indices to synthesize in one band the textural information used for the OO classification. The proposed approach is implemented in a user-friendly, freely available GEE code useful to perform the OO classification, tuning various parameters (e.g., choose the input bands, select the classification algorithm, test various segmentation scales) and compare it with a PB approach. The accuracy of OO and PB classifications can be assessed both visually and through two confusion matrices that can be used to calculate the relevant statistics (producer&rsquo;s, user&rsquo;s, overall accuracy (OA)). The proposed methodology was broadly tested in a 154 km2 study area, located in the Lake Trasimeno area (central Italy), using Landsat 8 (L8), Sentinel 2 (S2), and PlanetScope (PS) data. The area was selected considering its complex LULC mosaic mainly composed of artificial surfaces, annual and permanent crops, small lakes, and wooded areas. In the study area, the various tests produced interesting results on the different datasets (OA: PB RF (L8 = 72.7%, S2 = 82%, PS = 74.2), PB SVM (L8 = 79.1%, S2 = 80.2%, PS = 74.8%), OO RF (L8 = 64%, S2 = 89.3%, PS = 77.9), OO SVM (L8 = 70.4, S2 = 86.9%, PS = 73.9)). The broad code application demonstrated very good reliability of the whole process, even though the OO classification process resulted, sometimes, too demanding on higher resolution data, considering the available computational GEE resources.},
DOI = {10.3390/rs12223776}
}



@Article{s20226585,
AUTHOR = {Zhang, Zichen and Boubin, Jayson and Stewart, Christopher and Khanal, Sami},
TITLE = {Whole-Field Reinforcement Learning: A Fully Autonomous Aerial Scouting Method for Precision Agriculture},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6585},
URL = {https://www.mdpi.com/1424-8220/20/22/6585},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial systems (UAS) are increasingly used in precision agriculture to collect crop health related data. UAS can capture data more often and more cost-effectively than sending human scouts into the field. However, in large crop fields, flight time, and hence data collection, is limited by battery life. In a conventional UAS approach, human operators are required to exchange depleted batteries many times, which can be costly and time consuming. In this study, we developed a novel, fully autonomous aerial scouting approach that preserves battery life by sampling sections of a field for sensing and predicting crop health for the whole field. Our approach uses reinforcement learning (RL) and convolutional neural networks (CNN) to accurately and autonomously sample the field. To develop and test the approach, we ran flight simulations on an aerial image dataset collected from an 80-acre corn field. The excess green vegetation Index was used as a proxy for crop health condition. Compared to the conventional UAS scouting approach, the proposed scouting approach sampled 40% of the field, predicted crop health with 89.8% accuracy, reduced labor cost by 4.8&times; and increased agricultural profits by 1.36&times;.},
DOI = {10.3390/s20226585}
}



@Article{rs12223778,
AUTHOR = {Fu, Yuanyuan and Yang, Guijun and Li, Zhenhai and Song, Xiaoyu and Li, Zhenhong and Xu, Xingang and Wang, Pei and Zhao, Chunjiang},
TITLE = {Winter Wheat Nitrogen Status Estimation Using UAV-Based RGB Imagery and Gaussian Processes Regression},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3778},
URL = {https://www.mdpi.com/2072-4292/12/22/3778},
ISSN = {2072-4292},
ABSTRACT = {Predicting the crop nitrogen (N) nutrition status is critical for optimizing nitrogen fertilizer application. The present study examined the ability of multiple image features derived from unmanned aerial vehicle (UAV) RGB images for winter wheat N status estimation across multiple critical growth stages. The image features consisted of RGB-based vegetation indices (VIs), color parameters, and textures, which represented image features of different aspects and different types. To determine which N status indicators could be well-estimated, we considered two mass-based N status indicators (i.e., the leaf N concentration (LNC) and plant N concentration (PNC)) and two area-based N status indicators (i.e., the leaf N density (LND) and plant N density (PND)). Sixteen RGB-based VIs associated with crop growth were selected. Five color space models, including RGB, HSV, L*a*b*, L*c*h*, and L*u*v*, were used to quantify the winter wheat canopy color. The combination of Gaussian processes regression (GPR) and Gabor-based textures with four orientations and five scales was proposed to estimate the winter wheat N status. The gray level co-occurrence matrix (GLCM)-based textures with four orientations were extracted for comparison. The heterogeneity in the textures of different orientations was evaluated using the measures of mean and coefficient of variation (CV). The variable importance in projection (VIP) derived from partial least square regression (PLSR) and a band analysis tool based on Gaussian processes regression (GPR-BAT) were used to identify the best performing image features for the N status estimation. The results indicated that (1) the combination of RGB-based VIs or color parameters only could produce reliable estimates of PND and the GPR model based on the combination of color parameters yielded a higher accuracy for the estimation of PND (R2val = 0.571, RMSEval = 2.846 g/m2, and RPDval = 1.532), compared to that based on the combination of RGB-based VIs; (2) there was no significant heterogeneity in the textures of different orientations and the textures of 45 degrees were recommended in the winter wheat N status estimation; (3) compared with the RGB-based VIs and color parameters, the GPR model based on the Gabor-based textures produced a higher accuracy for the estimation of PND (R2val = 0.675, RMSEval = 2.493 g/m2, and RPDval = 1.748) and the PLSR model based on the GLCM-based textures produced a higher accuracy for the estimation of PNC (R2val = 0.612, RMSEval = 0.380%, and RPDval = 1.601); and (4) the combined use of RGB-based VIs, color parameters, and textures produced comparable estimation results to using textures alone. Both VIP-PLSR and GPR-BAT analyses confirmed that image textures contributed most to the estimation of winter wheat N status. The experimental results reveal the potential of image textures derived from high-definition UAV-based RGB images for the estimation of the winter wheat N status. They also suggest that a conventional low-cost digital camera mounted on a UAV could be well-suited for winter wheat N status monitoring in a fast and non-destructive way.},
DOI = {10.3390/rs12223778}
}



@Article{rs12223789,
AUTHOR = {Li, Bo and Gan, Zhigang and Chen, Daqing and Sergey Aleksandrovich, Dyachenko},
TITLE = {UAV Maneuvering Target Tracking in Uncertain Environments Based on Deep Reinforcement Learning and Meta-Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2072-4292/12/22/3789},
ISSN = {2072-4292},
ABSTRACT = {This paper combines deep reinforcement learning (DRL) with meta-learning and proposes a novel approach, named meta twin delayed deep deterministic policy gradient (Meta-TD3), to realize the control of unmanned aerial vehicle (UAV), allowing a UAV to quickly track a target in an environment where the motion of a target is uncertain. This approach can be applied to a variety of scenarios, such as wildlife protection, emergency aid, and remote sensing. We consider a multi-task experience replay buffer to provide data for the multi-task learning of the DRL algorithm, and we combine meta-learning to develop a multi-task reinforcement learning update method to ensure the generalization capability of reinforcement learning. Compared with the state-of-the-art algorithms, namely the deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3), experimental results show that the Meta-TD3 algorithm has achieved a great improvement in terms of both convergence value and convergence rate. In a UAV target tracking problem, Meta-TD3 only requires a few steps to train to enable a UAV to adapt quickly to a new target movement mode more and maintain a better tracking effectiveness.},
DOI = {10.3390/rs12223789}
}



@Article{rs12223783,
AUTHOR = {Khanal, Sami and KC, Kushal and Fulton, John P. and Shearer, Scott and Ozkan, Erdal},
TITLE = {Remote Sensing in Agriculture‚ÄîAccomplishments, Limitations, and Opportunities},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3783},
URL = {https://www.mdpi.com/2072-4292/12/22/3783},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34%), followed by the United States (20%) and China (11%). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability.},
DOI = {10.3390/rs12223783}
}



@Article{app10228189,
AUTHOR = {Lee, Sunmin and Baek, Won-Kyung and Jung, Hyung-Sup and Lee, Saro},
TITLE = {Susceptibility Mapping on Urban Landslides Using Deep Learning Approaches in Mt. Umyeon},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {8189},
URL = {https://www.mdpi.com/2076-3417/10/22/8189},
ISSN = {2076-3417},
ABSTRACT = {In recent years, the incidence of localized heavy rainfall has increased as abnormal weather events occur more frequently. In densely populated urban areas, this type of heavy rain can cause extreme landslide damage, so that it is necessary to estimate and analyze the susceptibility of future landslides. In this regard, deep learning (DL) methodologies have been used to identify areas prone to landslides recently. Therefore, in this study, DL methodologies, including a deep neural network (DNN), kernel-based DNN, and convolutional neural network (CNN) were used to identify areas where landslides could occur. As a detailed step for this purpose, landslide occurrence was first determined as landslide inventory through aerial photographs with comparative analysis using field survey data; a training set was built for model training through oversampling based on the landslide inventory. A total of 17 landslide influencing variables that influence the frequency of landslides by topography and geomorphology, as well as soil and forest variables, were selected to establish a landslide inventory. Then models were built using DNN, kernel-based DNN, and CNN models, and the susceptibility of landslides in the study area was determined. Model performance was evaluated through the average precision (AP) score and root mean square error (RMSE) for each of the three models. Finally, DNN, kernel-based DNN, and CNN models showed performances of 99.45%, 99.44%, and 99.41%, and RMSE values of 0.1694, 0.1806, and 0.1747, respectively. As a result, all three models showed similar performance, indicating excellent predictive ability of the models developed in this study. The information of landslides occurring in urban areas, which cause a great damage even with a small number of occurrences, can provide a basis for reference to the government and local authorities for urban landslide management.},
DOI = {10.3390/app10228189}
}



@Article{s20226623,
AUTHOR = {Kampczyk, Arkadiusz},
TITLE = {An Innovative Approach to Surveying the Geometry of Visibility Triangles at Railway Level Crossings},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6623},
URL = {https://www.mdpi.com/1424-8220/20/22/6623},
ISSN = {1424-8220},
ABSTRACT = {Railway level crossings (RLCs) in Poland are classified according to their protection systems. Category D, which is a form of passive RLC, aims to ensure safe and efficient operation. Surveying is essential to prepare and control the geometry of the visibility triangles used at RLCs. This article presents a new approach to monitoring the geometry of visibility triangles of RLCs using an electronic total station and a magnetic measuring square (MMS). Its main assumptions are presented together with the application of the innovative measuring instruments. Visibility is demonstrated taking into account the angles of intersection of the road axis with the track axis of the railway line and additional attributes related to the analysis and evaluation of general visibility conditions. The research highlights controversies that have received special attention against the background of the safety status of railway level crossings. As a case study, the RLC located on a single-track railway line in Poland is examined. The final section presents applications of the results obtained according to the proposed methodology. It is shown that the proposed approach is practical and effective. In addition to surveyors, the survey methodology can be used by road and rail traffic engineers and policy makers to further improve traffic safety at RLCs. This is an important global research task.},
DOI = {10.3390/s20226623}
}



@Article{rs12223797,
AUTHOR = {Radke, David and Radke, Daniel and Radke, John},
TITLE = {Beyond Measurement: Extracting Vegetation Height from High Resolution Imagery with Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3797},
URL = {https://www.mdpi.com/2072-4292/12/22/3797},
ISSN = {2072-4292},
ABSTRACT = {Measuring and monitoring the height of vegetation provides important insights into forest age and habitat quality. These are essential for the accuracy of applications that are highly reliant on up-to-date and accurate vegetation data. Current vegetation sensing practices involve ground survey, photogrammetry, synthetic aperture radar (SAR), and airborne light detection and ranging sensors (LiDAR). While these methods provide high resolution and accuracy, their hardware and collection effort prohibits highly recurrent and widespread collection. In response to the limitations of current methods, we designed Y-NET, a novel deep learning model to generate high resolution models of vegetation from highly recurrent multispectral aerial imagery and elevation data. Y-NET&rsquo;s architecture uses convolutional layers to learn correlations between different input features and vegetation height, generating an accurate vegetation surface model (VSM) at 1&times;1 m resolution. We evaluated Y-NET on 235 km2 of the East San Francisco Bay Area and find that Y-NET achieves low error from LiDAR when tested on new locations. Y-NET also achieves an R2 of 0.83 and can effectively model complex vegetation through side-by-side visual comparisons. Furthermore, we show that Y-NET is able to identify instances of vegetation growth and mitigation by comparing aerial imagery and LiDAR collected at different times.},
DOI = {10.3390/rs12223797}
}



@Article{rs12223808,
AUTHOR = {Su, Jinhua and Bai, Yanbing and Wang, Xingrui and Lu, Dong and Zhao, Bo and Yang, Hanfang and Mas, Erick and Koshimura, Shunichi},
TITLE = {Technical Solution Discussion for Key Challenges of Operational Convolutional Neural Network-Based Building-Damage Assessment from Satellite Imagery: Perspective from Benchmark xBD Dataset},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3808},
URL = {https://www.mdpi.com/2072-4292/12/22/3808},
ISSN = {2072-4292},
ABSTRACT = {Earth Observation satellite imaging helps building diagnosis during a disaster. Several models are put forward on the xBD dataset, which can be divided into two levels: the building level and the pixel level. Models from two levels evolve into several versions that will be reviewed in this paper. There are four key challenges hindering researchers from moving forward on this task, and this paper tries to give technical solutions. First, metrics on different levels could not be compared directly. We put forward a fairer metric and give a method to convert between metrics of two levels. Secondly, drone images may be another important source, but drone data may have only a post-disaster image. This paper shows and compares methods of directly detecting and generating. Thirdly, the class imbalance is a typical feature of the xBD dataset and leads to a bad F1 score for minor damage and major damage. This paper provides four specific data resampling strategies, which are Main-Label Over-Sampling (MLOS), Discrimination After Cropping (DAC), Dilation of Area with Minority (DAM) and Synthetic Minority Over-Sampling Technique (SMOTE), as well as cost-sensitive re-weighting schemes. Fourthly, faster prediction meets the need for a real-time situation. This paper recommends three specific methods, feature-map subtraction, parameter sharing, and knowledge distillation. Finally, we developed our AI-driven Damage Diagnose Platform (ADDP). This paper introduces the structure of ADDP and technical details. Customized settings, interface preview, and upload and download satellite images are major services our platform provides.},
DOI = {10.3390/rs12223808}
}



@Article{s20226680,
AUTHOR = {Sayeed, Mohd Abuzar and Kumar, Rajesh and Sharma, Vishal and Sayeed, Mohd Asim},
TITLE = {Efficient Deployment with Throughput Maximization for UAVs Communication Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {6680},
URL = {https://www.mdpi.com/1424-8220/20/22/6680},
ISSN = {1424-8220},
ABSTRACT = {The article presents a throughput maximization approach for UAV assisted ground networks. Throughput maximization involves minimizing delay and packet loss through UAV trajectory optimization, reinforcing the congested nodes and transmission channels. The aggressive reinforcement policy is achieved by characterizing nodes, links, and overall topology through delay, loss, throughput, and distance. A position-aware graph neural network (GNN) is used for characterization, prediction, and dynamic UAV trajectory enhancement. To establish correctness, the proposed approach is validated against optimized link state routing (OLSR) driven UAV assisted ground networks. The proposed approach considerably outperforms the classical approach by demonstrating significant gains in throughput and packet delivery ratio with notable decrements in delay and packet loss. The performance analysis of the proposed approach against software-defined UAVs (U-S) and UAVs as base stations (U-B) verifies the consistency and gains in average throughput while minimizing delay and packet loss. The scalability test of the proposed approach is performed by varying data rates and the number of UAVs.},
DOI = {10.3390/s20226680}
}



@Article{rs12223834,
AUTHOR = {Xia, Junshi and Yokoya, Naoto and Pham, Tien Dat},
TITLE = {Probabilistic Mangrove Species Mapping with Multiple-Source Remote-Sensing Datasets Using Label Distribution Learning in Xuan Thuy National Park, Vietnam},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3834},
URL = {https://www.mdpi.com/2072-4292/12/22/3834},
ISSN = {2072-4292},
ABSTRACT = {Mangrove forests play an important role in maintaining water quality, mitigating climate change impacts, and providing a wide range of ecosystem services. Effective identification of mangrove species using remote-sensing images remains a challenge. The combinations of multi-source remote-sensing datasets (with different spectral/spatial resolution) are beneficial to the improvement of mangrove tree species discrimination. In this paper, various combinations of remote-sensing datasets including Sentinel-1 dual-polarimetric synthetic aperture radar (SAR), Sentinel-2 multispectral, and Gaofen-3 full-polarimetric SAR data were used to classify the mangrove communities in Xuan Thuy National Park, Vietnam. The mixture of mangrove communities consisting of small and shrub mangrove patches is generally difficult to separate using low/medium spatial resolution. To alleviate this problem, we propose to use label distribution learning (LDL) to provide the probabilistic mapping of tree species, including Sonneratia caseolaris (SC), Kandelia obovata (KO), Aegiceras corniculatum (AC), Rhizophora stylosa (RS), and Avicennia marina (AM). The experimental results show that the best classification performance was achieved by an integration of Sentinel-2 and Gaofen-3 datasets, demonstrating that full-polarimetric Gaofen-3 data is superior to the dual-polarimetric Sentinel-1 data for mapping mangrove tree species in the tropics.},
DOI = {10.3390/rs12223834}
}



@Article{rs12223839,
AUTHOR = {Tian, Xiaomin and Chen, Long and Zhang, Xiaoli and Chen, Erxue},
TITLE = {Improved Prototypical Network Model for Forest Species Classification in Complex Stand},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {22},
ARTICLE-NUMBER = {3839},
URL = {https://www.mdpi.com/2072-4292/12/22/3839},
ISSN = {2072-4292},
ABSTRACT = {Deep learning has become an effective method for hyperspectral image classification. However, the high band correlation and data volume associated with airborne hyperspectral images, and the insufficiency of training samples, present challenges to the application of deep learning in airborne image classification. Prototypical networks are practical deep learning networks that have demonstrated effectiveness in handling small-sample classification. In this study, an improved prototypical network is proposed (by adding L2 regularization to the convolutional layer and dropout to the maximum pooling layer) to address the problem of overfitting in small-sample classification. The proposed network has an optimal sample window for classification, and the window size is related to the area and distribution of the study area. After performing dimensionality reduction using principal component analysis, the time required for training using hyperspectral images shortened significantly, and the test accuracy increased drastically. Furthermore, when the size of the sample window was 27 &times; 27 after dimensionality reduction, the overall accuracy of forest species classification was 98.53%, and the Kappa coefficient was 0.9838. Therefore, by using an improved prototypical network with a sample window of an appropriate size, the network yielded desirable classification results, thereby demonstrating its suitability for the fine classification and mapping of tree species.},
DOI = {10.3390/rs12223839}
}



@Article{f11121239,
AUTHOR = {Scharvogel, Daniel and Brandmeier, Melanie and Weis, Manuel},
TITLE = {A Deep Learning Approach for Calamity Assessment Using Sentinel-2 Data},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1239},
URL = {https://www.mdpi.com/1999-4907/11/12/1239},
ISSN = {1999-4907},
ABSTRACT = {The number of severe storm events has increased in recent decades due to climate change. These storms are one of the main causes for timber loss in European forests and damaged areas are prone to further degradation by, for example, bark beetle infestations. Usually, manual mapping of damaged areas based on aerial photographs is conducted by forest departments. This is very time-consuming and therefore automatic detection of windthrows based on active and passive remote sensing data is an ongoing research topic. In this study we evaluated state-of-the-art Convolutional Neural Networks (CNNs) in combination with Geographic Information Systems (GIS) for calamity assessment. The study area is in in the northern part of Hesse (Germany) and was covered by twelve Sentinel-2 scenes from 2018. Labels of damaged areas from the Friedericke storm (18 January 2018) were provided by HessenForst. We conducted several experiments based on a custom U-Net setup to derive the optimal architecture and input data as well as to assess the transferability of the model. Results highlight the possibility to detect damaged forest areas using Sentinel-2 data. Using a binary classification, accuracies of more than 92% were achieved with an Intersection over Union (IoU) score of 46.6%. The proposed workflow was integrated into ArcGIS and is suitable for fast detection of damaged areas directly after a storm and for disaster management but is limited by the deca-meter spatial resolution of the Sentinel-2 data.},
DOI = {10.3390/f11121239}
}



@Article{w12123300,
AUTHOR = {Elsayed, Salah and Hussein, Hend and Moghanm, Farahat S. and Khedher, Khaled M. and Eid, Ebrahem M. and Gad, Mohamed},
TITLE = {Application of Irrigation Water Quality Indices and Multivariate Statistical Techniques for Surface Water Quality Assessments in the Northern Nile Delta, Egypt},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3300},
URL = {https://www.mdpi.com/2073-4441/12/12/3300},
ISSN = {2073-4441},
ABSTRACT = {Under sustainable development conditions, the water quality of irrigation systems is a complex issue which involves the combined effects of several surface water management parameters. Therefore, this work aims to enhance the surface water quality assessment and geochemical controlling mechanisms and to assess the validation of surface water networks for irrigation using six Water Quality Indices (WQIs) supported by multivariate modelling techniques, such as Principal Component Regression (PCR), Support Vector Machine Regression (SVMR) and Stepwise Multiple Linear Regression (SMLR). A total of 110 surface water samples from a network of surface water cannels during the summers of 2018 and 2019 were collected for this research and standard analytical techniques were used to measure 21 physical and chemical parameters. The physicochemical properties revealed that the major ions concentrations were reported in the following order: Ca2+ &gt; Na+ &gt; Mg2+ &gt; K+ and alkalinity &gt; SO42&minus; &gt; Cl&minus; &gt; NO3&minus; &gt; F&minus;. The trace elements concentrations were reported in the following order: Fe &gt; Mn &gt; B &gt; Cr &gt; Pb &gt; Ni &gt; Cu &gt; Zn &gt; Cd. The surface water belongs to the Ca2+-Mg2+-HCO3&minus; and Ca2+-Mg2+-Cl&minus;-SO42&minus; water types, under a stress of silicate weathering and reverse ion exchange process. The computation of WQI values across two years revealed that 82% of samples represent a high class and the remaining 18% constitute a medium class of water quality for irrigation use with respect to the Irrigation Water Quality (IWQ) value, while the Sodium Percentage (Na%) values across two years indicated that 96% of samples fell into in a healthy class and 4% fell into in a permissible class for irrigation. In addition, the Sodium Absorption Ratio (SAR), Permeability Index (PI), Kelley Index (KI) and Residual Sodium Carbonate (RSC) values revealed that all surface water samples were appropriate for irrigation use. The PCR and SVMR indicated accurate and robust models that predict the six WQIs in both datasets of the calibration (Cal.) and validation (Val.), with R2 values varying from 0.48 to 0.99. The SMLR presented estimated the six WQIs well, with an R2 value that ranged from 0.66 to 0.99. In conclusion, WQIs and multivariate statistical analyses are effective and applicable for assessing the surface water quality. The PCR, SVMR and SMLR models provided robust and reliable estimates of the different indices and showed the highest R2 and the highest slopes values close to 1.00, as well as minimum values of RMSE in all models.},
DOI = {10.3390/w12123300}
}



@Article{rs12233855,
AUTHOR = {Tseng, Chun-Wei and Song, Cheng-En and Wang, Su-Fen and Chen, Yi-Chin and Tu, Jien-Yi and Yang, Ci-Jian and Chuang, Chih-Wei},
TITLE = {Application of High-Resolution Radar Rain Data to the Predictive Analysis of Landslide Susceptibility under Climate Change in the Laonong Watershed, Taiwan},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3855},
URL = {https://www.mdpi.com/2072-4292/12/23/3855},
ISSN = {2072-4292},
ABSTRACT = {Extreme rainfall has caused severe road damage and landslide disasters in mountainous areas. Rainfall forecasting derived from remote sensing data has been widely adopted for disaster prevention and early warning as a trend in recent years. By integrating high-resolution radar rain data, for example, the QPESUMS (quantitative precipitation estimation and segregation using multiple sensors) system provides a great opportunity to establish the extreme climate-based landslide susceptibility model, which would be helpful in the prevention of hillslope disasters under climate change. QPESUMS was adopted to obtain spatio-temporal rainfall patterns, and further, multi-temporal landslide inventories (2003&ndash;2018) would integrate with other explanatory factors and therefore, we can establish the logistic regression method for prediction of landslide susceptibility sites in the Laonong River watershed, which was devastated by Typhoon Morakot in 2009. Simulations of landslide susceptibility under the critical rainfall (300, 600, and 900 mm) were designed to verify the model&rsquo;s sensitivity. Due to the orographic effect, rainfall was concentrated at the low mountainous and middle elevation areas in the southern Laonong River watershed. Landslide change analysis indicates that the landslide ratio increased from 1.5% to 7.0% after Typhoon Morakot in 2009. Subsequently, the landslide ratio fluctuated between 3.5% and 4.5% after 2012, which indicates that the recovery of landslide areas is still in progress. The validation results showed that the calibrated model of 2005 is preferred in the general period, with an accuracy of 78%. For extreme rainfall typhoons, the calibrated model of 2009 would perform better (72%). This study presented that the integration of multi-temporal landslide inventories in a logistic regression model is capable of predicting rainfall-triggered landslide risk under climate change.},
DOI = {10.3390/rs12233855}
}



@Article{s20236732,
AUTHOR = {Qi, Haixia and Zhu, Bingyu and Wu, Zeyu and Liang, Yu and Li, Jianwen and Wang, Leidi and Chen, Tingting and Lan, Yubin and Zhang, Lei},
TITLE = {Estimation of Peanut Leaf Area Index from Unmanned Aerial Vehicle Multispectral Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6732},
URL = {https://www.mdpi.com/1424-8220/20/23/6732},
ISSN = {1424-8220},
ABSTRACT = {Leaf area index (LAI) is used to predict crop yield, and unmanned aerial vehicles (UAVs) provide new ways to monitor LAI. In this study, we used a fixed-wing UAV with multispectral cameras for remote sensing monitoring. We conducted field experiments with two peanut varieties at different planting densities to estimate LAI from multispectral images and establish a high-precision LAI prediction model. We used eight vegetation indices (VIs) and developed simple regression and artificial neural network (BPN) models for LAI and spectral VIs. The empirical model was calibrated to estimate peanut LAI, and the best model was selected from the coefficient of determination and root mean square error. The red (660 nm) and near-infrared (790 nm) bands effectively predicted peanut LAI, and LAI increased with planting density. The predictive accuracy of the multiple regression model was higher than that of the single linear regression models, and the correlations between Modified Red-Edge Simple Ratio Index (MSR), Ratio Vegetation Index (RVI), Normalized Difference Vegetation Index (NDVI), and LAI were higher than the other indices. The combined VI BPN model was more accurate than the single VI BPN model, and the BPN model accuracy was higher. Planting density affects peanut LAI, and reflectance-based vegetation indices can help predict LAI.},
DOI = {10.3390/s20236732}
}



@Article{a13120308,
AUTHOR = {Nguyen Duc, Duy and Tran Huu, Thong and Nananukul, Narameth},
TITLE = {A Dynamic Route-Planning System Based on Industry 4.0 Technology},
JOURNAL = {Algorithms},
VOLUME = {13},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {308},
URL = {https://www.mdpi.com/1999-4893/13/12/308},
ISSN = {1999-4893},
ABSTRACT = {Due to the availability of Industry 4.0 technology, the application of big data analytics to automated systems is possible. The distribution of products between warehouses or within a warehouse is an area that can benefit from automation based on Industry 4.0 technology. In this paper, the focus was on developing a dynamic route-planning system for automated guided vehicles within a warehouse. A dynamic routing problem with real-time obstacles was considered in this research. A key problem in this research area is the lack of a real-time route-planning algorithm that is suitable for the implementation on automated guided vehicles with limited computing resources. An optimization model, as well as machine learning methodologies for determining an operational route for the problem, is proposed. An internal layout of the warehouse of a large consumer product distributor was used to test the performance of the methodologies. A simulation environment based on Gazebo was developed and used for testing the implementation of the route-planning system. Computational results show that the proposed machine learning methodologies were able to generate routes with testing accuracy of up to 98% for a practical internal layout of a warehouse with 18 storage racks and 67 path segments. Managerial insights into how the machine learning configuration affects the prediction accuracy are also provided.},
DOI = {10.3390/a13120308}
}



@Article{robotics9040100,
AUTHOR = {Roy, Rapha√´lle N. and Drougard, Nicolas and Gateau, Thibault and Dehais, Fr√©d√©ric and Chanel, Caroline P. C.},
TITLE = {How Can Physiological Computing Benefit Human-Robot Interaction?},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {100},
URL = {https://www.mdpi.com/2218-6581/9/4/100},
ISSN = {2218-6581},
ABSTRACT = {As systems grow more automatized, the human operator is all too often overlooked. Although human-robot interaction (HRI) can be quite demanding in terms of cognitive resources, the mental states (MS) of the operators are not yet taken into account by existing systems. As humans are no providential agents, this lack can lead to hazardous situations. The growing number of neurophysiology and machine learning tools now allows for efficient operators&rsquo; MS monitoring. Sending feedback on MS in a closed-loop solution is therefore at hand. Involving a consistent automated planning technique to handle such a process could be a significant asset. This perspective article was meant to provide the reader with a synthesis of the significant literature with a view to implementing systems that adapt to the operator&rsquo;s MS to improve human-robot operations&rsquo; safety and performance. First of all, the need for this approach is detailed regarding remote operation, an example of HRI. Then, several MS identified as crucial for this type of HRI are defined, along with relevant electrophysiological markers. A focus is made on prime degraded MS linked to time-on-task and task demands, as well as collateral MS linked to system outputs (i.e., feedback and alarms). Lastly, the principle of symbiotic HRI is detailed and one solution is proposed to include the operator state vector into the system using a mixed-initiative decisional framework to drive such an interaction.},
DOI = {10.3390/robotics9040100}
}



@Article{en13236250,
AUTHOR = {Ayele, Yonas Zewdu and Aliyari, Mostafa and Griffiths, David and Droguett, Enrique Lopez},
TITLE = {Automatic Crack Segmentation for UAV-Assisted Bridge Inspection},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6250},
URL = {https://www.mdpi.com/1996-1073/13/23/6250},
ISSN = {1996-1073},
ABSTRACT = {Bridges are a critical piece of infrastructure in the network of road and rail transport system. Many of the bridges in Norway (in Europe) are at the end of their lifespan, therefore regular inspection and maintenance are critical to ensure the safety of their operations. However, the traditional inspection procedures and resources required are so time consuming and costly that there exists a significant maintenance backlog. The central thrust of this paper is to demonstrate the significant benefits of adapting a Unmanned Aerial Vehicle (UAV)-assisted inspection to reduce the time and costs of bridge inspection and established the research needs associated with the processing of the (big) data produced by such autonomous technologies. In this regard, a methodology is proposed for analysing the bridge damage that comprises three key stages, (i) data collection and model training, where one performs experiments and trials to perfect drone flights for inspection using case study bridges to inform and provide necessary (big) data for the second key stage, (ii) 3D construction, where one built 3D models that offer a permanent record of element geometry for each bridge asset, which could be used for navigation and control purposes, (iii) damage identification and analysis, where deep learning-based data analytics and modelling are applied for processing and analysing UAV image data and to perform bridge damage performance assessment. The proposed methodology is exemplified via UAV-assisted inspection of Skodsberg bridge, a 140 m prestressed concrete bridge, in the Viken county in eastern Norway.},
DOI = {10.3390/en13236250}
}



@Article{s20236780,
AUTHOR = {Fu, Yanhua and Xie, Hongfei and Mao, Yachun and Ren, Tao and Xiao, Dong},
TITLE = {Copper Content Inversion of Copper Ore Based on Reflectance Spectra and the VTELM Algorithm},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6780},
URL = {https://www.mdpi.com/1424-8220/20/23/6780},
ISSN = {1424-8220},
ABSTRACT = {Copper is an important national resource, which is widely used in various sectors of the national economy. The traditional detection of copper content in copper ore has the disadvantages of being time-consuming and high cost. Due to the many drawbacks of traditional detection methods, this paper proposes a new method for detecting copper content in copper ore, that is, through the spectral information of copper ore content detection method. First of all, we use chemical methods to analyze the copper content in a batch of copper ores, and accurately obtain the copper content in those ores. Then we do spectrometric tests on this batch of copper ore, and get accurate spectral data of copper ore. Based on the data obtained, we propose a new two hidden layer extreme learning machine algorithm with variable hidden layer nodes and use the regularization standard to constrain the extreme learning machine. Finally, the prediction model of copper content in copper ore is established by using the algorithm. Experiments show that this method of detecting copper ore content using spectral information is completely feasible, and the algorithm proposed in this paper can detect the copper content in copper ores faster and more accurately.},
DOI = {10.3390/s20236780}
}



@Article{rs12233892,
AUTHOR = {Egli, Sebastian and H√∂pke, Martin},
TITLE = {CNN-Based Tree Species Classification Using High Resolution RGB Image Data from Automated UAV Observations},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3892},
URL = {https://www.mdpi.com/2072-4292/12/23/3892},
ISSN = {2072-4292},
ABSTRACT = {Data on the distribution of tree species are often requested by forest managers, inventory agencies, foresters as well as private and municipal forest owners. However, the automated detection of tree species based on passive remote sensing data from aerial surveys is still not sufficiently developed to achieve reliable results independent of the phenological stage, time of day, season, tree vitality and prevailing atmospheric conditions. Here, we introduce a novel tree species classification approach based on high resolution RGB image data gathered during automated UAV flights that overcomes these insufficiencies. For the classification task, a computationally lightweight convolutional neural network (CNN) was designed. We show that with the chosen CNN model architecture, average classification accuracies of 92% can be reached independently of the illumination conditions and the phenological stages of four different tree species. We also show that a minimal ground sampling density of 1.6 cm/px is needed for the classification model to be able to make use of the spatial-structural information in the data. Finally, to demonstrate the applicability of the presented approach to derive spatially explicit tree species information, a gridded product is generated that yields an average classification accuracy of 88%.},
DOI = {10.3390/rs12233892}
}



@Article{sym12121965,
AUTHOR = {Zhu, Juncai and Wang, Zhizhong and Wang, Songwei and Chen, Shuli},
TITLE = {Moving Object Detection Based on Background Compensation and Deep Learning},
JOURNAL = {Symmetry},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1965},
URL = {https://www.mdpi.com/2073-8994/12/12/1965},
ISSN = {2073-8994},
ABSTRACT = {Detecting moving objects in a video sequence is an important problem in many vision-based applications. In particular, detecting moving objects when the camera is moving is a difficult problem. In this study, we propose a symmetric method for detecting moving objects in the presence of a dynamic background. First, a background compensation method is used to detect the proposed region of motion. Next, in order to accurately locate the moving objects, we propose a convolutional neural network-based method called YOLOv3-SOD for detecting all objects in the image, which is lightweight and specifically designed for small objects. Finally, the moving objects are determined by fusing the results obtained by motion detection and object detection. Missed detections are recalled according to the temporal and spatial information in adjacent frames. A dataset is not currently available specifically for moving object detection and recognition, and thus, we have released the MDR105 dataset comprising three classes with 105 videos. Our experiments demonstrated that the proposed algorithm can accurately detect moving objects in various scenarios with good overall performance.},
DOI = {10.3390/sym12121965}
}



@Article{rs12233925,
AUTHOR = {Pila≈°, Ivan and Ga≈°paroviƒá, Mateo and Novkiniƒá, Alan and Klobuƒçar, Damir},
TITLE = {Mapping of the Canopy Openings in Mixed Beech‚ÄìFir Forest at Sentinel-2 Subpixel Level Using UAV and Machine Learning Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3925},
URL = {https://www.mdpi.com/2072-4292/12/23/3925},
ISSN = {2072-4292},
ABSTRACT = {The presented study demonstrates a bi-sensor approach suitable for rapid and precise up-to-date mapping of forest canopy gaps for the larger spatial extent. The approach makes use of Unmanned Aerial Vehicle (UAV) red, green and blue (RGB) images on smaller areas for highly precise forest canopy mask creation. Sentinel-2 was used as a scaling platform for transferring information from the UAV to a wider spatial extent. Various approaches to an improvement in the predictive performance were examined: (I) the highest R2 of the single satellite index was 0.57, (II) the highest R2 using multiple features obtained from the single-date, S-2 image was 0.624, and (III) the highest R2 on the multitemporal set of S-2 images was 0.697. Satellite indices such as Atmospherically Resistant Vegetation Index (ARVI), Infrared Percentage Vegetation Index (IPVI), Normalized Difference Index (NDI45), Pigment-Specific Simple Ratio Index (PSSRa), Modified Chlorophyll Absorption Ratio Index (MCARI), Color Index (CI), Redness Index (RI), and Normalized Difference Turbidity Index (NDTI) were the dominant predictors in most of the Machine Learning (ML) algorithms. The more complex ML algorithms such as the Support Vector Machines (SVM), Random Forest (RF), Stochastic Gradient Boosting (GBM), Extreme Gradient Boosting (XGBoost), and Catboost that provided the best performance on the training set exhibited weaker generalization capabilities. Therefore, a simpler and more robust Elastic Net (ENET) algorithm was chosen for the final map creation.},
DOI = {10.3390/rs12233925}
}



@Article{rs12233926,
AUTHOR = {Deur, Martina and Ga≈°paroviƒá, Mateo and Balenoviƒá, Ivan},
TITLE = {Tree Species Classification in Mixed Deciduous Forests Using Very High Spatial Resolution Satellite Imagery and Machine Learning Methods},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3926},
URL = {https://www.mdpi.com/2072-4292/12/23/3926},
ISSN = {2072-4292},
ABSTRACT = {Spatially explicit information on tree species composition is important for both the forest management and conservation sectors. In combination with machine learning algorithms, very high-resolution satellite imagery may provide an effective solution to reduce the need for labor-intensive and time-consuming field-based surveys. In this study, we evaluated the possibility of using multispectral WorldView-3 (WV-3) satellite imagery for the classification of three main tree species (Quercus robur L., Carpinus betulus L., and Alnus glutinosa (L.) Geartn.) in a lowland, mixed deciduous forest in central Croatia. The pixel-based supervised classification was performed using two machine learning algorithms: random forest (RF) and support vector machine (SVM). Additionally, the contribution of gray level cooccurrence matrix (GLCM) texture features from WV-3 imagery in tree species classification was evaluated. Principal component analysis confirmed GLCM variance to be the most significant texture feature. Of the 373 visually interpreted reference polygons, 237 were used as training polygons and 136 were used as validation polygons. The validation results show relatively high overall accuracy (85%) for tree species classification based solely on WV-3 spectral characteristics and the RF classification approach. As expected, an improvement in classification accuracy was achieved by a combination of spectral and textural features. With the additional use of GLCM variance, the overall accuracy improved by 10% and 7% for RF and SVM classification approaches, respectively.},
DOI = {10.3390/rs12233926}
}



@Article{math8122140,
AUTHOR = {Kupervasser, Oleg and Kutomanov, Hennadii and Levi, Ori and Pukshansky, Vladislav and Yavich, Roman},
TITLE = {Using Deep Learning for Visual Navigation of Drone with Respect to 3D Ground Objects},
JOURNAL = {Mathematics},
VOLUME = {8},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2140},
URL = {https://www.mdpi.com/2227-7390/8/12/2140},
ISSN = {2227-7390},
ABSTRACT = {In the paper, visual navigation of a drone is considered. The drone navigation problem consists of two parts. The first part is finding the real position and orientation of the drone. The second part is finding the difference between desirable and real position and orientation of the drone and creation of the correspondent control signal for decreasing the difference. For the first part of the drone navigation problem, the paper presents a method for determining the coordinates of the drone camera with respect to known three-dimensional (3D) ground objects using deep learning. The algorithm has two stages. It causes the algorithm to be easy for interpretation by artificial neural network (ANN) and consequently increases its accuracy. At the first stage, we use the first ANN to find coordinates of the object origin projection. At the second stage, we use the second ANN to find the drone camera position and orientation. The algorithm has high accuracy (these errors were found for the validation set of images as differences between positions and orientations, obtained from a pretrained artificial neural network, and known positions and orientations), it is not sensitive to interference associated with changes in lighting, the appearance of external moving objects and the other phenomena where other methods of visual navigation are not effective. For the second part of the drone navigation problem, the paper presents a method for stabilization of drone flight controlled by autopilot with time delay. Indeed, image processing for navigation demands a lot of time and results in a time delay. However, the proposed method allows to get stable control in the presence of this time delay.},
DOI = {10.3390/math8122140}
}



@Article{designs4040052,
AUTHOR = {Sakharov, Vladimir and Chernyi, Sergei and Saburov, Sergey and Chertkov, Aleksandr},
TITLE = {Wavelet Transforms of Diagnosable Signals from Ship Power Complexes in a MATLAB Environment},
JOURNAL = {Designs},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2411-9660/4/4/52},
ISSN = {2411-9660},
ABSTRACT = {The use of digital technologies in the systems of diagnostics and monitoring of units of a ship&rsquo;s propulsion plant can significantly increase the efficiency and quality of assessing the technical condition of operated objects in the online mode as well as expand the class of practical problems to be solved. Digital processing of signals of complex configuration at a qualitatively new level is an indispensable condition for a critical improvement in the course of processing the current values of diagnosable parameters, increasing the reliability and trouble-free performance of a ship&rsquo;s technical equipment during operation. A method of approximation has been discussed in the paper. Moreover, the paper provides an algorithm for analyzing the signal of the indicator diagram of a marine diesel engine, the spectrum of which contains high-frequency components and short-term pulses indicating deviations from the normal operating mode, the assessment of which is practically impossible with the traditionally applied methods of spectral analysis of signals. The approximation method is based on the use of wavelet analysis, which makes it possible to deeply explore such modes. Examples of using wavelet analysis to approximate one-dimensional signals of elements and systems of a ship&rsquo;s power complexes are given.},
DOI = {10.3390/designs4040052}
}



@Article{s20236896,
AUTHOR = {Buzzy, Michael and Thesma, Vaishnavi and Davoodi, Mohammadreza and Mohammadpour Velni, Javad},
TITLE = {Real-Time Plant Leaf Counting Using Deep Object Detection Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6896},
URL = {https://www.mdpi.com/1424-8220/20/23/6896},
ISSN = {1424-8220},
ABSTRACT = {The use of deep neural networks (DNNs) in plant phenotyping has recently received considerable attention. By using DNNs, valuable insights into plant traits can be readily achieved. While these networks have made considerable advances in plant phenotyping, the results are processed too slowly to allow for real-time decision-making. Therefore, being able to perform plant phenotyping computations in real-time has become a critical part of precision agriculture and agricultural informatics. In this work, we utilize state-of-the-art object detection networks to accurately detect, count, and localize plant leaves in real-time. Our work includes the creation of an annotated dataset of Arabidopsis plants captured using Cannon Rebel XS camera. These images and annotations have been complied and made publicly available. This dataset is then fed into a Tiny-YOLOv3 network for training. The Tiny-YOLOv3 network is then able to converge and accurately perform real-time localization and counting of the leaves. We also create a simple robotics platform based on an Android phone and iRobot create2 to demonstrate the real-time capabilities of the network in the greenhouse. Additionally, a performance comparison is conducted between Tiny-YOLOv3 and Faster R-CNN. Unlike Tiny-YOLOv3, which is a single network that does localization and identification in a single pass, the Faster R-CNN network requires two steps to do localization and identification. While with Tiny-YOLOv3, inference time, F1 Score, and false positive rate (FPR) are improved compared to Faster R-CNN, other measures such as difference in count (DiC) and AP are worsened. Specifically, for our implementation of Tiny-YOLOv3, the inference time is under 0.01 s, the F1 Score is over 0.94, and the FPR is around 24%. Last, transfer learning using Tiny-YOLOv3 to detect larger leaves on a model trained only on smaller leaves is implemented. The main contributions of the paper are in creating dataset (shared with the research community), as well as the trained Tiny-YOLOv3 network for leaf localization and counting.},
DOI = {10.3390/s20236896}
}



@Article{s20236936,
AUTHOR = {Balaniuk, Remis and Isupova, Olga and Reece, Steven},
TITLE = {Mining and Tailings Dam Detection in Satellite Imagery Using Deep Learning},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6936},
URL = {https://www.mdpi.com/1424-8220/20/23/6936},
ISSN = {1424-8220},
ABSTRACT = {This work explores the combination of free cloud computing, free open-source software, and deep learning methods to analyze a real, large-scale problem: the automatic country-wide identification and classification of surface mines and mining tailings dams in Brazil. Locations of officially registered mines and dams were obtained from the Brazilian government open data resource. Multispectral Sentinel-2 satellite imagery, obtained and processed at the Google Earth Engine platform, was used to train and test deep neural networks using the TensorFlow 2 application programming interface (API) and Google Colaboratory (Colab) platform. Fully convolutional neural networks were used in an innovative way to search for unregistered ore mines and tailing dams in large areas of the Brazilian territory. The efficacy of the approach is demonstrated by the discovery of 263 mines that do not have an official mining concession. This exploratory work highlights the potential of a set of new technologies, freely available, for the construction of low cost data science tools that have high social impact. At the same time, it discusses and seeks to suggest practical solutions for the complex and serious problem of illegal mining and the proliferation of tailings dams, which pose high risks to the population and the environment, especially in developing countries.},
DOI = {10.3390/s20236936}
}



@Article{rs12233980,
AUTHOR = {Psomiadis, Emmanouil and Diakakis, Michalis and Soulis, Konstantinos X.},
TITLE = {Combining SAR and Optical Earth Observation with Hydraulic Simulation for Flood Mapping and Impact Assessment},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {3980},
URL = {https://www.mdpi.com/2072-4292/12/23/3980},
ISSN = {2072-4292},
ABSTRACT = {Timely mapping, measuring and impact assessment of flood events are crucial for the coordination of flood relief efforts and the elaboration of flood management and risk mitigation plans. However, this task is often challenging and time consuming with traditional land-based techniques. In this study, Sentinel-1 radar and Landsat images were utilized in collaboration with hydraulic modelling to obtain flood characteristics and land use/cover (LULC), and to assess flood impact in agricultural areas. Furthermore, indirect estimation of the recurrence interval of a flood event in a poorly gauged catchment was attempted by combining remote sensing (RS) and hydraulic modelling. To this end, a major flood event that occurred in Sperchios river catchment, in Central Greece, which is characterized by extensive farming activity was used as a case study. The synergistic usage of multitemporal RS products and hydraulic modelling has allowed the estimation of flood characteristics, such as extent, inundation depth, peak discharge, recurrence interval and inundation duration, providing valuable information for flood impact estimation and the future examination of flood hazard in poorly gauged basins. The capabilities of the ESA Sentinel-1 mission, which provides improved spatial and temporal analysis, allowing thus the mapping of the extent and temporal dynamics of flood events more accurately and independently from the weather conditions, were also highlighted. Both radar and optical data processing methods, i.e., thresholding, image differencing and water index calculation, provided similar and satisfactory results. Conclusively, multitemporal RS data and hydraulic modelling, with the selected techniques, can provide timely and useful flood observations during and right after flood disasters, applicable in a large part of the world where instrumental hydrological data are scarce and when an apace survey of the condition and information about temporal dynamics in the influenced region is crucial. However, future missions that will reduce further revisiting times will be valuable in this endeavor.},
DOI = {10.3390/rs12233980}
}



@Article{su122310150,
AUTHOR = {Zhu, Yongyan and Jeon, Seongwoo and Sung, Hyunchan and Kim, Yoonji and Park, Chiyoung and Cha, Sungeun and Jo, Hyun-woo and Lee, Woo-kyun},
TITLE = {Developing UAV-Based Forest Spatial Information and Evaluation Technology for Efficient Forest Management},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {10150},
URL = {https://www.mdpi.com/2071-1050/12/23/10150},
ISSN = {2071-1050},
ABSTRACT = {Forest spatial information is regularly established and managed as basic data for national forest planning and forest policy establishment. Among them, the grade of vegetation conservation shall be investigated and evaluated according to the value of vegetation conservation. As the collection of field data over large or remote areas is difficult, unmanned aerial vehicles (UAVs) are increasingly being used for this purpose. Consequently, there is a need for research on UAV-monitoring and three-dimensional (3D) image generation techniques. In this study, a new method that can efficiently collect and analyze UAV spatial data to survey and assess forests was developed. Both UAV-based and LiDAR imaging methods were evaluated in conjunction with the ground control point measurement method for forest surveys. In addition, by fusing the field survey database of each target site and the UAV optical and LiDAR images, the Gongju, Samcheok, and Seogwipo regions were analyzed based on deep learning. The kappa value showed 0.59, 0.47, and 0.78 accuracy for each of the sites in terms of vegetation type (artificial or natural), and 0.68, 0.53, and 0.62 accuracy in terms of vegetation layer structure. The results of comparative analysis with ecological natural maps by establishing vegetation conservation levels show that about 83.9% of the areas are consistent. The findings verified the applicability of this UAV-based approach for the construction of geospatial information on forests. The proposed method can be useful for improving the efficiency of the Vegetation Conservation Classification system and for conducting high-resolution monitoring in forests worldwide.},
DOI = {10.3390/su122310150}
}



@Article{s20236953,
AUTHOR = {Mondal, Sabyasachi and Tsourdos, Antonios},
TITLE = {Autonomous Addition of Agents to an Existing Group Using Genetic Algorithm},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6953},
URL = {https://www.mdpi.com/1424-8220/20/23/6953},
ISSN = {1424-8220},
ABSTRACT = {This paper presents an idea of how new agents can be added autonomously to a group of existing agents without changing the existing communication topology among them. Autonomous agent addition to existing Multi-Agent Systems (MASs) can give a strategic advantage during the execution of a critical beyond visual line-of-sight (BVLOS) mission. The addition of the agent essentially means that new connections with existing agents are established. It is obvious that the consensus control energy increases as the number of agent increases considering a specific consensus protocol. The objective of this work is to establish the new connections in a way such that the consensus energy increase due to the new agents is minimal. The updated topology, including new connections, must contain a spanning tree to maintain the stability of the MASs network. The updated optimal topology is obtained by solving minimum additional consensus control energy using the Two-Dimensional Genetic Algorithm. The results obtained are convincing.},
DOI = {10.3390/s20236953}
}



@Article{ma13235549,
AUTHOR = {Shin, Hyun Kyu and Ahn, Yong Han and Lee, Sang Hyo and Kim, Ha Young},
TITLE = {Automatic Concrete Damage Recognition Using Multi-Level Attention Convolutional Neural Network},
JOURNAL = {Materials},
VOLUME = {13},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {5549},
URL = {https://www.mdpi.com/1996-1944/13/23/5549},
PubMedID = {33291411},
ISSN = {1996-1944},
ABSTRACT = {There has been an increase in the deterioration of buildings and infrastructure in dense urban regions, and several defects in the structures are being exposed. To ensure the effective diagnosis of building conditions, vision-based automatic damage recognition techniques have been developed. However, conventional image processing techniques have some limitations in real-world situations owing to their manual feature extraction approach. To overcome these limitations, a convolutional neural network-based image recognition technique was adopted in this study, and a convolution-based concrete multi-damage recognition neural network (CMDnet) was developed. The image datasets consisted of 1981 types of concrete surface damages, including surface cracks, rebar exposure and delamination, as well as intact. Furthermore, it was experimentally demonstrated that the proposed model could accurately classify the damage types. The results obtained in this study reveal that the proposed model can recognize the different damage types from digital images of the surfaces of concrete structures. The trained CMDnet demonstrated a damage-detection accuracy of 98.9%. Moreover, the proposed model could be applied in automatic damage detection networks to achieve superior performance with regard to concrete surface damage detection and recognition, as well as accelerating efficient damage identification during the diagnosis of deteriorating structures used in civil engineering applications.},
DOI = {10.3390/ma13235549}
}



@Article{electronics9122076,
AUTHOR = {Mariscal-Harana, Jorge and Alarc√≥n, V√≠ctor and Gonz√°lez, Fidel and Calvente, Juan Jos√© and P√©rez-Grau, Francisco Javier and Viguria, Antidio and Ollero, An√≠bal},
TITLE = {Audio-Based Aircraft Detection System for Safe RPAS BVLOS Operations},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2076},
URL = {https://www.mdpi.com/2079-9292/9/12/2076},
ISSN = {2079-9292},
ABSTRACT = {For the Remotely Piloted Aircraft Systems (RPAS) market to continue its current growth rate, cost-effective &lsquo;Detect and Avoid&rsquo; systems that enable safe beyond visual line of sight (BVLOS) operations are critical. We propose an audio-based &lsquo;Detect and Avoid&rsquo; system, composed of microphones and an embedded computer, which performs real-time inferences using a sound event detection (SED) deep learning model. Two state-of-the-art SED models, YAMNet and VGGish, are fine-tuned using our dataset of aircraft sounds and their performances are compared for a wide range of configurations. YAMNet, whose MobileNet architecture is designed for embedded applications, outperformed VGGish both in terms of aircraft detection and computational performance. YAMNet&rsquo;s optimal configuration, with &gt;70% true positive rate and precision, results from combining data augmentation and undersampling with the highest available inference frequency (i.e., 10 Hz). While our proposed &lsquo;Detect and Avoid&rsquo; system already allows the detection of small aircraft from sound in real time, additional testing using multiple aircraft types is required. Finally, a larger training dataset, sensor fusion, or remote computations on cloud-based services could further improve system performance.},
DOI = {10.3390/electronics9122076}
}



@Article{logistics4040033,
AUTHOR = {Haji, Mona and Kerbache, Laoucine and Muhammad, Mahaboob and Al-Ansari, Tareq},
TITLE = {Roles of Technology in Improving Perishable Food Supply Chains},
JOURNAL = {Logistics},
VOLUME = {4},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {33},
URL = {https://www.mdpi.com/2305-6290/4/4/33},
ISSN = {2305-6290},
ABSTRACT = {Food supply chains are considered to be more complex systems than other types of supply chains. This complexity is due to the continuous changes taking place, particularly in ensuring the quality of food products throughout the entire supply chain, from growing, procurement of resources, production, and management of stock, to distribution to the final consumers. For that, food supply chain markets have become more highly developed in the use of modern technologies, and have begun to implement them in their logistical systems to satisfy their customers&rsquo; needs. The main objectives of this review are to identify the different technological implementations in different phases of the food supply chain processes and point out the key factors for using technologies to improve the characteristics of the perishable food supply chain. A total number of 137 articles were analyzed in this research to achieve these review objectives. Some of the various technologies found in different phases of the food supply chain were radio frequency identification (RFID), the Internet of Things (IoT), blockchain, three-dimensional printing (3DP), autonomous vehicles, and unmanned aerial vehicles (UAVs). These technologies were found in different phases of the food supply chain and improved the efficiency of supplying perishable foods. The review identified different characteristics of the perishable food supply chain. The main finding indicated that technological implementation enhances the efficiency and sustainability of the food supply chains and helps to retain perishable food characteristics.},
DOI = {10.3390/logistics4040033}
}



@Article{rs12234000,
AUTHOR = {Nevavuori, Petteri and Narra, Nathaniel and Linna, Petri and Lipping, Tarmo},
TITLE = {Crop Yield Prediction Using Multitemporal UAV Data and Spatio-Temporal Deep Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {4000},
URL = {https://www.mdpi.com/2072-4292/12/23/4000},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51% mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17% MAPE with four weekly frames from the beginning of the season.},
DOI = {10.3390/rs12234000}
}



@Article{rs12234003,
AUTHOR = {Li, Yansheng and Chen, Ruixian and Zhang, Yongjun and Zhang, Mi and Chen, Ling},
TITLE = {Multi-Label Remote Sensing Image Scene Classification by Combining a Convolutional Neural Network and a Graph Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {4003},
URL = {https://www.mdpi.com/2072-4292/12/23/4003},
ISSN = {2072-4292},
ABSTRACT = {As one of the fundamental tasks in remote sensing (RS) image understanding, multi-label remote sensing image scene classification (MLRSSC) is attracting increasing research interest. Human beings can easily perform MLRSSC by examining the visual elements contained in the scene and the spatio-topological relationships of these visual elements. However, most of existing methods are limited by only perceiving visual elements but disregarding the spatio-topological relationships of visual elements. With this consideration, this paper proposes a novel deep learning-based MLRSSC framework by combining convolutional neural network (CNN) and graph neural network (GNN), which is termed the MLRSSC-CNN-GNN. Specifically, the CNN is employed to learn the perception ability of visual elements in the scene and generate the high-level appearance features. Based on the trained CNN, one scene graph for each scene is further constructed, where nodes of the graph are represented by superpixel regions of the scene. To fully mine the spatio-topological relationships of the scene graph, the multi-layer-integration graph attention network (GAT) model is proposed to address MLRSSC, where the GAT is one of the latest developments in GNN. Extensive experiments on two public MLRSSC datasets show that the proposed MLRSSC-CNN-GNN can obtain superior performance compared with the state-of-the-art methods.},
DOI = {10.3390/rs12234003}
}



@Article{app10238754,
AUTHOR = {Sultan, Wajeeha and Anjum, Nadeem and Stansfield, Mark and Ramzan, Naeem},
TITLE = {Hybrid Local and Global Deep-Learning Architecture for Salient-Object Detection},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {8754},
URL = {https://www.mdpi.com/2076-3417/10/23/8754},
ISSN = {2076-3417},
ABSTRACT = {Salient-object detection is a fundamental and the most challenging problem in computer vision. This paper focuses on the detection of salient objects, especially in low-contrast images. To this end, a hybrid deep-learning architecture is proposed where features are extracted on both the local and global level. These features are then integrated to extract the exact boundary of the object of interest in an image. Experimentation was performed on five standard datasets, and results were compared with state-of-the-art approaches. Both qualitative and quantitative analyses showed the robustness of the proposed architecture.},
DOI = {10.3390/app10238754}
}



@Article{agronomy10121926,
AUTHOR = {Lyu, Hong-Kun and Yun, Sanghun and Choi, Byeongdae},
TITLE = {Machine Learning Feature Extraction Based on Binary Pixel Quantification Using Low-Resolution Images for Application of Unmanned Ground Vehicles in Apple Orchards},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1926},
URL = {https://www.mdpi.com/2073-4395/10/12/1926},
ISSN = {2073-4395},
ABSTRACT = {Deep learning and machine learning (ML) technologies have been implemented in various applications, and various agriculture technologies are being developed based on image-based object recognition technology. We propose an orchard environment free space recognition technology suitable for developing small-scale agricultural unmanned ground vehicle (UGV) autonomous mobile equipment using a low-cost lightweight processor. We designed an algorithm to minimize the amount of input data to be processed by the ML algorithm through low-resolution grayscale images and image binarization. In addition, we propose an ML feature extraction method based on binary pixel quantification that can be applied to an ML classifier to detect free space for autonomous movement of UGVs from binary images. Here, the ML feature is extracted by detecting the local-lowest points in segments of a binarized image and by defining 33 variables, including local-lowest points, to detect the bottom of a tree trunk. We trained six ML models to select a suitable ML model for trunk bottom detection among various ML models, and we analyzed and compared the performance of the trained models. The ensemble model demonstrated the best performance, and a test was performed using this ML model to detect apple tree trunks from 100 new images. Experimental results indicate that it is possible to recognize free space in an apple orchard environment by learning using approximately 100 low-resolution grayscale images.},
DOI = {10.3390/agronomy10121926}
}



@Article{electronics9122091,
AUTHOR = {Wolf, √Åd√°m and Troll, P√©ter and Romeder-Finger, Stefan and Archenti, Andreas and Sz√©ll, K√°roly and Galambos, P√©ter},
TITLE = {A Benchmark of Popular Indoor 3D Reconstruction Technologies: Comparison of ARCore and RTAB-Map},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2091},
URL = {https://www.mdpi.com/2079-9292/9/12/2091},
ISSN = {2079-9292},
ABSTRACT = {The fast evolution in computational and sensor technologies brings previously niche solutions to a wider userbase. As such, 3D reconstruction technologies are reaching new use-cases in scientific and everyday areas where they were not present before. Cost-effective and easy-to-use solutions include camera-based 3D scanning techniques, such as photogrammetry. This paper provides an overview of the available solutions and discusses in detail the depth-image based Real-time Appearance-based Mapping (RTAB-Map) technique as well as a smartphone-based solution that utilises ARCore, the Augmented Reality (AR) framework of Google. To qualitatively compare the two 3D reconstruction technologies, a simple length measurement-based method was applied with a purpose-designed reference object. The captured data were then analysed by a processing algorithm. In addition to the experimental results, specific case studies are briefly discussed, evaluating the applicability based on the capabilities of the technologies. As such, the paper presents the use-case of interior surveying in an automated laboratory as well as an example for using the discussed techniques for landmark surveying. The major findings are that point clouds created with these technologies provide a direction- and shape-accurate model, but those contain mesh continuity errors, and the estimated scale factor has a large standard deviation.},
DOI = {10.3390/electronics9122091}
}



@Article{rs12244010,
AUTHOR = {Liu, Xiang and Liu, Huiyu and Datta, Pawanjeet and Frey, Julian and Koch, Barbara},
TITLE = {Mapping an Invasive Plant Spartina alterniflora by Combining an Ensemble One-Class Classification Algorithm with a Phenological NDVI Time-Series Analysis Approach in Middle Coast of Jiangsu, China},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4010},
URL = {https://www.mdpi.com/2072-4292/12/24/4010},
ISSN = {2072-4292},
ABSTRACT = {Spartina alterniflora (S. alterniflora) is one of the worst plant invaders in the coastal wetlands of China. Accurate and repeatable mapping of S. alterniflora invasion is essential to develop cost-effective management strategies for conserving native biodiversity. Traditional remote-sensing-based mapping methods require a lot of fieldwork for sample collection. Moreover, our ability to detect this invasive species is still limited because of poor spectral separability between S. alterniflora and its co-dominant native plants. Therefore, we proposed a novel scheme that uses an ensemble one-class classifier (EOCC) in combination with phenological Normalized Difference Vegetation Index (NDVI) time-series analysis (TSA) to detect S. alterniflora. We evaluated the performance of the EOCC algorithm in two scenarios, i.e., single-scene analysis (SSA) and NDVI-TSA in the core zones of Yancheng National Natural Reserve (YNNR). Meanwhile, a fully supervised classifier support vector machine (SVM) was tested in the two scenarios for comparison. With these scenarios, the crucial phenological stages and the advantage of phenological NDVI-TSA in S. alterniflora recognition were also investigated. Results indicated the EOCC using only positive training data performed similarly well with the SVM trained on complete training data in the YNNR. Moreover, the EOCC algorithm presented a more robust transferability with notably higher classification accuracy than the SVM when being transferred to a second site, without a second training. Furthermore, when combined with the phenological NDVI-TSA, the EOCC algorithm presented more balanced sensitivity&ndash;specificity result, showing slightly better transferability than it performed in the best phenological stage (i.e., senescence stage of November). The achieved results (overall accuracy (OA), Kappa, and true skill statistic (TSS) were 92.92%, 0.843, and 0.834 for the YNNR, and OA, Kappa, and TSS were 90.94%, 0.815, and 0.825 for transferability to the non-training site) suggest that our detection scheme has a high potential for the mapping of S. alterniflora across different areas, and the EOCC algorithm can be a viable alternative to traditional supervised classification method for invasive plant detection.},
DOI = {10.3390/rs12244010}
}



@Article{en13246496,
AUTHOR = {Pierdicca, Roberto and Paolanti, Marina and Felicetti, Andrea and Piccinini, Fabio and Zingaretti, Primo},
TITLE = {Automatic Faults Detection of Photovoltaic Farms: solAIr, a Deep Learning-Based System for Thermal Images},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {6496},
URL = {https://www.mdpi.com/1996-1073/13/24/6496},
ISSN = {1996-1073},
ABSTRACT = {Renewable energy sources will represent the only alternative to limit fossil fuel usage and pollution. For this reason, photovoltaic (PV) power plants represent one of the main systems adopted to produce clean energy. Monitoring the state of health of a system is fundamental. However, these techniques are time demanding, cause stops to the energy generation, and often require laboratory instrumentation, thus being not cost-effective for frequent inspections. Moreover, PV plants are often located in inaccessible places, making any intervention dangerous. In this paper, we propose solAIr, an artificial intelligence system based on deep learning for anomaly cells detection in photovoltaic images obtained from unmanned aerial vehicles equipped with a thermal infrared sensor. The proposed anomaly cells detection system is based on the mask region-based convolutional neural network (Mask R-CNN) architecture, adopted because it simultaneously performs object detection and instance segmentation, making it useful for the automated inspection task. The proposed system is trained and evaluated on the photovoltaic thermal images dataset, a publicly available dataset collected for this work. Furthermore, the performances of three state-of-art deep neural networks, (DNNs) including UNet, FPNet and LinkNet, are compared and evaluated. Results show the effectiveness and the suitability of the proposed approach in terms of intersection over union (IoU) and the Dice coefficient.},
DOI = {10.3390/en13246496}
}



@Article{electronics9122099,
AUTHOR = {Gosiewski, Zdzis≈Çaw and Kwa≈õniewski, Konrad},
TITLE = {Time Minimization of Rescue Action Realized by an Autonomous Vehicle},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2099},
URL = {https://www.mdpi.com/2079-9292/9/12/2099},
ISSN = {2079-9292},
ABSTRACT = {In rescue operations, the full time of action plays important role. It is the sum of the planning, travel, and manipulation (in the action place) phase times. The time minimization of the first two phases by autonomous vehicle for remote action is considered in the paper. For a known a priori map, the path planning consists of local optimal decisions collected next in the general algorithm of the optimal path. Such an approach significantly reduces time of path planning. The robot features and known sparse obstacles reduce the allowable robot speeds. The time of travel is calculated from an allowable velocity profile. Therefore, it can be used to estimate the travel performance. Genetic algorithm and random search-based methods for path finding with travel time optimization are used and compared in the paper. All the proposed time optimization solutions of rescue operations are checked during computer simulations, and results of the simulations are presented.},
DOI = {10.3390/electronics9122099}
}



@Article{s20247061,
AUTHOR = {Yang, Zhao and Tang, Rong and Bao, Jie and Lu, Jiahuan and Zhang, Zhijie},
TITLE = {A Real-Time Trajectory Prediction Method of Small-Scale Quadrotors Based on GPS Data and Neural Network},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7061},
URL = {https://www.mdpi.com/1424-8220/20/24/7061},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a real-time trajectory prediction method for quadrotors based on a bidirectional gated recurrent unit model. Historical trajectory data of ten types of quadrotors were obtained. The bidirectional gated recurrent units were constructed and utilized to learn the historic data. The prediction results were compared with the traditional gated recurrent unit method to test its prediction performance. The efficiency of the proposed algorithm was investigated by comparing the training loss and training time. The results over the testing datasets showed that the proposed model produced better prediction results than the baseline models for all scenarios of the testing datasets. It was also found that the proposed model can converge to a stable state faster than the traditional gated recurrent unit model. Moreover, various types of training samples were applied and compared. With the same randomly selected test datasets, the performance of the prediction model can be improved by selecting the historical trajectory samples of the quadrotors close to the weight or volume of the target quadrotor for training. In addition, the performance of stable trajectory samples is significantly better than that with unstable trajectory segments with a frequent change of speed and direction with large angles.},
DOI = {10.3390/s20247061}
}



@Article{s20247071,
AUTHOR = {Kashiyama, Takehiro and Sobue, Hideaki and Sekimoto, Yoshihide},
TITLE = {Sky Monitoring System for Flying Object Detection Using 4K Resolution Camera},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7071},
URL = {https://www.mdpi.com/1424-8220/20/24/7071},
ISSN = {1424-8220},
ABSTRACT = {The use of drones and other unmanned aerial vehicles has expanded rapidly in recent years. These devices are expected to enter practical use in various fields, such as taking measurements through aerial photography and transporting small and lightweight objects. Simultaneously, concerns over these devices being misused for terrorism or other criminal activities have increased. In response, several sensor systems have been developed to monitor drone flights. In particular, with the recent progress of deep neural network technology, the monitoring of systems using image processing has been proposed. This study developed a monitoring system for flying objects using a 4K camera and a state-of-the-art convolutional neural network model to achieve real-time processing. We installed a monitoring system in a high-rise building in an urban area during this study and evaluated the precision with which it could detect flying objects at different distances under different weather conditions. The results obtained provide important information for determining the accuracy of monitoring systems with image processing in practice.},
DOI = {10.3390/s20247071}
}



@Article{app10248833,
AUTHOR = {Acci√≥n, √Ålvaro and Arg√ºello, Francisco and Heras, Dora B.},
TITLE = {Dual-Window Superpixel Data Augmentation for Hyperspectral Image Classification},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {8833},
URL = {https://www.mdpi.com/2076-3417/10/24/8833},
ISSN = {2076-3417},
ABSTRACT = {Deep learning (DL) has been shown to obtain superior results for classification tasks in the field of remote sensing hyperspectral imaging. Superpixel-based techniques can be applied to DL, significantly decreasing training and prediction times, but the results are usually far from satisfactory due to overfitting. Data augmentation techniques alleviate the problem by synthetically generating new samples from an existing dataset in order to improve the generalization capabilities of the classification model. In this paper we propose a novel data augmentation framework in the context of superpixel-based DL called dual-window superpixel (DWS). With DWS, data augmentation is performed over patches centered on the superpixels obtained by the application of simple linear iterative clustering (SLIC) superpixel segmentation. DWS is based on dividing the input patches extracted from the superpixels into two regions and independently applying transformations over them. As a result, four different data augmentation techniques are proposed that can be applied to a superpixel-based CNN classification scheme. An extensive comparison in terms of classification accuracy with other data augmentation techniques from the literature using two datasets is also shown. One of the datasets consists of small hyperspectral small scenes commonly found in the literature. The other consists of large multispectral vegetation scenes of river basins. The experimental results show that the proposed approach increases the overall classification accuracy for the selected datasets. In particular, two of the data augmentation techniques introduced, namely, dual-flip and dual-rotate, obtained the best results.},
DOI = {10.3390/app10248833}
}



@Article{rs12244040,
AUTHOR = {Xu, Ke and Zhang, Jingchao and Li, Huaimin and Cao, Weixing and Zhu, Yan and Jiang, Xiaoping and Ni, Jun},
TITLE = {Spectrum- and RGB-D-Based Image Fusion for the Prediction of Nitrogen Accumulation in Wheat},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4040},
URL = {https://www.mdpi.com/2072-4292/12/24/4040},
ISSN = {2072-4292},
ABSTRACT = {The accurate estimation of nitrogen accumulation is of great significance to nitrogen fertilizer management in wheat production. To overcome the shortcomings of spectral technology, which ignores the anisotropy of canopy structure when predicting the nitrogen accumulation in wheat, resulting in low accuracy and unstable prediction results, we propose a method for predicting wheat nitrogen accumulation based on the fusion of spectral and canopy structure features. After depth images are repaired using a hole-filling algorithm, RGB images and depth images are fused through IHS transformation, and textural features of the fused images are then extracted in order to express the three-dimensional structural information of the canopy. The fused images contain depth information of the canopy, which breaks through the limitation of extracting canopy structure features from a two-dimensional image. By comparing the experimental results of multiple regression analyses and BP neural networks, we found that the characteristics of the canopy structure effectively compensated for the model prediction of nitrogen accumulation based only on spectral characteristics. Our prediction model displayed better accuracy and stability, with prediction accuracy values (R2) based on BP neural network for the leaf layer nitrogen accumulation (LNA) and shoot nitrogen accumulation (SNA) during a full growth period of 0.74 and 0.73, respectively, and corresponding relative root mean square errors (RRMSEs) of 40.13% and 35.73%.},
DOI = {10.3390/rs12244040}
}



@Article{s20247091,
AUTHOR = {Monteleone, Sergio and Moraes, Edmilson Alves de and Tondato de Faria, Brenno and Aquino Junior, Plinio Thomaz and Maia, Rodrigo Filev and Neto, Andr√© Torre and Toscano, Attilio},
TITLE = {Exploring the Adoption of Precision Agriculture for Irrigation in the Context of Agriculture 4.0: The Key Role of Internet of Things},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7091},
URL = {https://www.mdpi.com/1424-8220/20/24/7091},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the concept of Agriculture 4.0 has emerged as an evolution of precision agriculture (PA) through the diffusion of the Internet of things (IoT). There is a perception that the PA adoption is occurring at a slower pace than expected. Little research has been carried out about Agriculture 4.0, as well as to farmer behavior and operations management. This work explores what drives the adoption of PA in the Agriculture 4.0 context, focusing on farmer behavior and operations management. As a result of a multimethod approach, the factors explaining the PA adoption in the Agriculture 4.0 context and a model of irrigation operations management are proposed. Six simulation scenarios are performed to study the relationships among the factors involved in irrigation planning. Empirical findings contribute to a better understanding of what Agriculture 4.0 is and to expand the possibilities of IoT in the PA domain. This work also contributes to the discussion on Agriculture 4.0, thanks to multidisciplinary research bringing together the different perspectives of PA, IoT and operations management. Moreover, this research highlights the key role of IoT, considering the farmer&rsquo;s possible choice to adopt several IoT sensing technologies for data collection.},
DOI = {10.3390/s20247091}
}



@Article{a13120333,
AUTHOR = {Celis, Ra√∫l de and Solano, Pablo and Cadarso, Luis},
TITLE = {Applying Neural Networks in Aerial Vehicle Guidance to Simplify Navigation Systems},
JOURNAL = {Algorithms},
VOLUME = {13},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {333},
URL = {https://www.mdpi.com/1999-4893/13/12/333},
ISSN = {1999-4893},
ABSTRACT = {The Guidance, Navigation and Control (GNC) of air and space vehicles has been one of the spearheads of research in the aerospace field in recent times. Using Global Navigation Satellite Systems (GNSS) and inertial navigation systems, accuracy may be detached from range. However, these sensor-based GNC systems may cause significant errors in determining attitude and position. These effects can be ameliorated using additional sensors, independent of cumulative errors. The quadrant photodetector semiactive laser is a good candidate for such a purpose. However, GNC systems&rsquo; development and construction costs are high. Reducing costs, while maintaining safety and accuracy standards, is key for development in aerospace engineering. Advanced algorithms for getting such standards while eliminating sensors are cornerstone. The development and application of machine learning techniques to GNC poses an innovative path for reducing complexity and costs. Here, a new nonlinear hybridization algorithm, which is based on neural networks, to estimate the gravity vector is presented. Using a neural network means that once it is trained, the physical-mathematical foundations of flight are not relevant; it is the network that returns dynamics to be fed to the GNC algorithm. The gravity vector, which can be accurately predicted, is used to determine vehicle attitude without calling for gyroscopes. Nonlinear simulations based on real flight dynamics are used to train the neural networks. Then, the approach is tested and simulated together with a GNC system. Monte Carlo analysis is conducted to determine performance when uncertainty arises. Simulation results prove that the performance of the presented approach is robust and precise in a six-degree-of-freedom simulation environment.},
DOI = {10.3390/a13120333}
}



@Article{jimaging6120137,
AUTHOR = {Bhuiyan, Md Abul Ehsan and Witharana, Chandi and Liljedahl, Anna K.},
TITLE = {Use of Very High Spatial Resolution Commercial Satellite Imagery and Deep Learning to Automatically Map Ice-Wedge Polygons across Tundra Vegetation Types},
JOURNAL = {Journal of Imaging},
VOLUME = {6},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {137},
URL = {https://www.mdpi.com/2313-433X/6/12/137},
ISSN = {2313-433X},
ABSTRACT = {We developed a high-throughput mapping workflow, which centers on deep learning (DL) convolutional neural network (CNN) algorithms on high-performance distributed computing resources, to automatically characterize ice-wedge polygons (IWPs) from sub-meter resolution commercial satellite imagery. We applied a region-based CNN object instance segmentation algorithm, namely the Mask R-CNN, to automatically detect and classify IWPs in North Slope of Alaska. The central goal of our study was to systematically expound the DLCNN model interoperability across varying tundra types (sedge, tussock sedge, and non-tussock sedge) and image scene complexities to refine the understanding of opportunities and challenges for regional-scale mapping applications. We corroborated quantitative error statistics along with detailed visual inspections to gauge the IWP detection accuracies. We found promising model performances (detection accuracies: 89% to 96% and classification accuracies: 94% to 97%) for all candidate image scenes with varying tundra types. The mapping workflow discerned the IWPs by exhibiting low absolute mean relative error (AMRE) values (0.17&ndash;0.23). Results further suggest the importance of increasing the variability of training samples when practicing transfer-learning strategy to map IWPs across heterogeneous tundra cover types. Overall, our findings demonstrate the robust performances of IWPs mapping workflow in multiple tundra landscapes.},
DOI = {10.3390/jimaging6120137}
}



@Article{rs12244070,
AUTHOR = {Ells√§√üer, Florian and R√∂ll, Alexander and Ahongshangbam, Joyson and Waite, Pierre-Andr√© and Hendrayanto and Schuldt, Bernhard and H√∂lscher, Dirk},
TITLE = {Predicting Tree Sap Flux and Stomatal Conductance from Drone-Recorded Surface Temperatures in a Mixed Agroforestry System‚ÄîA Machine Learning Approach},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4070},
URL = {https://www.mdpi.com/2072-4292/12/24/4070},
ISSN = {2072-4292},
ABSTRACT = {Plant transpiration is a key element in the hydrological cycle. Widely used methods for its assessment comprise sap flux techniques for whole-plant transpiration and porometry for leaf stomatal conductance. Recently emerging approaches based on surface temperatures and a wide range of machine learning techniques offer new possibilities to quantify transpiration. The focus of this study was to predict sap flux and leaf stomatal conductance based on drone-recorded and meteorological data and compare these predictions with in-situ measured transpiration. To build the prediction models, we applied classical statistical approaches and machine learning algorithms. The field work was conducted in an oil palm agroforest in lowland Sumatra. Random forest predictions yielded the highest congruence with measured sap flux (r2 = 0.87 for trees and r2 = 0.58 for palms) and confidence intervals for intercept and slope of a Passing-Bablok regression suggest interchangeability of the methods. Differences in model performance are indicated when predicting different tree species. Predictions for stomatal conductance were less congruent for all prediction methods, likely due to spatial and temporal offsets of the measurements. Overall, the applied drone and modelling scheme predicts whole-plant transpiration with high accuracy. We conclude that there is large potential in machine learning approaches for ecological applications such as predicting transpiration.},
DOI = {10.3390/rs12244070}
}



@Article{rs12244080,
AUTHOR = {Kavats, Olena and Khramov, Dmitriy and Sergieieva, Kateryna and Vasyliev, Volodymyr},
TITLE = {Monitoring of Sugarcane Harvest in Brazil Based on Optical and SAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4080},
URL = {https://www.mdpi.com/2072-4292/12/24/4080},
ISSN = {2072-4292},
ABSTRACT = {The algorithms for determining sugarcane harvest dates are proposed; the algorithms allow the ability to monitor large areas and are based on the publicly available Synthetic Aperture Radar (SAR) and optical satellite data. Algorithm 1 uses the NDVI (Normalized Difference Vegetation Index) time series derived from Sentinel-2 data. Sharp and continuous decrease in the NDVI values is the main sign of sugarcane harvest. The NDVI time series allows the ability to determine most harvest dates. The best estimates of the sugarcane areas harvested per month have been obtained from March to August 2018 when cloudy pixel percentage is less than 45% of the image area. Algorithm 2 of the harvest monitoring uses the coherence time series derived from Sentinel-1 Single Look Complex (SLC) images and optical satellite data. Low coherence, demonstrating sharp growth upon the harvest completion, corresponds to the harvest period. The NDVI time series trends were used to refine the algorithm. It is supposed that the descending NDVI trend corresponds to harvest. The algorithms were used to identify the harvest dates and calculate the harvested areas of the reference sample of 574 sugarcane parcels with a total area of 3745 ha in the state of S&atilde;o Paulo, Brazil. The harvested areas identified by visual interpretation coincide with the optical-data algorithm (algorithm 1) by 97%; the coincidence with the algorithm based on SAR and optical data (algorithm 2) is 90%. The main practical applications of the algorithms are harvest monitoring and identification of the harvested fields to estimate the harvested area.},
DOI = {10.3390/rs12244080}
}



@Article{s20247176,
AUTHOR = {Gou, Huabei and Guo, Xiao and Lou, Wenjie and Ou, Jiajun and Yuan, Jiace},
TITLE = {Path Following Control for Underactuated Airships with Magnitude and Rate Saturation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7176},
URL = {https://www.mdpi.com/1424-8220/20/24/7176},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a reinforcement learning (RL) based path following strategy for underactuated airships with magnitude and rate saturation. The Markov decision process (MDP) model for the control problem is established. Then an error bounded line-of-sight (LOS) guidance law is investigated to restrain the state space. Subsequently, a proximal policy optimization (PPO) algorithm is employed to approximate the optimal action policy through trial and error. Since the optimal action policy is generated from the action space, the magnitude and rate saturation can be avoided. The simulation results, involving circular, general, broken-line, and anti-wind path following tasks, demonstrate that the proposed control scheme can transfer to new tasks without adaptation, and possesses satisfying real-time performance and robustness.},
DOI = {10.3390/s20247176}
}



@Article{rs12244091,
AUTHOR = {Hussain, Nazar and Farooque, Aitazaz A. and Schumann, Arnold W. and McKenzie-Gopsill, Andrew and Esau, Travis and Abbas, Farhat and Acharya, Bishnu and Zaman, Qamar},
TITLE = {Design and Development of a Smart Variable Rate Sprayer Using Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4091},
URL = {https://www.mdpi.com/2072-4292/12/24/4091},
ISSN = {2072-4292},
ABSTRACT = {The uniform application (UA) of agrochemicals results in the over-application of harmful chemicals, increases crop input costs, and deteriorates the environment when compared with variable rate application (VA). A smart variable rate sprayer (SVRS) was designed, developed, and tested using deep learning (DL) for VA application of agrochemicals. Real-time testing of the SVRS took place for detecting and spraying and/or skipping lambsquarters weed and early blight infected and healthy potato plants. About 24,000 images were collected from potato fields in Prince Edward Island and New Brunswick under varying sunny, cloudy, and partly cloudy conditions and processed/trained using YOLOv3 and tiny-YOLOv3 models. Due to faster performance, the tiny-YOLOv3 was chosen to deploy in SVRS. A laboratory experiment was designed under factorial arrangements, where the two spraying techniques (UA and VA) and the three weather conditions (cloudy, partly cloudy, and sunny) were the two independent variables with spray volume consumption as a response variable. The experimental treatments had six repetitions in a 2 √ó 3 factorial design. Results of the two-way ANOVA showed a significant effect of spraying application techniques on volume consumption of spraying liquid (p-value &lt; 0.05). There was no significant effect of weather conditions and interactions between the two independent variables on volume consumption during weeds and simulated diseased plant detection experiments (p-value &gt; 0.05). The SVRS was able to save 42 and 43% spraying liquid during weeds and simulated diseased plant detection experiments, respectively. Water sensitive papers‚Äô analysis showed the applicability of SVRS for VA with &gt;40% savings of spraying liquid by SVRS when compared with UA. Field applications of this technique would reduce the crop input costs and the environmental risks in conditions (weed and disease) like experimental testing.},
DOI = {10.3390/rs12244091}
}



@Article{rs12244104,
AUTHOR = {Chadwick, Andrew J. and Goodbody, Tristan R. H. and Coops, Nicholas C. and Hervieux, Anne and Bater, Christopher W. and Martens, Lee A. and White, Barry and R√∂eser, Dominik},
TITLE = {Automatic Delineation and Height Measurement of Regenerating Conifer Crowns under Leaf-Off Conditions Using UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4104},
URL = {https://www.mdpi.com/2072-4292/12/24/4104},
ISSN = {2072-4292},
ABSTRACT = {The increasing use of unmanned aerial vehicles (UAV) and high spatial resolution imagery from associated sensors necessitates the continued advancement of efficient means of image processing to ensure these tools are utilized effectively. This is exemplified in the field of forest management, where the extraction of individual tree crown information stands to benefit operational budgets. We explored training a region-based convolutional neural network (Mask R-CNN) to automatically delineate individual tree crown (ITC) polygons in regenerating forests (14 years after harvest) using true colour red-green-blue (RGB) imagery with an average ground sampling distance (GSD) of 3 cm. We predicted ITC polygons to extract height information using canopy height models generated from digital aerial photogrammetric (DAP) point clouds. Our approach yielded an average precision of 0.98, an average recall of 0.85, and an average F1 score of 0.91 for the delineation of ITC. Remote height measurements were strongly correlated with field height measurements (r2 = 0.93, RMSE = 0.34 m). The mean difference between DAP-derived and field-collected height measurements was &minus;0.37 m and &minus;0.24 m for white spruce (Picea glauca) and lodgepole pine (Pinus contorta), respectively. Our results show that accurate ITC delineation in young, regenerating stands is possible with fine-spatial resolution RGB imagery and that predicted ITC can be used in combination with DAP to estimate tree height.},
DOI = {10.3390/rs12244104}
}



@Article{electronics9122156,
AUTHOR = {Li, Shunli and Zhang, Qiuyi and Li, Jinlun and Zhao, Hongxin and Yin, Xiaoxing and Yang, Mei},
TITLE = {Monopulse Antenna Based on Singular Spoof Surface Plasmon Polariton Structure for Angle Measurement},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2156},
URL = {https://www.mdpi.com/2079-9292/9/12/2156},
ISSN = {2079-9292},
ABSTRACT = {Direction finding and target tracking make demanding requirements on the measurement of incoming angles of electromagnetic waves. A monopulse antenna, based on the singular symmetric spoof surface plasmon polariton (SSPP) structure, is proposed for high-accuracy angle sensing. The singular SSPP structure is composed of periodic corrugated grooves for the confinement of the electromagnetic fields. Due to the microstrip&ndash;coplanar waveguide transition, the fields along both sides of the SSPP add constructively to form the endfire beam at the sum port and destructively to form the null radiation in the endfire direction at the difference port. An optimization based on the team progress algorithm is adopted to facilitate this antenna design. A prototype is designed and fabricated to validate the design principle, and measured results agree with the simulation. The proposed antenna shows a wide bandwidth ranging from 5.0 GHz to 7.5 GHz for both the sum and difference ports with the return loss greater than 10 dB, realizing a relative bandwidth of 40%. The isolation for the sum and difference ports is higher than 21 dB, and the null depth is larger than 20 dB over the entire operating range, which is favorable for the high accuracy angle sensing and measurement. This monopulse antenna has broad prospect in angle measuring systems such as direction finding and radar tracking scenes.},
DOI = {10.3390/electronics9122156}
}



@Article{rs12244118,
AUTHOR = {Wang, Nan and Xue, Jie and Peng, Jie and Biswas, Asim and He, Yong and Shi, Zhou},
TITLE = {Integrating Remote Sensing and Landscape Characteristics to Estimate Soil Salinity Using Machine Learning Methods: A Case Study from Southern Xinjiang, China},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4118},
URL = {https://www.mdpi.com/2072-4292/12/24/4118},
ISSN = {2072-4292},
ABSTRACT = {Soil salinization, one of the most severe global land degradation problems, leads to the loss of arable land and declines in crop yields. Monitoring the distribution of salinized soil and degree of salinization is critical for management, remediation, and utilization of salinized soil; however, there is a lack of thorough assessment of various data sources including remote sensing and landscape characteristics for estimating soil salinity in arid and semi-arid areas. The overall goal of this study was to develop a framework for estimating soil salinity in diverse landscapes by fusing information from satellite images, landscape characteristics, and appropriate machine learning models. To explore the spatial distribution of soil salinity in southern Xinjiang, China, as a case study, we obtained 151 soil samples in a field campaign, which were analyzed in laboratory for soil electrical conductivity. A total of 35 indices including remote sensing classifiers (11), terrain attributes (3), vegetation spectral indices (8), and salinity spectral indices (13) were calculated or derived and correlated with soil salinity. Nine were used to model and estimate soil salinity using four predictive modelling approaches: partial least squares regression (PLSR), convolutional neural network (CNN), support vector machine (SVM) learning, and random forest (RF). Testing datasets were divided into vegetation-covered and bare soil samples and were used for accuracy assessment. The RF model was the best regression model in this study, with R2 = 0.75, and was most effective in revealing the spatial characteristics of salt distribution. Importance analysis and path modeling of independent variables indicated that environmental factors and soil salinity indices including digital elevation model (DEM), B10, and green atmospherically resistant vegetation index (GARI) showed the strongest contribution in soil salinity estimation. This showed a great promise in the measurement and monitoring of soil salinity in arid and semi-arid areas from the integration of remote sensing, landscape characteristics, and using machine learning model.},
DOI = {10.3390/rs12244118}
}



@Article{app10249013,
AUTHOR = {Manrique Escobar, Camilo Andr√©s and Pappalardo, Carmine Maria and Guida, Domenico},
TITLE = {A Parametric Study of a Deep Reinforcement Learning Control System Applied to the Swing-Up Problem of the Cart-Pole},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {9013},
URL = {https://www.mdpi.com/2076-3417/10/24/9013},
ISSN = {2076-3417},
ABSTRACT = {In this investigation, the nonlinear swing-up problem associated with the cart-pole system modeled as a multibody dynamical system is solved by developing a deep Reinforcement Learning (RL) controller. Furthermore, the sensitivity analysis of the deep RL controller applied to the cart-pole swing-up problem is carried out. To this end, the influence of modifying the physical properties of the system and the presence of dry friction forces are analyzed employing the cumulative reward during the task. Extreme limits for the modifications of the parameters are determined to prove that the neural network architecture employed in this work features enough learning capability to handle the task under modifications as high as 90% on the pendulum mass, as well as a 100% increment on the cart mass. As expected, the presence of dry friction greatly affects the performance of the controller. However, a post-training of the agent in the modified environment takes only thirty-nine episodes to find the optimal control policy, resulting in a promising path for further developments of robust controllers.},
DOI = {10.3390/app10249013}
}



@Article{rs12244135,
AUTHOR = {Rajendran, Ganesh B. and Kumarasamy, Uma M. and Zarro, Chiara and Divakarachari, Parameshachari B. and Ullo, Silvia L.},
TITLE = {Land-Use and Land-Cover Classification Using a Human Group-Based Particle Swarm Optimization Algorithm with an LSTM Classifier on Hybrid Pre-Processing Remote-Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4135},
URL = {https://www.mdpi.com/2072-4292/12/24/4135},
ISSN = {2072-4292},
ABSTRACT = {Land-use and land-cover (LULC) classification using remote sensing imagery plays a vital role in many environment modeling and land-use inventories. In this study, a hybrid feature optimization algorithm along with a deep learning classifier is proposed to improve the performance of LULC classification, helping to predict wildlife habitat, deteriorating environmental quality, haphazard elements, etc. LULC classification is assessed using Sat 4, Sat 6 and Eurosat datasets. After the selection of remote-sensing images, normalization and histogram equalization methods are used to improve the quality of the images. Then, a hybrid optimization is accomplished by using the local Gabor binary pattern histogram sequence (LGBPHS), the histogram of oriented gradient (HOG) and Haralick texture features, for the feature extraction from the selected images. The benefits of this hybrid optimization are a high discriminative power and invariance to color and grayscale images. Next, a human group-based particle swarm optimization (PSO) algorithm is applied to select the optimal features, whose benefits are a fast convergence rate and ease of implementation. After selecting the optimal feature values, a long short-term memory (LSTM) network is utilized to classify the LULC classes. Experimental results showed that the human group-based PSO algorithm with a LSTM classifier effectively well differentiates the LULC classes in terms of classification accuracy, recall and precision. A maximum improvement of 6.03% on Sat 4 and 7.17% on Sat 6 in LULC classification is reached when the proposed human group-based PSO with LSTM is compared to individual LSTM, PSO with LSTM, and Human Group Optimization (HGO) with LSTM. Moreover, an improvement of 2.56% in accuracy is achieved, compared to the existing models, GoogleNet, Visual Geometric Group (VGG), AlexNet, ConvNet, when the proposed method is applied.},
DOI = {10.3390/rs12244135}
}



@Article{agronomy10121989,
AUTHOR = {Armenta-Medina, Dagoberto and Ramirez-delReal, Tania A. and Villanueva-V√°squez, Daniel and Mejia-Aguirre, Cristian},
TITLE = {Trends on Advanced Information and Communication Technologies for Improving Agricultural Productivities: A Bibliometric Analysis},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1989},
URL = {https://www.mdpi.com/2073-4395/10/12/1989},
ISSN = {2073-4395},
ABSTRACT = {In this work, an exhaustive revision is given of the literature associated with advanced information and communication technologies in agriculture within a window of 25 years using bibliometric tools enabled to detect of the main actors, structure, and dynamics in the scientific papers. The main findings are a trend of growth in the dynamics of publications associated with advanced information and communication technologies in agriculture productivity. Another assertion is that countries, like the USA, China, and Brazil, stand out in many publications due to allocating more resources to research, development, and agricultural productivity. In addition, the collaboration networks between countries are frequently in regions with closer cultural and idiomatic ties; additionally, terms&rsquo; occurrence are obtained with Louvain algorithm predominating four clusters: precision agriculture, smart agriculture, remote sensing, and climate smart agriculture. Finally, the thematic-map characterization with Callon&rsquo;s density and centrality is applied in three periods. The first period of thematic analysis shows a transition in detecting the variability of a nutrient, such as nitrogen, through the help of immature georeferenced techniques, towards greater remote sensing involvement. In the transition from the second to the third stage, the maturation of technologies, such as unmanned aerial vehicles, wireless sensor networks, and the machine learning area, is observed.},
DOI = {10.3390/agronomy10121989}
}



@Article{electronics9122178,
AUTHOR = {Lee, Hojun and Kang, Minhee and Song, Jaein and Hwang, Keeyeon},
TITLE = {The Detection of Black Ice Accidents for Preventative Automated Vehicles Using Convolutional Neural Networks},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2178},
URL = {https://www.mdpi.com/2079-9292/9/12/2178},
ISSN = {2079-9292},
ABSTRACT = {Automated Vehicles (AVs) are expected to dramatically reduce traffic accidents that have occurred when using human driving vehicles (HVs). However, despite the rapid development of AVs, accidents involving AVs can occur even in ideal situations. Therefore, in order to enhance their safety, &ldquo;preventive design&rdquo; for accidents is continuously required. Accordingly, the &ldquo;preventive design&rdquo; that prevents accidents in advance is continuously required to enhance the safety of AVs. Specially, black ice with characteristics that are difficult to identify with the naked eye&mdash;the main cause of major accidents in winter vehicles&mdash;is expected to cause serious injuries in the era of AVs, and measures are needed to prevent them. Therefore, this study presents a Convolutional Neural Network (CNN)-based black ice detection plan to prevent traffic accidents of AVs caused by black ice. Due to the characteristic of black ice that is formed only in a certain environment, we augmented image data and learned road environment images. Tests showed that the proposed CNN model detected black ice with 96% accuracy and reproducibility. It is expected that the CNN model for black ice detection proposed in this study will contribute to improving the safety of AVs and prevent black ice accidents in advance.},
DOI = {10.3390/electronics9122178}
}



@Article{rs12244149,
AUTHOR = {Samarin, Maxim and Zweifel, Lauren and Roth, Volker and Alewell, Christine},
TITLE = {Identifying Soil Erosion Processes in Alpine Grasslands on Aerial Imagery with a U-Net Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4149},
URL = {https://www.mdpi.com/2072-4292/12/24/4149},
ISSN = {2072-4292},
ABSTRACT = {Erosion in alpine grasslands is a major threat to ecosystem services of alpine soils. Natural causes for the occurrence of soil erosion are steep topography and prevailing climate conditions in combination with soil fragility. To increase our understanding of ongoing erosion processes and support sustainable land-use management, there is a need to acquire detailed information on spatial occurrence and temporal trends. Existing approaches to identify these trends are typically laborious, have lack of transferability to other regions, and are consequently only applicable to smaller regions. In order to overcome these limitations and create a sophisticated erosion monitoring tool capable of large-scale analysis, we developed a model based on U-Net, a fully convolutional neural network, to map different erosion processes on high-resolution aerial images (RGB, 0.25&ndash;0.5 m). U-Net was trained on a high-quality data set consisting of labeled erosion sites mapped with object-based image analysis (OBIA) for the Urseren Valley (Central Swiss Alps) for five aerial images (16 year period). We used the U-Net model to map the same study area and conduct quality assessments based on a held-out test region and a temporal transferability test on new images. Erosion classes are assigned according to their type (shallow landslide and sites with reduced vegetation affected by sheet erosion) or land-use impacts (livestock trails and larger management affected areas). We show that results obtained by OBIA and U-Net follow similar linear trends for the 16 year study period, exhibiting increases in total degraded area of 167% and 201%, respectively. Segmentations of eroded sites are generally in good agreement, but also display method-specific differences, which lead to an overall precision of 73%, a recall of 84%, and a F1-score of 78%. Our results show that U-Net is transferable to spatially (within our study area) and temporally unseen data (data from new years) and is therefore a method suitable to efficiently and successfully capture the temporal trends and spatial heterogeneity of degradation in alpine grasslands. Additionally, U-Net is a powerful and robust tool to map erosion sites in a predictive manner utilising large amounts of new aerial imagery.},
DOI = {10.3390/rs12244149}
}



@Article{rs12244143,
AUTHOR = {Hassanijalilian, Oveis and Igathinathane, C. and Bajwa, Sreekala and Nowatzki, John},
TITLE = {Rating Iron Deficiency in Soybean Using Image Processing and Decision-Tree Based Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {4143},
URL = {https://www.mdpi.com/2072-4292/12/24/4143},
ISSN = {2072-4292},
ABSTRACT = {The most efficient way of soybean (Glycine max (L.) Merrill) iron deficiency chlorosis (IDC) management is to select a tolerant cultivar suitable for the specific growing condition. These cultivars are selected by field experts based on IDC visual ratings. However, this visual rating method is laborious, expensive, time-consuming, subjective, and impractical on larger scales. Therefore, a modern digital image-based method using tree-based machine learning classifier models for rating soybean IDC at plot-scale was developed. Data were collected from soybean IDC cultivar trial plots. Images were processed with MATLAB and corrected for light intensity by using a standard color board in the image. The three machine learning models used in this study were decision tree (DT), random forest (RF), and adaptive boosting (AdaBoost). Calculated indices from images, such as dark green color index (DGCI), canopy size, and pixel counts into DGCI ranges and IDC visual scoring were used as input and target variables to train these models. Metrics such as precision, recall, and f1-score were used to assess the performance of the classifier models. Among all three models, AdaBoost had the best performance (average f1-score = 0.75) followed by RF and DT the least. Therefore, a ready-to-use methodology of image processing with AdaBoost model for soybean IDC rating was recommended. The developed method can be easily adapted to smartphone applications or scaled-up using images from aerial platforms.},
DOI = {10.3390/rs12244143}
}



@Article{ijgi9120759,
AUTHOR = {Zang, Yufu and Li, Bijun and Xiao, Xiongwu and Zhu, Jianfeng and Meng, Fancong},
TITLE = {An Efficient Probabilistic Registration Based on Shape Descriptor for Heritage Field Inspection},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {759},
URL = {https://www.mdpi.com/2220-9964/9/12/759},
ISSN = {2220-9964},
ABSTRACT = {Heritage documentation is implemented by digitally recording historical artifacts for the conservation and protection of these cultural heritage objects. As efficient spatial data acquisition tools, laser scanners have been widely used to collect highly accurate three-dimensional (3D) point clouds without damaging the original structure and the environment. To ensure the integrity and quality of the collected data, field inspection (i.e., on-spot checking the data quality) should be carried out to determine the need for additional measurements (i.e., extra laser scanning for areas with quality issues such as data missing and quality degradation). To facilitate inspection of all collected point clouds, especially checking the quality issues in overlaps between adjacent scans, all scans should be registered together. Thus, a point cloud registration method that is able to register scans fast and robustly is required. To fulfill the aim, this study proposes an efficient probabilistic registration for free-form cultural heritage objects by integrating the proposed principal direction descriptor and curve constraints. We developed a novel shape descriptor based on a local frame of principal directions. Within the frame, its density and distance feature images were generated to describe the shape of the local surface. We then embedded the descriptor into a probabilistic framework to reject ambiguous matches. Spatial curves were integrated as constraints to delimit the solution space. Finally, a multi-view registration was used to refine the position and orientation of each scan for the field inspection. Comprehensive experiments show that the proposed method was able to perform well in terms of rotation error, translation error, robustness, and runtime and outperformed some commonly used approaches.},
DOI = {10.3390/ijgi9120759}
}



@Article{s20247332,
AUTHOR = {Mawrence, Raphael and Munniks, Sandra and Valente, Jo√£o},
TITLE = {Calibration of Electrochemical Sensors for Nitrogen Dioxide Gas Detection Using Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {24},
ARTICLE-NUMBER = {7332},
URL = {https://www.mdpi.com/1424-8220/20/24/7332},
ISSN = {1424-8220},
ABSTRACT = {For years, urban air quality networks have been set up by private organizations and governments to monitor toxic gases like NO2. However, these networks can be very expensive to maintain, so their distribution is usually widely spaced, leaving gaps in the spatial resolution of the resulting air quality data. Recently, electrochemical sensors and their integration with unmanned aerial vehicles (UAVs) have attempted to fill these gaps through various experiments, none of which have considered the influence of a UAV when calibrating the sensors. Accordingly, this research attempts to improve the reliability of NO2 measurements detected from electrochemical sensors while on board an UAV by introducing rotor speed as part of the calibration model. This is done using a DJI Matrice 100 quadcopter and Alphasense sensors, which are calibrated using regression calculations in different environments. This produces a predictive r-squared up to 0.97. The sensors are then calibrated with rotor speed as an additional variable while on board the UAV and flown in a series of flights to evaluate the performance of the model, which produces a predictive r-squared up to 0.80. This methodological approach can be used to obtain more reliable NO2 measurements in future outdoor experiments that include electrochemical sensor integration with UAV&rsquo;s.},
DOI = {10.3390/s20247332}
}



@Article{agriculture10120653,
AUTHOR = {Bolfe, √âdson Luis and Jorge, L√∫cio Andr√© de Castro and Sanches, Ieda Del‚ÄôArco and Luchiari J√∫nior, Ariovaldo and da Costa, Cinthia Cabral and Victoria, Daniel de Castro and Inamasu, Ricardo Yassushi and Grego, C√©lia Regina and Ferreira, Victor Rodrigues and Ramirez, Andrea Restrepo},
TITLE = {Precision and Digital Agriculture: Adoption of Technologies and Perception of Brazilian Farmers},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {653},
URL = {https://www.mdpi.com/2077-0472/10/12/653},
ISSN = {2077-0472},
ABSTRACT = {The rapid population growth has driven the demand for more food, fiber, energy, and water, which is associated to an increase in the need to use natural resources in a more sustainable way. The use of precision agriculture machinery and equipment since the 1990s has provided important productive gains and maximized the use of agricultural inputs. The growing connectivity in the rural environment, in addition to its greater integration with data from sensor systems, remote sensors, equipment, and smartphones have paved the way for new concepts from the so-called Agriculture 4.0 or Digital Agriculture. This article presents the results of a survey carried out with 504 Brazilian farmers about the digital technologies in use, as well as current and future applications, perceived benefits, and challenges. The questionnaire was prepared, organized, and made available to the public through the online platform LimeSurvey and was available from 17 April to 2 June 2020. The primary data obtained for each question previously defined were consolidated and analyzed statistically. The results indicate that 84% of the interviewed farmers use at least one digital technology in their production system that differs according to technological complexity level. The main perceived benefit refers to the perception of increased productivity and the main challenges are the acquisition costs of machines, equipment, software, and connectivity. It is also noteworthy that 95% of farmers would like to learn more about new technologies to strengthen the agricultural development in their properties.},
DOI = {10.3390/agriculture10120653}
}



@Article{s21010014,
AUTHOR = {Dong, Mei and Wu, Hongyu and Hu, Hui and Azzam, Rafig and Zhang, Liang and Zheng, Zengrong and Gong, Xiaonan},
TITLE = {Deformation Prediction of Unstable Slopes Based on Real-Time Monitoring and DeepAR Model},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {14},
URL = {https://www.mdpi.com/1424-8220/21/1/14},
ISSN = {1424-8220},
ABSTRACT = {With increased urbanization, accidents related to slope instability are frequently encountered in construction sites. The deformation and failure mechanism of a landslide is a complex dynamic process, which seriously threatens people&rsquo;s lives and property. Currently, prediction and early warning of a landslide can be effectively performed by using Internet of Things (IoT) technology to monitor the landslide deformation in real time and an artificial intelligence algorithm to predict the deformation trend. However, if a slope failure occurs during the construction period, the builders and decision-makers find it challenging to effectively apply IoT technology to monitor the emergency and assist in proposing treatment measures. Moreover, for projects during operation (e.g., a motorway in a mountainous area), no recognized artificial intelligence algorithm exists that can forecast the deformation of steep slopes using the huge data obtained from monitoring devices. In this context, this paper introduces a real-time wireless monitoring system with multiple sensors for retrieving high-frequency overall data that can describe the deformation feature of steep slopes. The system was installed in the Qili connecting line of a motorway in Zhejiang Province, China, to provide a technical support for the design and implementation of safety solutions for the steep slopes. Most of the devices were retained to monitor the slopes even after construction. The machine learning Probabilistic Forecasting with Autoregressive Recurrent Networks (DeepAR) model based on time series and probabilistic forecasting was introduced into the project to predict the slope displacement. The predictive accuracy of the DeepAR model was verified by the mean absolute error, the root mean square error and the goodness of fit. This study demonstrates that the presented monitoring system and the introduced predictive model had good safety control ability during construction and good prediction accuracy during operation. The proposed approach will be helpful to assess the safety of excavated slopes before constructing new infrastructures.},
DOI = {10.3390/s21010014}
}



@Article{rs13010039,
AUTHOR = {Carvalho, Osmar Luiz Ferreira de and de Carvalho J√∫nior, Osmar Ab√≠lio and Albuquerque, Anesmar Olino de and Bem, Pablo Pozzobon de and Silva, Cristiano Rosa and Ferreira, Pedro Henrique Guimar√£es and Moura, Rebeca dos Santos de and Gomes, Roberto Arnaldo Trancoso and Guimar√£es, Renato Fontes and Borges, D√≠bio Leandro},
TITLE = {Instance Segmentation for Large, Multi-Channel Remote Sensing Imagery Using Mask-RCNN and a Mosaicking Approach},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2072-4292/13/1/39},
ISSN = {2072-4292},
ABSTRACT = {Instance segmentation is the state-of-the-art in object detection, and there are numerous applications in remote sensing data where these algorithms can produce significant results. Nevertheless, one of the main problems is that most algorithms use Red, Green, and Blue (RGB) images, whereas Satellite images often present more channels that can be crucial to improve performance. Therefore, the present work brings three contributions: (a) conversion system from ground truth polygon data into the Creating Common Object in Context (COCO) annotation format; (b) Detectron2 software source code adaptation and application on multi-channel imagery; and (c) large scene image mosaicking. We applied the procedure in a Center Pivot Irrigation System (CPIS) dataset with ground truth produced by the Brazilian National Water Agency (ANA) and Landsat-8 Operational Land Imager (OLI) imagery (7 channels with 30-m resolution). Center pivots are a modern irrigation system technique with massive growth potential in Brazil and other world areas. The round shapes with different textures, colors, and spectral behaviors make it appropriate to use Deep Learning instance segmentation. We trained the model using 512 &times; 512-pixel sized patches using seven different backbone structures (ResNet50- Feature Pyramid Network (FPN), Resnet50-DC5, ResNet50-C4, Resnet101-FPN, Resnet101-DC5, ResNet101-FPN, and ResNeXt101-FPN). The model evaluation used standard COCO metrics (Average Precision (AP), AP50, AP75, APsmall, APmedium, and AR100). ResNeXt101-FPN had the best results, with a 3% advantage over the second-best model (ResNet101-FPN). We also compared the ResNeXt101-FPN model in the seven-channel and RGB imagery, where the multi-channel model had a 3% advantage, demonstrating great improvement using a larger number of channels. This research is also the first with a mosaicking algorithm using instance segmentation models, where we tested in a 1536 &times; 1536-pixel image using a non-max suppression sorted by area method. The proposed methodology is innovative and suitable for many other remote sensing problems and medical imagery that often present more channels.},
DOI = {10.3390/rs13010039}
}



@Article{rs13010052,
AUTHOR = {Maung, Win Sithu and Sasaki, Jun},
TITLE = {Assessing the Natural Recovery of Mangroves after Human Disturbance Using Neural Network Classification and Sentinel-2 Imagery in Wunbaik Mangrove Forest, Myanmar},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2072-4292/13/1/52},
ISSN = {2072-4292},
ABSTRACT = {In this study, we examined the natural recovery of mangroves in abandoned shrimp ponds located in the Wunbaik Mangrove Forest (WMF) in Myanmar using artificial neural network (ANN) classification and a change detection approach with Sentinel-2 satellite images. In 2020, we conducted various experiments related to mangrove classification by tuning input features and hyper-parameters. The selected ANN model was used with a transfer learning approach to predict the mangrove distribution in 2015. Changes were detected using classification results from 2015 and 2020. Naturally recovering mangroves were identified by extracting the change detection results of three abandoned shrimp ponds selected during field investigation. The proposed method yielded an overall accuracy of 95.98%, a kappa coefficient of 0.92, mangrove and non-mangrove precisions of 0.95 and 0.98, respectively, recalls of 0.96, and F1 scores of 0.96 for the 2020 classification. For the 2015 prediction, transfer learning improved model performance, resulting in an overall accuracy of 97.20%, a kappa coefficient of 0.94, mangrove and non-mangrove precisions of 0.98 and 0.96, respectively, recalls of 0.98 and 0.97, and F1 scores of 0.96. The change detection results showed that mangrove forests in the WMF slightly decreased between 2015 and 2020. Naturally recovering mangroves were detected at approximately 50% of each abandoned site within a short abandonment period. This study demonstrates that the ANN method using Sentinel-2 imagery and topographic and canopy height data can produce reliable results for mangrove classification. The natural recovery of mangroves presents a valuable opportunity for mangrove rehabilitation at human-disturbed sites in the WMF.},
DOI = {10.3390/rs13010052}
}



@Article{en14010077,
AUTHOR = {Abbas, Ghulam and Asad, Muhammad Usman and Gu, Jason and Alelyani, Salem and Balas, Valentina E. and Rashid Hussain, Mohammad and Farooq, Umar and Awan, Ahmed Bilal and Raza, Ali and Chang, Chunqi},
TITLE = {Multivariable Unconstrained Pattern Search Method for Optimizing Digital PID Controllers Applied to Isolated Forward Converter},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {77},
URL = {https://www.mdpi.com/1996-1073/14/1/77},
ISSN = {1996-1073},
ABSTRACT = {Most of the traditional PID tuning methods are heuristic in nature. The heuristic approach-based tuned PID controllers show only nominal performance. In addition, in the case of a digital redesign approach, mapping of the heuristically-designed continuous-time PID controllers into discrete-time PID controllers and in case of the direct digital design approach, mapping of the continuous-time plant (forward converter) into the discrete-time plant, results in frequency distortion (or warping). Besides this, nonlinear elements such as ADC and DAC, and delay in the digital control loop deteriorate the control performance. There is a need to tune conventionally-designed digital controllers to enhance performance. This paper proposes optimized discrete-time PID controllers for a forward DC&ndash;DC converter operating in continuous conduction mode (CCM). The considered conventional digital PID controllers designed on the basis of the digital redesign and direct digital approaches are tuned by one of the multivariable unconstrained pattern search methods named Hooke&ndash;Jeeves (H&ndash;J) search method to ensure excellent output voltage regulation performance against the changes in input voltage and load current. Numerical results show that the H&ndash;J-based optimized PID compensated forward converter system shows tremendous improvement in performance compared to its unoptimized counterpart and simulated annealing (SA)-based compensated system, thus justifying the applicability of the H&ndash;J method for enhancing the performance.},
DOI = {10.3390/en14010077}
}



@Article{rs13010054,
AUTHOR = {Biffi, Leonardo Joso√© and Mitishita, Edson and Liesenberg, Veraldo and Santos, Anderson Aparecido dos and Gon√ßalves, Diogo Nunes and Estrabis, Nayara Vasconcelos and Silva, Jonathan de Andrade and Osco, Lucas Prado and Ramos, Ana Paula Marques and Centeno, Jorge Antonio Silva and Schimalski, Marcos Benedito and Rufato, Leo and Neto, S√≠lvio Lu√≠s Rafaeli and Marcato Junior, Jos√© and Gon√ßalves, Wesley Nunes},
TITLE = {ATSS Deep Learning-Based Approach to Detect Apple Fruits},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2072-4292/13/1/54},
ISSN = {2072-4292},
ABSTRACT = {In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. Specifically, in fruit detection problems, several recent works were developed using Deep Learning (DL) methods applied in images acquired in different acquisition levels. However, the increasing use of anti-hail plastic net cover in commercial orchards highlights the importance of terrestrial remote sensing systems. Apples are one of the most highly-challenging fruits to be detected in images, mainly because of the target occlusion problem occurrence. Additionally, the introduction of high-density apple tree orchards makes the identification of single fruits a real challenge. To support farmers to detect apple fruits efficiently, this paper presents an approach based on the Adaptive Training Sample Selection (ATSS) deep learning method applied to close-range and low-cost terrestrial RGB images. The correct identification supports apple production forecasting and gives local producers a better idea of forthcoming management practices. The main advantage of the ATSS method is that only the center point of the objects is labeled, which is much more practicable and realistic than bounding-box annotations in heavily dense fruit orchards. Additionally, we evaluated other object detection methods such as RetinaNet, Libra Regions with Convolutional Neural Network (R-CNN), Cascade R-CNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), and High-Resolution Network (HRNet). The study area is a highly-dense apple orchard consisting of Fuji Suprema apple fruits (Malus domestica Borkh) located in a smallholder farm in the state of Santa Catarina (southern Brazil). A total of 398 terrestrial images were taken nearly perpendicularly in front of the trees by a professional camera, assuring both a good vertical coverage of the apple trees in terms of heights and overlapping between picture frames. After, the high-resolution RGB images were divided into several patches for helping the detection of small and/or occluded apples. A total of 3119, 840, and 2010 patches were used for training, validation, and testing, respectively. Moreover, the proposed method&rsquo;s generalization capability was assessed by applying simulated image corruptions to the test set images with different severity levels, including noise, blurs, weather, and digital processing. Experiments were also conducted by varying the bounding box size (80, 100, 120, 140, 160, and 180 pixels) in the image original for the proposed approach. Our results showed that the ATSS-based method slightly outperformed all other deep learning methods, between 2.4% and 0.3%. Also, we verified that the best result was obtained with a bounding box size of 160 &times; 160 pixels. The proposed method was robust regarding most of the corruption, except for snow, frost, and fog weather conditions. Finally, a benchmark of the reported dataset is also generated and publicly available.},
DOI = {10.3390/rs13010054}
}



@Article{rs13010071,
AUTHOR = {Xu, Zhiyong and Zhang, Weicun and Zhang, Tianxiang and Li, Jiangyun},
TITLE = {HRCNet: High-Resolution Context Extraction Network for Semantic Segmentation of Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {71},
URL = {https://www.mdpi.com/2072-4292/13/1/71},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation is a significant method in remote sensing image (RSIs) processing and has been widely used in various applications. Conventional convolutional neural network (CNN)-based semantic segmentation methods are likely to lose the spatial information in the feature extraction stage and usually pay little attention to global context information. Moreover, the imbalance of category scale and uncertain boundary information meanwhile exists in RSIs, which also brings a challenging problem to the semantic segmentation task. To overcome these problems, a high-resolution context extraction network (HRCNet) based on a high-resolution network (HRNet) is proposed in this paper. In this approach, the HRNet structure is adopted to keep the spatial information. Moreover, the light-weight dual attention (LDA) module is designed to obtain global context information in the feature extraction stage and the feature enhancement feature pyramid (FEFP) structure is promoted and employed to fuse the contextual information of different scales. In addition, to achieve the boundary information, we design the boundary aware (BA) module combined with the boundary aware loss (BAloss) function. The experimental results evaluated on Potsdam and Vaihingen datasets show that the proposed approach can significantly improve the boundary and segmentation performance up to 92.0% and 92.3% on overall accuracy scores, respectively. As a consequence, it is envisaged that the proposed HRCNet model will be an advantage in remote sensing images segmentation.},
DOI = {10.3390/rs13010071}
}



@Article{electronics10010027,
AUTHOR = {Mun, Hyunsu and Lee, Youngseok},
TITLE = {Internet Traffic Classification with Federated Learning},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {27},
URL = {https://www.mdpi.com/2079-9292/10/1/27},
ISSN = {2079-9292},
ABSTRACT = {As Internet traffic classification is a typical problem for ISPs or mobile carriers, there have been a lot of studies based on statistical packet header information, deep packet inspection, or machine learning. Due to recent advances in end-to-end encryption and dynamic port policies, machine or deep learning has been an essential key to improve the accuracy of packet classification. In addition, ISPs or mobile carriers should carefully deal with the privacy issue while collecting user packets for accounting or security. The recent development of distributed machine learning, called federated learning, collaboratively carries out machine learning jobs on the clients without uploading data to a central server. Although federated learning provides an on-device learning framework towards user privacy protection, its feasibility and performance of Internet traffic classification have not been fully examined. In this paper, we propose a federated-learning traffic classification protocol (FLIC), which can achieve an accuracy comparable to centralized deep learning for Internet application identification without privacy leakage. FLIC can classify new applications on-the-fly when a participant joins in learning with a new application, which has not been done in previous works. By implementing the prototype of FLIC clients and a server with TensorFlow, the clients gather packets, perform the on-device training job and exchange the training results with the FLIC server. In addition, we demonstrate that federated learning-based packet classification achieves an accuracy of 88% under non-independent and identically distributed (non-IID) traffic across clients. When a new application that can be classified dynamically as a client participates in learning was added, an accuracy of 92% was achieved.},
DOI = {10.3390/electronics10010027}
}



@Article{s21010210,
AUTHOR = {Park, Dongsuk and Lee, Seungeui and Park, SeongUk and Kwak, Nojun},
TITLE = {Radar-Spectrogram-Based UAV Classification Using Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {210},
URL = {https://www.mdpi.com/1424-8220/21/1/210},
ISSN = {1424-8220},
ABSTRACT = {With the upsurge in the use of Unmanned Aerial Vehicles (UAVs) in various fields, detecting and identifying them in real-time are becoming important topics. However, the identification of UAVs is difficult due to their characteristics such as low altitude, slow speed, and small radar cross-section (LSS). With the existing deterministic approach, the algorithm becomes complex and requires a large number of computations, making it unsuitable for real-time systems. Hence, effective alternatives enabling real-time identification of these new threats are needed. Deep learning-based classification models learn features from data by themselves and have shown outstanding performance in computer vision tasks. In this paper, we propose a deep learning-based classification model that learns the micro-Doppler signatures (MDS) of targets represented on radar spectrogram images. To enable this, first, we recorded five LSS targets (three types of UAVs and two different types of human activities) with a frequency modulated continuous wave (FMCW) radar in various scenarios. Then, we converted signals into spectrograms in the form of images by Short time Fourier transform (STFT). After the data refinement and augmentation, we made our own radar spectrogram dataset. Secondly, we analyzed characteristics of the radar spectrogram dataset with the ResNet-18 model and designed the ResNet-SP model with less computation, higher accuracy and stability based on the ResNet-18 model. The results show that the proposed ResNet-SP has a training time of 242 s and an accuracy of 83.39%, which is superior to the ResNet-18 that takes 640 s for training with an accuracy of 79.88%.},
DOI = {10.3390/s21010210}
}



@Article{e23010056,
AUTHOR = {Niu, Haoyu and Wei, Jiamin and Chen, YangQuan},
TITLE = {Optimal Randomness for Stochastic Configuration Network (SCN) with Heavy-Tailed Distributions},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {56},
URL = {https://www.mdpi.com/1099-4300/23/1/56},
PubMedID = {33396383},
ISSN = {1099-4300},
ABSTRACT = {Stochastic Configuration Network (SCN) has a powerful capability for regression and classification analysis. Traditionally, it is quite challenging to correctly determine an appropriate architecture for a neural network so that the trained model can achieve excellent performance for both learning and generalization. Compared with the known randomized learning algorithms for single hidden layer feed-forward neural networks, such as Randomized Radial Basis Function (RBF) Networks and Random Vector Functional-link (RVFL), the SCN randomly assigns the input weights and biases of the hidden nodes in a supervisory mechanism. Since the parameters in the hidden layers are randomly generated in uniform distribution, hypothetically, there is optimal randomness. Heavy-tailed distribution has shown optimal randomness in an unknown environment for finding some targets. Therefore, in this research, the authors used heavy-tailed distributions to randomly initialize weights and biases to see if the new SCN models can achieve better performance than the original SCN. Heavy-tailed distributions, such as L&eacute;vy distribution, Cauchy distribution, and Weibull distribution, have been used. Since some mixed distributions show heavy-tailed properties, the mixed Gaussian and Laplace distributions were also studied in this research work. Experimental results showed improved performance for SCN with heavy-tailed distributions. For the regression model, SCN-L&eacute;vy, SCN-Mixture, SCN-Cauchy, and SCN-Weibull used less hidden nodes to achieve similar performance with SCN. For the classification model, SCN-Mixture, SCN-L&eacute;vy, and SCN-Cauchy have higher test accuracy of 91.5%, 91.7% and 92.4%, respectively. Both are higher than the test accuracy of the original SCN.},
DOI = {10.3390/e23010056}
}



@Article{agriculture11010022,
AUTHOR = {Rahman, Mohammad Fatin Fatihur and Fan, Shurui and Zhang, Yan and Chen, Lei},
TITLE = {A Comparative Study on Application of Unmanned Aerial Vehicle Systems in Agriculture},
JOURNAL = {Agriculture},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {22},
URL = {https://www.mdpi.com/2077-0472/11/1/22},
ISSN = {2077-0472},
ABSTRACT = {Presently in agriculture, there is much ample scope for drone and UAS (Unmanned Aircraft System) development. Because of their low cost and small size, these devices have the ability to help many developing countries with economic prosperity. The entire aggregation of financial investments in the agricultural area has increased appreciably in recent years. Sooth to say, agriculture remains a massive part of the world&rsquo;s commercial growth, and due to some complications, the agriculture fields withstand massive losses. Pets and destructive insects seem to be the primary reasons for certain degenerative diseases. It minimizes the potential productivity of the crops. For increasing the quality of the plants, fertilizers and pesticides are appropriately applied. Using UAVs (Unmanned Aerial Vehicles) for spraying pesticides and fertilizing materials is an exuberant contraption. It adequately reduces the rate of health dilemma and the number of workers, which is quite an impressive landmark. Willing producers are also adopting UAVs in agriculture to soil and field analysis, seed sowing, lessen the time and costs correlated with crop scouting, and field mapping. It is rapid, and it can sensibly diminish a farmer&rsquo;s workload, which is significantly a part of the agricultural revolution. This article aims to proportionally represent the concept of agricultural purposed UAV clear to the neophytes. First, this paper outlines the harmonic framework of the agricultural UAV, and then it abundantly illustrates the methods and materials. Finally, the article portrays the outcome.},
DOI = {10.3390/agriculture11010022}
}



@Article{rs13010123,
AUTHOR = {Guo, Anting and Huang, Wenjiang and Dong, Yingying and Ye, Huichun and Ma, Huiqin and Liu, Bo and Wu, Wenbin and Ren, Yu and Ruan, Chao and Geng, Yun},
TITLE = {Wheat Yellow Rust Detection Using UAV-Based Hyperspectral Technology},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {123},
URL = {https://www.mdpi.com/2072-4292/13/1/123},
ISSN = {2072-4292},
ABSTRACT = {Yellow rust is a worldwide disease that poses a serious threat to the safety of wheat production. Numerous studies on near-surface hyperspectral remote sensing at the leaf scale have achieved good results for disease monitoring. The next step is to monitor the disease at the field scale, which is of great significance for disease control. In our study, an unmanned aerial vehicle (UAV) equipped with a hyperspectral sensor was used to obtain hyperspectral images at the field scale. Vegetation indices (VIs) and texture features (TFs) extracted from the UAV-based hyperspectral images and their combination were used to establish partial least-squares regression (PLSR)-based disease monitoring models in different infection periods. In addition, we resampled the original images with 1.2 cm spatial resolution to images with different spatial resolutions (3 cm, 5 cm, 7 cm, 10 cm, 15 cm, and 20 cm) to evaluate the effect of spatial resolution on disease monitoring accuracy. The findings showed that the VI-based model had the highest monitoring accuracy (R2 = 0.75) in the mid-infection period. The TF-based model could be used to monitor yellow rust at the field scale and obtained the highest R2 in the mid- and late-infection periods (0.65 and 0.82, respectively). The VI-TF-based models had the highest accuracy in each infection period and outperformed the VI-based or TF-based models. The spatial resolution had a negligible influence on the VI-based monitoring accuracy, but significantly influenced the TF-based monitoring accuracy. Furthermore, the optimal spatial resolution for monitoring yellow rust using the VI-TF-based model in each infection period was 10 cm. The findings provide a reference for accurate disease monitoring using UAV hyperspectral images.},
DOI = {10.3390/rs13010123}
}



@Article{s21010231,
AUTHOR = {Jiang, Weiheng and Wu, Xiaogang and Wang, Yimou and Chen, Bolin and Feng, Wenjiang and Jin, Yi},
TITLE = {Time‚ÄìFrequency-Analysis-Based Blind Modulation Classification for Multiple-Antenna Systems},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {231},
URL = {https://www.mdpi.com/1424-8220/21/1/231},
PubMedID = {33401416},
ISSN = {1424-8220},
ABSTRACT = {Blind modulation classification is an important step in implementing cognitive radio networks. The multiple-input multiple-output (MIMO) technique is widely used in military and civil communication systems. Due to the lack of prior information about channel parameters and the overlapping of signals in MIMO systems, the traditional likelihood-based and feature-based approaches cannot be applied in these scenarios directly. Hence, in this paper, to resolve the problem of blind modulation classification in MIMO systems, the time&ndash;frequency analysis method based on the windowed short-time Fourier transform was used to analyze the time&ndash;frequency characteristics of time-domain modulated signals. Then, the extracted time&ndash;frequency characteristics are converted into red&ndash;green&ndash;blue (RGB) spectrogram images, and the convolutional neural network based on transfer learning was applied to classify the modulation types according to the RGB spectrogram images. Finally, a decision fusion module was used to fuse the classification results of all the receiving antennas. Through simulations, we analyzed the classification performance at different signal-to-noise ratios (SNRs); the results indicate that, for the single-input single-output (SISO) network, our proposed scheme can achieve 92.37% and 99.12% average classification accuracy at SNRs of &minus;4 and 10 dB, respectively. For the MIMO network, our scheme achieves 80.42% and 87.92% average classification accuracy at &minus;4 and 10 dB, respectively. The proposed method greatly improves the accuracy of modulation classification in MIMO networks.},
DOI = {10.3390/s21010231}
}



@Article{land10010029,
AUTHOR = {Papp, Levente and van Leeuwen, Boudewijn and Szilassi, P√©ter and Tobak, Zal√°n and Szatm√°ri, J√≥zsef and √Årvai, M√°ty√°s and M√©sz√°ros, J√°nos and P√°sztor, L√°szl√≥},
TITLE = {Monitoring Invasive Plant Species Using Hyperspectral Remote Sensing Data},
JOURNAL = {Land},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {29},
URL = {https://www.mdpi.com/2073-445X/10/1/29},
ISSN = {2073-445X},
ABSTRACT = {The species richness and biodiversity of vegetation in Hungary are increasingly threatened by invasive plant species brought in from other continents and foreign ecosystems. These invasive plant species have spread aggressively in the natural and semi-natural habitats of Europe. Common milkweed (Asclepias syriaca) is one of the species that pose the greatest ecological menace. Therefore, the primary purpose of the present study is to map and monitor the spread of common milkweed, the most common invasive plant species in Europe. Furthermore, the possibilities to detect and validate this special invasive plant by analyzing hyperspectral remote sensing data were investigated. In combination with field reference data, high-resolution hyperspectral aerial images acquired by an unmanned aerial vehicle (UAV) platform in 138 spectral bands in areas infected by common milkweed were examined. Then, support vector machine (SVM) and artificial neural network (ANN) classification algorithms were applied to the highly accurate field reference data. As a result, common milkweed individuals were distinguished in hyperspectral images, achieving an overall accuracy of 92.95% in the case of supervised SVM classification. Using the ANN model, an overall accuracy of 99.61% was achieved. To evaluate the proposed approach, two experimental tests were conducted, and in both cases, we managed to distinguish the individual specimens within the large variety of spreading invasive species in a study area of 2 ha, based on centimeter spatial resolution hyperspectral UAV imagery.},
DOI = {10.3390/land10010029}
}



@Article{app11010363,
AUTHOR = {Rold√°n-G√≥mez, Juan Jes√∫s and Gonz√°lez-Gironda, Eduardo and Barrientos, Antonio},
TITLE = {A Survey on Robotic Technologies for Forest Firefighting: Applying Drone Swarms to Improve Firefighters‚Äô Efficiency and Safety},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {363},
URL = {https://www.mdpi.com/2076-3417/11/1/363},
ISSN = {2076-3417},
ABSTRACT = {Forest firefighting missions encompass multiple tasks related to prevention, surveillance, and extinguishing. This work presents a complete survey of firefighters on the current problems in their work and the potential technological solutions. Additionally, it reviews the efforts performed by the academy and industry to apply different types of robots in the context of firefighting missions. Finally, all this information is used to propose a concept of operation for the comprehensive application of drone swarms in firefighting. The proposed system is a fleet of quadcopters that individually are only able to visit waypoints and use payloads, but collectively can perform tasks of surveillance, mapping, monitoring, etc. Three operator roles are defined, each one with different access to information and functions in the mission: mission commander, team leaders, and team members. These operators take advantage of virtual and augmented reality interfaces to intuitively get the information of the scenario and, in the case of the mission commander, control the drone swarm.},
DOI = {10.3390/app11010363}
}



@Article{drones5010004,
AUTHOR = {Flores, Donovan and Gonz√°lez-Hern√°ndez, Iv√°n and Lozano, Rogelio and Vazquez-Nicolas, Jesus Manuel and Hernandez Toral, Jorge Luis},
TITLE = {Automated Agave Detection and Counting Using a Convolutional Neural Network and Unmanned Aerial Systems},
JOURNAL = {Drones},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {4},
URL = {https://www.mdpi.com/2504-446X/5/1/4},
ISSN = {2504-446X},
ABSTRACT = {We present an automatic agave detection method for counting plants based on aerial data from a UAV (Unmanned Aerial Vehicle). Our objective is to autonomously count the number of agave plants in an area to aid management of the yield. An orthomosaic is obtained from agave plantations, which is then used to create a database. This database is in turn used to train a Convolutional Neural Network (CNN). The proposed method is based on computer image processing, and the CNN increases the detection performance of the approach. The main contribution of the present paper is to propose a method for agave plant detection with a high level of precision. In order to test the proposed method in a real agave plantation, we develop a UAV platform, which is equipped with several sensors to reach accurate counting. Therefore, our prototype can safely track a desired path to detect and count agave plants. For comparison purposes, we perform the same application using a simpler algorithm. The result shows that our proposed algorithm has better performance reaching an F1 score of 0.96 as opposed to 0.57 for the Haar algorithm. The obtained experimental results suggest that the proposed algorithm is robust and has considerable potential to help farmers manage agave agroecosystems.},
DOI = {10.3390/drones5010004}
}



@Article{rs13010127,
AUTHOR = {Yeh, Chia-Cheng and Chang, Yang-Lang and Alkhaleefah, Mohammad and Hsu, Pai-Hui and Eng, Weiyong and Koo, Voon-Chet and Huang, Bormin and Chang, Lena},
TITLE = {YOLOv3-Based Matching Approach for Roof Region Detection from Drone Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {127},
URL = {https://www.mdpi.com/2072-4292/13/1/127},
ISSN = {2072-4292},
ABSTRACT = {Due to the large data volume, the UAV image stitching and matching suffers from high computational cost. The traditional feature extraction algorithms&mdash;such as Scale-Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), and Oriented FAST Rotated BRIEF (ORB)&mdash;require heavy computation to extract and describe features in high-resolution UAV images. To overcome this issue, You Only Look Once version 3 (YOLOv3) combined with the traditional feature point matching algorithms is utilized to extract descriptive features from the drone dataset of residential areas for roof detection. Unlike the traditional feature extraction algorithms, YOLOv3 performs the feature extraction solely on the proposed candidate regions instead of the entire image, thus the complexity of the image matching is reduced significantly. Then, all the extracted features are fed into Structural Similarity Index Measure (SSIM) to identify the corresponding roof region pair between consecutive image sequences. In addition, the candidate corresponding roof pair by our architecture serves as the coarse matching region pair and limits the search range of features matching to only the detected roof region. This further improves the feature matching consistency and reduces the chances of wrong feature matching. Analytical results show that the proposed method is 13&times; faster than the traditional image matching methods with comparable performance.},
DOI = {10.3390/rs13010127}
}



@Article{s21010256,
AUTHOR = {Han, Pengfei and Mei, Han and Liu, Di and Zeng, Ning and Tang, Xiao and Wang, Yinghong and Pan, Yuepeng},
TITLE = {Calibrations of Low-Cost Air Pollution Monitoring Sensors for CO, NO2, O3, and SO2},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {256},
URL = {https://www.mdpi.com/1424-8220/21/1/256},
PubMedID = {33401737},
ISSN = {1424-8220},
ABSTRACT = {Pollutant gases, such as CO, NO2, O3, and SO2 affect human health, and low-cost sensors are an important complement to regulatory-grade instruments in pollutant monitoring. Previous studies focused on one or several species, while comprehensive assessments of multiple sensors remain limited. We conducted a 12-month field evaluation of four Alphasense sensors in Beijing and used single linear regression (SLR), multiple linear regression (MLR), random forest regressor (RFR), and neural network (long short-term memory (LSTM)) methods to calibrate and validate the measurements with nearby reference measurements from national monitoring stations. For performances, CO &gt; O3 &gt; NO2 &gt; SO2 for the coefficient of determination (R2) and root mean square error (RMSE). The MLR did not increase the R2 after considering the temperature and relative humidity influences compared with the SLR (with R2 remaining at approximately 0.6 for O3 and 0.4 for NO2). However, the RFR and LSTM models significantly increased the O3, NO2, and SO2 performances, with the R2 increasing from 0.3&ndash;0.5 to &gt;0.7 for O3 and NO2, and the RMSE decreasing from 20.4 to 13.2 ppb for NO2. For the SLR, there were relatively larger biases, while the LSTMs maintained a close mean relative bias of approximately zero (e.g., &lt;5% for O3 and NO2), indicating that these sensors combined with the LSTMs are suitable for hot spot detection. We highlight that the performance of LSTM is better than that of random forest and linear methods. This study assessed four electrochemical air quality sensors and different calibration models, and the methodology and results can benefit assessments of other low-cost sensors.},
DOI = {10.3390/s21010256}
}



@Article{rs13010132,
AUTHOR = {Zhou, Ning and Lau, Lawrence and Bai, Ruibin and Moore, Terry},
TITLE = {A Genetic Optimization Resampling Based Particle Filtering Algorithm for Indoor Target Tracking},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {132},
URL = {https://www.mdpi.com/2072-4292/13/1/132},
ISSN = {2072-4292},
ABSTRACT = {In indoor target tracking based on wireless sensor networks, the particle filtering algorithm has been widely used because of its outstanding performance in coping with highly non-linear problems. Resampling is generally required to address the inherent particle degeneracy problem in the particle filter. However, traditional resampling methods cause the problem of particle impoverishment. This problem degrades positioning accuracy and robustness and sometimes may even result in filtering divergence and tracking failure. In order to mitigate the particle impoverishment and improve positioning accuracy, this paper proposes an improved genetic optimization based resampling method. This resampling method optimizes the distribution of resampled particles by the five operators, i.e., selection, roughening, classification, crossover, and mutation. The proposed resampling method is then integrated into the particle filtering framework to form a genetic optimization resampling based particle filtering (GORPF) algorithm. The performance of the GORPF algorithm is tested by a one-dimensional tracking simulation and a three-dimensional indoor tracking experiment. Both test results show that with the aid of the proposed resampling method, the GORPF has better robustness against particle impoverishment and achieves better positioning accuracy than several existing target tracking algorithms. Moreover, the GORPF algorithm owns an affordable computation load for real-time applications.},
DOI = {10.3390/rs13010132}
}



@Article{rs13020162,
AUTHOR = {Qin, Jun and Wang, Biao and Wu, Yanlan and Lu, Qi and Zhu, Haochen},
TITLE = {Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {162},
URL = {https://www.mdpi.com/2072-4292/13/2/162},
ISSN = {2072-4292},
ABSTRACT = {Pine nematode is a highly contagious disease that causes great damage to the world&rsquo;s pine forest resources. Timely and accurate identification of pine nematode disease can help to control it. At present, there are few research on pine nematode disease identification, and it is difficult to accurately identify and locate nematode disease in a single pine by existing methods. This paper proposes a new network, SCANet (spatial-context-attention network), to identify pine nematode disease based on unmanned aerial vehicle (UAV) multi-spectral remote sensing images. In this method, a spatial information retention module is designed to reduce the loss of spatial information; it preserves the shallow features of pine nematode disease and expands the receptive field to enhance the extraction of deep features through a context information module. SCANet reached an overall accuracy of 79% and a precision and recall of around 0.86, and 0.91, respectively. In addition, 55 disease points among 59 known disease points were identified, which is better than other methods (DeepLab V3+, DenseNet, and HRNet). This paper presents a fast, precise, and practical method for identifying nematode disease and provides reliable technical support for the surveillance and control of pine wood nematode disease.},
DOI = {10.3390/rs13020162}
}



@Article{s21020343,
AUTHOR = {Bjerge, Kim and Nielsen, Jakob Bonde and Sepstrup, Martin Videb√¶k and Helsing-Nielsen, Flemming and H√∏ye, Toke Thomas},
TITLE = {An Automated Light Trap to Monitor Moths (Lepidoptera) Using Computer Vision-Based Tracking and Deep Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {343},
URL = {https://www.mdpi.com/1424-8220/21/2/343},
PubMedID = {33419136},
ISSN = {1424-8220},
ABSTRACT = {Insect monitoring methods are typically very time-consuming and involve substantial investment in species identification following manual trapping in the field. Insect traps are often only serviced weekly, resulting in low temporal resolution of the monitoring data, which hampers the ecological interpretation. This paper presents a portable computer vision system capable of attracting and detecting live insects. More specifically, the paper proposes detection and classification of species by recording images of live individuals attracted to a light trap. An Automated Moth Trap (AMT) with multiple light sources and a camera was designed to attract and monitor live insects during twilight and night hours. A computer vision algorithm referred to as Moth Classification and Counting (MCC), based on deep learning analysis of the captured images, tracked and counted the number of insects and identified moth species. Observations over 48 nights resulted in the capture of more than 250,000 images with an average of 5675 images per night. A customized convolutional neural network was trained on 2000 labeled images of live moths represented by eight different classes, achieving a high validation F1-score of 0.93. The algorithm measured an average classification and tracking F1-score of 0.71 and a tracking detection rate of 0.79. Overall, the proposed computer vision system and algorithm showed promising results as a low-cost solution for non-destructive and automatic monitoring of moths.},
DOI = {10.3390/s21020343}
}



@Article{robotics10010012,
AUTHOR = {Lim, Yixiang and Pongsakornsathien, Nichakorn and Gardi, Alessandro and Sabatini, Roberto and Kistan, Trevor and Ezer, Neta and Bursch, Daniel J.},
TITLE = {Adaptive Human-Robot Interactions for Multiple Unmanned Aerial Vehicles},
JOURNAL = {Robotics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {12},
URL = {https://www.mdpi.com/2218-6581/10/1/12},
ISSN = {2218-6581},
ABSTRACT = {Advances in unmanned aircraft systems (UAS) have paved the way for progressively higher levels of intelligence and autonomy, supporting new modes of operation, such as the one-to-many (OTM) concept, where a single human operator is responsible for monitoring and coordinating the tasks of multiple unmanned aerial vehicles (UAVs). This paper presents the development and evaluation of cognitive human-machine interfaces and interactions (CHMI2) supporting adaptive automation in OTM applications. A CHMI2 system comprises a network of neurophysiological sensors and machine-learning based models for inferring user cognitive states, as well as the adaptation engine containing a set of transition logics for control/display functions and discrete autonomy levels. Models of the user&rsquo;s cognitive states are trained on past performance and neurophysiological data during an offline calibration phase, and subsequently used in the online adaptation phase for real-time inference of these cognitive states. To investigate adaptive automation in OTM applications, a scenario involving bushfire detection was developed where a single human operator is responsible for tasking multiple UAV platforms to search for and localize bushfires over a wide area. We present the architecture and design of the UAS simulation environment that was developed, together with various human-machine interface (HMI) formats and functions, to evaluate the CHMI2 system&rsquo;s feasibility through human-in-the-loop (HITL) experiments. The CHMI2 module was subsequently integrated into the simulation environment, providing the sensing, inference, and adaptation capabilities needed to realise adaptive automation. HITL experiments were performed to verify the CHMI2 module&rsquo;s functionalities in the offline calibration and online adaptation phases. In particular, results from the online adaptation phase showed that the system was able to support real-time inference and human-machine interface and interaction (HMI2) adaptation. However, the accuracy of the inferred workload was variable across the different participants (with a root mean squared error (RMSE) ranging from 0.2 to 0.6), partly due to the reduced number of neurophysiological features available as real-time inputs and also due to limited training stages in the offline calibration phase. To improve the performance of the system, future work will investigate the use of alternative machine learning techniques, additional neurophysiological input features, and a more extensive training stage.},
DOI = {10.3390/robotics10010012}
}



@Article{su13020503,
AUTHOR = {Zhao, Rongkun and Li, Yuechen and Ma, Mingguo},
TITLE = {Mapping Paddy Rice with Satellite Remote Sensing: A Review},
JOURNAL = {Sustainability},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {503},
URL = {https://www.mdpi.com/2071-1050/13/2/503},
ISSN = {2071-1050},
ABSTRACT = {Paddy rice is a staple food of three billion people in the world. Timely and accurate estimation of the paddy rice planting area and paddy rice yield can provide valuable information for the government, planners and decision makers to formulate policies. This article reviews the existing paddy rice mapping methods presented in the literature since 2010, classifies these methods, and analyzes and summarizes the basic principles, advantages and disadvantages of these methods. According to the data sources used, the methods are divided into three categories: (I) Optical mapping methods based on remote sensing; (II) Mapping methods based on microwave remote sensing; and (III) Mapping methods based on the integration of optical and microwave remote sensing. We found that the optical remote sensing data sources are mainly MODIS, Landsat, and Sentinel-2, and the emergence of Sentinel-1 data has promoted research on radar mapping methods for paddy rice. Multisource data integration further enhances the accuracy of paddy rice mapping. The best methods are phenology algorithms, paddy rice mapping combined with machine learning, and multisource data integration. Innovative methods include the time series similarity method, threshold method combined with mathematical models, and object-oriented image classification. With the development of computer technology and the establishment of cloud computing platforms, opportunities are provided for obtaining large-scale high-resolution rice maps. Multisource data integration, paddy rice mapping under different planting systems and the connection with global changes are the focus of future development priorities.},
DOI = {10.3390/su13020503}
}



@Article{en14020294,
AUTHOR = {Kulsinskas, Andrius and Durdevic, Petar and Ortiz-Arroyo, Daniel},
TITLE = {Internal Wind Turbine Blade Inspections Using UAVs: Analysis and Design Issues},
JOURNAL = {Energies},
VOLUME = {14},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {294},
URL = {https://www.mdpi.com/1996-1073/14/2/294},
ISSN = {1996-1073},
ABSTRACT = {Interior and exterior wind turbine blade inspections are necessary to extend the lifetime of wind turbine generators. The use of unmanned vehicles is an alternative to exterior wind turbine blade inspections performed by technicians that require the use of cranes and ropes. Interior wind turbine blade inspections are even more challenging due to the confined spaces, lack of illumination, and the presence of potentially harmful internal structural components. Additionally, the cost of manned interior wind turbine blade inspections is a major limiting factor. This paper analyses all aspects of the viability of using manually controlled or autonomous aerial vehicles for interior wind turbine blade inspections. We discuss why the size, weight, and flight time of a vehicle, in addition to the structure of the wind turbine blade, are the main limiting factors in performing internal blade inspections. We also describe the design issues that must be considered to provide autonomy to unmanned vehicles and the control system, the sensors that can be used, and introduce some of the algorithms for localization, obstacle avoidance and path planning that are best suited for the task. Lastly, we briefly describe which non-destructive test instrumentation can be used for the purpose.},
DOI = {10.3390/en14020294}
}



@Article{app11020543,
AUTHOR = {Zhang, Tianxiang and Su, Jinya and Xu, Zhiyong and Luo, Yulin and Li, Jiangyun},
TITLE = {Sentinel-2 Satellite Imagery for Urban Land Cover Classification by Optimized Random Forest Classifier},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {543},
URL = {https://www.mdpi.com/2076-3417/11/2/543},
ISSN = {2076-3417},
ABSTRACT = {Land cover classification is able to reflect the potential natural and social process in urban development, providing vital information to stakeholders. Recent solutions on land cover classification are generally addressed by remotely sensed imagery and supervised classification methods. However, a high-performance classifier is desirable but challenging due to the existence of model hyperparameters. Conventional approaches generally rely on manual tuning, which is time-consuming and far from satisfying. Therefore, this work aims to propose a systematic method to automatically tune the hyperparameters by Bayesian parameter optimization for the random forest classifier. The recently launched Sentinel-2A/B satellites are drawn to provide the remote sensing imageries for land cover classification case study in Beijing, China, which have the best spectral/spatial resolutions among the freely available satellites. The improved random forest with Bayesian parameter optimization is compared against the support vector machine (SVM) and random forest (RF) with default hyperparameters by discriminating five land cover classes including building, tree, road, water, and crop field. Comparative experimental results show that the optimized RF classifier outperforms the conventional SVM and the RF with default hyperparameters in terms of accuracy, precision, and recall. The effects of band/feature number and the band usefulness are also assessed. It is envisaged that the improved classifier for Sentinel-2 satellite image processing can find a wide range of applications where high-resolution satellite imagery classification is applicable.},
DOI = {10.3390/app11020543}
}



@Article{app11020546,
AUTHOR = {Xie, Jiajia and Zhou, Rui and Liu, Yuan and Luo, Jun and Xie, Shaorong and Peng, Yan and Pu, Huayan},
TITLE = {Reinforcement-Learning-Based Asynchronous Formation Control Scheme for Multiple Unmanned Surface Vehicles},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {546},
URL = {https://www.mdpi.com/2076-3417/11/2/546},
ISSN = {2076-3417},
ABSTRACT = {The high performance and efficiency of multiple unmanned surface vehicles (multi-USV) promote the further civilian and military applications of coordinated USV. As the basis of multiple USVs&rsquo; cooperative work, considerable attention has been spent on developing the decentralized formation control of the USV swarm. Formation control of multiple USV belongs to the geometric problems of a multi-robot system. The main challenge is the way to generate and maintain the formation of a multi-robot system. The rapid development of reinforcement learning provides us with a new solution to deal with these problems. In this paper, we introduce a decentralized structure of the multi-USV system and employ reinforcement learning to deal with the formation control of a multi-USV system in a leader&ndash;follower topology. Therefore, we propose an asynchronous decentralized formation control scheme based on reinforcement learning for multiple USVs. First, a simplified USV model is established. Simultaneously, the formation shape model is built to provide formation parameters and to describe the physical relationship between USVs. Second, the advantage deep deterministic policy gradient algorithm (ADDPG) is proposed. Third, formation generation policies and formation maintenance policies based on the ADDPG are proposed to form and maintain the given geometry structure of the team of USVs during movement. Moreover, three new reward functions are designed and utilized to promote policy learning. Finally, various experiments are conducted to validate the performance of the proposed formation control scheme. Simulation results and contrast experiments demonstrate the efficiency and stability of the formation control scheme.},
DOI = {10.3390/app11020546}
}



@Article{s21020391,
AUTHOR = {Bigazzi, Luca and Gherardini, Stefano and Innocenti, Giacomo and Basso, Michele},
TITLE = {Development of Non Expensive Technologies for Precise Maneuvering of Completely Autonomous Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {391},
URL = {https://www.mdpi.com/1424-8220/21/2/391},
PubMedID = {33429920},
ISSN = {1424-8220},
ABSTRACT = {In this paper, solutions for precise maneuvering of an autonomous small (e.g., 350-class) Unmanned Aerial Vehicles (UAVs) are designed and implemented from smart modifications of non expensive mass market technologies. The considered class of vehicles suffers from light load, and, therefore, only a limited amount of sensors and computing devices can be installed on-board. Then, to make the prototype capable of moving autonomously along a fixed trajectory, a &ldquo;cyber-pilot&rdquo;, able on demand to replace the human operator, has been implemented on an embedded control board. This cyber-pilot overrides the commands thanks to a custom hardware signal mixer. The drone is able to localize itself in the environment without ground assistance by using a camera possibly mounted on a 3 Degrees Of Freedom (DOF) gimbal suspension. A computer vision system elaborates the video stream pointing out land markers with known absolute position and orientation. This information is fused with accelerations from a 6-DOF Inertial Measurement Unit (IMU) to generate a &ldquo;virtual sensor&rdquo; which provides refined estimates of the pose, the absolute position, the speed and the angular velocities of the drone. Due to the importance of this sensor, several fusion strategies have been investigated. The resulting data are, finally, fed to a control algorithm featuring a number of uncoupled digital PID controllers which work to bring to zero the displacement from the desired trajectory.},
DOI = {10.3390/s21020391}
}



@Article{s21020395,
AUTHOR = {Wei, Ziang and Fernandes, Henrique and Herrmann, Hans-Georg and Tarpani, Jose Ricardo and Osman, Ahmad},
TITLE = {A Deep Learning Method for the Impact Damage Segmentation of Curve-Shaped CFRP Specimens Inspected by Infrared Thermography},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {395},
URL = {https://www.mdpi.com/1424-8220/21/2/395},
PubMedID = {33429939},
ISSN = {1424-8220},
ABSTRACT = {Advanced materials such as continuous carbon fiber-reinforced thermoplastic (CFRP) laminates are commonly used in many industries, mainly because of their strength, stiffness to weight ratio, toughness, weldability, and repairability. Structural components working in harsh environments such as satellites are permanently exposed to some sort of damage during their lifetimes. To detect and characterize these damages, non-destructive testing and evaluation techniques are essential tools, especially for composite materials. In this study, artificial intelligence was applied in combination with infrared thermography to detected and segment impact damage on curved laminates that were previously submitted to a severe thermal stress cycles and subsequent ballistic impacts. Segmentation was performed on both mid-wave and long-wave infrared sequences obtained simultaneously during pulsed thermography experiments by means of a deep neural network. A deep neural network was trained for each wavelength. Both networks generated satisfactory results. The model trained with mid-wave images achieved an F1-score of 92.74% and the model trained with long-wave images achieved an F1-score of 87.39%.},
DOI = {10.3390/s21020395}
}



@Article{s21020396,
AUTHOR = {Galyaev, Andrey A. and Lysenko, Pavel V. and Yakhno, Victor P.},
TITLE = {2D Optimal Trajectory Planning Problem in Threat Environment for UUV with Non-Uniform Radiation Pattern},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/1424-8220/21/2/396},
PubMedID = {33429963},
ISSN = {1424-8220},
ABSTRACT = {Path planning is necessary in many applications using unmanned underwater vehicles (UUVs). The main class of tasks is the planning of safe routes with minimal energy costs and/or minimal levels of emitted physical and information signals. Since the action planner is on board the UUV, the main focus is on methods and algorithms that allow it to build reference trajectories while minimizing the number of calculations. The study is devoted to the problem of the optimal route planning for a UUV with a non-uniform radiation pattern. The problem is stated in the form of two point variational problem for which necessary and sufficient optimality conditions are proved. Particular attention is paid to cases where optimality conditions are not met. These cases are directly related to found specific forms of a radiation pattern. Sufficient optimality conditions are extended on the class of two-link and multi-link motion paths. Software tools have been developed and computer simulations have been performed for various types of radiation patterns.},
DOI = {10.3390/s21020396}
}



@Article{rs13020197,
AUTHOR = {Dirscherl, Mariel and Dietz, Andreas J. and Kneisel, Christof and Kuenzer, Claudia},
TITLE = {A Novel Method for Automated Supraglacial Lake Mapping in Antarctica Using Sentinel-1 SAR Imagery and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {197},
URL = {https://www.mdpi.com/2072-4292/13/2/197},
ISSN = {2072-4292},
ABSTRACT = {Supraglacial meltwater accumulation on ice sheets can be a main driver for accelerated ice discharge, mass loss, and global sea-level-rise. With further increasing surface air temperatures, meltwater-induced hydrofracturing, basal sliding, or surface thinning will cumulate and most likely trigger unprecedented ice mass loss on the Greenland and Antarctic ice sheets. While the Greenland surface hydrological network as well as its impacts on ice dynamics and mass balance has been studied in much detail, Antarctic supraglacial lakes remain understudied with a circum-Antarctic record of their spatio-temporal development entirely lacking. This study provides the first automated supraglacial lake extent mapping method using Sentinel-1 synthetic aperture radar (SAR) imagery over Antarctica and complements the developed optical Sentinel-2 supraglacial lake detection algorithm presented in our companion paper. In detail, we propose the use of a modified U-Net for semantic segmentation of supraglacial lakes in single-polarized Sentinel-1 imagery. The convolutional neural network (CNN) is implemented with residual connections for optimized performance as well as an Atrous Spatial Pyramid Pooling (ASPP) module for multiscale feature extraction. The algorithm is trained on 21,200 Sentinel-1 image patches and evaluated in ten spatially or temporally independent test acquisitions. In addition, George VI Ice Shelf is analyzed for intra-annual lake dynamics throughout austral summer 2019/2020 and a decision-level fused Sentinel-1 and Sentinel-2 maximum lake extent mapping product is presented for January 2020 revealing a more complete supraglacial lake coverage (~770 km2) than the individual single-sensor products. Classification results confirm the reliability of the proposed workflow with an average Kappa coefficient of 0.925 and a F1-score of 93.0% for the supraglacial water class across all test regions. Furthermore, the algorithm is applied in an additional test region covering supraglacial lakes on the Greenland ice sheet which further highlights the potential for spatio-temporal transferability. Future work involves the integration of more training data as well as intra-annual analyses of supraglacial lake occurrence across the whole continent and with focus on supraglacial lake development throughout a summer melt season and into Antarctic winter.},
DOI = {10.3390/rs13020197}
}



@Article{f12010066,
AUTHOR = {Korznikov, Kirill A. and Kislov, Dmitry E. and Altman, Jan and Dole≈æal, Ji≈ô√≠ and Vozmishcheva, Anna S. and Krestov, Pavel V.},
TITLE = {Using U-Net-Like Deep Convolutional Neural Networks for Precise Tree Recognition in Very High Resolution RGB (Red, Green, Blue) Satellite Images},
JOURNAL = {Forests},
VOLUME = {12},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/1999-4907/12/1/66},
ISSN = {1999-4907},
ABSTRACT = {Very high resolution satellite imageries provide an excellent foundation for precise mapping of plant communities and even single plants. We aim to perform individual tree recognition on the basis of very high resolution RGB (red, green, blue) satellite images using deep learning approaches for northern temperate mixed forests in the Primorsky Region of the Russian Far East. We used a pansharpened satellite RGB image by GeoEye-1 with a spatial resolution of 0.46 m/pixel, obtained in late April 2019. We parametrized the standard U-Net convolutional neural network (CNN) and trained it in manually delineated satellite images to solve the satellite image segmentation problem. For comparison purposes, we also applied standard pixel-based classification algorithms, such as random forest, k-nearest neighbor classifier, naive Bayes classifier, and quadratic discrimination. Pattern-specific features based on grey level co-occurrence matrices (GLCM) were computed to improve the recognition ability of standard machine learning methods. The U-Net-like CNN allowed us to obtain precise recognition of Mongolian poplar (Populus suaveolens Fisch. ex Loudon s.l.) and evergreen coniferous trees (Abies holophylla Maxim., Pinus koraiensis Siebold &amp; Zucc.). We were able to distinguish species belonging to either poplar or coniferous groups but were unable to separate species within the same group (i.e. A. holophylla and P. koraiensis were not distinguishable). The accuracy of recognition was estimated by several metrics and exceeded values obtained for standard machine learning approaches. In contrast to pixel-based recognition algorithms, the U-Net-like CNN does not lead to an increase in false-positive decisions when facing green-colored objects that are similar to trees. By means of U-Net-like CNN, we obtained a mean accuracy score of up to 0.96 in our computational experiments. The U-Net-like CNN recognizes tree crowns not as a set of pixels with known RGB intensities but as spatial objects with a specific geometry and pattern. This CNN&rsquo;s specific feature excludes misclassifications related to objects of similar colors as objects of interest. We highlight that utilization of satellite images obtained within the suitable phenological season is of high importance for successful tree recognition. The suitability of the phenological season is conceptualized as a group of conditions providing highlighting objects of interest over other components of vegetation cover. In our case, the use of satellite images captured in mid-spring allowed us to recognize evergreen fir and pine trees as the first class of objects (&ldquo;conifers&rdquo;) and poplars as the second class, which were in a leafless state among other deciduous tree species.},
DOI = {10.3390/f12010066}
}



@Article{rs13020216,
AUTHOR = {Wang, Yutang and Wang, Jia and Chang, Shuping and Sun, Lu and An, Likun and Chen, Yuhan and Xu, Jiangqi},
TITLE = {Classification of Street Tree Species Using UAV Tilt Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {216},
URL = {https://www.mdpi.com/2072-4292/13/2/216},
ISSN = {2072-4292},
ABSTRACT = {As an important component of the urban ecosystem, street trees have made an outstanding contribution to alleviating urban environmental pollution. Accurately extracting tree characteristics and species information can facilitate the monitoring and management of street trees, as well as aiding landscaping and studies of urban ecology. In this study, we selected the suburban areas of Beijing and Zhangjiakou and investigated six representative street tree species using unmanned aerial vehicle (UAV) tilt photogrammetry. We extracted five tree attributes and four combined attribute parameters and used four types of commonly-used machine learning classification algorithms as classifiers for tree species classification. The results show that random forest (RF), support vector machine (SVM), and back propagation (BP) neural network provide better classification results when using combined parameters for tree species classification, compared with those using individual tree attributes alone; however, the K-nearest neighbor (KNN) algorithm produced the opposite results. The best combination for classification is the BP neural network using combined attributes, with a classification precision of 89.1% and F-measure of 0.872, and we conclude that this approach best meets the requirements of street tree surveys. The results also demonstrate that optical UAV tilt photogrammetry combined with a machine learning classification algorithm is a low-cost, high-efficiency, and high-precision method for tree species classification.},
DOI = {10.3390/rs13020216}
}



@Article{jmse9010065,
AUTHOR = {Xu, Jin and Pan, Xinxiang and Jia, Baozhu and Wu, Xuerui and Liu, Peng and Li, Bo},
TITLE = {Oil Spill Detection Using LBP Feature and K-Means Clustering in Shipborne Radar Image},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {9},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {65},
URL = {https://www.mdpi.com/2077-1312/9/1/65},
ISSN = {2077-1312},
ABSTRACT = {Oil spill accidents have seriously harmed the marine environment. Effective oil spill monitoring can provide strong scientific and technological support for emergency response of law enforcement departments. Shipborne radar can be used to monitor oil spills immediately after the accident. In this paper, the original shipborne radar image collected by the teaching-practice ship Yukun of Dalian Maritime University during the oil spill accident of Dalian on 16 July 2010 was taken as the research data, and an oil spill detection method was proposed by using LBP texture feature and K-means algorithm. First, Laplacian operator, Otsu algorithm, and mean filter were used to suppress the co-frequency interference noises and high brightness pixels. Then the gray intensity correction matrix was used to reduce image nonuniformity. Next, using LBP texture feature and K-means clustering algorithm, the effective oil spill regions were extracted. Finally, the adaptive threshold was applied to identify the oil films. This method can automatically detect oil spills in shipborne radar image. It can provide a guarantee for real-time monitoring of oil spill accidents.},
DOI = {10.3390/jmse9010065}
}



@Article{w13020147,
AUTHOR = {Moeini, Mohammadreza and Shojaeizadeh, Ali and Geza, Mengistu},
TITLE = {Supervised Machine Learning for Estimation of Total Suspended Solids in Urban Watersheds},
JOURNAL = {Water},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {147},
URL = {https://www.mdpi.com/2073-4441/13/2/147},
ISSN = {2073-4441},
ABSTRACT = {Machine Learning (ML) algorithms provide an alternative for the prediction of pollutant concentration. We compared eight ML algorithms (Linear Regression (LR), uniform weighting k-Nearest Neighbor (UW-kNN), variable weighting k-Nearest Neighbor (VW-kNN), Support Vector Regression (SVR), Artificial Neural Network (ANN), Regression Tree (RT), Random Forest (RF), and Adaptive Boosting (AdB)) to evaluate the feasibility of ML approaches for estimation of Total Suspended Solids (TSS) using the national stormwater quality database. Six factors were used as features to train the algorithms with TSS concentration as the target parameter: Drainage area, land use, percent of imperviousness, rainfall depth, runoff volume, and antecedent dry days. Comparisons among the ML methods demonstrated a higher degree of variability in model performance, with the coefficient of determination (R2) and Nash&ndash;Sutcliffe (NSE) values ranging from 0.15 to 0.77. The Root Mean Square (RMSE) values ranged from 110 mg/L to 220 mg/L. The best fit was obtained using the AdB and RF models, with R2 values of 0.77 and 0.74 in the training step and 0.67 and 0.64 in the prediction step. The NSE values were 0.76 and 0.72 in the training step and 0.67 and 0.62 in the prediction step. The predictions from AdB were sensitive to all six factors. However, the sensitivity level was variable.},
DOI = {10.3390/w13020147}
}



@Article{ijgi10010022,
AUTHOR = {Yu, Tong and Wu, Wenjin and Gong, Chen and Li, Xinwu},
TITLE = {Residual Multi-Attention Classification Network for A Forest Dominated Tropical Landscape Using High-Resolution Remote Sensing Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {22},
URL = {https://www.mdpi.com/2220-9964/10/1/22},
ISSN = {2220-9964},
ABSTRACT = {Tropical forests are of vital importance for maintaining biodiversity, regulating climate and material cycles while facing deforestation, agricultural reclamation, and managing various pressures. Remote sensing (RS) can support effective monitoring and mapping approaches for tropical forests, and to facilitate this we propose a deep neural network with an encoder&ndash;decoder architecture here to classify tropical forests and their environment. To deal with the complexity of tropical landscapes, this method utilizes a multi-scale convolution neural network (CNN) to expand the receptive field and extract multi-scale features. The model refines the features with several attention modules and fuses them through an upsampling module. A two-stage training strategy is proposed to alleviate misclassifications caused by sample imbalances. A joint loss function based on cross-entropy loss and the generalized Dice loss is applied in the first stage, and the second stage used the focal loss to fine-tune the weights. As a case study, we use Hainan tropical reserves to test the performance of this model. Compared with four state-of-the-art (SOTA) semantic segmentation networks, our network achieves the best performance with two Hainan datasets (mean intersection over union (MIoU) percentages of 85.78% and 82.85%). We also apply the new model to classify a public true color dataset which has 17 semantic classes and obtain results with an 83.75% MIoU. This further demonstrates the applicability and potential of this model in complex classification tasks.},
DOI = {10.3390/ijgi10010022}
}



@Article{rs13020232,
AUTHOR = {Canata, Tatiana Fernanda and Wei, Marcelo Chan Fu and Maldaner, Leonardo Felipe and Molin, Jos√© Paulo},
TITLE = {Sugarcane Yield Mapping Using High-Resolution Imagery Data and Machine Learning Technique},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {232},
URL = {https://www.mdpi.com/2072-4292/13/2/232},
ISSN = {2072-4292},
ABSTRACT = {Yield maps provide essential information to guide precision agriculture (PA) practices. Yet, on-board yield monitoring for sugarcane can be challenging. At the same time, orbital images have been widely used for indirect crop yield estimation for many crops like wheat, corn, and rice, but not for sugarcane. Due to this, the objective of this study is to explore the potential of multi-temporal imagery data as an alternative for sugarcane yield mapping. The study was based on developing predictive sugarcane yield models integrating time-series orbital imaging and a machine learning technique. A commercial sugarcane site was selected, and Sentinel-2 images were acquired from the beginning of the ratoon sprouting until harvesting of two consecutive cropping seasons. The predictive yield models RF (Random forest) and MLR (Multiple Linear Regression) were developed using orbital images and yield maps generated by a commercial sensor-system on harvesting. Original yield data were filtered and interpolated with the same spatial resolution of the orbital images. The entire dataset was divided into training and testing datasets. Spectral bands, especially the near-infrared at tillering crop stage showed greater contribution to predicting sugarcane yield than the use of derived spectral vegetation indices. The Root Mean Squared Error (RMSE) obtained for the RF regression based on multiple spectral bands was 4.63 Mg ha&minus;1 with an R2 of 0.70 for the testing dataset. Overall, the RF regression had better performance than the MLR to predict sugarcane yield.},
DOI = {10.3390/rs13020232}
}



@Article{bdcc5010002,
AUTHOR = {Fenu, Gianni and Malloci, Francesca Maridina},
TITLE = {Forecasting Plant and Crop Disease: An Explorative Study on Current Algorithms},
JOURNAL = {Big Data and Cognitive Computing},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/2504-2289/5/1/2},
ISSN = {2504-2289},
ABSTRACT = {Every year, plant diseases cause a significant loss of valuable food crops around the world. The plant and crop disease management practice implemented in order to mitigate damages have changed considerably. Today, through the application of new information and communication technologies, it is possible to predict the onset or change in the severity of diseases using modern big data analysis techniques. In this paper, we present an analysis and classification of research studies conducted over the past decade that forecast the onset of disease at a pre-symptomatic stage (i.e., symptoms not visible to the naked eye) or at an early stage. We examine the specific approaches and methods adopted, pre-processing techniques and data used, performance metrics, and expected results, highlighting the issues encountered. The results of the study reveal that this practice is still in its infancy and that many barriers need to be overcome.},
DOI = {10.3390/bdcc5010002}
}



@Article{rs13020239,
AUTHOR = {Shao, Zhenfeng and Zhou, Zifan and Huang, Xiao and Zhang, Ya},
TITLE = {MRENet: Simultaneous Extraction of Road Surface and Road Centerline in Complex Urban Scenes from Very High-Resolution Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {239},
URL = {https://www.mdpi.com/2072-4292/13/2/239},
ISSN = {2072-4292},
ABSTRACT = {Automatic extraction of the road surface and road centerline from very high-resolution (VHR) remote sensing images has always been a challenging task in the field of feature extraction. Most existing road datasets are based on data with simple and clear backgrounds under ideal conditions, such as images derived from Google Earth. Therefore, the studies on road surface extraction and road centerline extraction under complex scenes are insufficient. Meanwhile, most existing efforts addressed these two tasks separately, without considering the possible joint extraction of road surface and centerline. With the introduction of multitask convolutional neural network models, it is possible to carry out these two tasks simultaneously by facilitating information sharing within a multitask deep learning model. In this study, we first design a challenging dataset using remote sensing images from the GF-2 satellite. The dataset contains complex road scenes with manually annotated images. We then propose a two-task and end-to-end convolution neural network, termed Multitask Road-related Extraction Network (MRENet), for road surface extraction and road centerline extraction. We take features extracted from the road as the condition of centerline extraction, and the information transmission and parameter sharing between the two tasks compensate for the potential problem of insufficient road centerline samples. In the network design, we use atrous convolutions and a pyramid scene parsing pooling module (PSP pooling), aiming to expand the network receptive field, integrate multilevel features, and obtain more abundant information. In addition, we use a weighted binary cross-entropy function to alleviate the background imbalance problem. Experimental results show that the proposed algorithm outperforms several comparative methods in the aspects of classification precision and visual interpretation.},
DOI = {10.3390/rs13020239}
}



@Article{s21020507,
AUTHOR = {Wang, Le and Xiang, Lirong and Tang, Lie and Jiang, Huanyu},
TITLE = {A Convolutional Neural Network-Based Method for Corn Stand Counting in the Field},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {507},
URL = {https://www.mdpi.com/1424-8220/21/2/507},
PubMedID = {33450839},
ISSN = {1424-8220},
ABSTRACT = {Accurate corn stand count in the field at early season is of great interest to corn breeders and plant geneticists. However, the commonly used manual counting method is time consuming, laborious, and prone to error. Nowadays, unmanned aerial vehicles (UAV) tend to be a popular base for plant-image-collecting platforms. However, detecting corn stands in the field is a challenging task, primarily because of camera motion, leaf fluttering caused by wind, shadows of plants caused by direct sunlight, and the complex soil background. As for the UAV system, there are mainly two limitations for early seedling detection and counting. First, flying height cannot ensure a high resolution for small objects. It is especially difficult to detect early corn seedlings at around one week after planting, because the plants are small and difficult to differentiate from the background. Second, the battery life and payload of UAV systems cannot support long-duration online counting work. In this research project, we developed an automated, robust, and high-throughput method for corn stand counting based on color images extracted from video clips. A pipeline developed based on the YoloV3 network and Kalman filter was used to count corn seedlings online. The results demonstrate that our method is accurate and reliable for stand counting, achieving an accuracy of over 98% at growth stages V2 and V3 (vegetative stages with two and three visible collars) with an average frame rate of 47 frames per second (FPS). This pipeline can also be mounted easily on manned cart, tractor, or field robotic systems for online corn counting.},
DOI = {10.3390/s21020507}
}



@Article{s21020513,
AUTHOR = {Lemaire, Pierre and Crispim-Junior, Carlos Fernando and Robinault, Lionel and Tougne, Laure},
TITLE = {Registering Unmanned Aerial Vehicle Videos in the Long Term},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {513},
URL = {https://www.mdpi.com/1424-8220/21/2/513},
PubMedID = {33450881},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have become a very popular way of acquiring video within contexts such as remote data acquisition or surveillance. Unfortunately, their viewpoint is often unstable, which tends to impact the automatic processing of their video flux negatively. To counteract the effects of an inconsistent viewpoint, two video processing strategies are classically adopted, namely registration and stabilization, which tend to be affected by distinct issues, namely jitter and drifting. Following our prior work, we suggest that the motion estimators used in both situations can be modeled to take into account their inherent errors. By acknowledging that drifting and jittery errors are of a different nature, we propose a combination that is able to limit their influence and build a hybrid solution for jitter-free video registration. In this work, however, our modeling was restricted to 2D-rigid transforms, which are rather limited in the case of airborne videos. In the present paper, we extend and refine the theoretical ground of our previous work. This addition allows us to show how to practically adapt our previous work to perspective transforms, which our study shows to be much more accurate for this problem. A lightweight implementation enables us to automatically register stationary UAV videos in real time. Our evaluation includes traffic surveillance recordings of up to 2 h and shows the potential of the proposed approach when paired with background subtraction tasks.},
DOI = {10.3390/s21020513}
}



@Article{jcs5010022,
AUTHOR = {Boaretto, Joel and Fotouhi, Mohammad and Tende, Eduardo and Aver, Gustavo Francisco and Marcon, Victoria Rafaela Ritzel and Cordeiro, Guilherme Lu√≠s and Bergmann, Carlos P√©rez and Vannucchi de Camargo, Felipe},
TITLE = {Biomimetics and Composite Materials toward Efficient Mobility: A Review},
JOURNAL = {Journal of Composites Science},
VOLUME = {5},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {22},
URL = {https://www.mdpi.com/2504-477X/5/1/22},
ISSN = {2504-477X},
ABSTRACT = {The development of new materials has always been strictly related to the rise of new technologies and progressively efficient systems. However, cutting-edge materials might not be enough to ensure the effectiveness of a given product if the design guidelines used do not favor the specific advantages of this material. Polymeric composites are known for their excellent mechanical properties, but current manufacturing techniques and the relatively narrow expertise in the field amongst engineers impose the challenge to provide the most suitable designs to certain applications. Bio-inspired designs, supported by thousands of years of evolution of nature, have shown to be extremely profitable tools for the design of optimized yet structurally complex shapes in which the tailoring aspect of polymeric composites perfectly fit. Bearing in mind the current but old-fashioned designs of auto-parts and vehicles built with metals with little or no topological optimization, the present work addresses how biomimicry is being applied in the mobility industry nowadays to provide lightweight structures and efficient designs. A general overview of biomimicry is made regarding vehicles, approaching how the use of composite materials has already contributed to successful cases.},
DOI = {10.3390/jcs5010022}
}



@Article{rs13020260,
AUTHOR = {Nguyen, Ha Trang and Lopez Caceres, Maximo Larry and Moritake, Koma and Kentsch, Sarah and Shu, Hase and Diez, Yago},
TITLE = {Individual Sick Fir Tree (Abies mariesii) Identification in Insect Infested Forests by Means of UAV Images and Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {260},
URL = {https://www.mdpi.com/2072-4292/13/2/260},
ISSN = {2072-4292},
ABSTRACT = {Insect outbreaks are a recurrent natural phenomenon in forest ecosystems expected to increase due to climate change. Recent advances in Unmanned Aerial Vehicles (UAV) and Deep Learning (DL) Networks provide us with tools to monitor them. In this study we used nine orthomosaics and normalized Digital Surface Models (nDSM) to detect and classify healthy and sick Maries fir trees as well as deciduous trees. This study aims at automatically classifying treetops by means of a novel computer vision treetops detection algorithm and the adaptation of existing DL architectures. Considering detection alone, the accuracy results showed 85.70% success. In terms of detection and classification, we were able to detect/classify correctly 78.59% of all tree classes (39.64% for sick fir). However, with data augmentation, detection/classification percentage of the sick fir class rose to 73.01% at the cost of the result accuracy of all tree classes that dropped 63.57%. The implementation of UAV, computer vision and DL techniques contribute to the development of a new approach to evaluate the impact of insect outbreaks in forest.},
DOI = {10.3390/rs13020260}
}



@Article{electronics10020169,
AUTHOR = {Hashima, Sherief and ElHalawany, Basem M. and Hatano, Kohei and Wu, Kaishun and Mohamed, Ehab Mahmoud},
TITLE = {Leveraging Machine-Learning for D2D Communications in 5G/Beyond 5G Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {169},
URL = {https://www.mdpi.com/2079-9292/10/2/169},
ISSN = {2079-9292},
ABSTRACT = {Device-to-device (D2D) communication is a promising paradigm for the fifth generation (5G) and beyond 5G (B5G) networks. Although D2D communication provides several benefits, including limited interference, energy efficiency, reduced delay, and network overhead, it faces a lot of technical challenges such as network architecture, and neighbor discovery, etc. The complexity of configuring D2D links and managing their interference, especially when using millimeter-wave (mmWave), inspire researchers to leverage different machine-learning (ML) techniques to address these problems towards boosting the performance of D2D networks. In this paper, a comprehensive survey about recent research activities on D2D networks will be explored with putting more emphasis on utilizing mmWave and ML methods. After exploring existing D2D research directions accompanied with their existing conventional solutions, we will show how different ML techniques can be applied to enhance the D2D networks performance over using conventional ways. Then, still open research directions in ML applications on D2D networks will be investigated including their essential needs. A case study of applying multi-armed bandit (MAB) as an efficient online ML tool to enhance the performance of neighbor discovery and selection (NDS) in mmWave D2D networks will be presented. This case study will put emphasis on the high potency of using ML solutions over using the conventional non-ML based methods for highly improving the average throughput performance of mmWave NDS.},
DOI = {10.3390/electronics10020169}
}



@Article{aerospace8010018,
AUTHOR = {Wada, Daichi and Araujo-Estrada, Sergio A. and Windsor, Shane},
TITLE = {Unmanned Aerial Vehicle Pitch Control Using Deep Reinforcement Learning with Discrete Actions in Wind Tunnel Test},
JOURNAL = {Aerospace},
VOLUME = {8},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2226-4310/8/1/18},
ISSN = {2226-4310},
ABSTRACT = {Deep reinforcement learning is a promising method for training a nonlinear attitude controller for fixed-wing unmanned aerial vehicles. Until now, proof-of-concept studies have demonstrated successful attitude control in simulation. However, detailed experimental investigations have not yet been conducted. This study applied deep reinforcement learning for one-degree-of-freedom pitch control in wind tunnel tests with the aim of gaining practical understandings of attitude control application. Three controllers with different discrete action choices, that is, elevator angles, were designed. The controllers with larger action rates exhibited better performance in terms of following angle-of-attack commands. The root mean square errors for tracking angle-of-attack commands decreased from 3.42&deg; to 1.99&deg; as the maximum action rate increased from 10&deg;/s to 50&deg;/s. The comparison between experimental and simulation results showed that the controller with a smaller action rate experienced the friction effect, and the controllers with larger action rates experienced fluctuating behaviors in elevator maneuvers owing to delay. The investigation of the effect of friction and delay on pitch control highlighted the importance of conducting experiments to understand actual control performances, specifically when the controllers were trained with a low-fidelity model.},
DOI = {10.3390/aerospace8010018}
}



@Article{brainsci11010106,
AUTHOR = {Andreu-Perez, Ana R. and Kiani, Mehrin and Andreu-Perez, Javier and Reddy, Pratusha and Andreu-Abela, Jaime and Pinto, Maria and Izzetoglu, Kurtulus},
TITLE = {Single-Trial Recognition of Video Gamer‚Äôs Expertise from Brain Haemodynamic and Facial Emotion Responses},
JOURNAL = {Brain Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {106},
URL = {https://www.mdpi.com/2076-3425/11/1/106},
PubMedID = {33466787},
ISSN = {2076-3425},
ABSTRACT = {With an increase in consumer demand of video gaming entertainment, the game industry is exploring novel ways of game interaction such as providing direct interfaces between the game and the gamers&rsquo; cognitive or affective responses. In this work, gamer&rsquo;s brain activity has been imaged using functional near infrared spectroscopy (fNIRS) whilst they watch video of a video game (League of Legends) they play. A video of the face of the participants is also recorded for each of a total of 15 trials where a trial is defined as watching a gameplay video. From the data collected, i.e., gamer&rsquo;s fNIRS data in combination with emotional state estimation from gamer&rsquo;s facial expressions, the expertise level of the gamers has been decoded per trial in a multi-modal framework comprising of unsupervised deep feature learning and classification by state-of-the-art models. The best tri-class classification accuracy is obtained using a cascade of random convolutional kernel transform (ROCKET) feature extraction method and deep classifier at 91.44%. This is the first work that aims at decoding expertise level of gamers using non-restrictive and portable technologies for brain imaging, and emotional state recognition derived from gamers&rsquo; facial expressions. This work has profound implications for novel designs of future human interactions with video games and brain-controlled games.},
DOI = {10.3390/brainsci11010106}
}



@Article{rs13020278,
AUTHOR = {Zheng, Qiong and Ye, Huichun and Huang, Wenjiang and Dong, Yingying and Jiang, Hao and Wang, Chongyang and Li, Dan and Wang, Li and Chen, Shuisen},
TITLE = {Integrating Spectral Information and Meteorological Data to Monitor Wheat Yellow Rust at a Regional Scale: A Case Study},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {278},
URL = {https://www.mdpi.com/2072-4292/13/2/278},
ISSN = {2072-4292},
ABSTRACT = {Wheat yellow rust has a severe impact on wheat production and threatens food security in China; as such, an effective monitoring method is necessary at the regional scale. We propose a model for yellow rust monitoring based on Sentinel-2 multispectral images and a series of two-stage vegetation indices and meteorological data. Sensitive spectral vegetation indices (single- and two-stage indices) and meteorological features for wheat yellow rust discrimination were selected using the random forest method. Wheat yellow rust monitoring models were established using three different classification methods: linear discriminant analysis (LDA), support vector machine (SVM), and artificial neural network (ANN). The results show that models based on two-stage indices (i.e., those calculated using images from two different days) significantly outperform single-stage index models (i.e., those calculated using an image from a single day), the overall accuracy improved from 63.2% to 78.9%. The classification accuracies of models combining a vegetation index with meteorological feature are higher than those of pure vegetation index models. Among them, the model based on two-stage vegetation indices and meteorological features performs best, with a classification accuracy exceeding 73.7%. The SVM algorithm performed best for wheat yellow rust monitoring among the three algorithms; its classification accuracy (84.2%) was ~10.5% and 5.3% greater than those of LDA and ANN, respectively. Combined with crop growth and environmental information, our model has great potential for monitoring wheat yellow rust at a regional scale. Future work will focus on regional-scale monitoring and forecasting of crop disease.},
DOI = {10.3390/rs13020278}
}



@Article{s21020570,
AUTHOR = {Biundini, Iago Z. and Pinto, Milena F. and Melo, Aurelio G. and Marcato, Andre L. M. and Hon√≥rio, Leonardo M. and Aguiar, Maria J. R.},
TITLE = {A Framework for Coverage Path Planning Optimization Based on Point Cloud for Structural Inspection},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {570},
URL = {https://www.mdpi.com/1424-8220/21/2/570},
PubMedID = {33467417},
ISSN = {1424-8220},
ABSTRACT = {Different practical applications have emerged in the last few years, requiring periodic and detailed inspections to verify possible structural changes. Inspections using Unmanned Aerial Vehicles (UAVs) should minimize flight time due to battery time restrictions and identify the terrain&rsquo;s topographic features. In this sense, Coverage Path Planning (CPP) aims at finding the best path to coverage of a determined area respecting the operation&rsquo;s restrictions. Photometric information from the terrain is used to create routes or even refine paths already created. Therefore, this research&rsquo;s main contribution is developing a methodology that uses a metaheuristic algorithm based on point cloud data to inspect slope and dams structures. The technique was applied in a simulated and real scenario to verify its effectiveness. The results showed an increasing 3D reconstructions&rsquo; quality observing optimizing photometric and mission time criteria.},
DOI = {10.3390/s21020570}
}



@Article{s21020581,
AUTHOR = {Zhang, Xiaomin and Zhao, Zhiyao and Wang, Zhaoyang and Wang, Xiaoyi},
TITLE = {Fault Detection and Identification Method for Quadcopter Based on Airframe Vibration Signals},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {581},
URL = {https://www.mdpi.com/1424-8220/21/2/581},
PubMedID = {33467463},
ISSN = {1424-8220},
ABSTRACT = {Quadcopters are widely used in a variety of military and civilian mission scenarios. Real-time online detection of the abnormal state of the quadcopter is vital to the safety of aircraft. Existing data-driven fault detection methods generally usually require numerous sensors to collect data. However, quadcopter airframe space is limited. A large number of sensors cannot be loaded, meaning that it is difficult to use additional sensors to capture fault signals for quadcopters. In this paper, without additional sensors, a Fault Detection and Identification (FDI) method for quadcopter blades based on airframe vibration signals is proposed using the airborne acceleration sensor. This method integrates multi-axis data information and effectively detects and identifies quadcopter blade faults through Long and Short-Term Memory (LSTM) network models. Through flight experiments, the quadcopter triaxial accelerometer data are collected for airframe vibration signals at first. Then, the wavelet packet decomposition method is employed to extract data features, and the standard deviations of the wavelet packet coefficients are employed to form the feature vector. Finally, the LSTM-based FDI model is constructed for quadcopter blade FDI. The results show that the method can effectively detect and identify quadcopter blade faults with a better FDI performance and a higher model accuracy compared with the Back Propagation (BP) neural network-based FDI model.},
DOI = {10.3390/s21020581}
}



@Article{rs13020289,
AUTHOR = {Debella-Gilo, Misganu and Gjertsen, Arnt Kristian},
TITLE = {Mapping Seasonal Agricultural Land Use Types Using Deep Learning on Sentinel-2 Image Time Series},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {289},
URL = {https://www.mdpi.com/2072-4292/13/2/289},
ISSN = {2072-4292},
ABSTRACT = {The size and location of agricultural fields that are in active use and the type of use during the growing season are among the vital information that is needed for the careful planning and forecasting of agricultural production at national and regional scales. In areas where such data are not readily available, an independent seasonal monitoring method is needed. Remote sensing is a widely used tool to map land use types, although there are some limitations that can partly be circumvented by using, among others, multiple observations, careful feature selection and appropriate analysis methods. Here, we used Sentinel-2 satellite image time series (SITS) over the land area of Norway to map three agricultural land use classes: cereal crops, fodder crops (grass) and unused areas. The Multilayer Perceptron (MLP) and two variants of the Convolutional Neural Network (CNN), are implemented on SITS data of four different temporal resolutions. These enabled us to compare twelve model-dataset combinations to identify the model-dataset combination that results in the most accurate predictions. The CNN is implemented in the spectral and temporal dimensions instead of the conventional spatial dimension. Rather than using existing deep learning architectures, an autotuning procedure is implemented so that the model hyperparameters are empirically optimized during the training. The results obtained on held-out test data show that up to 94% overall accuracy and 90% Cohen&rsquo;s Kappa can be obtained when the 2D CNN is applied on the SITS data with a temporal resolution of 7 days. This is closely followed by the 1D CNN on the same dataset. However, the latter performs better than the former in predicting data outside the training set. It is further observed that cereal is predicted with the highest accuracy, followed by grass. Predicting the unused areas has been found to be difficult as there is no distinct surface condition that is common for all unused areas.},
DOI = {10.3390/rs13020289}
}



@Article{geomatics1010004,
AUTHOR = {Moreni, Mael and Theau, Jerome and Foucher, Samuel},
TITLE = {Train Fast While Reducing False Positives: Improving Animal Classification Performance Using Convolutional Neural Networks},
JOURNAL = {Geomatics},
VOLUME = {1},
YEAR = {2021},
NUMBER = {1},
PAGES = {34--49},
URL = {https://www.mdpi.com/2673-7418/1/1/4},
ISSN = {2673-7418},
ABSTRACT = {The combination of unmanned aerial vehicles (UAV) with deep learning models has the capacity to replace manned aircrafts for wildlife surveys. However, the scarcity of animals in the wild often leads to highly unbalanced, large datasets for which even a good detection method can return a large amount of false detections. Our objectives in this paper were to design a training method that would reduce training time, decrease the number of false positives and alleviate the fine-tuning effort of an image classifier in a context of animal surveys. We acquired two highly unbalanced datasets of deer images with a UAV and trained a Resnet-18 classifier using hard-negative mining and a series of recent techniques. Our method achieved sub-decimal false positive rates on two test sets (1 false positive per 19,162 and 213,312 negatives respectively), while training on small but relevant fractions of the data. The resulting training times were therefore significantly shorter than they would have been using the whole datasets. This high level of efficiency was achieved with little tuning effort and using simple techniques. We believe this parsimonious approach to dealing with highly unbalanced, large datasets could be particularly useful to projects with either limited resources or extremely large datasets.},
DOI = {10.3390/geomatics1010004}
}



@Article{agronomy11010174,
AUTHOR = {Li, Haolu and Wang, Guojie and Dong, Zhen and Wei, Xikun and Wu, Mengjuan and Song, Huihui and Amankwah, Solomon Obiri Yeboah},
TITLE = {Identifying Cotton Fields from Remote Sensing Images Using Multiple Deep Learning Networks},
JOURNAL = {Agronomy},
VOLUME = {11},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {174},
URL = {https://www.mdpi.com/2073-4395/11/1/174},
ISSN = {2073-4395},
ABSTRACT = {Remote sensing imageries processed through empirical and deterministic approaches help predict multiple agronomic traits throughout the growing season. Accurate identification of cotton crop from remotely sensed imageries is a significant task in precision agriculture. This study aims to utilize a deep learning-based framework for cotton crop field identification with Gaofen-1 (GF-1) high-resolution (16 m) imageries in Wei-Ku region, China. An optimized model for the pixel-wise multidimensional densely connected convolutional neural network (DenseNet) was used. Four widely-used classic convolutional neural networks (CNNs), including ResNet, VGG, SegNet, and DeepLab v3+, were also used for accuracy assessment. The results infer that DenseNet can identify cotton crop features within a relatively shorter time about 5 h for training convergence. The model performance was examined by multiple indicators (P, F1, R, and mIou) produced through the confusion matrix, and the derived cotton fields were then visualized. The DenseNet model has illustrated considerable improvements in comparison with the preceding mainstream models. The results showed that the retrieval precision was 0.948, F1 score was 0.953, and mIou was 0.911. Furthermore, its performance is relatively better in discriminating cotton crop fields&rsquo; fine structures when clouds, mountain shadows, and urban built up.},
DOI = {10.3390/agronomy11010174}
}



@Article{rs13020310,
AUTHOR = {Zou, Kunlin and Chen, Xin and Zhang, Fan and Zhou, Hang and Zhang, Chunlong},
TITLE = {A Field Weed Density Evaluation Method Based on UAV Imaging and Modified U-Net},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {310},
URL = {https://www.mdpi.com/2072-4292/13/2/310},
ISSN = {2072-4292},
ABSTRACT = {Weeds are one of the main factors affecting the yield and quality of agricultural products. Accurate evaluation of weed density is of great significance for field management, especially precision weeding. In this paper, a weed density calculating and mapping method in the field is proposed. An unmanned aerial vehicle (UAV) was used to capture field images. The excess green minus excess red index, combined with the minimum error threshold segmentation method, was used to segment green plants and bare land. A modified U-net was used to segment crops from images. After removing the bare land and crops from the field, images of weeds were obtained. The weed density was evaluated by the ratio of weed area to total area on the segmented image. The accuracy of the green plant segmentation was 93.5%. In terms of crop segmentation, the intersection over union (IoU) was 93.40%, and the segmentation time of a single image was 35.90 ms. Finally, the determination coefficient of the UAV evaluated weed density and the manually observed weed density was 0.94, and the root mean square error was 0.03. With the proposed method, the weed density of a field can be effectively evaluated from UAV images, hence providing critical information for precision weeding.},
DOI = {10.3390/rs13020310}
}



@Article{s21020638,
AUTHOR = {Ni, Ming and Wang, Hongjie and Liu, Xudong and Liao, Yilin and Fu, Lin and Wu, Qianqian and Mu, Jiong and Chen, Xiaoyan and Li, Jun},
TITLE = {Design of Variable Spray System for Plant Protection UAV Based on CFD Simulation and Regression Analysis},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {638},
URL = {https://www.mdpi.com/1424-8220/21/2/638},
PubMedID = {33477600},
ISSN = {1424-8220},
ABSTRACT = {Multi-rotor unmanned aerial vehicles (UAVs) for plant protection are widely used in China&rsquo;s agricultural production. However, spray droplets often drift and distribute nonuniformly, thereby harming its utilization and the environment. A variable spray system is designed, discussed, and verified to solve this problem. The distribution characteristics of droplet deposition under different spray states (flight state, environment state, nozzle state) are obtained through computational fluid dynamics simulation. In the verification experiment, the wind velocity error of most sample points is less than 1 m/s, and the deposition ratio error is less than 10%, indicating that the simulation is reliable. A simulation data set is used to train support vector regression and back propagation neural network with multiple parameters. An optimal regression model with the root mean square error of 6.5% is selected. The UAV offset and nozzle flow of the variable spray system can be obtained in accordance with the current spray state by multi-sensor fusion and the predicted deposition distribution characteristics. The farmland experiment shows that the deposition volume error between the prediction and experiment is within 30%, thereby proving the effectiveness of the system. This article provides a reference for the improvement of UAV intelligent spray system.},
DOI = {10.3390/s21020638}
}



@Article{ijgi10010039,
AUTHOR = {Zhou, Kai and Xie, Yan and Gao, Zhan and Miao, Fang and Zhang, Lei},
TITLE = {FuNet: A Novel Road Extraction Network with Fusion of Location Data and Remote Sensing Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2220-9964/10/1/39},
ISSN = {2220-9964},
ABSTRACT = {Road semantic segmentation is unique and difficult. Road extraction from remote sensing imagery often produce fragmented road segments leading to road network disconnection due to the occlusion of trees, buildings, shadows, cloud, etc. In this paper, we propose a novel fusion network (FuNet) with fusion of remote sensing imagery and location data, which plays an important role of location data in road connectivity reasoning. A universal iteration reinforcement (IteR) module is embedded into FuNet to enhance the ability of network learning. We designed the IteR formula to repeatedly integrate original information and prediction information and designed the reinforcement loss function to control the accuracy of road prediction output. Another contribution of this paper is the use of histogram equalization data pre-processing to enhance image contrast and improve the accuracy by nearly 1%. We take the excellent D-LinkNet as the backbone network, designing experiments based on the open dataset. The experiment result shows that our method improves over the compared advanced road extraction methods, which not only increases the accuracy of road extraction, but also improves the road topological connectivity.},
DOI = {10.3390/ijgi10010039}
}



@Article{ijgi10010041,
AUTHOR = {Kadhim, Israa and Abed, Fanar M.},
TITLE = {The Potential of LiDAR and UAV-Photogrammetric Data Analysis to Interpret Archaeological Sites: A Case Study of Chun Castle in South-West England},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {10},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {41},
URL = {https://www.mdpi.com/2220-9964/10/1/41},
ISSN = {2220-9964},
ABSTRACT = {With the increasing demands to use remote sensing approaches, such as aerial photography, satellite imagery, and LiDAR in archaeological applications, there is still a limited number of studies assessing the differences between remote sensing methods in extracting new archaeological finds. Therefore, this work aims to critically compare two types of fine-scale remotely sensed data: LiDAR and an Unmanned Aerial Vehicle (UAV) derived Structure from Motion (SfM) photogrammetry. To achieve this, aerial imagery and airborne LiDAR datasets of Chun Castle were acquired, processed, analyzed, and interpreted. Chun Castle is one of the most remarkable ancient sites in Cornwall County (Southwest England) that had not been surveyed and explored by non-destructive techniques. The work outlines the approaches that were applied to the remotely sensed data to reveal potential remains: Visualization methods (e.g., hillshade and slope raster images), ISODATA clustering, and Support Vector Machine (SVM) algorithms. The results display various archaeological remains within the study site that have been successfully identified. Applying multiple methods and algorithms have successfully improved our understanding of spatial attributes within the landscape. The outcomes demonstrate how raster derivable from inexpensive approaches can be used to identify archaeological remains and hidden monuments, which have the possibility to revolutionize archaeological understanding.},
DOI = {10.3390/ijgi10010041}
}



