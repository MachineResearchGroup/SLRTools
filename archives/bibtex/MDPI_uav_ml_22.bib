
@Article{geosciences10060219,
AUTHOR = {Błaszczak-Bąk, Wioleta and Janicka, Joanna and Suchocki, Czesław and Masiero, Andrea and Sobieraj-Żłobińska, Anna},
TITLE = {Down-Sampling of Large LiDAR Dataset in the Context of Off-Road Objects Extraction},
JOURNAL = {Geosciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {219},
URL = {https://www.mdpi.com/2076-3263/10/6/219},
ISSN = {2076-3263},
ABSTRACT = {Nowadays, LiDAR (Light Detection and Ranging) is used in many fields, such as transportation. Thanks to the recent technological improvements, the current generation of LiDAR mapping instruments available on the market allows to acquire up to millions of three-dimensional (3D) points per second. On the one hand, such improvements allowed the development of LiDAR-based systems with increased productivity, enabling the quick acquisition of detailed 3D descriptions of the objects of interest. However, on the other hand, the extraction of the information of interest from such huge amount of acquired data can be quite challenging and time demanding. Motivated by such observation, this paper proposes the use of the Optimum Dataset method in order to ease and speed up the information extraction phase by significantly reducing the size of the acquired dataset while preserving (retain) the information of interest. This paper focuses on the data reduction of LiDAR datasets acquired on roads, with the goal of extraction the off-road objects. Mostly motivated by the need of mapping roads and quickly determining car position along a road, the development of efficient methods for the extraction of such kind of information is becoming a hot topic in the research community.},
DOI = {10.3390/geosciences10060219}
}



@Article{rs12111838,
AUTHOR = {Zhang, Zhao and Flores, Paulo and Igathinathane, C. and L. Naik, Dayakar and Kiran, Ravi and Ransom, Joel K.},
TITLE = {Wheat Lodging Detection from UAS Imagery Using Machine Learning Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {1838},
URL = {https://www.mdpi.com/2072-4292/12/11/1838},
ISSN = {2072-4292},
ABSTRACT = {The current mainstream approach of using manual measurements and visual inspections for crop lodging detection is inefficient, time-consuming, and subjective. An innovative method for wheat lodging detection that can overcome or alleviate these shortcomings would be welcomed. This study proposed a systematic approach for wheat lodging detection in research plots (372 experimental plots), which consisted of using unmanned aerial systems (UAS) for aerial imagery acquisition, manual field evaluation, and machine learning algorithms to detect the occurrence or not of lodging. UAS imagery was collected on three different dates (23 and 30 July 2019, and 8 August 2019) after lodging occurred. Traditional machine learning and deep learning were evaluated and compared in this study in terms of classification accuracy and standard deviation. For traditional machine learning, five types of features (i.e. gray level co-occurrence matrix, local binary pattern, Gabor, intensity, and Hu-moment) were extracted and fed into three traditional machine learning algorithms (i.e., random forest (RF), neural network, and support vector machine) for detecting lodged plots. For the datasets on each imagery collection date, the accuracies of the three algorithms were not significantly different from each other. For any of the three algorithms, accuracies on the first and last date datasets had the lowest and highest values, respectively. Incorporating standard deviation as a measurement of performance robustness, RF was determined as the most satisfactory. Regarding deep learning, three different convolutional neural networks (simple convolutional neural network, VGG-16, and GoogLeNet) were tested. For any of the single date datasets, GoogLeNet consistently had superior performance over the other two methods. Further comparisons between RF and GoogLeNet demonstrated that the detection accuracies of the two methods were not significantly different from each other (p &gt; 0.05); hence, the choice of any of the two would not affect the final detection accuracies. However, considering the fact that the average accuracy of GoogLeNet (93%) was larger than RF (91%), it was recommended to use GoogLeNet for wheat lodging detection. This research demonstrated that UAS RGB imagery, coupled with the GoogLeNet machine learning algorithm, can be a novel, reliable, objective, simple, low-cost, and effective (accuracy &gt; 90%) tool for wheat lodging detection.},
DOI = {10.3390/rs12111838}
}



@Article{app10113953,
AUTHOR = {de la Fuente Castillo, Víctor and Díaz-Álvarez, Alberto and Manso-Callejo, Miguel-Ángel and Serradilla García, Francisco},
TITLE = {Grammar Guided Genetic Programming for Network Architecture Search and Road Detection on Aerial Orthophotography},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3953},
URL = {https://www.mdpi.com/2076-3417/10/11/3953},
ISSN = {2076-3417},
ABSTRACT = {Photogrammetry involves aerial photography of the Earth&rsquo;s surface and subsequently processing the images to provide a more accurate depiction of the area (Orthophotography). It is used by the Spanish Instituto Geogr&aacute;fico Nacional to update road cartography but requires a significant amount of manual labor due to the need to perform visual inspection of all tiled images. Deep learning techniques (artificial neural networks with more than one hidden layer) can perform road detection but it is still unclear how to find the optimal network architecture. Our main goal is the automatic design of deep neural network architectures with grammar-guided genetic programming. In this kind of evolutive algorithm, all the population individuals (here candidate network architectures) are constrained to rules specified by a grammar that defines valid and useful structural patterns to guide the search process. Grammar used includes well-known complex structures (e.g., Inception-like modules) combined with a custom designed mutation operator (dynamically links the mutation probability to structural diversity). Pilot results show that the system is able to design models for road detection that obtain test accuracies similar to that reached by state-of-the-art models when evaluated over a dataset from the Spanish National Aerial Orthophotography Plan.},
DOI = {10.3390/app10113953}
}



@Article{s20113245,
AUTHOR = {Zhang, Tianyao and Hu, Xiaoguang and Xiao, Jin and Zhang, Guofeng},
TITLE = {A Machine Learning Method for Vision-Based Unmanned Aerial Vehicle Systems to Understand Unknown Environments},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {11},
ARTICLE-NUMBER = {3245},
URL = {https://www.mdpi.com/1424-8220/20/11/3245},
ISSN = {1424-8220},
ABSTRACT = {What makes unmanned aerial vehicles (UAVs) intelligent is their capability of sensing and understanding new unknown environments. Some studies utilize computer vision algorithms like Visual Simultaneous Localization and Mapping (VSLAM) and Visual Odometry (VO) to sense the environment for pose estimation, obstacles avoidance and visual servoing. However, understanding the new environment (i.e., make the UAV recognize generic objects) is still an essential scientific problem that lacks a solution. Therefore, this paper takes a step to understand the items in an unknown environment. The aim of this research is to enable the UAV with basic understanding capability for a high-level UAV flock application in the future. Specially, firstly, the proposed understanding method combines machine learning and traditional algorithm to understand the unknown environment through RGB images; secondly, the You Only Look Once (YOLO) object detection system is integrated (based on TensorFlow) in a smartphone to perceive the position and category of 80 classes of objects in the images; thirdly, the method makes the UAV more intelligent and liberates the operator from labor; fourthly, detection accuracy and latency in working condition are quantitatively evaluated, and properties of generality (can be used in various platforms), transportability (easily deployed from one platform to another) and scalability (easily updated and maintained) for UAV flocks are qualitatively discussed. The experiments suggest that the method has enough accuracy to recognize various objects with high computational speed, and excellent properties of generality, transportability and scalability.},
DOI = {10.3390/s20113245}
}



@Article{s20123336,
AUTHOR = {Tang, Ta-Wei and Kuo, Wei-Han and Lan, Jauh-Hsiang and Ding, Chien-Fang and Hsu, Hakiem and Young, Hong-Tsu},
TITLE = {Anomaly Detection Neural Network with Dual Auto-Encoders GAN and Its Industrial Inspection Applications},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3336},
URL = {https://www.mdpi.com/1424-8220/20/12/3336},
ISSN = {1424-8220},
ABSTRACT = {Recently, researchers have been studying methods to introduce deep learning into automated optical inspection (AOI) systems to reduce labor costs. However, the integration of deep learning in the industry may encounter major challenges such as sample imbalance (defective products that only account for a small proportion). Therefore, in this study, an anomaly detection neural network, dual auto-encoder generative adversarial network (DAGAN), was developed to solve the problem of sample imbalance. With skip-connection and dual auto-encoder architecture, the proposed method exhibited excellent image reconstruction ability and training stability. Three datasets, namely public industrial detection training set, MVTec AD, with mobile phone screen glass and wood defect detection datasets, were used to verify the inspection ability of DAGAN. In addition, training with a limited amount of data was proposed to verify its detection ability. The results demonstrated that the areas under the curve (AUCs) of DAGAN were better than previous generative adversarial network-based anomaly detection models in 13 out of 17 categories in these datasets, especially in categories with high variability or noise. The maximum AUC improvement was 0.250 (toothbrush). Moreover, the proposed method exhibited better detection ability than the U-Net auto-encoder, which indicates the function of discriminator in this application. Furthermore, the proposed method had a high level of AUCs when using only a small amount of training data. DAGAN can significantly reduce the time and cost of collecting and labeling data when it is applied to industrial detection.},
DOI = {10.3390/s20123336}
}



@Article{rs12121924,
AUTHOR = {Miura, Hiroyuki and Aridome, Tomohiro and Matsuoka, Masashi},
TITLE = {Deep Learning-Based Identification of Collapsed, Non-Collapsed and Blue Tarp-Covered Buildings from Post-Disaster Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1924},
URL = {https://www.mdpi.com/2072-4292/12/12/1924},
ISSN = {2072-4292},
ABSTRACT = {A methodology for the automated identification of building damage from post-disaster aerial images was developed based on convolutional neural network (CNN) and building damage inventories. The aerial images and the building damage data obtained in the 2016 Kumamoto, and the 1995 Kobe, Japan earthquakes were analyzed. Since the roofs of many moderately damaged houses are covered with blue tarps immediately after disasters, not only collapsed and non-collapsed buildings but also the buildings covered with blue tarps were identified by the proposed method. The CNN architecture developed in this study correctly classifies the building damage with the accuracy of approximately 95 % in both earthquake data. We applied the developed CNN model to aerial images in Chiba, Japan, damaged by the typhoon in September 2019. The result shows that more than 90 % of the building damage are correctly classified by the CNN model.},
DOI = {10.3390/rs12121924}
}



@Article{rs12121933,
AUTHOR = {Wang, Mingchang and Zhang, Haiming and Sun, Weiwei and Li, Sheng and Wang, Fengyan and Yang, Guodong},
TITLE = {A Coarse-to-Fine Deep Learning Based Land Use Change Detection Method for High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1933},
URL = {https://www.mdpi.com/2072-4292/12/12/1933},
ISSN = {2072-4292},
ABSTRACT = {In recent decades, high-resolution (HR) remote sensing images have shown considerable potential for providing detailed information for change detection. The traditional change detection methods based on HR remote sensing images mostly only detect a single land type or only the change range, and cannot simultaneously detect the change of all object types and pixel-level range changes in the area. To overcome this difficulty, we propose a new coarse-to-fine deep learning-based land-use change detection method. We independently created a new scene classification dataset called NS-55, and innovatively considered the adaptation relationship between the convolutional neural network (CNN) and the scene complexity by selecting the CNN that best fit the scene complexity. The CNN trained by NS-55 was used to detect the category of the scene, define the final category of the scene according to the majority voting method, and obtain the changed scene by comparison to obtain the so-called coarse change result. Then, we created a multi-scale threshold (MST) method, which is a new method for obtaining high-quality training samples. We used the high-quality samples selected by MST to train the deep belief network to obtain the pixel-level range change detection results. By mapping coarse scene changes to range changes, we could obtain fine multi-type land-use change detection results. Experiments were conducted on the Multi-temporal Scene Wuhan dataset and aerial images of a particular area of Dapeng New District, Shenzhen, where promising results were achieved by the proposed method. This demonstrates that the proposed method is practical, easy-to-implement, and the NS-55 dataset is physically justified. The proposed method has the potential to be applied in the large scale land use fine change detection problem and qualitative and quantitative research on land use/cover change based on HR remote sensing data.},
DOI = {10.3390/rs12121933}
}



@Article{ijgi9060397,
AUTHOR = {Wagle, Nimisha and Acharya, Tri Dev},
TITLE = {Past and Present Practices of Topographic Base Map Database Update in Nepal},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {397},
URL = {https://www.mdpi.com/2220-9964/9/6/397},
ISSN = {2220-9964},
ABSTRACT = {Topographic Base Maps (TBMs) are those maps that portray ground relief as the form of contour lines and show planimetric details. Various other maps like geomorphological maps, contour maps, and land use planning maps are derived from topographical maps. In this constantly changing world, the update of TBMs is indispensable. In Nepal, their update and maintenance are done by the Survey Department (SD) as a national mapping agency. This paper presents the history of topographical mapping and the reasons for the lack of updates. Currently, the SD is updating the TBM database using panchromatic and multispectral images from the Zi Yuan-3 (ZY-3) satellite with a resolution of 2.1 and 5.8 m, respectively. The updated methodology includes the orthorectification of images, the pansharpening of images, field data collection, digitization, change detection, and updating, the overlay of vector data and field verification, data quality control, and printing map production. A TBM in the Dang district of Nepal is presented as casework to show the changes in the area and issues faced during the update. Though the present digitizing procedure is time-consuming and labor-intensive, the use of high-resolution imagery has made mapping accurate and has produced high-quality maps. However, audit and automation can be introduced from the experiences of other countries for accurate and frequent updates of the TBM database in Nepal.},
DOI = {10.3390/ijgi9060397}
}



@Article{s20123414,
AUTHOR = {Gong, Dawei and He, Zhiheng and Ye, Xiaolong and Fang, Ziyun},
TITLE = {Visual Saliency Detection for Over-Temperature Regions in 3D Space via Dual-Source Images},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {3414},
URL = {https://www.mdpi.com/1424-8220/20/12/3414},
ISSN = {1424-8220},
ABSTRACT = {To allow mobile robots to visually observe the temperature of equipment in complex industrial environments and work on temperature anomalies in time, it is necessary to accurately find the coordinates of temperature anomalies and obtain information on the surrounding obstacles. This paper proposes a visual saliency detection method for hypertemperature in three-dimensional space through dual-source images. The key novelty of this method is that it can achieve accurate salient object detection without relying on high-performance hardware equipment. First, the redundant point clouds are removed through adaptive sampling to reduce the computational memory. Second, the original images are merged with infrared images and the dense point clouds are surface-mapped to visually display the temperature of the reconstructed surface and use infrared imaging characteristics to detect the plane coordinates of temperature anomalies. Finally, transformation mapping is coordinated according to the pose relationship to obtain the spatial position. Experimental results show that this method not only displays the temperature of the device directly but also accurately obtains the spatial coordinates of the heat source without relying on a high-performance computing platform.},
DOI = {10.3390/s20123414}
}



@Article{app10124207,
AUTHOR = {Asokan, Anju and Anitha, J. and Ciobanu, Monica and Gabor, Andrei and Naaji, Antoanela and Hemanth, D. Jude},
TITLE = {Image Processing Techniques for Analysis of Satellite Images for Historical Maps Classification—An Overview},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {4207},
URL = {https://www.mdpi.com/2076-3417/10/12/4207},
ISSN = {2076-3417},
ABSTRACT = {Historical maps classification has become an important application in today&rsquo;s scenario of everchanging land boundaries. Historical map changes include the change in boundaries of cities/states, vegetation regions, water bodies and so forth. Change detection in these regions are mainly carried out via satellite images. Hence, an extensive knowledge on satellite image processing is necessary for historical map classification applications. An exhaustive analysis on the merits and demerits of many satellite image processing methods are discussed in this paper. Though several computational methods are available, different methods perform differently for the various satellite image processing applications. Wrong selection of methods will lead to inferior results for a specific application. This work highlights the methods and the suitable satellite imaging methods associated with these applications. Several comparative analyses are also performed in this work to show the suitability of several methods. This work will help support the selection of innovative solutions for the different problems associated with satellite image processing applications.},
DOI = {10.3390/app10124207}
}



@Article{jmse8060449,
AUTHOR = {Abaspur Kazerouni, Iman and Dooly, Gerard and Toal, Daniel},
TITLE = {Underwater Image Enhancement and Mosaicking System Based on A-KAZE Feature Matching},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {8},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {449},
URL = {https://www.mdpi.com/2077-1312/8/6/449},
ISSN = {2077-1312},
ABSTRACT = {Feature extraction and matching is a key component in image stitching and a critical step in advancing image reconstructions, machine vision and robotic perception algorithms. This paper presents a fast and robust underwater image mosaicking system based on (2D)2PCA and A-KAZE key-points extraction and optimal seam-line methods. The system utilizes image enhancement as a preprocessing step to improve quality and allow for greater keyframe extraction and matching performance, leading to better quality mosaicking. The application focus of this paper is underwater imaging and it demonstrates the suitability of the developed system in advanced underwater reconstructions. The results show that the proposed method can address the problems of noise, mismatching and quality issues which are typically found in underwater image datasets. The results demonstrate the proposed method as scale-invariant and show improvements in terms of processing speed and system robustness over other methods found in the literature.},
DOI = {10.3390/jmse8060449}
}



@Article{ijgi9060403,
AUTHOR = {Zhang, Xueyan},
TITLE = {Village-Level Homestead and Building Floor Area Estimates Based on UAV Imagery and U-Net Algorithm},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {9},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {403},
URL = {https://www.mdpi.com/2220-9964/9/6/403},
ISSN = {2220-9964},
ABSTRACT = {China&rsquo;s rural population has declined markedly with the acceleration of urbanization and industrialization, but the area under rural homesteads has continued to expand. Proper rural land use and management require large-scale, efficient, and low-cost rural residential surveys; however, such surveys are time-consuming and difficult to accomplish. Unmanned aerial vehicle (UAV) technology coupled with a deep learning architecture and 3D modelling can provide a potential alternative to traditional surveys for gathering rural homestead information. In this study, a method to estimate the village-level homestead area, a 3D-based building height model (BHM), and the number of building floors based on UAV imagery and the U-net algorithm was developed, and the respective estimation accuracies were found to be 0.92, 0.99, and 0.89. This method is rapid and inexpensive compared to the traditional time-consuming and costly household surveys, and, thus, it is of great significance to the ongoing use and management of rural homestead information, especially with regards to the confirmation of homestead property rights in China. Further, the proposed combination of UAV imagery and U-net technology may have a broader application in rural household surveys, as it can provide more information for decision-makers to grasp the current state of the rural socio-economic environment.},
DOI = {10.3390/ijgi9060403}
}



@Article{app10124247,
AUTHOR = {Zhong, Kefeng and Teng, Shuai and Liu, Gen and Chen, Gongfa and Cui, Fangsen},
TITLE = {Structural Damage Features Extracted by Convolutional Neural Networks from Mode Shapes},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {4247},
URL = {https://www.mdpi.com/2076-3417/10/12/4247},
ISSN = {2076-3417},
ABSTRACT = {This paper aims to locate damaged rods in a three-dimensional (3D) steel truss and reveals some internal working mechanisms of the convolutional neural network (CNN), which is based on the first-order modal parameters and CNN. The CNN training samples (including a large number of damage scenarios) are created by ABAQUS and PYTHON scripts. The mode shapes and mode curvature differences are taken as the inputs of the CNN training samples, respectively, and the damage locating accuracy of the CNN is investigated. Finally, the features extracted from each convolutional layer of the CNN are checked to reveal some internal working mechanisms of the CNN and explain the specific meanings of some features. The results show that the CNN-based damage detection method using mode shapes as the inputs has a higher locating accuracy for all damage degrees, while the method using mode curvature differences as the inputs has a lower accuracy for the targets with a low damage degree; however, with the increase of the target damage degree, it gradually achieves the same good locating accuracy as mode shapes. The features extracted from each convolutional layer show that the CNN can obtain the difference between the sample to be classified and the average of training samples in shallow layers, and then amplify the difference in the subsequent convolutional layer, which is similar to a power function, finally it produces a distinguishable peak signal at the damage location. Then a damage locating method is derived from the feature extraction of the CNN. All of these results indicate that the CNN using first-order modal parameters not only has a powerful damage location ability, but also opens up a new way to extract damage features from the measurement data.},
DOI = {10.3390/app10124247}
}



@Article{rs12121990,
AUTHOR = {Wagner, Matthias P. and Oppelt, Natascha},
TITLE = {Deep Learning and Adaptive Graph-Based Growing Contours for Agricultural Field Extraction},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1990},
URL = {https://www.mdpi.com/2072-4292/12/12/1990},
ISSN = {2072-4292},
ABSTRACT = {Field mapping and information on agricultural landscapes is of increasing importance for many applications. Monitoring schemes and national cadasters provide a rich source of information but their maintenance and regular updating is costly and labor-intensive. Automatized mapping of fields based on remote sensing imagery may aid in this task and allow for a faster and more regular observation. Although remote sensing has seen extensive use in agricultural research topics, such as plant health monitoring, crop type classification, yield prediction, and irrigation, field delineation and extraction has seen comparatively little research interest. In this study, we present a field boundary detection technique based on deep learning and a variety of image features, and combine it with the graph-based growing contours (GGC) method to extract agricultural fields in a study area in northern Germany. The boundary detection step only requires red, green, and blue (RGB) data and is therefore largely independent of the sensor used. We compare different image features based on color and luminosity information and evaluate their usefulness for the task of field boundary detection. A model based on texture metrics, gradient information, Hessian matrix eigenvalues, and local statistics showed good results with accuracies up to 88.2%, an area under the ROC curve (AUC) of up to 0.94, and F1 score of up to 0.88. The exclusive use of these universal image features may also facilitate transferability to other regions. We further present modifications to the GGC method intended to aid in upscaling of the method through process acceleration with a minimal effect on results. We combined the boundary detection results with the GGC method for field polygon extraction. Results were promising, with the new GGC version performing similarly or better than the original version while experiencing an acceleration of 1.3&times; to 2.3&times; on different subsets and input complexities. Further research may explore other applications of the GGC method outside agricultural remote sensing and field extraction.},
DOI = {10.3390/rs12121990}
}



@Article{rs12122000,
AUTHOR = {Kwan, Chiman and Ayhan, Bulent and Budavari, Bence and Lu, Yan and Perez, Daniel and Li, Jiang and Bernabe, Sergio and Plaza, Antonio},
TITLE = {Deep Learning for Land Cover Classification Using Only a Few Bands},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2000},
URL = {https://www.mdpi.com/2072-4292/12/12/2000},
ISSN = {2072-4292},
ABSTRACT = {There is an emerging interest in using hyperspectral data for land cover classification. The motivation behind using hyperspectral data is the notion that increasing the number of narrowband spectral channels would provide richer spectral information and thus help improve the land cover classification performance. Although hyperspectral data with hundreds of channels provide detailed spectral signatures, the curse of dimensionality might lead to degradation in the land cover classification performance. Moreover, in some practical applications, hyperspectral data may not be available due to cost, data storage, or bandwidth issues, and RGB and near infrared (NIR) could be the only image bands available for land cover classification. Light detection and ranging (LiDAR) data is another type of data to assist land cover classification especially if the land covers of interest have different heights. In this paper, we examined the performance of two Convolutional Neural Network (CNN)-based deep learning algorithms for land cover classification using only four bands (RGB+NIR) and five bands (RGB+NIR+LiDAR), where these limited number of image bands were augmented using Extended Multi-attribute Profiles (EMAP). The deep learning algorithms were applied to a well-known dataset used in the 2013 IEEE Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest. With EMAP augmentation, the two deep learning algorithms were observed to achieve better land cover classification performance using only four bands as compared to that using all 144 hyperspectral bands.},
DOI = {10.3390/rs12122000}
}



@Article{designs4020015,
AUTHOR = {Parvaresh, Ahmad and Abrazeh, Saber and Mohseni, Saeid-Reza and Zeitouni, Meisam Jahanshahi and Gheisarnejad, Meysam and Khooban, Mohammad-Hassan},
TITLE = {A Novel Deep Learning Backstepping Controller-Based Digital Twins Technology for Pitch Angle Control of Variable Speed Wind Turbine},
JOURNAL = {Designs},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2411-9660/4/2/15},
ISSN = {2411-9660},
ABSTRACT = {This paper proposes a deep deterministic policy gradient (DDPG) based nonlinear integral backstepping (NIB) in combination with model free control (MFC) for pitch angle control of variable speed wind turbine. In particular, the controller has been presented as a digital twin (DT) concept, which is an increasingly growing method in a variety of applications. In DDPG-NIB-MFC, the pitch angle is considered as the control input that depends on the optimal rotor speed, which is usually derived from effective wind speed. The system stability according to the Lyapunov theory can be achieved by the recursive nature of the backstepping theory and the integral action has been used to compensate for the steady-state error. Moreover, due to the nonlinear characteristics of wind turbines, the MFC aims to handle the un-modeled system dynamics and disturbances. The DDPG algorithm with actor-critic structure has been added in the proposed control structure to efficiently and adaptively tune the controller parameters embedded in the NIB controller. Under this effort, a digital twin of a presented controller is defined as a real-time and probabilistic model which is implemented on the digital signal processor (DSP) computing device. To ensure the performance of the proposed approach and output behavior of the system, software-in-loop (SIL) and hardware-in-loop (HIL) testing procedures have been considered. From the simulation and implementation outcomes, it can be concluded that the proposed backstepping controller based DDPG is more effective, robust, and adaptive than the backstepping and proportional-integral (PI) controllers optimized by particle swarm optimization (PSO) in the presence of uncertainties and disturbances.},
DOI = {10.3390/designs4020015}
}



@Article{rs12122002,
AUTHOR = {Panagiotou, Emmanouil and Chochlakis, Georgios and Grammatikopoulos, Lazaros and Charou, Eleni},
TITLE = {Generating Elevation Surface from a Single RGB Remotely Sensed Image Using Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2002},
URL = {https://www.mdpi.com/2072-4292/12/12/2002},
ISSN = {2072-4292},
ABSTRACT = {Generating Digital Elevation Models (DEM) from satellite imagery or other data sources constitutes an essential tool for a plethora of applications and disciplines, ranging from 3D flight planning and simulation, autonomous driving and satellite navigation, such as GPS, to modeling water flow, precision farming and forestry. The task of extracting this 3D geometry from a given surface hitherto requires a combination of appropriately collected corresponding samples and/or specialized equipment, as inferring the elevation from single image data is out of reach for contemporary approaches. On the other hand, Artificial Intelligence (AI) and Machine Learning (ML) algorithms have experienced unprecedented growth in recent years as they can extrapolate rules in a data-driven manner and retrieve convoluted, nonlinear one-to-one mappings, such as an approximate mapping from satellite imagery to DEMs. Therefore, we propose an end-to-end Deep Learning (DL) approach to construct this mapping and to generate an absolute or relative point cloud estimation of a DEM given a single RGB satellite (Sentinel-2 imagery in this work) or drone image. The model has been readily extended to incorporate available information from the non-visible electromagnetic spectrum. Unlike existing methods, we only exploit one image for the production of the elevation data, rendering our approach less restrictive and constrained, but suboptimal compared to them at the same time. Moreover, recent advances in software and hardware allow us to make the inference and the generation extremely fast, even on moderate hardware. We deploy Conditional Generative Adversarial networks (CGAN), which are the state-of-the-art approach to image-to-image translation. We expect our work to serve as a springboard for further development in this field and to foster the integration of such methods in the process of generating, updating and analyzing DEMs.},
DOI = {10.3390/rs12122002}
}



@Article{rs12122012,
AUTHOR = {Kucharczyk, Maja and Hay, Geoffrey J. and Ghaffarian, Salar and Hugenholtz, Chris H.},
TITLE = {Geographic Object-Based Image Analysis: A Primer and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2012},
URL = {https://www.mdpi.com/2072-4292/12/12/2012},
ISSN = {2072-4292},
ABSTRACT = {Geographic object-based image analysis (GEOBIA) is a remote sensing image analysis paradigm that defines and examines image-objects: groups of neighboring pixels that represent real-world geographic objects. Recent reviews have examined methodological considerations and highlighted how GEOBIA improves upon the 30+ year pixel-based approach, particularly for H-resolution imagery. However, the literature also exposes an opportunity to improve guidance on the application of GEOBIA for novice practitioners. In this paper, we describe the theoretical foundations of GEOBIA and provide a comprehensive overview of the methodological workflow, including: (i) software-specific approaches (open-source and commercial); (ii) best practices informed by research; and (iii) the current status of methodological research. Building on this foundation, we then review recent research on the convergence of GEOBIA with deep convolutional neural networks, which we suggest is a new form of GEOBIA. Specifically, we discuss general integrative approaches and offer recommendations for future research. Overall, this paper describes the past, present, and anticipated future of GEOBIA in a novice-accessible format, while providing innovation and depth to experienced practitioners.},
DOI = {10.3390/rs12122012}
}



@Article{ai1020021,
AUTHOR = {Barbedo, Jayme Garcia Arnal},
TITLE = {Detecting and Classifying Pests in Crops Using Proximal Images and Machine Learning: A Review},
JOURNAL = {AI},
VOLUME = {1},
YEAR = {2020},
NUMBER = {2},
PAGES = {312--328},
URL = {https://www.mdpi.com/2673-2688/1/2/21},
ISSN = {2673-2688},
ABSTRACT = {Pest management is among the most important activities in a farm. Monitoring all different species visually may not be effective, especially in large properties. Accordingly, considerable research effort has been spent towards the development of effective ways to remotely monitor potential infestations. A growing number of solutions combine proximal digital images with machine learning techniques, but since species and conditions associated to each study vary considerably, it is difficult to draw a realistic picture of the actual state of the art on the subject. In this context, the objectives of this article are (1) to briefly describe some of the most relevant investigations on the subject of automatic pest detection using proximal digital images and machine learning; (2) to provide a unified overview of the research carried out so far, with special emphasis to research gaps that still linger; (3) to propose some possible targets for future research.},
DOI = {10.3390/ai1020021}
}



@Article{rs12122026,
AUTHOR = {Bowler, Ellen and Fretwell, Peter T. and French, Geoffrey and Mackiewicz, Michal},
TITLE = {Using Deep Learning to Count Albatrosses from Space: Assessing Results in Light of Ground Truth Uncertainty},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2026},
URL = {https://www.mdpi.com/2072-4292/12/12/2026},
ISSN = {2072-4292},
ABSTRACT = {Many wildlife species inhabit inaccessible environments, limiting researchers ability to conduct essential population surveys. Recently, very high resolution (sub-metre) satellite imagery has enabled remote monitoring of certain species directly from space; however, manual analysis of the imagery is time-consuming, expensive and subjective. State-of-the-art deep learning approaches can automate this process; however, often image datasets are small, and uncertainty in ground truth labels can affect supervised training schemes and the interpretation of errors. In this paper, we investigate these challenges by conducting both manual and automated counts of nesting Wandering Albatrosses on four separate islands, captured by the 31 cm resolution WorldView-3 sensor. We collect counts from six observers, and train a convolutional neural network (U-Net) using leave-one-island-out cross-validation and different combinations of ground truth labels. We show that (1) interobserver variation in manual counts is significant and differs between the four islands, (2) the small dataset can limit the networks ability to generalise to unseen imagery and (3) the choice of ground truth labels can have a significant impact on our assessment of network performance. Our final results show the network detects albatrosses as accurately as human observers for two of the islands, while in the other two misclassifications are largely caused by the presence of noise, cloud cover and habitat, which was not present in the training dataset. While the results show promise, we stress the importance of considering these factors for any study where data is limited and observer confidence is variable.},
DOI = {10.3390/rs12122026}
}



@Article{rs12122051,
AUTHOR = {Pamart, Anthony and Morlet, François and De Luca, Livio and Veron, Philippe},
TITLE = {A Robust and Versatile Pipeline for Automatic Photogrammetric-Based Registration of Multimodal Cultural Heritage Documentation},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {2051},
URL = {https://www.mdpi.com/2072-4292/12/12/2051},
ISSN = {2072-4292},
ABSTRACT = {Imaging techniques and Image Based-Modeling (IBM) practices in the field of Cultural Heritage (CH) studies are nowadays no longer used as one-shot applications but as various and complex scenarios involving multiple modalities; sensors, scales, spectral bands and temporalities utilized by various experts. Current use of Structure from Motion and photogrammetric methods necessitates some improvements in iterative registration to ease the growing complexity in the management of the scientific imaging applied on heritage assets. In this context, the co-registration of photo-documentation among other imaging resources is a key step in order to move towards data fusion and collaborative semantic enrichment scenarios. This paper presents the recent development of a Totally Automated Co-registration and Orientation library (TACO) based on the interoperability of open-source solutions to conduct photogrammetric-based registration. The proposed methodology addresses and solves some gaps in term of robustness and versatility in the field of incremental and global orientation of image-sets dedicated to CH practices.},
DOI = {10.3390/rs12122051}
}



@Article{w12061825,
AUTHOR = {Muhadi, Nur Atirah and Abdullah, Ahmad Fikri and Bejo, Siti Khairunniza and Mahadi, Muhammad Razif and Mijic, Ana},
TITLE = {Image Segmentation Methods for Flood Monitoring System},
JOURNAL = {Water},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1825},
URL = {https://www.mdpi.com/2073-4441/12/6/1825},
ISSN = {2073-4441},
ABSTRACT = {Flood disasters are considered annual disasters in Malaysia due to their consistent occurrence. They are among the most dangerous disasters in the country. Lack of data during flood events is the main constraint to improving flood monitoring systems. With the rapid development of information technology, flood monitoring systems using a computer vision approach have gained attention over the last decade. Computer vision requires an image segmentation technique to understand the content of the image and to facilitate analysis. Various segmentation algorithms have been developed to improve results. This paper presents a comparative study of image segmentation techniques used in extracting water information from digital images. The segmentation methods were evaluated visually and statistically. To evaluate the segmentation methods statistically, the dice similarity coefficient and the Jaccard index were calculated to measure the similarity between the segmentation results and the ground truth images. Based on the experimental results, the hybrid technique obtained the highest values among the three methods, yielding an average of 97.70% for the dice score and 95.51% for the Jaccard index. Therefore, we concluded that the hybrid technique is a promising segmentation method compared to the others in extracting water features from digital images.},
DOI = {10.3390/w12061825}
}



@Article{s20133664,
AUTHOR = {Zhang, Qichen and Zhu, Meiqiang and Zou, Liang and Li, Ming and Zhang, Yong},
TITLE = {Learning Reward Function with Matching Network for Mapless Navigation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {3664},
URL = {https://www.mdpi.com/1424-8220/20/13/3664},
ISSN = {1424-8220},
ABSTRACT = {Deep reinforcement learning (DRL) has been successfully applied in mapless navigation. An important issue in DRL is to design a reward function for evaluating actions of agents. However, designing a robust and suitable reward function greatly depends on the designer&rsquo;s experience and intuition. To address this concern, we consider employing reward shaping from trajectories on similar navigation tasks without human supervision, and propose a general reward function based on matching network (MN). The MN-based reward function is able to gain the experience by pre-training through trajectories on different navigation tasks and accelerate the training speed of DRL in new tasks. The proposed reward function keeps the optimal strategy of DRL unchanged. The simulation results on two static maps show that the DRL converge with less iterations via the learned reward function than the state-of-the-art mapless navigation methods. The proposed method performs well in dynamic maps with partially moving obstacles. Even when test maps are different from training maps, the proposed strategy is able to complete the navigation tasks without additional training.},
DOI = {10.3390/s20133664}
}



@Article{su12135323,
AUTHOR = {Lin, Yao-Chin and Yeh, Ching-Chuan and Chen, Wei-Hung and Liu, Wei-Chun and Wang, Jyun-Jie},
TITLE = {The Use of Big Data for Sustainable Development in Motor Production Line Issues},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {5323},
URL = {https://www.mdpi.com/2071-1050/12/13/5323},
ISSN = {2071-1050},
ABSTRACT = {This study explores big data gathered from motor production lines to gain a better understanding of production line issues. Motor products from Solen Electric Company&rsquo;s motor production lines were used to predict failure points based on big data analytics, where 3606 datapoints from the company&rsquo;s testing equipment were statistically analyzed. The current study focused on secondary data and expert interview results to further define the relevant statistical dimensions. Only 14 of the original 88 detection parameters were required for monitoring the production line. The relationships between these parameters and the relevant motor components were established to indicate how an abnormal reading may be interpreted to quickly resolve an issue. Thus, a theoretical model for the monitoring of the motor production line was proposed. Further implications and practical suggestions are also offered to improve the production lines. This study explores big data analysis and smart manufacturing and demonstrates the promise of these technologies in improving production line efficiency and reducing waste to promote sustainable production goals. Big data thus constitute the core technology for advancing production lines into Industry 4.0 and promoting industry sustainability.},
DOI = {10.3390/su12135323}
}



@Article{app10134574,
AUTHOR = {Ghaffarian, Saman and Rezaie Farhadabad, Ali and Kerle, Norman},
TITLE = {Post-Disaster Recovery Monitoring with Google Earth Engine},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {4574},
URL = {https://www.mdpi.com/2076-3417/10/13/4574},
ISSN = {2076-3417},
ABSTRACT = {Post-disaster recovery is a complex process in terms of measuring its progress after a disaster and understanding its components and influencing factors. During this process, disaster planners and governments need reliable information to make decisions towards building the affected region back to normal (pre-disaster), or even improved, conditions. Hence, it is essential to use methods to understand the dynamics/variables of the post-disaster recovery process, and rapid and cost-effective data and tools to monitor the process. Google Earth Engine (GEE) provides free access to vast amounts of remote sensing (RS) data and a powerful computing environment in a cloud platform, making it an attractive tool to analyze earth surface data. In this study we assessed the suitability of GEE to analyze and track recovery. To do so, we employed GEE to assess the recovery process over a three-year period after Typhoon Haiyan, which struck Leyte island, in the Philippines, in 2013. We developed an approach to (i) generate cloud and shadow-free image composites from Landsat 7 and 8 satellite imagery and produce land cover classification data using the Random Forest method, and (ii) generate damage and recovery maps based on post-classification change analysis. The method produced land cover maps with accuracies &gt;88%. We used the model to produce damage and three time-step recovery maps for 62 municipalities on Leyte island. The results showed that most of the municipalities had recovered after three years in terms of returning to the pre-disaster situation based on the selected land cover change analysis. However, more analysis (e.g., functional assessment) based on detailed data (e.g., land use maps) is needed to evaluate the more complex and subtle socio-economic aspects of the recovery. The study showed that GEE has good potential for monitoring the recovery process for extensive regions. However, the most important limitation is the lack of very-high-resolution RS data that are critical to assess the process in detail, in particular in complex urban environments.},
DOI = {10.3390/app10134574}
}



@Article{rs12132136,
AUTHOR = {Veeranampalayam Sivakumar, Arun Narenthiran and Li, Jiating and Scott, Stephen and Psota, Eric and J. Jhala, Amit and Luck, Joe D. and Shi, Yeyin},
TITLE = {Comparison of Object Detection and Patch-Based Classification Deep Learning Models on Mid- to Late-Season Weed Detection in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {2136},
URL = {https://www.mdpi.com/2072-4292/12/13/2136},
ISSN = {2072-4292},
ABSTRACT = {Mid- to late-season weeds that escape from the routine early-season weed management threaten agricultural production by creating a large number of seeds for several future growing seasons. Rapid and accurate detection of weed patches in field is the first step of site-specific weed management. In this study, object detection-based convolutional neural network models were trained and evaluated over low-altitude unmanned aerial vehicle (UAV) imagery for mid- to late-season weed detection in soybean fields. The performance of two object detection models, Faster RCNN and the Single Shot Detector (SSD), were evaluated and compared in terms of weed detection performance using mean Intersection over Union (IoU) and inference speed. It was found that the Faster RCNN model with 200 box proposals had similar good weed detection performance to the SSD model in terms of precision, recall, f1 score, and IoU, as well as a similar inference time. The precision, recall, f1 score and IoU were 0.65, 0.68, 0.66 and 0.85 for Faster RCNN with 200 proposals, and 0.66, 0.68, 0.67 and 0.84 for SSD, respectively. However, the optimal confidence threshold of the SSD model was found to be much lower than that of the Faster RCNN model, which indicated that SSD might have lower generalization performance than Faster RCNN for mid- to late-season weed detection in soybean fields using UAV imagery. The performance of the object detection model was also compared with patch-based CNN model. The Faster RCNN model yielded a better weed detection performance than the patch-based CNN with and without overlap. The inference time of Faster RCNN was similar to patch-based CNN without overlap, but significantly less than patch-based CNN with overlap. Hence, Faster RCNN was found to be the best model in terms of weed detection performance and inference time among the different models compared in this study. This work is important in understanding the potential and identifying the algorithms for an on-farm, near real-time weed detection and management.},
DOI = {10.3390/rs12132136}
}



@Article{rs12132169,
AUTHOR = {Arce, Samuel and Vernon, Cory A. and Hammond, Joshua and Newell, Valerie and Janson, Joseph and Franke, Kevin W. and Hedengren, John D.},
TITLE = {Automated 3D Reconstruction Using Optimized View-Planning Algorithms for Iterative Development of Structure-from-Motion Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {13},
ARTICLE-NUMBER = {2169},
URL = {https://www.mdpi.com/2072-4292/12/13/2169},
ISSN = {2072-4292},
ABSTRACT = {Unsupervised machine learning algorithms (clustering, genetic, and principal component analysis) automate Unmanned Aerial Vehicle (UAV) missions as well as the creation and refinement of iterative 3D photogrammetric models with a next best view (NBV) approach. The novel approach uses Structure-from-Motion (SfM) to achieve convergence to a specified orthomosaic resolution by identifying edges in the point cloud and planning cameras that &ldquo;view&rdquo; the holes identified by edges without requiring an initial model. This iterative UAV photogrammetric method successfully runs in various Microsoft AirSim environments. Simulated ground sampling distance (GSD) of models reaches as low as     3.4     cm per pixel, and generally, successive iterations improve resolution. Besides analogous application in simulated environments, a field study of a retired municipal water tank illustrates the practical application and advantages of automated UAV iterative inspection of infrastructure using     63 %     fewer photographs than a comparable manual flight with analogous density point clouds obtaining a GSD of less than 3 cm per pixel. Each iteration qualitatively increases resolution according to a logarithmic regression, reduces holes in models, and adds details to model edges.},
DOI = {10.3390/rs12132169}
}



@Article{agriculture10070277,
AUTHOR = {García-Martínez, Héctor and Flores-Magdaleno, Héctor and Ascencio-Hernández, Roberto and Khalil-Gardezi, Abdul and Tijerina-Chávez, Leonardo and Mancilla-Villa, Oscar R. and Vázquez-Peña, Mario A.},
TITLE = {Corn Grain Yield Estimation from Vegetation Indices, Canopy Cover, Plant Density, and a Neural Network Using Multispectral and RGB Images Acquired with Unmanned Aerial Vehicles},
JOURNAL = {Agriculture},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {277},
URL = {https://www.mdpi.com/2077-0472/10/7/277},
ISSN = {2077-0472},
ABSTRACT = {Corn yields vary spatially and temporally in the plots as a result of weather, altitude, variety, plant density, available water, nutrients, and planting date; these are the main factors that influence crop yield. In this study, different multispectral and red-green-blue (RGB) vegetation indices were analyzed, as well as the digitally estimated canopy cover and plant density, in order to estimate corn grain yield using a neural network model. The relative importance of the predictor variables was also analyzed. An experiment was established with five levels of nitrogen fertilization (140, 200, 260, 320, and 380 kg/ha) and four replicates, in a completely randomized block design, resulting in 20 experimental polygons. Crop information was captured using two sensors (Parrot Sequoia_4.9, and DJI FC6310_8.8) mounted on an unmanned aerial vehicle (UAV) for two flight dates at 47 and 79 days after sowing (DAS). The correlation coefficient between the plant density, obtained through the digital count of corn plants, and the corn grain yield was 0.94; this variable was the one with the highest relative importance in the yield estimation according to Garson&rsquo;s algorithm. The canopy cover, digitally estimated, showed a correlation coefficient of 0.77 with respect to the corn grain yield, while the relative importance of this variable in the yield estimation was 0.080 and 0.093 for 47 and 79 DAS, respectively. The wide dynamic range vegetation index (WDRVI), plant density, and canopy cover showed the highest correlation coefficient and the smallest errors (R = 0.99, mean absolute error (MAE) = 0.028 t ha&minus;1, root mean square error (RMSE) = 0.125 t ha&minus;1) in the corn grain yield estimation at 47 DAS, with the WDRVI index and the density being the variables with the highest relative importance for this crop development date. For the 79 DAS flight, the combination of the normalized difference vegetation index (NDVI), normalized difference red edge (NDRE), WDRVI, excess green (EXG), triangular greenness index (TGI), and visible atmospherically resistant index (VARI), as well as plant density and canopy cover, generated the highest correlation coefficient and the smallest errors (R = 0.97, MAE = 0.249 t ha&minus;1, RMSE = 0.425 t ha&minus;1) in the corn grain yield estimation, where the density and the NDVI were the variables with the highest relative importance, with values of 0.295 and 0.184, respectively. However, the WDRVI, plant density, and canopy cover estimated the corn grain yield with acceptable precision (R = 0.96, MAE = 0.209 t ha&minus;1, RMSE = 0.449 t ha&minus;1). The generated neural network models provided a high correlation coefficient between the estimated and the observed corn grain yield, and also showed acceptable errors in the yield estimation. The spectral information registered through remote sensors mounted on unmanned aerial vehicles and its processing in vegetation indices, canopy cover, and plant density allowed the characterization and estimation of corn grain yield. Such information is very useful for decision-making and agricultural activities planning.},
DOI = {10.3390/agriculture10070277}
}



@Article{rs12142188,
AUTHOR = {Niculescu, Simona and Boissonnat, Jean-Baptiste and Lardeux, Cédric and Roberts, Dar and Hanganu, Jenica and Billey, Antoine and Constantinescu, Adrian and Doroftei, Mihai},
TITLE = {Synergy of High-Resolution Radar and Optical Images Satellite for Identification and Mapping of Wetland Macrophytes on the Danube Delta},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2188},
URL = {https://www.mdpi.com/2072-4292/12/14/2188},
ISSN = {2072-4292},
ABSTRACT = {In wetland environments, vegetation has an important role in ecological functioning. The main goal of this work was to identify an optimal combination of Sentinel-1 (S1), Sentinel-2 (S2), and Pleiades data using ground-reference data to accurately map wetland macrophytes in the Danube Delta. We tested several combinations of optical and Synthetic Aperture Radar (SAR) data rigorously at two levels. First, in order to reduce the confusion between reed (Phragmites australis (Cav.) Trin. ex Steud.) and other macrophyte communities, a time series analysis of S1 data was performed. The potential of S1 for detection of compact reed on plaur, compact reed on plaur/reed cut, open reed on plaur, pure reed, and reed on salinized soil was evaluated through time series of backscatter coefficient and coherence ratio images, calculated mainly according to the phenology of the reed. The analysis of backscattering coefficients allowed separation of reed classes that strongly overlapped. The coherence coefficient showed that C-band SAR repeat pass interferometric coherence for cut reed detection is feasible. In the second section, random forest (RF) classification was applied to the S2, Pleiades, and S1 data and in situ observations to discriminate and map reed against other aquatic macrophytes (submerged aquatic vegetation (SAV), emergent macrophytes, some floating broad-leaved and floating vegetation of delta lakes). In addition, different optical indices were included in the RF. A total of 67 classification models were made in several sensor combinations with two series of validation samples (with the reed and without reed) using both a simple and more detailed classification schema. The results showed that reed is completely discriminable compared to other macrophyte communities with all sensor combinations. In all combinations, the model-based producer’s accuracy (PA) and user’s accuracy (UA) for reed with both nomenclatures were over 90%. The diverse combinations of sensors were valuable for improving the overall classification accuracy of all of the communities of aquatic macrophytes except Myriophyllum spicatum L.},
DOI = {10.3390/rs12142188}
}



@Article{s20143824,
AUTHOR = {Justa, Josef and Šmídl, Václav and Hamáček, Aleš},
TITLE = {Fast AHRS Filter for Accelerometer, Magnetometer, and Gyroscope Combination with Separated Sensor Corrections},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3824},
URL = {https://www.mdpi.com/1424-8220/20/14/3824},
ISSN = {1424-8220},
ABSTRACT = {A new predictor&ndash;corrector filter for attitude and heading reference systems (AHRS) using data from an orthogonal sensor combination of three accelerometers, three magnetometers and three gyroscopes is proposed. The filter uses the predictor&mdash;corrector structure, with prediction based on gyroscopes and independent correction steps for acceleration and magnetic field sensors. We propose two variants of the filter: (i) one using mathematical operations of special orthogonal group SO(3), that are accurate for nonlinear operations, for highest possible accuracy, and (ii) one using linearization of nonlinear operations for fast evaluation. Both approaches are quaternion-based filter realizations without redundant steps. The filters are compared to state of the art methods in this field on data recorded using low-cost microelectromechanical systems (MEMS) sensors with ground truth measured by the VICON optical system. Both filters achieved better accuracy than conventional methods at lower computational cost. The recorded data with ground truth reference and the source codes of both filters are publicly available.},
DOI = {10.3390/s20143824}
}



@Article{app10144735,
AUTHOR = {McClellan, Miranda and Cervelló-Pastor, Cristina and Sallent, Sebastià},
TITLE = {Deep Learning at the Mobile Edge: Opportunities for 5G Networks},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4735},
URL = {https://www.mdpi.com/2076-3417/10/14/4735},
ISSN = {2076-3417},
ABSTRACT = {Mobile edge computing (MEC) within 5G networks brings the power of cloud computing, storage, and analysis closer to the end user. The increased speeds and reduced delay enable novel applications such as connected vehicles, large-scale IoT, video streaming, and industry robotics. Machine Learning (ML) is leveraged within mobile edge computing to predict changes in demand based on cultural events, natural disasters, or daily commute patterns, and it prepares the network by automatically scaling up network resources as needed. Together, mobile edge computing and ML enable seamless automation of network management to reduce operational costs and enhance user experience. In this paper, we discuss the state of the art for ML within mobile edge computing and the advances needed in automating adaptive resource allocation, mobility modeling, security, and energy efficiency for 5G networks.},
DOI = {10.3390/app10144735}
}



@Article{electronics9071120,
AUTHOR = {Liang, Chao and Shanmugam, Bharanidharan and Azam, Sami and Karim, Asif and Islam, Ashraful and Zamani, Mazdak and Kavianpour, Sanaz and Idris, Norbik Bashah},
TITLE = {Intrusion Detection System for the Internet of Things Based on Blockchain and Multi-Agent Systems},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1120},
URL = {https://www.mdpi.com/2079-9292/9/7/1120},
ISSN = {2079-9292},
ABSTRACT = {With the popularity of Internet of Things (IoT) technology, the security of the IoT network has become an important issue. Traditional intrusion detection systems have their limitations when applied to the IoT network due to resource constraints and the complexity. This research focusses on the design, implementation and testing of an intrusion detection system which uses a hybrid placement strategy based on a multi-agent system, blockchain and deep learning algorithms. The system consists of the following modules: data collection, data management, analysis, and response. The National security lab&ndash;knowledge discovery and data mining NSL-KDD dataset is used to test the system. The results demonstrate the efficiency of deep learning algorithms when detecting attacks from the transport layer. The experiment indicates that deep learning algorithms are suitable for intrusion detection in IoT network environment.},
DOI = {10.3390/electronics9071120}
}



@Article{electronics9071121,
AUTHOR = {Kong, Weiren and Zhou, Deyun and Yang, Zhen and Zhao, Yiyang and Zhang, Kai},
TITLE = {UAV Autonomous Aerial Combat Maneuver Strategy Generation with Observation Error Based on State-Adversarial Deep Deterministic Policy Gradient and Inverse Reinforcement Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1121},
URL = {https://www.mdpi.com/2079-9292/9/7/1121},
ISSN = {2079-9292},
ABSTRACT = {With the development of unmanned aerial vehicle (UAV) and artificial intelligence (AI) technology, Intelligent UAV will be widely used in future autonomous aerial combat. Previous researches on autonomous aerial combat within visual range (WVR) have limitations due to simplifying assumptions, limited robustness, and ignoring sensor errors. In this paper, in order to consider the error of the aircraft sensors, we model the aerial combat WVR as a state-adversarial Markov decision process (SA-MDP), which introduce the small adversarial perturbations on state observations and these perturbations do not alter the environment directly, but can mislead the agent into making suboptimal decisions. Meanwhile, we propose a novel autonomous aerial combat maneuver strategy generation algorithm with high-performance and high-robustness based on state-adversarial deep deterministic policy gradient algorithm (SA-DDPG), which add a robustness regularizers related to an upper bound on performance loss at the actor-network. At the same time, a reward shaping method based on maximum entropy (MaxEnt) inverse reinforcement learning algorithm (IRL) is proposed to improve the aerial combat strategy generation algorithm&rsquo;s efficiency. Finally, the efficiency of the aerial combat strategy generation algorithm and the performance and robustness of the resulting aerial combat strategy is verified by simulation experiments. Our main contributions are three-fold. First, to introduce the observation errors of UAV, we are modeling air combat as SA-MDP. Second, to make the strategy network of air combat maneuver more robust in the presence of observation errors, we introduce regularizers into the policy gradient. Third, to solve the problem that air combat&rsquo;s reward function is too sparse, we use MaxEnt IRL to design a shaping reward to accelerate the convergence of SA-DDPG.},
DOI = {10.3390/electronics9071121}
}



@Article{agriengineering2030029,
AUTHOR = {Gao, Zongmei and Luo, Zhongwei and Zhang, Wen and Lv, Zhenzhen and Xu, Yanlei},
TITLE = {Deep Learning Application in Plant Stress Imaging: A Review},
JOURNAL = {AgriEngineering},
VOLUME = {2},
YEAR = {2020},
NUMBER = {3},
PAGES = {430--446},
URL = {https://www.mdpi.com/2624-7402/2/3/29},
ISSN = {2624-7402},
ABSTRACT = {Plant stress is one of major issues that cause significant economic loss for growers. The labor-intensive conventional methods for identifying the stressed plants constrain their applications. To address this issue, rapid methods are in urgent needs. Developments of advanced sensing and machine learning techniques trigger revolutions for precision agriculture based on deep learning and big data. In this paper, we reviewed the latest deep learning approaches pertinent to the image analysis of crop stress diagnosis. We compiled the current sensor tools and deep learning principles involved in plant stress phenotyping. In addition, we reviewed a variety of deep learning applications/functions with plant stress imaging, including classification, object detection, and segmentation, of which are closely intertwined. Furthermore, we summarized and discussed the current challenges and future development avenues in plant phenotyping.},
DOI = {10.3390/agriengineering2030029}
}



@Article{s20143923,
AUTHOR = {Jamil, Sonain and Fawad and Rahman, MuhibUr and Ullah, Amin and Badnava, Salman and Forsat, Masoud and Mirjavadi, Seyed Sajad},
TITLE = {Malicious UAV Detection Using Integrated Audio and Visual Features for Public Safety Applications},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3923},
URL = {https://www.mdpi.com/1424-8220/20/14/3923},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have become popular in surveillance, security, and remote monitoring. However, they also pose serious security threats to public privacy. The timely detection of a malicious drone is currently an open research issue for security provisioning companies. Recently, the problem has been addressed by a plethora of schemes. However, each plan has a limitation, such as extreme weather conditions and huge dataset requirements. In this paper, we propose a novel framework consisting of the hybrid handcrafted and deep feature to detect and localize malicious drones from their sound and image information. The respective datasets include sounds and occluded images of birds, airplanes, and thunderstorms, with variations in resolution and illumination. Various kernels of the support vector machine (SVM) are applied to classify the features. Experimental results validate the improved performance of the proposed scheme compared to other related methods.},
DOI = {10.3390/s20143923}
}



@Article{drones4030034,
AUTHOR = {Shahmoradi, Javad and Talebi, Elaheh and Roghanchi, Pedram and Hassanalian, Mostafa},
TITLE = {A Comprehensive Review of Applications of Drone Technology in the Mining Industry},
JOURNAL = {Drones},
VOLUME = {4},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {34},
URL = {https://www.mdpi.com/2504-446X/4/3/34},
ISSN = {2504-446X},
ABSTRACT = {This paper aims to provide a comprehensive review of the current state of drone technology and its applications in the mining industry. The mining industry has shown increased interest in the use of drones for routine operations. These applications include 3D mapping of the mine environment, ore control, rock discontinuities mapping, postblast rock fragmentation measurements, and tailing stability monitoring, to name a few. The article offers a review of drone types, specifications, and applications of commercially available drones for mining applications. Finally, the research needs for the design and implementation of drones for underground mining applications are discussed.},
DOI = {10.3390/drones4030034}
}



@Article{ani10071207,
AUTHOR = {Akçay, Hüseyin Gökhan and Kabasakal, Bekir and Aksu, Duygugül and Demir, Nusret and Öz, Melih and Erdoğan, Ali},
TITLE = {Automated Bird Counting with Deep Learning for Regional Bird Distribution Mapping},
JOURNAL = {Animals},
VOLUME = {10},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1207},
URL = {https://www.mdpi.com/2076-2615/10/7/1207},
PubMedID = {32708550},
ISSN = {2076-2615},
ABSTRACT = {A challenging problem in the field of avian ecology is deriving information on bird population movement trends. This necessitates the regular counting of birds which is usually not an easily-achievable task. A promising attempt towards solving the bird counting problem in a more consistent and fast way is to predict the number of birds in different regions from their photos. For this purpose, we exploit the ability of computers to learn from past data through deep learning which has been a leading sub-field of AI for image understanding. Our data source is a collection of on-ground photos taken during our long run of birding activity. We employ several state-of-the-art generic object-detection algorithms to learn to detect birds, each being a member of one of the 38 identified species, in natural scenes. The experiments revealed that computer-aided counting outperformed the manual counting with respect to both accuracy and time. As a real-world application of image-based bird counting, we prepared the spatial bird order distribution and species diversity maps of Turkey by utilizing the geographic information system (GIS) technology. Our results suggested that deep learning can assist humans in bird monitoring activities and increase citizen scientists&rsquo; participation in large-scale bird surveys.},
DOI = {10.3390/ani10071207}
}



@Article{app10144870,
AUTHOR = {Coviello, Luca and Cristoforetti, Marco and Jurman, Giuseppe and Furlanello, Cesare},
TITLE = {GBCNet: In-Field Grape Berries Counting for Yield Estimation by Dilated CNNs},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {4870},
URL = {https://www.mdpi.com/2076-3417/10/14/4870},
ISSN = {2076-3417},
ABSTRACT = {We introduce here the Grape Berries Counting Net (GBCNet), a tool for accurate fruit yield estimation from smartphone cameras, by adapting Deep Learning algorithms originally developed for crowd counting. We test GBCNet using cross-validation procedure on two original datasets CR1 and CR2 of grape pictures taken in-field before veraison. A total of 35,668 berries have been manually annotated for the task. GBCNet achieves good performances on both the seven grape varieties dataset CR1, although with a different accuracy level depending on the variety, and on the single variety dataset CR2: in particular Mean Average Error (MAE) ranges from 0.85% for Pinot Gris to 11.73% for Marzemino on CR1 and reaches 7.24% on the Teroldego CR2 dataset.},
DOI = {10.3390/app10144870}
}



@Article{s20143948,
AUTHOR = {Fu, Wenpeng and Liu, Ran and Wang, Heng and Ali, Rashid and He, Yongping and Cao, Zhiqiang and Qin, Zhenghong},
TITLE = {A Method of Multiple Dynamic Objects Identification and Localization Based on Laser and RFID},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3948},
URL = {https://www.mdpi.com/1424-8220/20/14/3948},
ISSN = {1424-8220},
ABSTRACT = {In an indoor environment, object identification and localization are paramount for human-object interaction. Visual or laser-based sensors can achieve the identification and localization of the object based on its appearance, but these approaches are computationally expensive and not robust against the environment with obstacles. Radio Frequency Identification (RFID) has a unique tag ID to identify the object, but it cannot accurately locate it. Therefore, in this paper, the data of RFID and laser range finder are fused for the better identification and localization of multiple dynamic objects in an indoor environment. The main method is to use the laser range finder to estimate the radial velocities of objects in a certain environment, and match them with the object&rsquo;s radial velocities estimated by the RFID phase. The method also uses a fixed time series as &ldquo;sliding time window&rdquo; to find the cluster with the highest similarity of each RFID tag in each window. Moreover, the Pearson correlation coefficient (PCC) is used in the update stage of the particle filter (PF) to estimate the moving path of each cluster in order to improve the accuracy in a complex environment with obstacles. The experiments were verified by a SCITOS G5 robot. The results show that this method can achieve an matching rate of 90.18% and a localization accuracy of 0.33m in an environment with the presence of obstacles. This method effectively improves the matching rate and localization accuracy of multiple objects in indoor scenes when compared to the Bray-Curtis (BC) similarity matching-based approach as well as the particle filter-based approach.},
DOI = {10.3390/s20143948}
}



@Article{f11070763,
AUTHOR = {Klemmt, Hans-Joachim and Seitz, Rudolf and Straub, Christoph},
TITLE = {Application of Haralick’s Texture Features for Rapid Detection of Windthrow Hotspots in Orthophotos},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {763},
URL = {https://www.mdpi.com/1999-4907/11/7/763},
ISSN = {1999-4907},
ABSTRACT = {Windthrow and storm damage are crucial issues in practical forestry. We propose a method for rapid detection of windthrow hotspots in airborne digital orthophotos. Therefore, we apply Haralick&rsquo;s texture features on 50 &times; 50 m cells of the orthophotos and classify the cells with a random forest algorithm. We apply the classification results from a training data set on a validation set. The overall classification accuracy of the proposed method varies between 76% for fine distinction of the cells and 96% for a distinction level that tried to detect only severe damaged cells. The proposed method enables the rapid detection of windthrow hotspots in forests immediately after their occurrence in single-date data. It is not adequate for the determination of areas with only single fallen trees. Future research will investigate the possibilities and limitations when applying the method on other data sources (e.g., optical satellite data).},
DOI = {10.3390/f11070763}
}



@Article{s20143954,
AUTHOR = {Ahmed, Habib and La, Hung Manh and Gucunski, Nenad},
TITLE = {Review of Non-Destructive Civil Infrastructure Evaluation for Bridges: State-of-the-Art Robotic Platforms, Sensors and Algorithms},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3954},
URL = {https://www.mdpi.com/1424-8220/20/14/3954},
ISSN = {1424-8220},
ABSTRACT = {The non-destructive evaluation (NDE) of civil infrastructure has been an active area of research in recent decades. The traditional inspection of civil infrastructure mostly relies on visual inspection using human inspectors. To facilitate this process, different sensors for data collection and techniques for data analyses have been used to effectively carry out this task in an automated fashion. This review-based study will examine some of the recent developments in the field of autonomous robotic platforms for NDE and the structural health monitoring (SHM) of bridges. Some of the salient features of this review-based study will be discussed in the light of the existing surveys and reviews that have been published in the recent past, which will enable the clarification regarding the novelty of the present review-based study. The review methodology will be discussed in sufficient depth, which will provide insights regarding some of the primary aspects of the review methodology followed by this review-based study. In order to provide an in-depth examination of the state-of-the-art, the current research will examine the three major research streams. The first stream relates to technological robotic platforms developed for NDE of bridges. The second stream of literature examines myriad sensors used for the development of robotic platforms for the NDE of bridges. The third stream of literature highlights different algorithms for the surface- and sub-surface-level analysis of bridges that have been developed by studies in the past. A number of challenges towards the development of robotic platforms have also been discussed.},
DOI = {10.3390/s20143954}
}



@Article{robotics9030054,
AUTHOR = {Al-Buraiki, Omar and Wu, Wenbo and Payeur, Pierre},
TITLE = {Probabilistic Allocation of Specialized Robots on Targets Detected Using Deep Learning Networks},
JOURNAL = {Robotics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {54},
URL = {https://www.mdpi.com/2218-6581/9/3/54},
ISSN = {2218-6581},
ABSTRACT = {Task allocation for specialized unmanned robotic agents is addressed in this paper. Based on the assumptions that each individual robotic agent possesses specialized capabilities and that targets representing the tasks to be performed in the surrounding environment impose specific requirements, the proposed approach computes task-agent fitting probabilities to efficiently match the available robotic agents with the detected targets. The framework is supported by a deep learning method with an object instance segmentation capability, Mask R-CNN, that is adapted to provide target object recognition and localization estimates from vision sensors mounted on the robotic agents. Experimental validation, for indoor search-and-rescue (SAR) scenarios, is conducted and results demonstrate the reliability and efficiency of the proposed approach.},
DOI = {10.3390/robotics9030054}
}



@Article{s20143973,
AUTHOR = {Xue, Dan and Yuan, Weiqi},
TITLE = {Dynamic Partition Gaussian Crack Detection Algorithm Based on Projection Curve Distribution},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {3973},
URL = {https://www.mdpi.com/1424-8220/20/14/3973},
ISSN = {1424-8220},
ABSTRACT = {When detecting the cracks in the tunnel lining image, due to uneven illumination, there are generally differences in brightness and contrast between the cracked pixels and the surrounding background pixels as well as differences in the widths of the cracked pixels, which bring difficulty in detecting and extracting cracks. Therefore, this paper proposes a dynamic partitioned Gaussian crack detection algorithm based on the projection curve distribution. First, according to the distribution of the image projection curve, the background pixels are dynamically partitioned. Second, a new dynamic partitioned Gaussian (DPG) model was established, and the set rules of partition boundary conditions, partition number, and partition corresponding threshold were defined. Then, the threshold and multi-scale Gaussian factors corresponding to different crack widths were substituted into the Gaussian model to detect cracks. Finally, crack morphology and the breakpoint connection algorithm were combined to complete the crack extraction. The algorithm was tested on the lining gallery captured on the site of the Tang-Ling-Shan Tunnel in Liaoning Province, China. The optimal parameters in the algorithm were estimated through the Recall, Precision, and Time curves. From two aspects of qualitative and quantitative analysis, the experimental results demonstrate that this algorithm could effectively eliminate the effect of uneven illumination on crack detection. After detection, Recall could reach more than 96%, and after extraction, Precision was increased by more than 70%.},
DOI = {10.3390/s20143973}
}



@Article{rs12142308,
AUTHOR = {Muhadi, Nur Atirah and Abdullah, Ahmad Fikri and Bejo, Siti Khairunniza and Mahadi, Muhammad Razif and Mijic, Ana},
TITLE = {The Use of LiDAR-Derived DEM in Flood Applications: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2308},
URL = {https://www.mdpi.com/2072-4292/12/14/2308},
ISSN = {2072-4292},
ABSTRACT = {Flood occurrence is increasing due to escalated urbanization and extreme climate change; hence, various studies on this issue and methods of flood monitoring and mapping are also increasing to reduce the severe impacts of flood disasters. The advancement of current technologies such as light detection and ranging (LiDAR) systems facilitated and improved flood applications. In a LiDAR system, a laser emits light that travels to the ground and reflects off objects like buildings and trees. The reflected light energy returns to the sensor, whereby the time interval is recorded. Since the conventional methods cannot produce high-resolution digital elevation model (DEM) data, which results in low accuracy of flood simulation results, LiDAR data are extensively used as an alternative. This review aims to study the potential and the applications of LiDAR-derived DEM in flood studies. It also provides insight into the operating principles of different LiDAR systems, system components, and advantages and disadvantages of each system. This paper discusses several topics relevant to flood studies from a LiDAR-derived DEM perspective. Furthermore, the challenges and future perspectives regarding DEM LiDAR data for flood mapping and assessment are also reviewed. This study demonstrates that LiDAR-derived data are useful in flood risk management, especially in the future assessment of flood-related problems.},
DOI = {10.3390/rs12142308}
}



@Article{rs12142313,
AUTHOR = {El Mahrad, Badr and Newton, Alice and Icely, John D. and Kacimi, Ilias and Abalansa, Samuel and Snoussi, Maria},
TITLE = {Contribution of Remote Sensing Technologies to a Holistic Coastal and Marine Environmental Management Framework: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {14},
ARTICLE-NUMBER = {2313},
URL = {https://www.mdpi.com/2072-4292/12/14/2313},
ISSN = {2072-4292},
ABSTRACT = {Coastal and marine management require the evaluation of multiple environmental threats and issues. However, there are gaps in the necessary data and poor access or dissemination of existing data in many countries around the world. This research identifies how remote sensing can contribute to filling these gaps so that environmental agencies, such as the United Nations Environmental Programme, European Environmental Agency, and International Union for Conservation of Nature, can better implement environmental directives in a cost-effective manner. Remote sensing (RS) techniques generally allow for uniform data collection, with common acquisition and reporting methods, across large areas. Furthermore, these datasets are sometimes open-source, mainly when governments finance satellite missions. Some of these data can be used in holistic, coastal and marine environmental management frameworks, such as the DAPSI(W)R(M) framework (Drivers–Activities–Pressures–State changes–Impacts (on Welfare)–Responses (as Measures), an updated version of Drivers–Pressures–State–Impact–Responses. The framework is a useful and holistic problem-structuring framework that can be used to assess the causes, consequences, and responses to change in the marine environment. Six broad classifications of remote data collection technologies are reviewed for their potential contribution to integrated marine management, including Satellite-based Remote Sensing, Aerial Remote Sensing, Unmanned Aerial Vehicles, Unmanned Surface Vehicles, Unmanned Underwater Vehicles, and Static Sensors. A significant outcome of this study is practical inputs into each component of the DAPSI(W)R(M) framework. The RS applications are not expected to be all-inclusive; rather, they provide insight into the current use of the framework as a foundation for developing further holistic resource technologies for management strategies in the future. A significant outcome of this research will deliver practical insights for integrated coastal and marine management and demonstrate the usefulness of RS to support the implementation of environmental goals, descriptors, targets, and policies, such as the Water Framework Directive, Marine Strategy Framework Directive, Ocean Health Index, and United Nations Sustainable Development Goals. Additionally, the opportunities and challenges of these technologies are discussed.},
DOI = {10.3390/rs12142313}
}



