
@Article{rs11111309,
AUTHOR = {Weinstein, Ben G. and Marconi, Sergio and Bohlman, Stephanie and Zare, Alina and White, Ethan},
TITLE = {Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1309},
URL = {https://www.mdpi.com/2072-4292/11/11/1309},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing can transform the speed, scale, and cost of biodiversity and forestry surveys. Data acquisition currently outpaces the ability to identify individual organisms in high resolution imagery. We outline an approach for identifying tree-crowns in RGB imagery while using a semi-supervised deep learning detection network. Individual crown delineation has been a long-standing challenge in remote sensing and available algorithms produce mixed results. We show that deep learning models can leverage existing Light Detection and Ranging (LIDAR)-based unsupervised delineation to generate trees that are used for training an initial RGB crown detection model. Despite limitations in the original unsupervised detection approach, this noisy training data may contain information from which the neural network can learn initial tree features. We then refine the initial model using a small number of higher-quality hand-annotated RGB images. We validate our proposed approach while using an open-canopy site in the National Ecological Observation Network. Our results show that a model using 434,551 self-generated trees with the addition of 2848 hand-annotated trees yields accurate predictions in natural landscapes. Using an intersection-over-union threshold of 0.5, the full model had an average tree crown recall of 0.69, with a precision of 0.61 for the visually-annotated data. The model had an average tree detection rate of 0.82 for the field collected stems. The addition of a small number of hand-annotated trees improved the performance over the initial self-supervised model. This semi-supervised deep learning approach demonstrates that remote sensing can overcome a lack of labeled training data by generating noisy data for initial training using unsupervised methods and retraining the resulting models with high quality labeled data.},
DOI = {10.3390/rs11111309}
}



@Article{s19112527,
AUTHOR = {Sakhakarmi, Sayan and Park, JeeWoong},
TITLE = {Investigation of Tactile Sensory System Configuration for Construction Hazard Perception},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2527},
URL = {https://www.mdpi.com/1424-8220/19/11/2527},
ISSN = {1424-8220},
ABSTRACT = {The application of tactile-based wearable devices to assist in navigation for people with low sight/low memory has demonstrated the feasibility of using such devices as a means of communication. Accordingly, a previous study in construction research investigated various parameters of tactile signals to develop a communicable system for potential application in construction hazard communication. However, the nature of construction limits the application of such devices to the body of construction workers, and it is important to understand sensor design parameters for improved communication, which has not been given significant attention yet. Therefore, this study aims to determine key design factors such as the number of motors, spacing between sensors and the layout of a tactile sensory system to be used for communicating construction hazards to workers. For this purpose, this study focused on identifying the number of motors based on extensive literature and the problem of construction safety as to hazard communication, determining the arrangement that allowed for effective delivery and perception of information with minimum effort. The researchers conducted two experimental studies: First, to determine the minimum spacing between vibration motors that allows for the identification of each individual motor with high accuracy; and second, to determine the layout of motors that is suitable for effective communication of multiple types of information. More importantly, the tactile-sensor configuration identified from this study allows the workers to learn the signal patterns easily in order to identify multiple types of information related to hazards. Using such a communication system on construction sites will assist in transmitting hazard-related information to workers, and thus, protect the lives of workers. Such wearable technologies enable the detection of individual-level hazards and prevent worker fatalities and severe injuries.},
DOI = {10.3390/s19112527}
}



@Article{rs11111342,
AUTHOR = {Zhang, Heng and Yang, Wen and Yu, Huai and Zhang, Haijian and Xia, Gui-Song},
TITLE = {Detecting Power Lines in UAV Images with Convolutional Features and Structured Constraints},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1342},
URL = {https://www.mdpi.com/2072-4292/11/11/1342},
ISSN = {2072-4292},
ABSTRACT = {Power line detection plays an important role in an automated UAV-based electricity inspection system, which is crucial for real-time motion planning and navigation along power lines. Previous methods which adopt traditional filters and gradients may fail to capture complete power lines due to noisy backgrounds. To overcome this, we develop an accurate power line detection method using convolutional and structured features. Specifically, we first build a convolutional neural network to obtain hierarchical responses from each layer. Simultaneously, the rich feature maps are integrated to produce a fusion output, then we extract the structured information including length, width, orientation and area from the coarsest feature map. Finally, we combine the fusion output with structured information to get a result with clear background. The proposed method fully exploits multiscale and structured prior information to conduct both accurate and efficient detection. In addition, we release two power line datasets due to the scarcity in the public domain. The method is evaluated on the well-annotated power line datasets and achieves competitive performance compared with state-of-the-art methods.},
DOI = {10.3390/rs11111342}
}



@Article{app9112389,
AUTHOR = {Zhou, Chengquan and Ye, Hongbao and Xu, Zhifu and Hu, Jun and Shi, Xiaoyan and Hua, Shan and Yue, Jibo and Yang, Guijun},
TITLE = {Estimating Maize-Leaf Coverage in Field Conditions by Applying a Machine Learning Algorithm to UAV Remote Sensing Images},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2389},
URL = {https://www.mdpi.com/2076-3417/9/11/2389},
ISSN = {2076-3417},
ABSTRACT = {Leaf coverage is an indicator of plant growth rate and predicted yield, and thus it is crucial to plant-breeding research. Robust image segmentation of leaf coverage from remote-sensing images acquired by unmanned aerial vehicles (UAVs) in varying environments can be directly used for large-scale coverage estimation, and is a key component of high-throughput field phenotyping. We thus propose an image-segmentation method based on machine learning to extract relatively accurate coverage information from the orthophoto generated after preprocessing. The image analysis pipeline, including dataset augmenting, removing background, classifier training and noise reduction, generates a set of binary masks to obtain leaf coverage from the image. We compare the proposed method with three conventional methods (Hue-Saturation-Value, edge-detection-based algorithm, random forest) and a frontier deep-learning method called DeepLabv3+. The proposed method improves indicators such as Qseg, Sr, Es and mIOU by 15% to 30%. The experimental results show that this approach is less limited by radiation conditions, and that the protocol can easily be implemented for extensive sampling at low cost. As a result, with the proposed method, we recommend using red-green-blue (RGB)-based technology in addition to conventional equipment for acquiring the leaf coverage of agricultural crops.},
DOI = {10.3390/app9112389}
}



@Article{rs11121405,
AUTHOR = {Bazine, Razika and Wu, Huayi and Boukhechba, Kamel},
TITLE = {Spatial Filtering in DCT Domain-Based Frameworks for Hyperspectral Imagery Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1405},
URL = {https://www.mdpi.com/2072-4292/11/12/1405},
ISSN = {2072-4292},
ABSTRACT = {In this article, we propose two effective frameworks for hyperspectral imagery classification based on spatial filtering in Discrete Cosine Transform (DCT) domain. In the proposed approaches, spectral DCT is performed on the hyperspectral image to obtain a spectral profile representation, where the most significant information in the transform domain is concentrated in a few low-frequency components. The high-frequency components that generally represent noisy data are further processed using a spatial filter to extract the remaining useful information. For the spatial filtering step, both two-dimensional DCT (2D-DCT) and two-dimensional adaptive Wiener filter (2D-AWF) are explored. After performing the spatial filter, an inverse spectral DCT is applied on all transformed bands including the filtered bands to obtain the final preprocessed hyperspectral data, which is subsequently fed into a linear Support Vector Machine (SVM) classifier. Experimental results using three hyperspectral datasets show that the proposed framework Cascade Spectral DCT Spatial Wiener Filter (CDCT-WF_SVM) outperforms several state-of-the-art methods in terms of classification accuracy, the sensitivity regarding different sizes of the training samples, and computational time.},
DOI = {10.3390/rs11121405}
}



@Article{app9122410,
AUTHOR = {Patel, Maharshi and Jernigan, Shaphan and Richardson, Rob and Ferguson, Scott and Buckner, Gregory},
TITLE = {Autonomous Robotics for Identification and Management of Invasive Aquatic Plant Species},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2410},
URL = {https://www.mdpi.com/2076-3417/9/12/2410},
ISSN = {2076-3417},
ABSTRACT = {Invasive aquatic plant species can expand rapidly throughout water bodies and cause severely adverse economic and ecological impacts. While mechanical, chemical, and biological methods exist for the identification and treatment of these invasive species, they are manually intensive, inefficient, costly, and can cause collateral ecological damage. To address current deficiencies in aquatic weed management, this paper details the development of a small fleet of fully autonomous boats capable of subsurface hydroacoustic imaging (to scan aquatic vegetation), machine learning (for automated weed identification), and herbicide deployment (for vegetation control). These capabilities aim to minimize manual labor and provide more efficient, safe (reduced chemical exposure to personnel), and timely weed management. Geotagged hydroacoustic imagery of three aquatic plant varieties (Hydrilla, Cabomba, and Coontail) was collected and used to create a software pipeline for subsurface aquatic weed classification and distribution mapping. Employing deep learning, the novel software achieved a classification accuracy of 99.06% after training.},
DOI = {10.3390/app9122410}
}



@Article{su11123278,
AUTHOR = {Voutos, Yorghos and Mylonas, Phivos and Katheniotis, John and Sofou, Anastasia},
TITLE = {A Survey on Intelligent Agricultural Information Handling Methodologies},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {3278},
URL = {https://www.mdpi.com/2071-1050/11/12/3278},
ISSN = {2071-1050},
ABSTRACT = {The term intelligent agriculture, or smart farming, typically involves the incorporation of computer science and information technologies into the traditional notion of farming. The latter utilizes plain machinery and equipment used for many decades and the only significant improvement made over the years has been the introduction of automation in the process. Still, at the beginning of the new century, there are ways and room for further vast improvements. More specifically, the low cost of rather advanced sensors and small-scale devices, now even connected to the Internet of Things (IoT), allowed them to be introduced in the process and used within agricultural production systems. New and emerging technologies and methodologies, like the utilization of cheap network storage, are expected to advance this development. In this sense, the main goals of this paper may be summarized as follows: (a) To identify, group, and acknowledge the current state-of-the-art research knowledge about intelligent agriculture approaches, (b) to categorize them according to meaningful data sources categories, and (c) to describe current efficient data processing and utilization aspects from the perspective of the main trends in the field.},
DOI = {10.3390/su11123278}
}



@Article{su11123339,
AUTHOR = {Agapiou, Athos},
TITLE = {Enhancement of Archaeological Proxies at Non-Homogenous Environments in Remotely Sensed Imagery},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {3339},
URL = {https://www.mdpi.com/2071-1050/11/12/3339},
ISSN = {2071-1050},
ABSTRACT = {Optical remote sensing has been widely used for the identification of archaeological proxies. Such proxies, known as crop or soil marks, can be detected in multispectral images due to their spectral signatures and the distinct contrast that they provide in relation to the surrounding area. The current availability of high-resolution satellite datasets has enabled researchers to provide new methodologies and algorithms that can further enhance archaeological proxies supporting thus image-interpretation. However, a critical point that remains unsolved is the detection of crop and soil marks in non-homogenous environments. In these areas, interpretation is problematic even after the application of sophisticated image enhancement analysis techniques due to the mixed landscape and spectral confusion produced from the high-resolution datasets. To overcome this problem, we propose an image-based methodology in which the vegetation is suppressed following the &ldquo;forced invariance&rdquo; method and then we apply a linear orthogonal transformation to the suppressed spectral bands. The new Red&ndash;Green&ndash;Blue (RGB) image corresponds to a new three-band spectral space where the three axes are linked with the crop mark, vegetation, and soil components. The study evaluates the proposed approach in the archaeological site of &ldquo;Nea Paphos&rdquo; in Cyprus using a WorldView-2 multispectral image aiming to overcome the limitations of the mixed environments.},
DOI = {10.3390/su11123339}
}



@Article{s19122722,
AUTHOR = {Higa, Kyota and Iwamoto, Kota},
TITLE = {Robust Shelf Monitoring Using Supervised Learning for Improving On-Shelf Availability in Retail Stores},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2722},
URL = {https://www.mdpi.com/1424-8220/19/12/2722},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a method to robustly monitor shelves in retail stores using supervised learning for improving on-shelf availability. To ensure high on-shelf availability, which is a key factor for improving profits in retail stores, we focus on understanding changes in products regarding increases/decreases in product amounts on the shelves. Our method first detects changed regions of products in an image by using background subtraction followed by moving object removal. It then classifies the detected change regions into several classes representing the actual changes on the shelves, such as &ldquo;product taken (decrease)&rdquo; and &ldquo;product replenished/returned (increase)&rdquo;, by supervised learning using convolutional neural networks. It finally updates the shelf condition representing the presence/absence of products using classification results and computes the product amount visible in the image as on-shelf availability using the updated shelf condition. Three experiments were conducted using two videos captured from a surveillance camera on the ceiling in a real store. Results of the first and second experiments show the effectiveness of the product change classification in our method. Results of the third experiment show that our method achieves a success rate of 89.6% for on-shelf availability when an error margin is within one product. With high accuracy, store clerks can maintain high on-shelf availability, enabling retail stores to increase profits.},
DOI = {10.3390/s19122722}
}



@Article{rs11121443,
AUTHOR = {Yao, Huang and Qin, Rongjun and Chen, Xiaoyu},
TITLE = {Unmanned Aerial Vehicle for Remote Sensing Applications—A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1443},
URL = {https://www.mdpi.com/2072-4292/11/12/1443},
ISSN = {2072-4292},
ABSTRACT = {The unmanned aerial vehicle (UAV) sensors and platforms nowadays are being used in almost every application (e.g., agriculture, forestry, and mining) that needs observed information from the top or oblique views. While they intend to be a general remote sensing (RS) tool, the relevant RS data processing and analysis methods are still largely ad-hoc to applications. Although the obvious advantages of UAV data are their high spatial resolution and flexibility in acquisition and sensor integration, there is in general a lack of systematic analysis on how these characteristics alter solutions for typical RS tasks such as land-cover classification, change detection, and thematic mapping. For instance, the ultra-high-resolution data (less than 10 cm of Ground Sampling Distance (GSD)) bring more unwanted classes of objects (e.g., pedestrian and cars) in land-cover classification; the often available 3D data generated from photogrammetric images call for more advanced techniques for geometric and spectral analysis. In this paper, we perform a critical review on RS tasks that involve UAV data and their derived products as their main sources including raw perspective images, digital surface models, and orthophotos. In particular, we focus on solutions that address the &ldquo;new&rdquo; aspects of the UAV data including (1) ultra-high resolution; (2) availability of coherent geometric and spectral data; and (3) capability of simultaneously using multi-sensor data for fusion. Based on these solutions, we provide a brief summary of existing examples of UAV-based RS in agricultural, environmental, urban, and hazards assessment applications, etc., and by discussing their practical potentials, we share our views in their future research directions and draw conclusive remarks.},
DOI = {10.3390/rs11121443}
}



@Article{app9122490,
AUTHOR = {Piltan, Farzin and Kim, Cheol-Hong and Kim, Jong-Myon},
TITLE = {Adaptive Fuzzy-Based Fault-Tolerant Control of a Continuum Robotic System for Maxillary Sinus Surgery},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2490},
URL = {https://www.mdpi.com/2076-3417/9/12/2490},
ISSN = {2076-3417},
ABSTRACT = {Continuum robots represent a class of highly sensitive, multiple-degrees-of-freedom robots that are biologically inspired. Because of their flexibility and accuracy, these robots can be used in maxillary sinus surgery. The design of an effective procedure with high accuracy, reliability, robust fault diagnosis, and fault-tolerant control for a surgical robot for the sinus is necessary to maintain the high performance and safety necessary for surgery on the maxillary sinus. Thus, a robust adaptive hybrid observation method using an adaptive, fuzzy auto regressive with exogenous input (ARX) Laguerre Takagi&ndash;Sugeno (T&ndash;S) fuzzy robust feedback linearization observer for a surgical robot is presented. To address the issues of system modeling, the fuzzy ARX-Laguerre technique is represented. In addition, a T&ndash;S fuzzy robust feedback linearization observer is applied to a fuzzy ARX-Laguerre to improve the accuracy of fault estimation, reliability, and robustness for the surgical robot in the presence of uncertainties. For fault-tolerant control in the presence of uncertainties and unknown conditions, an adaptive fuzzy observation-based feedback linearization technique is presented. The effectiveness of the proposed algorithm is tested with simulations. Experimental results show that the proposed method reduces the average position error from 35 mm to 2.45 mm in the presence of faults.},
DOI = {10.3390/app9122490}
}



@Article{s19122775,
AUTHOR = {Munaye, Yirga Yayeh and Lin, Hsin-Piao and Adege, Abebe Belay and Tarekegn, Getaneh Berie},
TITLE = {UAV Positioning for Throughput Maximization Using Deep Learning Approaches},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2775},
URL = {https://www.mdpi.com/1424-8220/19/12/2775},
ISSN = {1424-8220},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) as a communication platform has great practical importance for future wireless networks, especially for on-demand deployment for temporary and emergency conditions. The user throughput estimation in a wireless system depends on the data traffic load and the available capacity to support that load. In UAV-assisted communication, the position of the UAV is one major factor that affects the capacity available to the data flows being served. This study applies multi-layer perceptron (MLP) and long short term memory (LSTM) approaches to determine the position of a UAV that maximizes the overall system performance and user throughput. To analyze and evaluate the system performance, we apply the hybrid of MLP-LSTM for classification regression tasks and K-means algorithms for automatic clustering of classes. The implementation of our work is done through TensorFlow packages. The performance of our proposed system is compared with other approaches to give accurate and novel results for both classification and regression tasks of the user throughput maximization and UAV positioning. According to the results, 98% of the user throughput maximization accuracy is correctly classified. Moreover, the UAV positioning provides accuracy levels of 94.73%, 98.33%, and 99.53% for original datasets (scenario 1), reduced features on the estimated values of user throughput at each grid point (scenario 2), and reduced feature datasets collected on different days and grid points achieved maximum throughput (scenario 3), respectively.},
DOI = {10.3390/s19122775}
}



@Article{drones3020050,
AUTHOR = {Parrott, Elizabeth and Panter, Heather and Morrissey, Joanne and Bezombes, Frederic},
TITLE = {A Low Cost Approach to Disturbed Soil Detection Using Low Altitude Digital Imagery from an Unmanned Aerial Vehicle},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {2},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2504-446X/3/2/50},
ISSN = {2504-446X},
ABSTRACT = {Until recently, clandestine burial investigations relied upon witness statements to determine target search areas of soil and vegetation disturbance. Due to this, remote sensing technologies are increasingly used to detect fresh clandestine graves. However, despite the increased capabilities of remote sensing, clandestine burial searches remain resourcefully intensive as the police have little access to the technology when it is required. In contrast to this, Unmanned Aerial Vehicle (UAV) technology is increasingly popular amongst law enforcement worldwide. As such, this paper explores the use of digital imagery collected from a low cost UAV for the aided detection of disturbed soil sites indicative of fresh clandestine graves. This is done by assessing the unaltered UAV video output using image processing tools to detect sites of disturbance, therefore highlighting previously unrecognised capabilities of police UAVs. This preliminary investigation provides a low cost rapid approach to detecting fresh clandestine graves, further supporting the use of UAV technology by UK police.},
DOI = {10.3390/drones3020050}
}



@Article{s19122829,
AUTHOR = {Yu, Li and Tian, Yugang and Wu, Wei},
TITLE = {A Dark Target Detection Method Based on the Adjacency Effect: A Case Study on Crack Detection},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2829},
URL = {https://www.mdpi.com/1424-8220/19/12/2829},
ISSN = {1424-8220},
ABSTRACT = {Dark target detection is important for engineering applications but the existing methods do not consider the imaging environment of dark targets, such as the adjacency effect. The adjacency effect will affect the quantitative applications of remote sensing, especially for high contrast images and images with ever-increasing resolution. Further, most studies have focused on how to eliminate the adjacency effect and there is almost no research about the application of the adjacency effect. However, the adjacency effect leads to some unique characteristics for the dark target surrounded by a bright background. This paper utilizes these characteristics to assist in the detection of the dark object, and the low-high threshold detection strategy and the adaptive threshold selection method under the assumption of Gaussian distribution are designed. Meanwhile, preliminary case experiments are carried out on the crack detection of concrete slope protection. Finally, the experiment results show that it is feasible to utilize the adjacency effect for dark target detection.},
DOI = {10.3390/s19122829}
}



@Article{rs11121505,
AUTHOR = {Zhang, Heng and Eziz, Anwar and Xiao, Jian and Tao, Shengli and Wang, Shaopeng and Tang, Zhiyao and Zhu, Jiangling and Fang, Jingyun},
TITLE = {High-Resolution Vegetation Mapping Using eXtreme Gradient Boosting Based on Extensive Features},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1505},
URL = {https://www.mdpi.com/2072-4292/11/12/1505},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of vegetation is a premise for conserving, managing, and sustainably using vegetation resources, especially in conditions of intensive human activities and accelerating global changes. However, it is still challenging to produce high-resolution multiclass vegetation map in high accuracy, due to the incapacity of traditional mapping techniques in distinguishing mosaic vegetation classes with subtle differences and the paucity of fieldwork data. This study created a workflow by adopting a promising classifier, extreme gradient boosting (XGBoost), to produce accurate vegetation maps of two strikingly different cases (the Dzungarian Basin in China and New Zealand) based on extensive features and abundant vegetation data. For the Dzungarian Basin, a vegetation map with seven vegetation types, 17 subtypes, and 43 associations was produced with an overall accuracy of 0.907, 0.801, and 0.748, respectively. For New Zealand, a map of 10 habitats and a map of 41 vegetation classes were produced with 0.946, and 0.703 overall accuracy, respectively. The workflow incorporating simplified field survey procedures outperformed conventional field survey and remote sensing based methods in terms of accuracy and efficiency. In addition, it opens a possibility of building large-scale, high-resolution, and timely vegetation monitoring platforms for most terrestrial ecosystems worldwide with the aid of Google Earth Engine and citizen science programs.},
DOI = {10.3390/rs11121505}
}



@Article{rs11121507,
AUTHOR = {Cardenal, Javier and Fernández, Tomás and Pérez-García, José Luis and Gómez-López, José Miguel},
TITLE = {Measurement of Road Surface Deformation Using Images Captured from UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1507},
URL = {https://www.mdpi.com/2072-4292/11/12/1507},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a methodology for measuring road surface deformation due to terrain instability processes. The methodology is based on ultra-high resolution images acquired from unmanned aerial vehicles (UAVs). Flights are georeferenced by means of Structure from Motion (SfM) techniques. Dense point clouds, obtained using the multiple-view stereo (MVS) approach, are used to generate digital surface models (DSM) and high resolution orthophotographs (0.02 m GSD). The methodology has been applied to an unstable area located in La Guardia (Jaen, Southern Spain), where an active landslide was identified. This landslide affected some roads and accesses to a highway at the landslide foot. The detailed road deformation was monitored between 2012 and 2015 by means of eleven UAV flights of ultrahigh resolution covering an area of about 260 m × 90 m. The accuracy of the analysis has been established in 0.02 ± 0.01 m in XY and 0.04 ± 0.02 m in Z. Large deformations in the order of two meters were registered in the total period analyzed that resulted in maximum average rates of 0.62 m/month in the unstable area. Some boundary conditions were considered because of the low required flying height (&lt;50 m above ground level) in order to achieve a suitable image GSD, the fast landslide dynamic, continuous maintenance works on the affected roads and dramatic seasonal vegetation changes throughout the monitoring period. Finally, we have analyzed the relation of displacements to rainfalls in the area, finding a significant correlation between the two variables, as well as two different reactivation episodes.},
DOI = {10.3390/rs11121507}
}



@Article{electronics8070723,
AUTHOR = {Qiao, Dalei and Liu, Guangzhong and Zhang, Jun and Zhang, Qiangyong and Wu, Gongxing and Dong, Feng},
TITLE = {M3C: Multimodel-and-Multicue-Based Tracking by Detection of Surrounding Vessels in Maritime Environment for USV},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {723},
URL = {https://www.mdpi.com/2079-9292/8/7/723},
ISSN = {2079-9292},
ABSTRACT = {It is crucial for unmanned surface vessels (USVs) to detect and track surrounding vessels in real time to avoid collisions at sea. However, the harsh maritime environment poses great challenges to multitarget tracking (MTT). In this paper, a novel tracking by detection framework that integrates the multimodel and multicue (M3C) pipeline is proposed, which aims at improving the detection and tracking performance. Regarding the multimodel, we predicted the maneuver probability of a target vessel via the gated recurrent unit (GRU) model with an attention mechanism, and fused their respective outputs as the output of a kinematic filter. We developed a hybrid affinity model based on multi cues, such as the motion, appearance, and attitude of the ego vessel in the data association stage. By using the proposed ship re-identification approach, the tracker had the capability of appearance matching via metric learning. Experimental evaluation of two public maritime datasets showed that our method achieved state-of-the-art performance, not only in identity switches (IDS) but also in frame rates.},
DOI = {10.3390/electronics8070723}
}



@Article{rs11131510,
AUTHOR = {Fetai, Bujar and Oštir, Krištof and Kosmatin Fras, Mojca and Lisec, Anka},
TITLE = {Extraction of Visible Boundaries for Cadastral Mapping Based on UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1510},
URL = {https://www.mdpi.com/2072-4292/11/13/1510},
ISSN = {2072-4292},
ABSTRACT = {In order to transcend the challenge of accelerating the establishment of cadastres and to efficiently maintain them once established, innovative, and automated cadastral mapping techniques are needed. The focus of the research is on the use of high-resolution optical sensors on unmanned aerial vehicle (UAV) platforms. More specifically, this study investigates the potential of UAV-based cadastral mapping, where the ENVI feature extraction (FX) module has been used for data processing. The paper describes the workflow, which encompasses image pre-processing, automatic extraction of visible boundaries on the UAV imagery, and data post-processing. It shows that this approach should be applied when the UAV orthoimage is resampled to a larger ground sample distance (GSD). In addition, the findings show that it is important to filter the extracted boundary maps to improve the results. The results of the accuracy assessment showed that almost 80% of the extracted visible boundaries were correct. Based on the automatic extraction method, the proposed workflow has the potential to accelerate and facilitate the creation of cadastral maps, especially for developing countries. In developed countries, the extracted visible boundaries might be used for the revision of existing cadastral maps. However, in both cases, the extracted visible boundaries must be validated by landowners and other beneficiaries.},
DOI = {10.3390/rs11131510}
}



@Article{f10070537,
AUTHOR = {Tian, Jiarong and Dai, Tingting and Li, Haidong and Liao, Chengrui and Teng, Wenxiu and Hu, Qingwu and Ma, Weibo and Xu, Yannan},
TITLE = {A Novel Tree Height Extraction Approach for Individual Trees by Combining TLS and UAV Image-Based Point Cloud Integration},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {537},
URL = {https://www.mdpi.com/1999-4907/10/7/537},
ISSN = {1999-4907},
ABSTRACT = {Research Highlights: This study carried out a feasibility analysis on the tree height extraction of a planted coniferous forest with high canopy density by combining terrestrial laser scanner (TLS) and unmanned aerial vehicle (UAV) image&ndash;based point cloud data at small and midsize tree farms. Background and Objectives: Tree height is an important factor for forest resource surveys. This information plays an important role in forest structure evaluation and forest stock estimation. The objectives of this study were to solve the problem of underestimating tree height and to guarantee the precision of tree height extraction in medium and high-density planted coniferous forests. Materials and Methods: This study developed a novel individual tree localization (ITL)-based tree height extraction method to obtain preliminary results in a planted coniferous forest plots with 107 trees (Metasequoia). Then, the final accurate results were achieved based on the canopy height model (CHM) and CHM seed points (CSP). Results: The registration accuracy of the TLS and UAV image-based point cloud data reached 6 cm. The authors optimized the precision of tree height extraction using the ITL-based method by improving CHM resolution from 0.2 m to 0.1 m. Due to the overlapping of forest canopies, the CSP method failed to delineate all individual tree crowns in medium to high-density forest stands with the matching rates of about 75%. However, the accuracy of CSP-based tree height extraction showed obvious advantages compared with the ITL-based method. Conclusion: The proposed method provided a solid foundation for dynamically monitoring forest resources in a high-accuracy and low-cost way, especially in planted tree farms.},
DOI = {10.3390/f10070537}
}



@Article{rs11131540,
AUTHOR = {Wang, Yanjun and Chen, Qi and Zhu, Qing and Liu, Lin and Li, Chaokui and Zheng, Dunyong},
TITLE = {A Survey of Mobile Laser Scanning Applications and Key Techniques over Urban Areas},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1540},
URL = {https://www.mdpi.com/2072-4292/11/13/1540},
ISSN = {2072-4292},
ABSTRACT = {Urban planning and management need accurate three-dimensional (3D) data such as light detection and ranging (LiDAR) point clouds. The mobile laser scanning (MLS) data, with up to millimeter-level accuracy and point density of a few thousand points/m2, have gained increasing attention in urban applications. Substantial research has been conducted in the past decade. This paper conducted a comprehensive survey of urban applications and key techniques based on MLS point clouds. We first introduce the key characteristics of MLS systems and the corresponding point clouds, and present the challenges and opportunities of using the data. Next, we summarize the current applications of using MLS over urban areas, including transportation infrastructure mapping, building information modeling, utility surveying and mapping, vegetation inventory, and autonomous vehicle driving. Then, we review common key issues for processing and analyzing MLS point clouds, including classification methods, object recognition, data registration, data fusion, and 3D city modeling. Finally, we discuss the future prospects for MLS technology and urban applications.},
DOI = {10.3390/rs11131540}
}



@Article{rs11131550,
AUTHOR = {Koch, Tobias and Körner, Marco and Fraundorfer, Friedrich},
TITLE = {Automatic and Semantically-Aware 3D UAV Flight Planning for Image-Based 3D Reconstruction},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1550},
URL = {https://www.mdpi.com/2072-4292/11/13/1550},
ISSN = {2072-4292},
ABSTRACT = {Small-scaled unmanned aerial vehicles (UAVs) emerge as ideal image acquisition platforms due to their high maneuverability even in complex and tightly built environments. The acquired images can be utilized to generate high-quality 3D models using current multi-view stereo approaches. However, the quality of the resulting 3D model highly depends on the preceding flight plan which still requires human expert knowledge, especially in complex urban and hazardous environments. In terms of safe flight plans, practical considerations often define prohibited and restricted airspaces to be accessed with the vehicle. We propose a 3D UAV path planning framework designed for detailed and complete small-scaled 3D reconstructions considering the semantic properties of the environment allowing for user-specified restrictions on the airspace. The generated trajectories account for the desired model resolution and the demands on a successful photogrammetric reconstruction. We exploit semantics from an initial flight to extract the target object and to define restricted and prohibited airspaces which have to be avoided during the path planning process to ensure a safe and short UAV path, while still aiming to maximize the object reconstruction quality. The path planning problem is formulated as an orienteering problem and solved via discrete optimization exploiting submodularity and photogrammetrical relevant heuristics. An evaluation of our method on a customized synthetic scene and on outdoor experiments suggests the real-world capability of our methodology by providing feasible, short and safe flight plans for the generation of detailed 3D reconstruction models.},
DOI = {10.3390/rs11131550}
}



@Article{app9132656,
AUTHOR = {Zhang, Hanxue and Shen, Chong and Chen, Xuemei and Cao, Huiliang and Zhao, Donghua and Huang, Haoqian and Guo, Xiaoting},
TITLE = {An Enhanced Fusion Strategy for Reliable Attitude Measurement Utilizing Vision and Inertial Sensors},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2656},
URL = {https://www.mdpi.com/2076-3417/9/13/2656},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we present a radial basis function (RBF) and cubature Kalman filter (CKF) based enhanced fusion strategy for vision and inertial integrated attitude measurement for sampling frequency discrepancy and divergence. First, the multi-frequency problem of the integrated system and the reason for attitude divergence are analyzed. Second, the filter equation and attitude differential equation are constructed to calculate attitudes separately in time series when visual and inertial data are available or when there are only inertial data. Third, attitude errors between inertial and vision are sent to the input layer of RBF for training. After this, through the activation function of the hidden layer, the errors are transferred to the output layer for weighting the sums, and the training model is established. To overcome the problem of divergence inherent in a multi-frequency system, the well-trained RBF, which can output the attitude errors, is utilized to compensate the attitudes calculated by pure inertial data. Finally, semi-physical simulation experiments under different scenarios are performed to validate the effectiveness and superiority of the proposed scheme in accurate attitude measurements and enhanced anti-divergence capability.},
DOI = {10.3390/app9132656}
}



@Article{act8030053,
AUTHOR = {Kidd, Robert},
TITLE = {Artificial Immune Systems: An Overview for Faulting Actuators},
JOURNAL = {Actuators},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {53},
URL = {https://www.mdpi.com/2076-0825/8/3/53},
ISSN = {2076-0825},
ABSTRACT = {This paper reviews Artificial Immune Systems (AIS) that can be implemented to compensate for actuators that are in a faulted state or operating abnormally. Eventually, all actuators will fail or wear out, and these actuator faults must be managed if a system is to operate safely. The AIS are adaptive algorithms which are inherently well-suited to these situations by treating these faults as infections that must be combated. However, the computational intensity of these algorithms has caused them to have limited success in real-time situations. With the advent of distributed and cloud-based computing these algorithms have begun to be feasible for diagnosing faulted actuators and then generating compensating controllers in near-real-time. To encourage the application of AIS to these situations, this work presents research for the fundamental operating principles of AIS, their applications, and a brief case-study on their applicability to fault compensation by considering an overactuated rover with four independent drive wheels and independent front and rear steering.},
DOI = {10.3390/act8030053}
}



@Article{rs11131554,
AUTHOR = {Zhang, Xin and Han, Liangxiu and Dong, Yingying and Shi, Yue and Huang, Wenjiang and Han, Lianghao and González-Moreno, Pablo and Ma, Huiqin and Ye, Huichun and Sobeih, Tam},
TITLE = {A Deep Learning-Based Approach for Automated Yellow Rust Disease Detection from High-Resolution Hyperspectral UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1554},
URL = {https://www.mdpi.com/2072-4292/11/13/1554},
ISSN = {2072-4292},
ABSTRACT = {Yellow rust in winter wheat is a widespread and serious fungal disease, resulting in significant yield losses globally. Effective monitoring and accurate detection of yellow rust are crucial to ensure stable and reliable wheat production and food security. The existing standard methods often rely on manual inspection of disease symptoms in a small crop area by agronomists or trained surveyors. This is costly, time consuming and prone to error due to the subjectivity of surveyors. Recent advances in unmanned aerial vehicles (UAVs) mounted with hyperspectral image sensors have the potential to address these issues with low cost and high efficiency. This work proposed a new deep convolutional neural network (DCNN) based approach for automated crop disease detection using very high spatial resolution hyperspectral images captured with UAVs. The proposed model introduced multiple Inception-Resnet layers for feature extraction and was optimized to establish the most suitable depth and width of the network. Benefiting from the ability of convolution layers to handle three-dimensional data, the model used both spatial and spectral information for yellow rust detection. The model was calibrated with hyperspectral imagery collected by UAVs in five different dates across a whole crop cycle over a well-controlled field experiment with healthy and rust infected wheat plots. Its performance was compared across sampling dates and with random forest, a representative of traditional classification methods in which only spectral information was used. It was found that the method has high performance across all the growing cycle, particularly at late stages of the disease spread. The overall accuracy of the proposed model (0.85) was higher than that of the random forest classifier (0.77). These results showed that combining both spectral and spatial information is a suitable approach to improving the accuracy of crop disease detection with high resolution UAV hyperspectral images.},
DOI = {10.3390/rs11131554}
}



@Article{sym11070842,
AUTHOR = {Ryu, June-Woo and Pham, Quoc-Viet and Luan, Huynh N. T. and Hwang, Won-Joo and Kim, Jong-Deok and Lee, Jung-Tae},
TITLE = {Multi-Access Edge Computing Empowered Heterogeneous Networks: A Novel Architecture and Potential Works},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {842},
URL = {https://www.mdpi.com/2073-8994/11/7/842},
ISSN = {2073-8994},
ABSTRACT = {One of the most promising approaches to address the mismatch between computation- intensive applications and computation-limited end devices is multi-access edge computing (MEC). To overcome the rapid increase in traffic volume and offload the traffic from macrocells, a massive number of small cells have been deployed, so-called heterogeneous networks (HetNets). Strongly motivated by the close integration of MEC and HetNets, in this paper, we propose an envisioned architecture of MEC-empowered HetNets, where both wireless and wired backhaul solutions are supported, flying base stations (BSs) can be equipped with MEC servers, and mobile users (MUs) need both communication and computation resources for their computationally heavy tasks. Subsequently, we provide the research progress summary of task offloading and resource allocation in the proposed MEC-empowered unmanned aerial vehicle (UAV)-assisted heterogeneous networks. We complete this article by spotlighting key challenges and open future directives for researches.},
DOI = {10.3390/sym11070842}
}



@Article{rs11131561,
AUTHOR = {Klouček, Tomáš and Komárek, Jan and Surový, Peter and Hrach, Karel and Janata, Přemysl and Vašíček, Bedřich},
TITLE = {The Use of UAV Mounted Sensors for Precise Detection of Bark Beetle Infestation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1561},
URL = {https://www.mdpi.com/2072-4292/11/13/1561},
ISSN = {2072-4292},
ABSTRACT = {The bark beetle (Ips typographus) disturbance represents serious environmental and economic issue and presents a major challenge for forest management. A timely detection of bark beetle infestation is therefore necessary to reduce losses. Besides wood production, a bark beetle outbreak affects the forest ecosystem in many other ways including the water cycle, nutrient cycle, or carbon fixation. On that account, (not just) European temperate coniferous forests may become endangered ecosystems. Our study was performed in the unmanaged zone of the Krkono&scaron;e Mountains National Park in the northern part of the Czech Republic where the natural spreading of bark beetle is slow and, therefore, allow us to continuously monitor the infested trees that are, in contrast to managed forests, not being removed. The aim of this work is to evaluate possibilities of unmanned aerial vehicle (UAV)-mounted low-cost RGB and modified near-infrared sensors for detection of different stages of infested trees at the individual level, using a retrospective time series for recognition of still green but already infested trees (so-called green attack). A mosaic was created from the UAV imagery, radiometrically calibrated for surface reflectance, and five vegetation indices were calculated; the reference data about the stage of bark beetle infestation was obtained through a combination of field survey and visual interpretation of an orthomosaic. The differences of vegetation indices between infested and healthy trees over four time points were statistically evaluated and classified using the Maximum Likelihood classifier. Achieved results confirm our assumptions that it is possible to use a low-cost UAV-based sensor for detection of various stages of bark beetle infestation across seasons; with increasing time after infection, distinguishing infested trees from healthy ones grows easier. The best performance was achieved by the Greenness Index with overall accuracy of 78%&ndash;96% across the time periods. The performance of the indices based on near-infrared band was lower.},
DOI = {10.3390/rs11131561}
}



@Article{su11133637,
AUTHOR = {Lee, SangSik and Jeong, YiNa and Son, SuRak and Lee, ByungKwan},
TITLE = {A Self-Predictable Crop Yield Platform (SCYP) Based On Crop Diseases Using Deep Learning},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3637},
URL = {https://www.mdpi.com/2071-1050/11/13/3637},
ISSN = {2071-1050},
ABSTRACT = {This paper proposes a self-predictable crop yield platform (SCYP) based on crop diseases using deep learning that collects weather information (temperature, humidity, sunshine, precipitation, etc.) and farm status information (harvest date, disease information, crop status, ground temperature, etc.), diagnoses crop diseases by using convolutional neural network (CNN), and predicts crop yield based on factors such as climate change, crop diseases, and others by using artificial neural network (ANN). The SCYP consists of an image preprocessing module (IPM) to determine crop diseases through the Google Vision API and image resizing, a crop disease diagnosis module (CDDM) based on CNN to diagnose the types and extent of crop diseases through photographs, and a crop yield prediction module (CYPM) based on ANN by using information of crop diseases, remaining time until harvest (based on the date), current temperature, humidity and precipitation (amount of snowfall) in the area, sunshine amount, ground temperature, atmospheric pressure, moisture evaporation in the ground, etc. Four experiments were conducted to verify the efficiency of the SCYP. In the CDMM, the accuracy and operation time of each model were measured using three neural network models: CNN, region-CNN(R-CNN), and you only look once (YOLO). In the CYPM, rectified linear unit (ReLU), Sigmoid, and Step activation functions were compared to measure ANN accuracy. The accuracy of CNN was about 3.5% higher than that of R-CNN and about 5.4% higher than that of YOLO. The operation time of CNN was about 37 s less than that of R-CNN and about 72 s less than that of YOLO. The CDDM had slightly less operation time, but in this paper, we prefer accuracy over operation time to diagnose crop diseases efficiently and accurately. When the activation function of the ANN used in the CYPM was ReLU, the accuracy of the ANN was 2% higher than that of Sigmoid and 7% higher than that of Step. The CYPM prediction was about 34% more accurate when using multiple diseases than when not using them. Therefore, the SCYP can predict farm yields more accurately than traditional methods.},
DOI = {10.3390/su11133637}
}



@Article{agronomy9070354,
AUTHOR = {Zhang, Xiuhua and Derival, Magda and Albrecht, Ute and Ampatzidis, Yiannis},
TITLE = {Evaluation of a Ground Penetrating Radar to Map the Root Architecture of HLB-Infected Citrus Trees},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {354},
URL = {https://www.mdpi.com/2073-4395/9/7/354},
ISSN = {2073-4395},
ABSTRACT = {This paper investigates the influences of several limiting factors on the performance of ground penetrating radar (GPR) in accurately detecting huanglongbing (HLB)-infected citrus roots and determining their main structural characteristics. First, single-factor experiments were conducted to evaluate GPR performance. The factors that were evaluated were (i) root diameter; (ii) root moisture level; (iii) root depth; (iv) root spacing; (v) survey angle; and, (vi) soil moisture level. Second, two multi-factor field experiments were conducted to evaluate the performance of the GPR in complex orchard environments. The GPR generated a hyperbola in the radar profile upon root detection; the diameter of the root was successfully determined according to the width of the hyperbola when the roots were larger than 6 mm in diameter. The GPR also distinguished live from dead roots, a capability that is indispensable for studying the effects of soil-borne and other diseases on the citrus tree root system. The GPR can distinguish the roots only if their horizontal distance is greater than 10 cm and their vertical distance is greater than 5 cm if two or more roots are in proximity. GPR technology can be applied to determine the efficacy of advanced crop production strategies, especially under the pressures of disease and environmental stresses.},
DOI = {10.3390/agronomy9070354}
}



@Article{rs11131584,
AUTHOR = {Chen, Yang and Lee, Won Suk and Gan, Hao and Peres, Natalia and Fraisse, Clyde and Zhang, Yanchao and He, Yong},
TITLE = {Strawberry Yield Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoimages},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1584},
URL = {https://www.mdpi.com/2072-4292/11/13/1584},
ISSN = {2072-4292},
ABSTRACT = {Strawberry growers in Florida suffer from a lack of efficient and accurate yield forecasts for strawberries, which would allow them to allocate optimal labor and equipment, as well as other resources for harvesting, transportation, and marketing. Accurate estimation of the number of strawberry flowers and their distribution in a strawberry field is, therefore, imperative for predicting the coming strawberry yield. Usually, the number of flowers and their distribution are estimated manually, which is time-consuming, labor-intensive, and subjective. In this paper, we develop an automatic strawberry flower detection system for yield prediction with minimal labor and time costs. The system used a small unmanned aerial vehicle (UAV) (DJI Technology Co., Ltd., Shenzhen, China) equipped with an RGB (red, green, blue) camera to capture near-ground images of two varieties (Sensation and Radiance) at two different heights (2 m and 3 m) and built orthoimages of a 402 m2 strawberry field. The orthoimages were automatically processed using the Pix4D software and split into sequential pieces for deep learning detection. A faster region-based convolutional neural network (R-CNN), a state-of-the-art deep neural network model, was chosen for the detection and counting of the number of flowers, mature strawberries, and immature strawberries. The mean average precision (mAP) was 0.83 for all detected objects at 2 m heights and 0.72 for all detected objects at 3 m heights. We adopted this model to count strawberry flowers in November and December from 2 m aerial images and compared the results with a manual count. The average deep learning counting accuracy was 84.1% with average occlusion of 13.5%. Using this system could provide accurate counts of strawberry flowers, which can be used to forecast future yields and build distribution maps to help farmers observe the growth cycle of strawberry fields.},
DOI = {10.3390/rs11131584}
}



@Article{rs11131594,
AUTHOR = {Qiu, Heqian and Li, Hongliang and Wu, Qingbo and Meng, Fanman and Ngan, King Ngi and Shi, Hengcan},
TITLE = {A2RMNet: Adaptively Aspect Ratio Multi-Scale Network for Object Detection in Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1594},
URL = {https://www.mdpi.com/2072-4292/11/13/1594},
ISSN = {2072-4292},
ABSTRACT = {Object detection is a significant and challenging problem in the study area of remote sensing and image analysis. However, most existing methods are easy to miss or incorrectly locate objects due to the various sizes and aspect ratios of objects. In this paper, we propose a novel end-to-end Adaptively Aspect Ratio Multi-Scale Network (A     2    RMNet) to solve this problem. On the one hand, we design a multi-scale feature gate fusion network to adaptively integrate the multi-scale features of objects. This network is composed of gate fusion modules, refine blocks and region proposal networks. On the other hand, an aspect ratio attention network is leveraged to preserve the aspect ratios of objects, which alleviates the excessive shape distortions of objects caused by aspect ratio changes during training. Experiments show that the proposed A     2    RMNet significantly outperforms the previous state of the arts on the DOTA dataset, NWPU VHR-10 dataset, RSOD dataset and UCAS-AOD dataset by     5.73 %    ,     7.06 %    ,     3.27 %     and     2.24 %    , respectively.},
DOI = {10.3390/rs11131594}
}



@Article{su11133682,
AUTHOR = {Yu, Jiajie and Ji, Yanjie and Gao, Liangpeng and Gao, Qi},
TITLE = {Optimization of Metro Passenger Organizing of Alighting and Boarding Processes: Simulated Evidence from the Metro Station in Nanjing, China},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3682},
URL = {https://www.mdpi.com/2071-1050/11/13/3682},
ISSN = {2071-1050},
ABSTRACT = {Since the long dwell time and chaotic crowds make metro trips inefficient and dissatisfying, the importance of optimizing alighting and boarding processes has become more prominent. This paper focuses on the adjustment of passenger organizing modes. Using field data from the metro station in Nanjing, China, a micro-simulation model of alighting and boarding processes based on an improved social force paradigm was built to simulate the movement of passengers under different passenger organizing modes. Unit flow rate, delay, and social force work (SFW) jointly reflect the efficiency and, especially, the physical energy consumption of passengers under each mode. It was found that when passengers alighted and boarded by different doors, efficiency reached its optimal level which was 76.92% higher than the status quo of Nanjing, and the physical energy consumption was reduced by 16.30%. Both the findings and the model can provide support for passenger organizing in metro stations, and the concept of SFW can be applied to other scenes simulated by the social force model, such as evacuations of large-scale activities, to evaluate the physical energy consumption of people.},
DOI = {10.3390/su11133682}
}



@Article{s19132965,
AUTHOR = {Contreras-Cruz, Marco Antonio and Ramirez-Paredes, Juan Pablo and Hernandez-Belmonte, Uriel Haile and Ayala-Ramirez, Victor},
TITLE = {Vision-Based Novelty Detection Using Deep Features and Evolved Novelty Filters for Specific Robotic Exploration and Inspection Tasks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2965},
URL = {https://www.mdpi.com/1424-8220/19/13/2965},
ISSN = {1424-8220},
ABSTRACT = {One of the essential abilities in animals is to detect novelties within their environment. From the computational point of view, novelty detection consists of finding data that are different in some aspect to the known data. In robotics, researchers have incorporated novelty modules in robots to develop automatic exploration and inspection tasks. The visual sensor is one of the preferred sensors to perform this task. However, there exist problems as illumination changes, occlusion, and scale, among others. Besides, novelty detectors vary their performance depending on the specific application scenario. In this work, we propose a visual novelty detection framework for specific exploration and inspection tasks based on evolved novelty detectors. The system uses deep features to represent the visual information captured by the robots and applies a global optimization technique to design novelty detectors for specific robotics applications. We verified the performance of the proposed system against well-established state-of-the-art methods in a challenging scenario. This scenario was an outdoor environment covering typical problems in computer vision such as illumination changes, occlusion, and geometric transformations. The proposed framework presented high-novelty detection accuracy with competitive or even better results than the baseline methods.},
DOI = {10.3390/s19132965}
}



@Article{s19132993,
AUTHOR = {Wang, Chaoqun and Wang, Jiankun and Li, Chenming and Ho, Danny and Cheng, Jiyu and Yan, Tingfang and Meng, Lili and Meng, Max Q.-H.},
TITLE = {Safe and Robust Mobile Robot Navigation in Uneven Indoor Environments},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2993},
URL = {https://www.mdpi.com/1424-8220/19/13/2993},
ISSN = {1424-8220},
ABSTRACT = {Complex environments pose great challenges for autonomous mobile robot navigation. In this study, we address the problem of autonomous navigation in 3D environments with staircases and slopes. An integrated system for safe mobile robot navigation in 3D complex environments is presented and both the perception and navigation capabilities are incorporated into the modular and reusable framework. Firstly, to distinguish the slope from the staircase in the environment, the robot builds a 3D OctoMap of the environment with a novel Simultaneously Localization and Mapping (SLAM) framework using the information of wheel odometry, a 2D laser scanner, and an RGB-D camera. Then, we introduce the traversable map, which is generated by the multi-layer 2D maps extracted from the 3D OctoMap. This traversable map serves as the input for autonomous navigation when the robot faces slopes and staircases. Moreover, to enable robust robot navigation in 3D environments, a novel camera re-localization method based on regression forest towards stable 3D localization is incorporated into this framework. In addition, we utilize a variable step size Rapidly-exploring Random Tree (RRT) method which can adjust the exploring step size automatically without tuning this parameter manually according to the environment, so that the navigation efficiency is improved. The experiments are conducted in different kinds of environments and the output results demonstrate that the proposed system enables the robot to navigate efficiently and robustly in complex 3D environments.},
DOI = {10.3390/s19132993}
}



@Article{rs11131617,
AUTHOR = {Wang, Jicheng and Shen, Li and Qiao, Wenfan and Dai, Yanshuai and Li, Zhilin},
TITLE = {Deep Feature Fusion with Integration of Residual Connection and Attention Model for Classification of VHR Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1617},
URL = {https://www.mdpi.com/2072-4292/11/13/1617},
ISSN = {2072-4292},
ABSTRACT = {The classification of very-high-resolution (VHR) remote sensing images is essential in many applications. However, high intraclass and low interclass variations in these kinds of images pose serious challenges. Fully convolutional network (FCN) models, which benefit from a powerful feature learning ability, have shown impressive performance and great potential. Nevertheless, only classification results with coarse resolution can be obtained from the original FCN method. Deep feature fusion is often employed to improve the resolution of outputs. Existing strategies for such fusion are not capable of properly utilizing the low-level features and considering the importance of features at different scales. This paper proposes a novel, end-to-end, fully convolutional network to integrate a multiconnection ResNet model and a class-specific attention model into a unified framework to overcome these problems. The former fuses multilevel deep features without introducing any redundant information from low-level features. The latter can learn the contributions from different features of each geo-object at each scale. Extensive experiments on two open datasets indicate that the proposed method can achieve class-specific scale-adaptive classification results and it outperforms other state-of-the-art methods. The results were submitted to the International Society for Photogrammetry and Remote Sensing (ISPRS) online contest for comparison with more than 50 other methods. The results indicate that the proposed method (ID: SWJ_2) ranks #1 in terms of overall accuracy, even though no additional digital surface model (DSM) data that were offered by ISPRS were used and no postprocessing was applied.},
DOI = {10.3390/rs11131617}
}



@Article{s19133014,
AUTHOR = {Jalil, Bushra and Leone, Giuseppe Riccardo and Martinelli, Massimo and Moroni, Davide and Pascali, Maria Antonietta and Berton, Andrea},
TITLE = {Fault Detection in Power Equipment via an Unmanned Aerial System Using Multi Modal Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3014},
URL = {https://www.mdpi.com/1424-8220/19/13/3014},
ISSN = {1424-8220},
ABSTRACT = {The power transmission lines are the link between power plants and the points of consumption, through substations. Most importantly, the assessment of damaged aerial power lines and rusted conductors is of extreme importance for public safety; hence, power lines and associated components must be periodically inspected to ensure a continuous supply and to identify any fault and defect. To achieve these objectives, recently, Unmanned Aerial Vehicles (UAVs) have been widely used; in fact, they provide a safe way to bring sensors close to the power transmission lines and their associated components without halting the equipment during the inspection, and reducing operational cost and risk. In this work, a drone, equipped with multi-modal sensors, captures images in the visible and infrared domain and transmits them to the ground station. We used state-of-the-art computer vision methods to highlight expected faults (i.e., hot spots) or damaged components of the electrical infrastructure (i.e., damaged insulators). Infrared imaging, which is invariant to large scale and illumination changes in the real operating environment, supported the identification of faults in power transmission lines; while a neural network is adapted and trained to detect and classify insulators from an optical video stream. We demonstrate our approach on data captured by a drone in Parma, Italy.},
DOI = {10.3390/s19133014}
}



@Article{rs11131627,
AUTHOR = {Kartoziia, Andrei},
TITLE = {Assessment of the Ice Wedge Polygon Current State by Means of UAV Imagery Analysis (Samoylov Island, the Lena Delta)},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1627},
URL = {https://www.mdpi.com/2072-4292/11/13/1627},
ISSN = {2072-4292},
ABSTRACT = {Modern degradation of Arctic permafrost promotes changes in tundra landscapes and leads to degradation of ice wedge polygons, which are the most widespread landforms of Arctic wetlands. Status assessment of polygon degradation is important for various environmental studies. We have applied the geographic information systems’ (GIS) analysis of data from unmanned aerial vehicles (UAV) to accurately assess the status of ice wedge polygon degradation on Samoylov Island. We used several modern models of polygon degradation for revealing polygon types, which obviously correspond to different stages of degradation. Manual methods of mapping and a high spatial resolution of used UAV data allowed for a high degree of accuracy in the identification of all land units. The study revealed the following: 41.79% of the first terrace surface was composed of non-degraded polygonal tundra; 18.37% was composed of polygons, which had signs of thermokarst activity and corresponded to various stages of degradation in the models; and 39.84% was composed of collapsed polygons, slopes, valleys, and water bodies, excluding ponds of individual polygons. This study characterizes the current status of polygonal tundra degradation of the first terrace surface on Samoylov Island. Our assessment reflects the landscape condition of the first terrace surface of Samoylov Island, which is the typical island of the southern part of the Lena Delta. Moreover, the study illustrates the potential of UAV data GIS analysis for highly accurate investigations of Arctic landscape changes.},
DOI = {10.3390/rs11131627}
}



@Article{rs11141636,
AUTHOR = {Lai, Xudong and Yang, Jingru and Li, Yongxu and Wang, Mingwei},
TITLE = {A Building Extraction Approach Based on the Fusion of LiDAR Point Cloud and Elevation Map Texture Features},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1636},
URL = {https://www.mdpi.com/2072-4292/11/14/1636},
ISSN = {2072-4292},
ABSTRACT = {Building extraction is an important way to obtain information in urban planning, land management, and other fields. As remote sensing has various advantages such as large coverage and real-time capability, it becomes an essential approach for building extraction. Among various remote sensing technologies, the capability of providing 3D features makes the LiDAR point cloud become a crucial means for building extraction. However, the LiDAR point cloud has difficulty distinguishing objects with similar heights, in which case texture features are able to extract different objects in a 2D image. In this paper, a building extraction method based on the fusion of point cloud and texture features is proposed, and the texture features are extracted by using an elevation map that expresses the height of each point. The experimental results show that the proposed method obtains better extraction results than that of other texture feature extraction methods and ENVI software in all experimental areas, and the extraction accuracy is always higher than 87%, which is satisfactory for some practical work.},
DOI = {10.3390/rs11141636}
}



@Article{app9142808,
AUTHOR = {Peng, Yahui and Liu, Xiaochen and Shen, Chong and Huang, Haoqian and Zhao, Donghua and Cao, Huiliang and Guo, Xiaoting},
TITLE = {An Improved Optical Flow Algorithm Based on Mask-R-CNN and K-Means for Velocity Calculation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2808},
URL = {https://www.mdpi.com/2076-3417/9/14/2808},
ISSN = {2076-3417},
ABSTRACT = {Aiming at enhancing the accuracy and reliability of velocity calculation in vision navigation, an improved method is proposed in this paper. The method integrates Mask-R-CNN (Mask Region-based Convolutional Neural Network) and K-Means with the pyramid Lucas Kanade algorithm in order to reduce the harmful effect of moving objects on velocity calculation. Firstly, Mask-R-CNN is used to recognize the objects which have motions relative to the ground and covers them with masks to enhance the similarity between pixels and to reduce the impacts of the noisy moving pixels. Then, the pyramid Lucas Kanade algorithm is used to calculate the optical flow value. Finally, the value is clustered by the K-Means algorithm to abandon the outliers, and vehicle velocity is calculated by the processed optical flow. The prominent advantages of the proposed algorithm are (i) decreasing the bad impacts to velocity calculation, due to the objects which have relative motions; (ii) obtaining the correct optical flow sets and velocity calculation outputs with less fluctuation; and (iii) the applicability enhancement of the optical flow algorithm in complex navigation environment. The proposed algorithm is tested by actual experiments. Results with superior precision and reliability show the feasibility and effectiveness of the proposed method for vehicle velocity calculation in vision navigation system.},
DOI = {10.3390/app9142808}
}



@Article{s19143106,
AUTHOR = {Zhou, Chengquan and Ye, Hongbao and Hu, Jun and Shi, Xiaoyan and Hua, Shan and Yue, Jibo and Xu, Zhifu and Yang, Guijun},
TITLE = {Automated Counting of Rice Panicle by Applying Deep Learning Model to Images from Unmanned Aerial Vehicle Platform},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3106},
URL = {https://www.mdpi.com/1424-8220/19/14/3106},
ISSN = {1424-8220},
ABSTRACT = {The number of panicles per unit area is a common indicator of rice yield and is of great significance to yield estimation, breeding, and phenotype analysis. Traditional counting methods have various drawbacks, such as long delay times and high subjectivity, and they are easily perturbed by noise. To improve the accuracy of rice detection and counting in the field, we developed and implemented a panicle detection and counting system that is based on improved region-based fully convolutional networks, and we use the system to automate rice-phenotype measurements. The field experiments were conducted in target areas to train and test the system and used a rotor light unmanned aerial vehicle equipped with a high-definition RGB camera to collect images. The trained model achieved a precision of 0.868 on a held-out test set, which demonstrates the feasibility of this approach. The algorithm can deal with the irregular edge of the rice panicle, the significantly different appearance between the different varieties and growing periods, the interference due to color overlapping between panicle and leaves, and the variations in illumination intensity and shading effects in the field. The result is more accurate and efficient recognition of rice-panicles, which facilitates rice breeding. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a global scale.},
DOI = {10.3390/s19143106}
}



@Article{s19143115,
AUTHOR = {Wei, Yang and Wang, Hao and Tsang, Kim Fung and Liu, Yucheng and Wu, Chung Kit and Zhu, Hongxu and Chow, Yuk-Tak and Hung, Faan Hei},
TITLE = {Proximity Environmental Feature Based Tree Health Assessment Scheme Using Internet of Things and Machine Learning Algorithm},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3115},
URL = {https://www.mdpi.com/1424-8220/19/14/3115},
ISSN = {1424-8220},
ABSTRACT = {Improperly grown trees may cause huge hazards to the environment and to humans, through e.g., climate change, soil erosion, etc. A proximity environmental feature-based tree health assessment (PTA) scheme is proposed to prevent these hazards by providing guidance for early warning methods of potential poor tree health. In PTA development, tree health is defined and evaluated based on proximity environmental features (PEFs). The PEF takes into consideration the seven surrounding ambient features that strongly impact tree health. The PEFs were measured by the deployed smart sensors surrounding trees. A database composed of tree health and relative PEFs was established for further analysis. An adaptive data identifying (ADI) algorithm is applied to exclude the influence of interference factors in the database. Finally, the radial basis function (RBF) neural network (NN), a machine leaning algorithm, has been identified as the appropriate tool with which to correlate tree health and PEFs to establish the PTA algorithm. One of the salient features of PTA is that the algorithm can evaluate, and thus monitor, tree health remotely and automatically from smart sensor data by taking advantage of the well-established internet of things (IoT) network and machine learning algorithm.},
DOI = {10.3390/s19143115}
}



@Article{rs11141678,
AUTHOR = {Fu, Yongyong and Ye, Ziran and Deng, Jinsong and Zheng, Xinyu and Huang, Yibo and Yang, Wu and Wang, Yaohua and Wang, Ke},
TITLE = {Finer Resolution Mapping of Marine Aquaculture Areas Using WorldView-2 Imagery and a Hierarchical Cascade Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1678},
URL = {https://www.mdpi.com/2072-4292/11/14/1678},
ISSN = {2072-4292},
ABSTRACT = {Marine aquaculture plays an important role in seafood supplement, economic development, and coastal ecosystem service provision. The precise delineation of marine aquaculture areas from high spatial resolution (HSR) imagery is vital for the sustainable development and management of coastal marine resources. However, various sizes and detailed structures of marine objects make it difficult for accurate mapping from HSR images by using conventional methods. Therefore, this study attempts to extract marine aquaculture areas by using an automatic labeling method based on the convolutional neural network (CNN), i.e., an end-to-end hierarchical cascade network (HCNet). Specifically, for marine objects of various sizes, we propose to improve the classification performance by utilizing multi-scale contextual information. Technically, based on the output of a CNN encoder, we employ atrous convolutions to capture multi-scale contextual information and aggregate them in a hierarchical cascade way. Meanwhile, for marine objects with detailed structures, we propose to refine the detailed information gradually by using a series of long-span connections with fine resolution features from the shallow layers. In addition, to decrease the semantic gaps between features in different levels, we propose to refine the feature space (i.e., channel and spatial dimensions) using an attention-based module. Experimental results show that our proposed HCNet can effectively identify and distinguish different kinds of marine aquaculture, with 98% of overall accuracy. It also achieves better classification performance compared with object-based support vector machine and state-of-the-art CNN-based methods, such as FCN-32s, U-Net, and DeeplabV2. Our developed method lays a solid foundation for the intelligent monitoring and management of coastal marine resources.},
DOI = {10.3390/rs11141678}
}



@Article{make1030046,
AUTHOR = {Manzo, Mario},
TITLE = {Graph-Based Image Matching for Indoor Localization},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {1},
YEAR = {2019},
NUMBER = {3},
PAGES = {785--804},
URL = {https://www.mdpi.com/2504-4990/1/3/46},
ISSN = {2504-4990},
ABSTRACT = {Graphs are a very useful framework for representing information. In general, these data structures are used in different application domains where data of interest are described in terms of local and spatial relations. In this context, the aim is to propose an alternative graph-based image representation. An image is encoded by a Region Adjacency Graph (RAG), based on Multicolored Neighborhood (MCN) clustering. This representation is integrated into a Content-Based Image Retrieval (CBIR) system, designed for the vision-based positioning task. The image matching phase, in the CBIR system, is managed with an approach of attributed graph matching, named the extended-VF algorithm. Evaluated in a context of indoor localization, the proposed system reports remarkable performance.},
DOI = {10.3390/make1030046}
}



@Article{en12142706,
AUTHOR = {Ejaz, Waleed and Azam, Muhammad Awais and Saadat, Salman and Iqbal, Farkhund and Hanan, Abdul},
TITLE = {Unmanned Aerial Vehicles enabled IoT Platform for Disaster Management},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2706},
URL = {https://www.mdpi.com/1996-1073/12/14/2706},
ISSN = {1996-1073},
ABSTRACT = {Efficient and reliable systems are required to detect and monitor disasters such as wildfires as well as to notify the people in the disaster-affected areas. Internet of Things (IoT) is the key paradigm that can address the multitude problems related to disaster management. In addition, an unmanned aerial vehicles (UAVs)-enabled IoT platform connected via cellular network can further enhance the robustness of the disaster management system. The UAV-enabled IoT platform is based on three main research areas: (i) ground IoT network; (ii) communication technologies for ground and aerial connectivity; and (iii) data analytics. In this paper, we provide a holistic view of a UAVs-enabled IoT platform which can provide ubiquitous connectivity to both aerial and ground users in challenging environments such as wildfire management. We then highlight key challenges for the design of an efficient and reliable IoT platform. We detail a case study targeting the design of an efficient ground IoT network that can detect and monitor fire and send notifications to people using named data networking (NDN) architecture. The use of NDN architecture in a sensor network for IoT integrates pull-based communication to enable reliable and efficient message dissemination in the network and to notify the users as soon as possible in case of disastrous situations. The results of the case study show the enormous impact on the performance of IoT platform for wildfire management. Lastly, we draw the conclusion and outline future research directions in this field.},
DOI = {10.3390/en12142706}
}



@Article{drones3030058,
AUTHOR = {Akhloufi, Moulay A. and Arola, Sebastien and Bonnet, Alexandre},
TITLE = {Drones Chasing Drones: Reinforcement Learning and Deep Search Area Proposal},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {58},
URL = {https://www.mdpi.com/2504-446X/3/3/58},
ISSN = {2504-446X},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are very popular and increasingly used in different applications. Today, the use of multiple UAVs and UAV swarms are attracting more interest from the research community, leading to the exploration of topics such as UAV cooperation, multi-drone autonomous navigation, etc. In this work, we propose two approaches for UAV pursuit-evasion. The first approach uses deep reinforcement learning to predict the actions to apply to the follower UAV to keep track of the target UAV. The second approach uses a deep object detector and a search area proposal (SAP) to predict the position of the target UAV in the next frame for tracking purposes. The two approaches are promising and lead to a higher tracking accuracy with an intersection over union (IoU) above the selected threshold. We also show that the deep SAP-based approach improves the detection of distant objects that cover small areas in the image. The efficiency of the proposed algorithms is demonstrated in outdoor tracking scenarios using real UAVs.},
DOI = {10.3390/drones3030058}
}



@Article{rs11141692,
AUTHOR = {Farooq, Adnan and Jia, Xiuping and Hu, Jiankun and Zhou, Jun},
TITLE = {Multi-Resolution Weed Classification via Convolutional Neural Network and Superpixel Based Local Binary Pattern Using Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1692},
URL = {https://www.mdpi.com/2072-4292/11/14/1692},
ISSN = {2072-4292},
ABSTRACT = {Automatic weed detection and classification faces the challenges of large intraclass variation and high spectral similarity to other vegetation. With the availability of new high-resolution remote sensing data from various platforms and sensors, it is possible to capture both spectral and spatial characteristics of weed species at multiple scales. Effective multi-resolution feature learning is then desirable to extract distinctive intensity, texture and shape features of each category of weed to enhance the weed separability. We propose a feature extraction method using a Convolutional Neural Network (CNN) and superpixel based Local Binary Pattern (LBP). Both middle and high level spatial features are learned using the CNN. Local texture features from superpixel-based LBP are extracted, and are also used as input to Support Vector Machines (SVM) for weed classification. Experimental results on the hyperspectral and remote sensing datasets verify the effectiveness of the proposed method, and show that it outperforms several feature extraction approaches.},
DOI = {10.3390/rs11141692}
}



@Article{rs11141694,
AUTHOR = {Mekhalfi, Mohamed Lamine and Bejiga, Mesay Belete and Soresina, Davide and Melgani, Farid and Demir, Begüm},
TITLE = {Capsule Networks for Object Detection in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1694},
URL = {https://www.mdpi.com/2072-4292/11/14/1694},
ISSN = {2072-4292},
ABSTRACT = {Recent advances in Convolutional Neural Networks (CNNs) have attracted great attention in remote sensing due to their high capability to model high-level semantic content of Remote Sensing (RS) images. However, CNNs do not explicitly retain the relative position of objects in an image and, thus, the effectiveness of the obtained features is limited in the framework of the complex object detection problems. To address this problem, in this paper we introduce Capsule Networks (CapsNets) for object detection in Unmanned Aerial Vehicle-acquired images. Unlike CNNs, CapsNets extract and exploit the information content about objects&rsquo; relative position across several layers, which enables parsing crowded scenes with overlapping objects. Experimental results obtained on two datasets for car and solar panel detection problems show that CapsNets provide similar object detection accuracies when compared to state-of-the-art deep models with significantly reduced computational time. This is due to the fact that CapsNets emphasize dynamic routine instead of the depth.},
DOI = {10.3390/rs11141694}
}



@Article{rs11141698,
AUTHOR = {Xu, Jin and Wang, Haixia and Cui, Can and Liu, Peng and Zhao, Yang and Li, Bo},
TITLE = {Oil Spill Segmentation in Ship-Borne Radar Images with an Improved Active Contour Model},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1698},
URL = {https://www.mdpi.com/2072-4292/11/14/1698},
ISSN = {2072-4292},
ABSTRACT = {Oil spills cause serious damage to marine ecosystems and environments. The application of ship-borne radars to monitor oil spill emergencies and rescue operations has shown promise, but has not been well-studied. This paper presents an improved Active Contour Model (ACM) for oil film detection in ship-borne radar images using pixel area threshold parameters. After applying a pre-processing scheme with a Laplace operator, an Otsu threshold, and mean and median filtering, the shape and area of the oil film can be calculated rapidly. Compared with other ACMs, the improved Local Binary Fitting (LBF) model is robust and has a fast calculation speed for uniform ship-borne radar sea clutter images. The proposed method achieves better results and higher operation efficiency than other automatic and semi-automatic methods for oil film detection in ship-borne radar images. Furthermore, it provides a scientific basis to assess pollution scope and estimate the necessary cleaning materials during oil spills.},
DOI = {10.3390/rs11141698}
}



@Article{rs11141708,
AUTHOR = {Cao, Shuang and Yu, Yongtao and Guan, Haiyan and Peng, Daifeng and Yan, Wanqian},
TITLE = {Affine-Function Transformation-Based Object Matching for Vehicle Detection from Unmanned Aerial Vehicle Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1708},
URL = {https://www.mdpi.com/2072-4292/11/14/1708},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection from remote sensing images plays a significant role in transportation related applications. However, the scale variations, orientation variations, illumination variations, and partial occlusions of vehicles, as well as the image qualities, bring great challenges for accurate vehicle detection. In this paper, we present an affine-function transformation-based object matching framework for vehicle detection from unmanned aerial vehicle (UAV) images. First, meaningful and non-redundant patches are generated through a superpixel segmentation strategy. Then, the affine-function transformation-based object matching framework is applied to a vehicle template and each of the patches for vehicle existence estimation. Finally, vehicles are detected and located after matching cost thresholding, vehicle location estimation, and multiple response elimination. Quantitative evaluations on two UAV image datasets show that the proposed method achieves an average completeness, correctness, quality, and F1-measure of 0.909, 0.969, 0.883, and 0.938, respectively. Comparative studies also demonstrate that the proposed method achieves compatible performance with the Faster R-CNN and outperforms the other eight existing methods in accurately detecting vehicles of various conditions.},
DOI = {10.3390/rs11141708}
}



@Article{rs11141713,
AUTHOR = {Jozdani, Shahab Eddin and Johnson, Brian Alan and Chen, Dongmei},
TITLE = {Comparing Deep Neural Networks, Ensemble Classifiers, and Support Vector Machine Algorithms for Object-Based Urban Land Use/Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1713},
URL = {https://www.mdpi.com/2072-4292/11/14/1713},
ISSN = {2072-4292},
ABSTRACT = {With the advent of high-spatial resolution (HSR) satellite imagery, urban land use/land cover (LULC) mapping has become one of the most popular applications in remote sensing. Due to the importance of context information (e.g., size/shape/texture) for classifying urban LULC features, Geographic Object-Based Image Analysis (GEOBIA) techniques are commonly employed for mapping urban areas. Regardless of adopting a pixel- or object-based framework, the selection of a suitable classifier is of critical importance for urban mapping. The popularity of deep learning (DL) (or deep neural networks (DNNs)) for image classification has recently skyrocketed, but it is still arguable if, or to what extent, DL methods can outperform other state-of-the art ensemble and/or Support Vector Machines (SVM) algorithms in the context of urban LULC classification using GEOBIA. In this study, we carried out an experimental comparison among different architectures of DNNs (i.e., regular deep multilayer perceptron (MLP), regular autoencoder (RAE), sparse, autoencoder (SAE), variational autoencoder (AE), convolutional neural networks (CNN)), common ensemble algorithms (Random Forests (RF), Bagging Trees (BT), Gradient Boosting Trees (GB), and Extreme Gradient Boosting (XGB)), and SVM to investigate their potential for urban mapping using a GEOBIA approach. We tested the classifiers on two RS images (with spatial resolutions of 30 cm and 50 cm). Based on our experiments, we drew three main conclusions: First, we found that the MLP model was the most accurate classifier. Second, unsupervised pretraining with the use of autoencoders led to no improvement in the classification result. In addition, the small difference in the classification accuracies of MLP from those of other models like SVM, GB, and XGB classifiers demonstrated that other state-of-the-art machine learning classifiers are still versatile enough to handle mapping of complex landscapes. Finally, the experiments showed that the integration of CNN and GEOBIA could not lead to more accurate results than the other classifiers applied.},
DOI = {10.3390/rs11141713}
}



@Article{pr7070464,
AUTHOR = {Gong, Qingwu and Tan, Si and Wang, Yubo and Liu, Dong and Qiao, Hui and Wu, Liuchuang},
TITLE = {Online Operation Risk Assessment of the Wind Power System of the Convolution Neural Network (CNN) Considering Multiple Random Factors},
JOURNAL = {Processes},
VOLUME = {7},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {464},
URL = {https://www.mdpi.com/2227-9717/7/7/464},
ISSN = {2227-9717},
ABSTRACT = {In order to solve the problem of the inaccuracy of the traditional online operation risk assessment model based on a physical mechanism and the inability to adapt to the actual operation of massive online operation monitoring data, this paper proposes an online operation risk assessment of the wind power system of the convolution neural network (CNN) considering multiple random factors. This paper analyzes multiple random factors of the wind power system, including uncertain wind power output, load fluctuations, frequent changes in operation patterns, and the electrical equipment failure rate, and generates the sample data based on multi-random factors. It uses the CNN algorithm network, offline training to obtain the risk assessment model, and online application to obtain the real-time online operation risk state of the wind power system. Finally, the online operation risk assessment model is verified by simulation using the standard network of 39 nodes of 10 machines New England system. The results prove that the risk assessment model presented in this paper is more rapid and suitable for online application.},
DOI = {10.3390/pr7070464}
}



@Article{app9142908,
AUTHOR = {Zhou, Qiang and Li, Xin},
TITLE = {Deep Homography Estimation and Its Application to Wall Maps of Wall-Climbing Robots},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2908},
URL = {https://www.mdpi.com/2076-3417/9/14/2908},
ISSN = {2076-3417},
ABSTRACT = {When locating wall-climbing robots with vision-based methods, locating and controlling the wall-climbing robot in the pixel coordinate of the wall map is an effective alternative that eliminates the need to calibrate the internal and external parameters of the camera. The estimation accuracy of the homography matrix between the camera image and the wall map directly impacts the pixel positioning accuracy of the wall-climbing robot in the wall map. In this study, we focused on the homography estimation between the camera image and wall map. We proposed HomographyFpnNet and obtained a smaller homography estimation error for a center-aligned image pair compared with the state of the art. The proposed hierarchical HomographyFpnNet for a non-center-aligned image pair significantly outperforms the method based on artificially designed features + Random Sample Consensus. The experiments conducted with a trained three-stage hierarchical HomographyFpnNet model on wall images of climbing robots also achieved small mean corner pixel error and proved its potential for estimating the homography between the wall map and camera images. The three-stage hierarchical HomographyFpnNet model has an average processing time of 10.8 ms on a GPU. The real-time processing speed satisfies the requirements of wall-climbing robots.},
DOI = {10.3390/app9142908}
}



@Article{agronomy9070403,
AUTHOR = {Tsolakis, Naoum and Bechtsis, Dimitrios and Bochtis, Dionysis},
TITLE = {AgROS: A Robot Operating System Based Emulation Tool for Agricultural Robotics},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {403},
URL = {https://www.mdpi.com/2073-4395/9/7/403},
ISSN = {2073-4395},
ABSTRACT = {This research aims to develop a farm management emulation tool that enables agrifood producers to effectively introduce advanced digital technologies, like intelligent and autonomous unmanned ground vehicles (UGVs), in real-world field operations. To that end, we first provide a critical taxonomy of studies investigating agricultural robotic systems with regard to: (i) the analysis approach, i.e., simulation, emulation, real-world implementation; (ii) farming operations; and (iii) the farming type. Our analysis demonstrates that simulation and emulation modelling have been extensively applied to study advanced agricultural machinery while the majority of the extant research efforts focuses on harvesting/picking/mowing and fertilizing/spraying activities; most studies consider a generic agricultural layout. Thereafter, we developed AgROS, an emulation tool based on the Robot Operating System, which could be used for assessing the efficiency of real-world robot systems in customized fields. The AgROS allows farmers to select their actual field from a map layout, import the landscape of the field, add characteristics of the actual agricultural layout (e.g., trees, static objects), select an agricultural robot from a predefined list of commercial systems, import the selected UGV into the emulation environment, and test the robot&rsquo;s performance in a quasi-real-world environment. AgROS supports farmers in the ex-ante analysis and performance evaluation of robotized precision farming operations while lays the foundations for realizing &ldquo;digital twins&rdquo; in agriculture.},
DOI = {10.3390/agronomy9070403}
}



@Article{robotics8030059,
AUTHOR = {Iannace, Gino and Ciaburro, Giuseppe and Trematerra, Amelia},
TITLE = {Fault Diagnosis for UAV Blades Using Artificial Neural Network},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2218-6581/8/3/59},
ISSN = {2218-6581},
ABSTRACT = {In recent years, unmanned aerial vehicles (UAVs) have been used in several fields including, for example, archaeology, cargo transport, conservation, healthcare, filmmaking, hobbies and recreational use. UAVs are aircraft characterized by the absence of a human pilot on board. The extensive use of these devices has highlighted maintenance problems with regard to the propellers, which represent the source of propulsion of the aircraft. A defect in the propellers of a drone can cause the aircraft to fall to the ground and its consequent destruction, and it also constitutes a safety problem for objects and people that are in the range of action of the aircraft. In this study, the measurements of the noise emitted by a UAV were used to build a classification model to detect unbalanced blades in a UAV propeller. To simulate the fault condition, two strips of paper tape were applied to the upper surface of a blade. The paper tape created a substantial modification of the aerodynamics of the blade, and this modification characterized the noise produced by the blade in its rotation. Then, a model based on artificial neural network algorithms was built to detect unbalanced blades in a UAV propeller. This model showed high accuracy (0.9763), indicating a high number of correct detections and suggests the adoption of this tool to verify the operating conditions of a UAV. The test must be performed indoors; from the measurements of the noise produced by the UAV it is possible to identify an imbalance in the propeller blade.},
DOI = {10.3390/robotics8030059}
}



@Article{rs11141725,
AUTHOR = {Xia, Xue and Persello, Claudio and Koeva, Mila},
TITLE = {Deep Fully Convolutional Networks for Cadastral Boundary Detection from UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1725},
URL = {https://www.mdpi.com/2072-4292/11/14/1725},
ISSN = {2072-4292},
ABSTRACT = {There is a growing demand for cheap and fast cadastral mapping methods to face the challenge of 70% global unregistered land rights. As traditional on-site field surveying is time-consuming and labor intensive, imagery-based cadastral mapping has in recent years been advocated by fit-for-purpose (FFP) land administration. However, owing to the semantic gap between the high-level cadastral boundary concept and low-level visual cues in the imagery, improving the accuracy of automatic boundary delineation remains a major challenge. In this research, we use imageries acquired by Unmanned Aerial Vehicles (UAV) to explore the potential of deep Fully Convolutional Networks (FCNs) for cadastral boundary detection in urban and semi-urban areas. We test the performance of FCNs against other state-of-the-art techniques, including Multi-Resolution Segmentation (MRS) and Globalized Probability of Boundary (gPb) in two case study sites in Rwanda. Experimental results show that FCNs outperformed MRS and gPb in both study areas and achieved an average accuracy of 0.79 in precision, 0.37 in recall and 0.50 in F-score. In conclusion, FCNs are able to effectively extract cadastral boundaries, especially when a large proportion of cadastral boundaries are visible. This automated method could minimize manual digitization and reduce field work, thus facilitating the current cadastral mapping and updating practices.},
DOI = {10.3390/rs11141725}
}



@Article{s19143212,
AUTHOR = {Zhou, Sanzhang and Kang, Feng and Li, Wenbin and Kan, Jiangming and Zheng, Yongjun and He, Guojian},
TITLE = {Extracting Diameter at Breast Height with a Handheld Mobile LiDAR System in an Outdoor Environment},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3212},
URL = {https://www.mdpi.com/1424-8220/19/14/3212},
ISSN = {1424-8220},
ABSTRACT = {Mobile laser scanning (MLS) is widely used in the mapping of forest environments. It has become important for extracting the parameters of forest trees using the generated environmental map. In this study, a three-dimensional point cloud map of a forest area was generated by using the Velodyne VLP-16 LiDAR system, so as to extract the diameter at breast height (DBH) of individual trees. The Velodyne VLP-16 LiDAR system and inertial measurement units (IMU) were used to construct a mobile measurement platform for generating 3D point cloud maps for forest areas. The 3D point cloud map in the forest area was processed offline, and the ground point cloud was removed by the random sample consensus (RANSAC) algorithm. The trees in the experimental area were segmented by the European clustering algorithm, and the DBH component of the tree point cloud was extracted and projected onto a 2D plane, fitting the DBH of the trees using the RANSAC algorithm in the plane. A three-dimensional point cloud map of 71 trees was generated in the experimental area, and estimated the DBH. The mean and variance of the absolute error were 0.43 cm and 0.50, respectively. The relative error of the whole was 2.27%, the corresponding variance was 15.09, and the root mean square error (RMSE) was 0.70 cm. The experimental results were good and met the requirements of forestry mapping, and the application value and significance were presented.},
DOI = {10.3390/s19143212}
}



@Article{app9142917,
AUTHOR = {Chen, Yan and Zhang, Chengming and Wang, Shouyi and Li, Jianping and Li, Feng and Yang, Xiaoxia and Wang, Yuanyuan and Yin, Leikun},
TITLE = {Extracting Crop Spatial Distribution from Gaofen 2 Imagery Using a Convolutional Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2917},
URL = {https://www.mdpi.com/2076-3417/9/14/2917},
ISSN = {2076-3417},
ABSTRACT = {Using satellite remote sensing has become a mainstream approach for extracting crop spatial distribution. Making edges finer is a challenge, while simultaneously extracting crop spatial distribution information from high-resolution remote sensing images using a convolutional neural network (CNN). Based on the characteristics of the crop area in the Gaofen 2 (GF-2) images, this paper proposes an improved CNN to extract fine crop areas. The CNN comprises a feature extractor and a classifier. The feature extractor employs a spectral feature extraction unit to generate spectral features, and five coding-decoding-pair units to generate five level features. A linear model is used to fuse features of different levels, and the fusion results are up-sampled to obtain a feature map consistent with the structure of the input image. This feature map is used by the classifier to perform pixel-by-pixel classification. In this study, the SegNet and RefineNet models and 21 GF-2 images of Feicheng County, Shandong Province, China, were chosen for comparison experiment. Our approach had an accuracy of 93.26%, which is higher than those of the existing SegNet (78.12%) and RefineNet (86.54%) models. This demonstrates the superiority of the proposed method in extracting crop spatial distribution information from GF-2 remote sensing images.},
DOI = {10.3390/app9142917}
}



@Article{s19143217,
AUTHOR = {Cho, Jaechan and Jung, Yongchul and Kim, Dong-Sun and Lee, Seongjoo and Jung, Yunho},
TITLE = {Moving Object Detection Based on Optical Flow Estimation and a Gaussian Mixture Model for Advanced Driver Assistance Systems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3217},
URL = {https://www.mdpi.com/1424-8220/19/14/3217},
ISSN = {1424-8220},
ABSTRACT = {Most approaches for moving object detection (MOD) based on computer vision are limited to stationary camera environments. In advanced driver assistance systems (ADAS), however, ego-motion is added to image frames owing to the use of a moving camera. This results in mixed motion in the image frames and makes it difficult to classify target objects and background. In this paper, we propose an efficient MOD algorithm that can cope with moving camera environments. In addition, we present a hardware design and implementation results for the real-time processing of the proposed algorithm. The proposed moving object detector was designed using hardware description language (HDL) and its real-time performance was evaluated using an FPGA based test system. Experimental results demonstrate that our design achieves better detection performance than existing MOD systems. The proposed moving object detector was implemented with 13.2K logic slices, 104 DSP48s, and 163 BRAM and can support real-time processing of 30 fps at an operating frequency of 200 MHz.},
DOI = {10.3390/s19143217}
}



@Article{geosciences9070323,
AUTHOR = {Jakovljevic, Gordana and Govedarica, Miro and Alvarez-Taboada, Flor and Pajic, Vladimir},
TITLE = {Accuracy Assessment of Deep Learning Based Classification of LiDAR and UAV Points Clouds for DTM Creation and Flood Risk Mapping},
JOURNAL = {Geosciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2076-3263/9/7/323},
ISSN = {2076-3263},
ABSTRACT = {Digital elevation model (DEM) has been frequently used for the reduction and management of flood risk. Various classification methods have been developed to extract DEM from point clouds. However, the accuracy and computational efficiency need to be improved. The objectives of this study were as follows: (1) to determine the suitability of a new method to produce DEM from unmanned aerial vehicle (UAV) and light detection and ranging (LiDAR) data, using a raw point cloud classification and ground point filtering based on deep learning and neural networks (NN); (2) to test the convenience of rebalancing datasets for point cloud classification; (3) to evaluate the effect of the land cover class on the algorithm performance and the elevation accuracy; and (4) to assess the usability of the LiDAR and UAV structure from motion (SfM) DEM in flood risk mapping. In this paper, a new method of raw point cloud classification and ground point filtering based on deep learning using NN is proposed and tested on LiDAR and UAV data. The NN was trained on approximately 6 million points from which local and global geometric features and intensity data were extracted. Pixel-by-pixel accuracy assessment and visual inspection confirmed that filtering point clouds based on deep learning using NN is an appropriate technique for ground classification and producing DEM, as for the test and validation areas, both ground and non-ground classes achieved high recall (&gt;0.70) and high precision values (&gt;0.85), which showed that the two classes were well handled by the model. The type of method used for balancing the original dataset did not have a significant influence in the algorithm accuracy, and it was suggested not to use any of them unless the distribution of the generated and real data set will remain the same. Furthermore, the comparisons between true data and LiDAR and a UAV structure from motion (UAV SfM) point clouds were analyzed, as well as the derived DEM. The root mean square error (RMSE) and the mean average error (MAE) of the DEM were 0.25 m and 0.05 m, respectively, for LiDAR data, and 0.59 m and &ndash;0.28 m, respectively, for UAV data. For all land cover classes, the UAV DEM overestimated the elevation, whereas the LIDAR DEM underestimated it. The accuracy was not significantly different in the LiDAR DEM for the different vegetation classes, while for the UAV DEM, the RMSE increased with the height of the vegetation class. The comparison of the inundation areas derived from true LiDAR and UAV data for different water levels showed that in all cases, the largest differences were obtained for the lowest water level tested, while they performed best for very high water levels. Overall, the approach presented in this work produced DEM from LiDAR and UAV data with the required accuracy for flood mapping according to European Flood Directive standards. Although LiDAR is the recommended technology for point cloud acquisition, a suitable alternative is also UAV SfM in hilly areas.},
DOI = {10.3390/geosciences9070323}
}



@Article{su11154007,
AUTHOR = {Ali, Haibat and Choi, Jae-ho},
TITLE = {A Review of Underground Pipeline Leakage and Sinkhole Monitoring Methods Based on Wireless Sensor Networking},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {4007},
URL = {https://www.mdpi.com/2071-1050/11/15/4007},
ISSN = {2071-1050},
ABSTRACT = {Major metropolitan cities worldwide have extensively invested to secure utilities and build state-of-the-art infrastructure related to underground fluid transportation. Sewer and water pipelines make our lives extremely convenient when they function appropriately. However, leakages in underground pipe mains causes sinkholes and drinking-water scarcity. Sinkholes are the complex problems stemming from the interaction of leaked water and ground. The aim of this work is to review the existing methods for monitoring leakage in underground pipelines, the sinkholes caused by these leakages, and the viability of wireless sensor networking (WSN) for monitoring leakages and sinkholes. Herein, the authors have discussed the methods based on different objectives and their applicability via various approaches&mdash;(1) patent analysis; (2) web-of-science analysis; (3) WSN-based pipeline leakage and sinkhole monitoring. The study shows that the research on sinkholes due to leakages in sewer and water pipelines by using WSN is still in a premature stage and needs extensive investigation and research contributions. Additionally, the authors have suggested prospects for future research by comparing, analyzing, and classifying the reviewed methods. This study advocates collocating WSN, Internet of things, and artificial intelligence with pipeline monitoring methods to resolve the issues of the sinkhole occurrence.},
DOI = {10.3390/su11154007}
}



@Article{app9152961,
AUTHOR = {Cao, Mingwei and Jia, Wei and Lv, Zhihan and Zheng, Liping and Liu, Xiaoping},
TITLE = {Superpixel-Based Feature Tracking for Structure from Motion},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2961},
URL = {https://www.mdpi.com/2076-3417/9/15/2961},
ISSN = {2076-3417},
ABSTRACT = {Feature tracking in image collections significantly affects the efficiency and accuracy of Structure from Motion (SFM). Insufficient correspondences may result in disconnected structures and incomplete components, while the redundant correspondences containing incorrect ones may yield to folded and superimposed structures. In this paper, we present a Superpixel-based feature tracking method for structure from motion. In the proposed method, we first propose to use a joint approach to detect local keypoints and compute descriptors. Second, the superpixel-based approach is used to generate labels for the input image. Third, we combine the Speed Up Robust Feature and binary test in the generated label regions to produce a set of combined descriptors for the detected keypoints. Fourth, the locality-sensitive hash (LSH)-based k nearest neighboring matching (KNN) is utilized to produce feature correspondences, and then the ratio test approach is used to remove outliers from the previous matching collection. Finally, we conduct comprehensive experiments on several challenging benchmarking datasets including highly ambiguous and duplicated scenes. Experimental results show that the proposed method gets better performances with respect to the state of the art methods.},
DOI = {10.3390/app9152961}
}



@Article{su11154016,
AUTHOR = {Li, Dawei and Zhang, Yujia and Li, Cheng},
TITLE = {Mining Public Opinion on Transportation Systems Based on Social Media Data},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {4016},
URL = {https://www.mdpi.com/2071-1050/11/15/4016},
ISSN = {2071-1050},
ABSTRACT = {Public participation plays an important role of traffic planning and management, but it is a great challenge to collect and analyze public opinions for traffic problems on a large scale under traditional methods. Traffic management departments should appropriately adopt public opinions in order to formulate scientific and reasonable regulations and policies. At present, while increasing degree of public participation, data collection and processing should be accelerated to make up for the shortcomings of traditional planning. This paper focuses on text analysis using large data with temporal and spatial attributes of social network platform. Web crawler technology is used to obtain traffic-related text in mainstream social platforms. After basic treatment, the emotional tendency of the text is analyzed. Then, based on the probabilistic topic modeling (latent Dirichlet allocation model), the main opinions of the public are extracted, and the spatial and temporal characteristics of the data are summarized. Taking Nanjing Metro as an example, the existing problems are summarized from the public opinions and improvement measures are put forward, which proves the feasibility of providing technical support for public participation in public transport with social media big data.},
DOI = {10.3390/su11154016}
}



@Article{rs11151748,
AUTHOR = {Hariharan, Jeanette and Fuller, John and Ampatzidis, Yiannis and Abdulridha, Jaafar and Lerwill, Andrew},
TITLE = {Finite Difference Analysis and Bivariate Correlation of Hyperspectral Data for Detecting Laurel Wilt Disease and Nutritional Deficiency in Avocado},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1748},
URL = {https://www.mdpi.com/2072-4292/11/15/1748},
ISSN = {2072-4292},
ABSTRACT = {Laurel wilt (Lw) is a very destructive disease and poses a serious threat to the commercial production of avocado in Florida, USA. External symptoms of Lw are similar to those that are caused by other diseases and disorders. A rapid technique to distinguish Lw infected avocado from healthy trees and trees with other abiotic stressors is presented in this paper. A novel method was developed to analyze data from hyperspectral data using finite difference approximation (FDA) and bivariate correlation (BC) to discriminate Lw, Nitrogen (N), and Iron (Fe) deficiencies from healthy avocado plants. Several combinatorial methods were used in preprocessing the data, such as standard normal transformation of data, smoothing of the data, and polynomial fit. The FDA technique was derived using a Taylor Polynomial finite difference approximation. This FDA accentuates inflection points in the spectrum. These, in turn, reveal variance in the data that can be used to identify spectral signature associated with healthy and diseased states. By statistical correlation using the bivariate correlation coefficient of these enhanced spectral patterns, an algorithm (FDA-BC) for distinguishing Lw avocado leaves from all other categories of healthy or mineral deficient avocado leaves is achieved with an overall accuracy of 100%.},
DOI = {10.3390/rs11151748}
}



@Article{rs11151763,
AUTHOR = {Li, Songyang and Yuan, Fei and Ata-UI-Karim, Syed Tahir and Zheng, Hengbiao and Cheng, Tao and Liu, Xiaojun and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Cao, Qiang},
TITLE = {Combining Color Indices and Textures of UAV-Based Digital Imagery for Rice LAI Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1763},
URL = {https://www.mdpi.com/2072-4292/11/15/1763},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is a fundamental indicator of plant growth status in agronomic and environmental studies. Due to rapid advances in unmanned aerial vehicle (UAV) and sensor technologies, UAV-based remote sensing is emerging as a promising solution for monitoring crop LAI with great flexibility and applicability. This study aimed to determine the feasibility of combining color and texture information derived from UAV-based digital images for estimating LAI of rice (Oryza sativa L.). Rice field trials were conducted at two sites using different nitrogen application rates, varieties, and transplanting methods during 2016 to 2017. Digital images were collected using a consumer-grade UAV after sampling at key growth stages of tillering, stem elongation, panicle initiation and booting. Vegetation color indices (CIs) and grey level co-occurrence matrix-based textures were extracted from mosaicked UAV ortho-images for each plot. As a solution of using indices composed by two different textures, normalized difference texture indices (NDTIs) were calculated by two randomly selected textures. The relationships between rice LAIs and each calculated index were then compared using simple linear regression. Multivariate regression models with different input sets were further used to test the potential of combining CIs with various textures for rice LAI estimation. The results revealed that the visible atmospherically resistant index (VARI) based on three visible bands and the NDTI based on the mean textures derived from the red and green bands were the best for LAI retrieval in the CI and NDTI groups, respectively. Independent accuracy assessment showed that random forest (RF) exhibited the best predictive performance when combining CI and texture inputs (R2 = 0.84, RMSE = 0.87, MAE = 0.69). This study introduces a promising solution of combining color indices and textures from UAV-based digital imagery for rice LAI estimation. Future studies are needed on finding the best operation mode, suitable ground resolution, and optimal predictive methods for practical applications.},
DOI = {10.3390/rs11151763}
}



@Article{rs11151774,
AUTHOR = {Yi, Yaning and Zhang, Zhijie and Zhang, Wanchang and Zhang, Chuanrong and Li, Weidong and Zhao, Tian},
TITLE = {Semantic Segmentation of Urban Buildings from VHR Remote Sensing Imagery Using a Deep Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1774},
URL = {https://www.mdpi.com/2072-4292/11/15/1774},
ISSN = {2072-4292},
ABSTRACT = {Urban building segmentation is a prevalent research domain for very high resolution (VHR) remote sensing; however, various appearances and complicated background of VHR remote sensing imagery make accurate semantic segmentation of urban buildings a challenge in relevant applications. Following the basic architecture of U-Net, an end-to-end deep convolutional neural network (denoted as DeepResUnet) was proposed, which can effectively perform urban building segmentation at pixel scale from VHR imagery and generate accurate segmentation results. The method contains two sub-networks: One is a cascade down-sampling network for extracting feature maps of buildings from the VHR image, and the other is an up-sampling network for reconstructing those extracted feature maps back to the same size of the input VHR image. The deep residual learning approach was adopted to facilitate training in order to alleviate the degradation problem that often occurred in the model training process. The proposed DeepResUnet was tested with aerial images with a spatial resolution of 0.075 m and was compared in performance under the exact same conditions with six other state-of-the-art networks&mdash;FCN-8s, SegNet, DeconvNet, U-Net, ResUNet and DeepUNet. Results of extensive experiments indicated that the proposed DeepResUnet outperformed the other six existing networks in semantic segmentation of urban buildings in terms of visual and quantitative evaluation, especially in labeling irregular-shape and small-size buildings with higher accuracy and entirety. Compared with the U-Net, the F1 score, Kappa coefficient and overall accuracy of DeepResUnet were improved by 3.52%, 4.67% and 1.72%, respectively. Moreover, the proposed DeepResUnet required much fewer parameters than the U-Net, highlighting its significant improvement among U-Net applications. Nevertheless, the inference time of DeepResUnet is slightly longer than that of the U-Net, which is subject to further improvement.},
DOI = {10.3390/rs11151774}
}



@Article{s19153318,
AUTHOR = {Martínez, Carlos and Jiménez, Felipe},
TITLE = {Implementation of a Potential Field-Based Decision-Making Algorithm on Autonomous Vehicles for Driving in Complex Environments},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3318},
URL = {https://www.mdpi.com/1424-8220/19/15/3318},
ISSN = {1424-8220},
ABSTRACT = {Autonomous driving is undergoing huge developments nowadays. It is expected that its implementation will bring many benefits. Autonomous cars must deal with tasks at different levels. Although some of them are currently solved, and perception systems provide quite an accurate and complete description of the environment, high-level decisions are hard to obtain in challenging scenarios. Moreover, they must comply with safety, reliability and predictability requirements, road user acceptance, and comfort specifications. This paper presents a path planning algorithm based on potential fields. Potential models are adjusted so that their behavior is appropriate to the environment and the dynamics of the vehicle and they can face almost any unexpected scenarios. The response of the system considers the road characteristics (e.g., maximum speed, lane line curvature, etc.) and the presence of obstacles and other users. The algorithm has been tested on an automated vehicle equipped with a GPS receiver, an inertial measurement unit and a computer vision system in real environments with satisfactory results.},
DOI = {10.3390/s19153318}
}



@Article{s19153316,
AUTHOR = {Salhaoui, Marouane and Guerrero-González, Antonio and Arioua, Mounir and Ortiz, Francisco J. and El Oualkadi, Ahmed and Torregrosa, Carlos Luis},
TITLE = {Smart Industrial IoT Monitoring and Control System Based on UAV and Cloud Computing Applied to a Concrete Plant},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3316},
URL = {https://www.mdpi.com/1424-8220/19/15/3316},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are now considered one of the best remote sensing techniques for gathering data over large areas. They are now being used in the industry sector as sensing tools for proactively solving or preventing many issues, besides quantifying production and helping to make decisions. UAVs are a highly consistent technological platform for efficient and cost-effective data collection and event monitoring. The industrial Internet of things (IIoT) sends data from systems that monitor and control the physical world to data processing systems that cloud computing has shown to be important tools for meeting processing requirements. In fog computing, the IoT gateway links different objects to the internet. It can operate as a joint interface for different networks and support different communication protocols. A great deal of effort has been put into developing UAVs and multi-UAV systems. This paper introduces a smart IIoT monitoring and control system based on an unmanned aerial vehicle that uses cloud computing services and exploits fog computing as the bridge between IIoT layers. Its novelty lies in the fact that the UAV is automatically integrated into an industrial control system through an IoT gateway platform, while UAV photos are systematically and instantly computed and analyzed in the cloud. Visual supervision of the plant by drones and cloud services is integrated in real-time into the control loop of the industrial control system. As a proof of concept, the platform was used in a case study in an industrial concrete plant. The results obtained clearly illustrate the feasibility of the proposed platform in providing a reliable and efficient system for UAV remote control to improve product quality and reduce waste. For this, we studied the communication latency between the different IIoT layers in different IoT gateways.},
DOI = {10.3390/s19153316}
}



@Article{robotics8030062,
AUTHOR = {Thomas, Ajith and Hedley, John},
TITLE = {FumeBot: A Deep Convolutional Neural Network Controlled Robot},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {62},
URL = {https://www.mdpi.com/2218-6581/8/3/62},
ISSN = {2218-6581},
ABSTRACT = {This paper describes the development of a convolutional neural network for the control of a home monitoring robot (FumeBot). The robot is fitted with a Raspberry Pi for on board control and a Raspberry Pi camera is used as the data feed for the neural network. A wireless connection between the robot and a graphical user interface running on a laptop allows for the diagnostics and development of the neural network. The neural network, running on the laptop, was trained using a supervised training method. The robot was put through a series of obstacle courses to test its robustness, with the tests demonstrating that the controller has learned to navigate the obstacles to a reasonable level. The main problem identified in this work was that the neural controller did not have memory of past actions it took and a past state of the world resulting in obstacle collisions. Options to rectify this issue are suggested.},
DOI = {10.3390/robotics8030062}
}



@Article{s19153335,
AUTHOR = {Fuentes, Sigfredo and Tongson, Eden Jane and De Bei, Roberta and Gonzalez Viejo, Claudia and Ristic, Renata and Tyerman, Stephen and Wilkinson, Kerry},
TITLE = {Non-Invasive Tools to Detect Smoke Contamination in Grapevine Canopies, Berries and Wine: A Remote Sensing and Machine Learning Modeling Approach},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3335},
URL = {https://www.mdpi.com/1424-8220/19/15/3335},
ISSN = {1424-8220},
ABSTRACT = {Bushfires are becoming more frequent and intensive due to changing climate. Those that occur close to vineyards can cause smoke contamination of grapevines and grapes, which can affect wines, producing smoke-taint. At present, there are no available practical in-field tools available for detection of smoke contamination or taint in berries. This research proposes a non-invasive/in-field detection system for smoke contamination in grapevine canopies based on predictable changes in stomatal conductance patterns based on infrared thermal image analysis and machine learning modeling based on pattern recognition. A second model was also proposed to quantify levels of smoke-taint related compounds as targets in berries and wines using near-infrared spectroscopy (NIR) as inputs for machine learning fitting modeling. Results showed that the pattern recognition model to detect smoke contamination from canopies had 96% accuracy. The second model to predict smoke taint compounds in berries and wine fit the NIR data with a correlation coefficient (R) of 0.97 and with no indication of overfitting. These methods can offer grape growers quick, affordable, accurate, non-destructive in-field screening tools to assist in vineyard management practices to minimize smoke taint in wines with in-field applications using smartphones and unmanned aerial systems (UAS).},
DOI = {10.3390/s19153335}
}



@Article{rs11151780,
AUTHOR = {Böhler, Jonas E. and Schaepman, Michael E. and Kneubühler, Mathias},
TITLE = {Optimal Timing Assessment for Crop Separation Using Multispectral Unmanned Aerial Vehicle (UAV) Data and Textural Features},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1780},
URL = {https://www.mdpi.com/2072-4292/11/15/1780},
ISSN = {2072-4292},
ABSTRACT = {The separation of crop types is essential for many agricultural applications, particularly when within-season information is required. Generally, remote sensing may provide timely information with varying accuracy over the growing season, but in small structured agricultural areas, a very high spatial resolution may be needed that exceeds current satellite capabilities. This paper presents an experiment using spectral and textural features of NIR-red-green-blue (NIR-RGB) bands data sets acquired with an unmanned aerial vehicle (UAV). The study area is located in the Swiss Plateau, which has highly fragmented and small structured agricultural fields. The observations took place between May 5 and September 29, 2015 over 11 days. The analyses are based on a random forest (RF) approach, predicting crop separation metrics of all analyzed crops. Three temporal windows of observations based on accumulated growing degree days (AGDD) were identified: an early temporal window (515&ndash;1232 AGDD, 5 May&ndash;17 June 2015) with an average accuracy (AA) of 70&ndash;75%; a mid-season window (1362&ndash;2016 AGDD, 25 June&ndash;22 July 2015) with an AA of around 80%; and a late window (2626&ndash;3238 AGDD, 21 August&ndash;29 September 2015) with an AA of &lt;65%. Therefore, crop separation is most promising in the mid-season window, and an additional NIR band increases the accuracy significantly. However, discrimination of winter crops is most effective in the early window, adding further observational requirements to the first window.},
DOI = {10.3390/rs11151780}
}



@Article{electronics8080847,
AUTHOR = {Zhang, Dong and Raven, Lindsey Ann and Lee, Dah-Jye and Yu, Meng and Desai, Alok},
TITLE = {Hardware Friendly Robust Synthetic Basis Feature Descriptor},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {847},
URL = {https://www.mdpi.com/2079-9292/8/8/847},
ISSN = {2079-9292},
ABSTRACT = {Finding corresponding image features between two images is often the first step for many computer vision algorithms. This paper introduces an improved synthetic basis feature descriptor algorithm that describes and compares image features in an efficient and discrete manner with rotation and scale invariance. It works by performing a number of similarity tests between the feature region surrounding the feature point and a predetermined number of synthetic basis images to generate a feature descriptor that uniquely describes the feature region. Features in two images are matched by comparing their descriptors. By only storing the similarity of the feature region to each synthetic basis image, the overall storage size is greatly reduced. In short, this new binary feature descriptor is designed to provide high feature matching accuracy with computational simplicity, relatively low resource usage, and a hardware friendly design for real-time vision applications. Experimental results show that our algorithm produces higher precision rates and larger number of correct matches than the original version and other mainstream algorithms and is a good alternative for common computer vision applications. Two applications that often have to cope with scaling and rotation variations are included in this work to demonstrate its performance.},
DOI = {10.3390/electronics8080847}
}



@Article{app9153099,
AUTHOR = {Hashmi, Muhammad Zeeshan Ul Hasnain and Riaz, Qaiser and Hussain, Mehdi and Shahzad, Muhammad},
TITLE = {What Lies Beneath One’s Feet? Terrain Classification Using Inertial Data of Human Walk},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3099},
URL = {https://www.mdpi.com/2076-3417/9/15/3099},
ISSN = {2076-3417},
ABSTRACT = {The objective of this study was to investigate if the inertial data collected from normal human walk can be used to reveal the underlying terrain types. For this purpose, we recorded the gait patterns of normal human walk on six different terrain types with variation in hardness and friction using body mounted inertial sensors. We collected accelerations and angular velocities of 40 healthy subjects with two smartphones embedded inertial measurement units (MPU-6500) attached at two different body locations (chest and lower back). The recorded data were segmented with stride based segmentation approach and 194 tempo-spectral features were computed for each stride. We trained two machine learning classifiers, namely random forest and support vector machine, and cross validated the results with 10-fold cross-validation strategy. The classification tasks were performed on indoor&ndash;outdoor terrains, hard&ndash;soft terrains, and a combination of binary, ternary, quaternary, quinary and senary terrains. From the experimental results, the classification accuracies of 97% and 92% were achieved for indoor&ndash;outdoor and hard&ndash;soft terrains, respectively. The classification results for binary, ternary, quaternary, quinary and senary class classification were 96%, 94%, 92%, 90%, and 89%, respectively. These results demonstrate that the stride data collected with the low-level signals of a single IMU can be used to train classifiers and predict terrain types with high accuracy. Moreover, the problem at hand can be solved invariant of sensor type and sensor location.},
DOI = {10.3390/app9153099}
}



@Article{rs11151812,
AUTHOR = {Dash, Jonathan P. and Watt, Michael S. and Paul, Thomas S. H. and Morgenroth, Justin and Pearse, Grant D.},
TITLE = {Early Detection of Invasive Exotic Trees Using UAV and Manned Aircraft Multispectral and LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1812},
URL = {https://www.mdpi.com/2072-4292/11/15/1812},
ISSN = {2072-4292},
ABSTRACT = {Exotic conifers can provide significant ecosystem services, but in some environments, they have become invasive and threaten indigenous ecosystems. In New Zealand, this phenomenon is of considerable concern as the area occupied by invasive exotic trees is large and increasing rapidly. Remote sensing methods offer a potential means of identifying and monitoring land infested by these trees, enabling managers to efficiently allocate resources for their control. In this study, we sought to develop methods for remote detection of exotic invasive trees, namely Pinus sylvestris and P. ponderosa. Critically, the study aimed to detect these species prior to the onset of maturity and coning as this is important for preventing further spread. In the study environment in New Zealand&rsquo;s South Island, these species reach maturity and begin bearing cones at a young age. As such, detection of these smaller individuals requires specialist methods and very high-resolution remote sensing data. We examined the efficacy of classifiers developed using two machine learning algorithms with multispectral and laser scanning data collected from two platforms&mdash;manned aircraft and unmanned aerial vehicles (UAV). The study focused on a localized conifer invasion originating from a multi-species pine shelter belt in a grassland environment. This environment provided a useful means of defining the detection thresholds of the methods and technologies employed. An extensive field dataset including over 17,000 trees (height range = 1 cm to 476 cm) was used as an independent validation dataset for the detection methods developed. We found that data from both platforms and using both logistic regression and random forests for classification provided highly accurate (kappa     &lt; 0.996    ) detection of invasive conifers. Our analysis showed that the data from both UAV and manned aircraft was useful for detecting trees down to 1 m in height and therefore shorter than 99.3% of the coning individuals in the study dataset. We also explored the relative contribution of both multispectral and airborne laser scanning (ALS) data in the detection of invasive trees through fitting classification models with different combinations of predictors and found that the most useful models included data from both sensors. However, the combination of ALS and multispectral data did not significantly improve classification accuracy. We believe that this was due to the simplistic vegetation and terrain structure in the study site that resulted in uncomplicated separability of invasive conifers from other vegetation. This study provides valuable new knowledge of the efficacy of detecting invasive conifers prior to the onset of coning using high-resolution data from UAV and manned aircraft. This will be an important tool in managing the spread of these important invasive plants.},
DOI = {10.3390/rs11151812}
}



@Article{s19153410,
AUTHOR = {Lin, Lishan and Yang, Yuji and Cheng, Hui and Chen, Xuechen},
TITLE = {Autonomous Vision-Based Aerial Grasping for Rotorcraft Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3410},
URL = {https://www.mdpi.com/1424-8220/19/15/3410},
ISSN = {1424-8220},
ABSTRACT = {Autonomous vision-based aerial grasping is an essential and challenging task for aerial manipulation missions. In this paper, we propose a vision-based aerial grasping system for a Rotorcraft Unmanned Aerial Vehicle (UAV) to grasp a target object. The UAV system is equipped with a monocular camera, a 3-DOF robotic arm with a gripper and a Jetson TK1 computer. Efficient and reliable visual detectors and control laws are crucial for autonomous aerial grasping using limited onboard sensing and computational capabilities. To detect and track the target object in real time, an efficient proposal algorithm is presented to reliably estimate the region of interest (ROI), then a correlation filter-based classifier is developed to track the detected object. Moreover, a support vector regression (SVR)-based grasping position detector is proposed to improve the grasp success rate with high computational efficiency. Using the estimated grasping position and the UAV?Äôs states, novel control laws of the UAV and the robotic arm are proposed to perform aerial grasping. Extensive simulations and outdoor flight experiments have been implemented. The experimental results illustrate that the proposed vision-based aerial grasping system can autonomously and reliably grasp the target object while working entirely onboard.},
DOI = {10.3390/s19153410}
}



@Article{computers8030058,
AUTHOR = {Ferrag, Mohamed Amine and Maglaras, Leandros},
TITLE = {DeliveryCoin: An IDS and Blockchain-Based Delivery Framework for Drone-Delivered Services},
JOURNAL = {Computers},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {58},
URL = {https://www.mdpi.com/2073-431X/8/3/58},
ISSN = {2073-431X},
ABSTRACT = {In this paper, we propose an intrusion detection system (IDS) and Blockchain-based delivery framework, called DeliveryCoin, for drone-delivered services. The DeliveryCoin framework consists of four phases, including system initialization phase, creating the block, updating the blockchain, and intrusion detection phase. To achieve privacy-preservation, the DeliveryCoin framework employs hash functions and short signatures without random oracles and the Strong Diffie&ndash;Hellman (SDH) assumption in bilinear groups. To achieve consensus inside the blockchain-based delivery platform, we introduce a UAV-aided forwarding mechanism, named pBFTF. We also propose an IDS system in each macro eNB (5G) for detecting self-driving network attacks as well as false transactions between self-driving nodes. Furthermore, extensive simulations are conducted, and results confirm the efficiency of our proposed DeliveryCoin framework in terms of latency of blockchain consensus and accuracy.},
DOI = {10.3390/computers8030058}
}



@Article{s19163465,
AUTHOR = {Pongsakornsathien, Nichakorn and Lim, Yixiang and Gardi, Alessandro and Hilton, Samuel and Planke, Lars and Sabatini, Roberto and Kistan, Trevor and Ezer, Neta},
TITLE = {Sensor Networks for Aerospace Human-Machine Systems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3465},
URL = {https://www.mdpi.com/1424-8220/19/16/3465},
ISSN = {1424-8220},
ABSTRACT = {Intelligent automation and trusted autonomy are being introduced in aerospace cyber-physical systems to support diverse tasks including data processing, decision-making, information sharing and mission execution. Due to the increasing level of integration/collaboration between humans and automation in these tasks, the operational performance of closed-loop human-machine systems can be enhanced when the machine monitors the operator’s cognitive states and adapts to them in order to maximise the effectiveness of the Human-Machine Interfaces and Interactions (HMI2). Technological developments have led to neurophysiological observations becoming a reliable methodology to evaluate the human operator’s states using a variety of wearable and remote sensors. The adoption of sensor networks can be seen as an evolution of this approach, as there are notable advantages if these sensors collect and exchange data in real-time, while their operation is controlled remotely and synchronised. This paper discusses recent advances in sensor networks for aerospace cyber-physical systems, focusing on Cognitive HMI2 (CHMI2) implementations. The key neurophysiological measurements used in this context and their relationship with the operator’s cognitive states are discussed. Suitable data analysis techniques based on machine learning and statistical inference are also presented, as these techniques allow processing both neurophysiological and operational data to obtain accurate cognitive state estimations. Lastly, to support the development of sensor networks for CHMI2 applications, the paper addresses the performance characterisation of various state-of-the-art sensors and the propagation of measurement uncertainties through a machine learning-based inference engine. Results show that a proper sensor selection and integration can support the implementation of effective human-machine systems for various challenging aerospace applications, including Air Traffic Management (ATM), commercial airliner Single-Pilot Operations (SIPO), one-to-many Unmanned Aircraft Systems (UAS), and space operations management.},
DOI = {10.3390/s19163465}
}



@Article{rs11161859,
AUTHOR = {Dyson, Jack and Mancini, Adriano and Frontoni, Emanuele and Zingaretti, Primo},
TITLE = {Deep Learning for Soil and Crop Segmentation from Remotely Sensed Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1859},
URL = {https://www.mdpi.com/2072-4292/11/16/1859},
ISSN = {2072-4292},
ABSTRACT = {One of the most challenging problems in precision agriculture is to correctly identify and separate crops from the soil. Current precision farming algorithms based on artificially intelligent networks use multi-spectral or hyper-spectral data to derive radiometric indices that guide the operational management of agricultural complexes. Deep learning applications using these big data require sensitive filtering of raw data to effectively drive their hidden layer neural network architectures. Threshold techniques based on the normalized difference vegetation index (NDVI) or other similar metrics are generally used to simplify the development and training of deep learning neural networks. They have the advantage of being natural transformations of hyper-spectral or multi-spectral images that filter the data stream into a neural network, while reducing training requirements and increasing system classification performance. In this paper, to calculate a detailed crop/soil segmentation based on high resolution Digital Surface Model (DSM) data, we propose the redefinition of the radiometric index using a directional mathematical filter. To further refine the analysis, we feed this new radiometric index image of about 3500 &times; 4500 pixels into a relatively small Convolution Neural Network (CNN) designed for general image pattern recognition at 28 &times; 28 pixels to evaluate and resolve the vegetation correctly. We show that the result of applying a DSM filter to the NDVI radiometric index before feeding it into a Convolutional Neural Network can potentially improve crop separation hit rate by 65%.},
DOI = {10.3390/rs11161859}
}



@Article{app9163264,
AUTHOR = {Kang, Xujie and Li, Jing and Fan, Xiangtao and Wan, Wenhui},
TITLE = {Real-Time RGB-D Simultaneous Localization and Mapping Guided by Terrestrial LiDAR Point Cloud for Indoor 3-D Reconstruction and Camera Pose Estimation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3264},
URL = {https://www.mdpi.com/2076-3417/9/16/3264},
ISSN = {2076-3417},
ABSTRACT = {In recent years, low-cost and lightweight RGB and depth (RGB-D) sensors, such as Microsoft Kinect, have made available rich image and depth data, making them very popular in the field of simultaneous localization and mapping (SLAM), which has been increasingly used in robotics, self-driving vehicles, and augmented reality. The RGB-D SLAM constructs 3D environmental models of natural landscapes while simultaneously estimating camera poses. However, in highly variable illumination and motion blur environments, long-distance tracking can result in large cumulative errors and scale shifts. To address this problem in actual applications, in this study, we propose a novel multithreaded RGB-D SLAM framework that incorporates a highly accurate prior terrestrial Light Detection and Ranging (LiDAR) point cloud, which can mitigate cumulative errors and improve the system&rsquo;s robustness in large-scale and challenging scenarios. First, we employed deep learning to achieve system automatic initialization and motion recovery when tracking is lost. Next, we used terrestrial LiDAR point cloud to obtain prior data of the landscape, and then we applied the point-to-surface inductively coupled plasma (ICP) iterative algorithm to realize accurate camera pose control from the previously obtained LiDAR point cloud data, and finally expanded its control range in the local map construction. Furthermore, an innovative double window segment-based map optimization method is proposed to ensure consistency, better real-time performance, and high accuracy of map construction. The proposed method was tested for long-distance tracking and closed-loop in two different large indoor scenarios. The experimental results indicated that the standard deviation of the 3D map construction is 10 cm in a mapping distance of 100 m, compared with the LiDAR ground truth. Further, the relative cumulative error of the camera in closed-loop experiments is 0.09%, which is twice less than that of the typical SLAM algorithm (3.4%). Therefore, the proposed method was demonstrated to be more robust than the ORB-SLAM2 algorithm in complex indoor environments.},
DOI = {10.3390/app9163264}
}



@Article{app9163277,
AUTHOR = {Chen, Bo and Hua, Chunsheng and Li, Decai and He, Yuqing and Han, Jianda},
TITLE = {Intelligent Human–UAV Interaction System with Joint Cross-Validation over Action–Gesture Recognition and Scene Understanding},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3277},
URL = {https://www.mdpi.com/2076-3417/9/16/3277},
ISSN = {2076-3417},
ABSTRACT = {We propose an intelligent human&ndash;unmanned aerial vehicle (UAV) interaction system, in which, instead of using the conventional remote controller, the UAV flight actions are controlled by a deep learning-based action&ndash;gesture joint detection system. The Resnet-based scene-understanding algorithm is introduced into the proposed system to enable the UAV to adjust its flight strategy automatically, according to the flying conditions. Meanwhile, both the deep learning-based action detection and multi-feature cascade gesture recognition methods are employed by a cross-validation process to create the corresponding flight action. The effectiveness and efficiency of the proposed system are confirmed by its application to controlling the flight action of a real flying UAV for more than 3 h.},
DOI = {10.3390/app9163277}
}



@Article{s19163517,
AUTHOR = {Ji, Zheng and Liao, Yifan and Zheng, Li and Wu, Liang and Yu, Manzhu and Feng, Yanjie},
TITLE = {An Assembled Detector Based on Geometrical Constraint for Power Component Recognition},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3517},
URL = {https://www.mdpi.com/1424-8220/19/16/3517},
ISSN = {1424-8220},
ABSTRACT = {The intelligent inspection of power lines and other difficult-to-access structures and facilities has been greatly enhanced by the use of Unmanned Aerial Vehicles (UAVs), which allow inspection in a safe, efficient, and high-quality fashion. This paper analyzes the characteristics of a scene containing power equipment and the operation mode of UAVs. A low-cost virtual scene is created, and a training sample for the power-line components is generated quickly. Taking a vibration-damper as the main object, an assembled detector based on geometrical constraint (ADGC) is proposed and is used to analyze the virtual dataset. The geometric positional relationship is used as the constraint, and the Faster Region with Convolutional Neural Network (R-CNN), Deformable Part Model (DPM), and Haar cascade classifiers are combined, which allows the features of different classifiers, such as contour, shape, and texture to be fully used. By combining the characteristics of virtual data and real data using UAV images, the power components are detected by the ADGC. The result produced by the detector with relatively good performance can help expand the training set and achieve a better detection model. Moreover, this method can be smoothly transferred to other power-line facilities and other power-line scenarios.},
DOI = {10.3390/s19163517}
}



@Article{s19163542,
AUTHOR = {Lygouras, Eleftherios and Santavas, Nicholas and Taitzoglou, Anastasios and Tarchanidis, Konstantinos and Mitropoulos, Athanasios and Gasteratos, Antonios},
TITLE = {Unsupervised Human Detection with an Embedded Vision System on a Fully Autonomous UAV for Search and Rescue Operations},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3542},
URL = {https://www.mdpi.com/1424-8220/19/16/3542},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play a primary role in a plethora of technical and scientific fields owing to their wide range of applications. In particular, the provision of emergency services during the occurrence of a crisis event is a vital application domain where such aerial robots can contribute, sending out valuable assistance to both distressed humans and rescue teams. Bearing in mind that time constraints constitute a crucial parameter in search and rescue (SAR) missions, the punctual and precise detection of humans in peril is of paramount importance. The paper in hand deals with real-time human detection onboard a fully autonomous rescue UAV. Using deep learning techniques, the implemented embedded system was capable of detecting open water swimmers. This allowed the UAV to provide assistance accurately in a fully unsupervised manner, thus enhancing first responder operational capabilities. The novelty of the proposed system is the combination of global navigation satellite system (GNSS) techniques and computer vision algorithms for both precise human detection and rescue apparatus release. Details about hardware configuration as well as the system&rsquo;s performance evaluation are fully discussed.},
DOI = {10.3390/s19163542}
}



@Article{app9163359,
AUTHOR = {Yeom, Seokwon and Cho, In-Jun},
TITLE = {Detection and Tracking of Moving Pedestrians with a Small Unmanned Aerial Vehicle},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3359},
URL = {https://www.mdpi.com/2076-3417/9/16/3359},
ISSN = {2076-3417},
ABSTRACT = {Small unmanned aircraft vehicles (SUAVs) or drones are very useful for visual detection and tracking due to their efficiency in capturing scenes. This paper addresses the detection and tracking of moving pedestrians with an SUAV. The detection step consists of frame subtraction, followed by thresholding, morphological filter, and false alarm reduction, taking into consideration the true size of targets. The center of the detected area is input to the next tracking stage. Interacting multiple model (IMM) filtering estimates the state of vectors and covariance matrices, using multiple modes of Kalman filtering. In the experiments, a dozen people and one car are captured by a stationary drone above the road. The Kalman filter and the IMM filter with two or three modes are compared in the accuracy of the state estimation. The root-mean squared errors (RMSE) of position and velocity are obtained for each target and show the good accuracy in detecting and tracking the target position&mdash;the average detection rate is 96.5%. When the two-mode IMM filter is used, the minimum average position and velocity RMSE obtained are around 0.8 m and 0.59 m/s, respectively.},
DOI = {10.3390/app9163359}
}



@Article{rs11161916,
AUTHOR = {Corradino, Claudia and Ganci, Gaetana and Cappello, Annalisa and Bilotta, Giuseppe and Hérault, Alexis and Del Negro, Ciro},
TITLE = {Mapping Recent Lava Flows at Mount Etna Using Multispectral Sentinel-2 Images and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1916},
URL = {https://www.mdpi.com/2072-4292/11/16/1916},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of recent lava flows can provide significant insight into the development of flow fields that may aid in predicting future flow behavior. The task is challenging, due to both intrinsic properties of the phenomenon (e.g., lava flow resurfacing processes) and technical issues (e.g., the difficulty to survey a spatially extended lava flow with either aerial or ground instruments while avoiding hazardous locations). The huge amount of moderate to high resolution multispectral satellite data currently provides new opportunities for monitoring of extreme thermal events, such as eruptive phenomena. While retrieving boundaries of an active lava flow is relatively straightforward, problems arise when discriminating a recently cooled lava flow from older lava flow fields. Here, we present a new supervised classifier based on machine learning techniques to discriminate recent lava imaged in the MultiSpectral Imager (MSI) onboard Sentinel-2 satellite. Automated classification evaluates each pixel in a scene and then groups the pixels with similar values (e.g., digital number, reflectance, radiance) into a specified number of classes. Bands at the spatial resolution of 10 m (bands 2, 3, 4, 8) are used as input to the classifier. The training phase is performed on a small number of pixels manually labeled as covered by fresh lava, while the testing characterizes the entire lava flow field. Compared with ground-based measurements and actual lava flows of Mount Etna emplaced in 2017 and 2018, our automatic procedure provides excellent results in terms of accuracy, precision, and sensitivity.},
DOI = {10.3390/rs11161916}
}



@Article{rs11161915,
AUTHOR = {Roberts, Kevin C. and Lindsay, John B. and Berg, Aaron A.},
TITLE = {An Analysis of Ground-Point Classifiers for Terrestrial LiDAR},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1915},
URL = {https://www.mdpi.com/2072-4292/11/16/1915},
ISSN = {2072-4292},
ABSTRACT = {Previous literature has compared the performance of existing ground point classification (GPC) techniques on airborne LiDAR (ALS) data (LiDAR—light detection and ranging); however, their performance when applied to terrestrial LiDAR (TLS) data has not yet been addressed. This research tested the classification accuracy of five openly-available GPC algorithms on seven TLS datasets: Zhang et al.’s inverted cloth simulation (CSF), Kraus and Pfeiffer’s hierarchical weighted robust interpolation classifier (HWRI), Axelsson’s progressive TIN densification filter (TIN), Evans and Hudak’s multiscale curvature classification (MCC), and Vosselman’s modified slope-based filter (MSBF). Classification performance was analyzed using the kappa index of agreement (KIA) and rasterized spatial distribution of classification accuracy datasets generated through comparisons with manually classified reference datasets. The results identified a decrease in classification accuracy for the CSF and HWRI classification of low vegetation, for the HWRI and MCC classifications of variably sloped terrain, for the HWRI and TIN classifications of low outlier points, and for the TIN and MSBF classifications of off-terrain (OT) points without any ground points beneath. Additionally, the results show that while no single algorithm was suitable for use on all datasets containing varying terrain characteristics and OT object types, in general, a mathematical-morphology/slope-based method outperformed other methods, reporting a kappa score of 0.902.},
DOI = {10.3390/rs11161915}
}



@Article{rs11161922,
AUTHOR = {Guo, Shichen and Jin, Qizhao and Wang, Hongzhen and Wang, Xuezhi and Wang, Yangang and Xiang, Shiming},
TITLE = {Learnable Gated Convolutional Neural Network for Semantic Segmentation in Remote-Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1922},
URL = {https://www.mdpi.com/2072-4292/11/16/1922},
ISSN = {2072-4292},
ABSTRACT = {Semantic segmentation in high-resolution remote-sensing (RS) images is a fundamental task for RS-based urban understanding and planning. However, various types of artificial objects in urban areas make this task quite challenging. Recently, the use of Deep Convolutional Neural Networks (DCNNs) with multiscale information fusion has demonstrated great potential in enhancing performance. Technically, however, existing fusions are usually implemented by summing or concatenating feature maps in a straightforward way. Seldom do works consider the spatial importance for global-to-local context-information aggregation. This paper proposes a Learnable-Gated CNN (L-GCNN) to address this issue. Methodologically, the Taylor expression of the information-entropy function is first parameterized to design the gate function, which is employed to generate pixelwise weights for coarse-to-fine refinement in the L-GCNN. Accordingly, a Parameterized Gate Module (PGM) was designed to achieve this goal. Then, the single PGM and its densely connected extension were embedded into different levels of the encoder in the L-GCNN to help identify the discriminative feature maps at different scales. With the above designs, the L-GCNN is finally organized as a self-cascaded end-to-end architecture that is able to sequentially aggregate context information for fine segmentation. The proposed model was evaluated on two public challenging benchmarks, the ISPRS 2Dsemantic segmentation challenge Potsdam dataset and the Massachusetts building dataset. The experiment results demonstrate that the proposed method exhibited significant improvement compared with several related segmentation networks, including the FCN, SegNet, RefineNet, PSPNet, DeepLab and GSN.For example, on the Potsdam dataset, our method achieved a 93.65%     F 1     score and 88.06%     I o U     score for the segmentation of tiny cars in high-resolution RS images. As a conclusion, the proposed model showed potential for object segmentation from the RS images of buildings, impervious surfaces, low vegetation, trees and cars in urban settings, which largely varies in size and have confusing appearances.},
DOI = {10.3390/rs11161922}
}



@Article{s19163595,
AUTHOR = {Santos, Anderson Aparecido dos and Marcato Junior, José and Araújo, Márcio Santos and Di Martini, David Robledo and Tetila, Everton Castelão and Siqueira, Henrique Lopes and Aoki, Camila and Eltner, Anette and Matsubara, Edson Takashi and Pistori, Hemerson and Feitosa, Raul Queiroz and Liesenberg, Veraldo and Gonçalves, Wesley Nunes},
TITLE = {Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3595},
URL = {https://www.mdpi.com/1424-8220/19/16/3595},
ISSN = {1424-8220},
ABSTRACT = {Detection and classification of tree species from remote sensing data were performed using mainly multispectral and hyperspectral images and Light Detection And Ranging (LiDAR) data. Despite the comparatively lower cost and higher spatial resolution, few studies focused on images captured by Red-Green-Blue (RGB) sensors. Besides, the recent years have witnessed an impressive progress of deep learning methods for object detection. Motivated by this scenario, we proposed and evaluated the usage of Convolutional Neural Network (CNN)-based methods combined with Unmanned Aerial Vehicle (UAV) high spatial resolution RGB imagery for the detection of law protected tree species. Three state-of-the-art object detection methods were evaluated: Faster Region-based Convolutional Neural Network (Faster R-CNN), YOLOv3 and RetinaNet. A dataset was built to assess the selected methods, comprising 392 RBG images captured from August 2018 to February 2019, over a forested urban area in midwest Brazil. The target object is an important tree species threatened by extinction known as Dipteryx alata Vogel (Fabaceae). The experimental analysis delivered average precision around 92% with an associated processing times below 30 miliseconds.},
DOI = {10.3390/s19163595}
}



@Article{math7080755,
AUTHOR = {Ran, Xiangjin and Xue, Linfu and Zhang, Yanyan and Liu, Zeyu and Sang, Xuejia and He, Jinxin},
TITLE = {Rock Classification from Field Image Patches Analyzed Using a Deep Convolutional Neural Network},
JOURNAL = {Mathematics},
VOLUME = {7},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {755},
URL = {https://www.mdpi.com/2227-7390/7/8/755},
ISSN = {2227-7390},
ABSTRACT = {The automatic identification of rock type in the field would aid geological surveying, education, and automatic mapping. Deep learning is receiving significant research attention for pattern recognition and machine learning. Its application here has effectively identified rock types from images captured in the field. This paper proposes an accurate approach for identifying rock types in the field based on image analysis using deep convolutional neural networks. The proposed approach can identify six common rock types with an overall classification accuracy of 97.96%, thus outperforming other established deep-learning models and a linear model. The results show that the proposed approach based on deep learning represents an improvement in intelligent rock-type identification and solves several difficulties facing the automated identification of rock types in the field.},
DOI = {10.3390/math7080755}
}



@Article{rs11161952,
AUTHOR = {Du, Jinyang and Watts, Jennifer D. and Jiang, Lingmei and Lu, Hui and Cheng, Xiao and Duguay, Claude and Farina, Mary and Qiu, Yubao and Kim, Youngwook and Kimball, John S. and Tarolli, Paolo},
TITLE = {Remote Sensing of Environmental Changes in Cold Regions: Methods, Achievements and Challenges},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1952},
URL = {https://www.mdpi.com/2072-4292/11/16/1952},
ISSN = {2072-4292},
ABSTRACT = {Cold regions, including high-latitude and high-altitude landscapes, are experiencing profound environmental changes driven by global warming. With the advance of earth observation technology, remote sensing has become increasingly important for detecting, monitoring, and understanding environmental changes over vast and remote regions. This paper provides an overview of recent achievements, challenges, and opportunities for land remote sensing of cold regions by (a) summarizing the physical principles and methods in remote sensing of selected key variables related to ice, snow, permafrost, water bodies, and vegetation; (b) highlighting recent environmental nonstationarity occurring in the Arctic, Tibetan Plateau, and Antarctica as detected from satellite observations; (c) discussing the limits of available remote sensing data and approaches for regional monitoring; and (d) exploring new opportunities from next-generation satellite missions and emerging methods for accurate, timely, and multi-scale mapping of cold regions.},
DOI = {10.3390/rs11161952}
}



@Article{app9173448,
AUTHOR = {Zhang, Changwu and Tang, Yuchen and Zhou, Li and Liu, Hengzhu},
TITLE = {Late Line-of-Sight Check and Prioritized Trees for Path Planning},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3448},
URL = {https://www.mdpi.com/2076-3417/9/17/3448},
ISSN = {2076-3417},
ABSTRACT = {How to generate a safe and high-quality path for a moving robot is a classical issue. The relative solution is also suitable for computer games and animation. To make the problem simpler, it is common to discretize the continuous planning space into grids, with blocked cells to represent obstacles. However, grid-based path-finding algorithms usually cannot find the truly shortest path because the extending paths are constrained to grid edges. Meanwhile, Line-of-Sight Check (LoS-Check) is an efficient operation to find the shorter any-angle path by relaxing the constraint of grid edges, which has been successfully used in Theta*. Through reducing the number of LoS-Check operations in Theta*, a variant version called LazyTheta* speeds up the planning especially in the 3D environment. We propose Late LoS-Check A* (LLA*) to further reduce the LoS-Check amount. It uses the structure of the prioritized trees to partially update the gvalues of different successors that share the same parent (i.e., the parent of the current vertex waiting to be extended). The sufficient experiments on various benchmark maps show that LLA* costs less execution time than Lazy Theta* while generating shorter paths. If we just delay the LoS-Check, the path planned by LLA* will hardly be shorter than that of Lazy Theta*. Therefore, the key of LLA* is the discriminatory strategy, and we empirically explain the reason why both the path length and execution time of LLA* are shorter than those of Lazy Theta*.},
DOI = {10.3390/app9173448}
}



@Article{electronics8090915,
AUTHOR = {Maldonado-Bascón, Saturnino and Iglesias-Iglesias, Cristian and Martín-Martín, Pilar and Lafuente-Arroyo, Sergio},
TITLE = {Fallen People Detection Capabilities Using Assistive Robot},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {915},
URL = {https://www.mdpi.com/2079-9292/8/9/915},
ISSN = {2079-9292},
ABSTRACT = {One of the main problems in the elderly population and for people with functional disabilities is falling when they are not supervised. Therefore, there is a need for monitoring systems with fall detection functionality. Mobile robots are a good solution for keeping the person in sight when compared to static-view sensors. Mobile-patrol robots can be used for a group of people and systems are less intrusive than ones based on mobile robots. In this paper, we propose a novel vision-based solution for fall detection based on a mobile-patrol robot that can correct its position in case of doubt. The overall approach can be formulated as an end-to-end solution based on two stages: person detection and fall classification. Deep learning-based computer vision is used for person detection and fall classification is done by using a learning-based Support Vector Machine (SVM) classifier. This approach mainly fulfills the following design requirements&mdash;simple to apply, adaptable, high performance, independent of person size, clothes, or the environment, low cost and real-time computing. Important to highlight is the ability to distinguish between a simple resting position and a real fall scene. One of the main contributions of this paper is the input feature vector to the SVM-based classifier. We evaluated the robustness of the approach using a realistic public dataset proposed in this paper called the Fallen Person Dataset (FPDS), with 2062 images and 1072 falls. The results obtained from different experiments indicate that the system has a high success rate in fall classification (precision of 100% and recall of 99.74%). Training the algorithm using our Fallen Person Dataset (FPDS) and testing it with other datasets showed that the algorithm is independent of the camera setup.},
DOI = {10.3390/electronics8090915}
}



@Article{rs11171976,
AUTHOR = {Hamdi, Zayd Mahmoud and Brandmeier, Melanie and Straub, Christoph},
TITLE = {Forest Damage Assessment Using Deep Learning on High Resolution Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1976},
URL = {https://www.mdpi.com/2072-4292/11/17/1976},
ISSN = {2072-4292},
ABSTRACT = {Storms can cause significant damage to forest areas, affecting biodiversity and infrastructure and leading to economic loss. Thus, rapid detection and mapping of windthrows are crucially important for forest management. Recent advances in computer vision have led to highly-accurate image classification algorithms such as Convolutional Neural Network (CNN) architectures. In this study, we tested and implemented an algorithm based on CNNs in an ArcGIS environment for automatic detection and mapping of damaged areas. The algorithm was trained and tested on a forest area in Bavaria, Germany. . It is a based on a modified U-Net architecture that was optimized for the pixelwise classification of multispectral aerial remote sensing data. The neural network was trained on labeled damaged areas from after-storm aerial orthophotos of a ca.     109 k  m 2      forest area with RGB and NIR bands and 0.2-m spatial resolution. Around     10 7     pixels of labeled data were used in the process. Once the network is trained, predictions on further datasets can be computed within seconds, depending on the size of the input raster and the computational power used. The overall accuracy on our test dataset was     92 %    . During visual validation, labeling errors were found in the reference data that somewhat biased the results because the algorithm in some instance performed better than the human labeling procedure, while missing areas affected by shadows. Our results are very good in terms of precision, and the methods introduced in this paper have several additional advantages compared to traditional methods: CNNs automatically detect high- and low-level features in the data, leading to high classification accuracies, while only one after-storm image is needed in comparison to two images for approaches based on change detection. Furthermore, flight parameters do not affect the results in the same way as for approaches that require DSMs and DTMs as the classification is only based on the image data themselves, and errors occurring in the computation of DSMs and DTMs do not affect the results with respect to the z component. The integration into the ArcGIS Platform allows a streamlined workflow for forest management, as the results can be accessed by mobile devices in the field to allow for high-accuracy ground-truthing and additional mapping that can be synchronized back into the database. Our results and the provided automatic workflow highlight the potential of deep learning on high-resolution imagery and GIS for fast and efficient post-disaster damage assessment as a first step of disaster management.},
DOI = {10.3390/rs11171976}
}



@Article{en12173234,
AUTHOR = {Joung, Jingon and Lee, Han Lim and Zhao, Jian and Kang, Xin},
TITLE = {Power Control Method for Energy Efficient Buffer-Aided Relay Systems},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3234},
URL = {https://www.mdpi.com/1996-1073/12/17/3234},
ISSN = {1996-1073},
ABSTRACT = {In this paper, a power control method is proposed for a buffer-aided relay node (RN) to enhance the energy efficiency of the RN system. By virtue of a buffer, the RN can reserve the data at the buffer when the the channel gain between an RN and a destination node (DN) is weaker than that between SN and RN. The RN then opportunistically forward the reserved data in the buffer according to channel condition between the RN and the DN. By exploiting the buffer, RN reduces transmit power when it reduces the transmit data rate and reserve the data in the buffer. Therefore, without any total throughput reduction, the power consumption of RN can be reduced, resulting in the energy efficiency (EE) improvement of the RN system. Furthermore, for the power control, we devise a simple power control method based on a two-dimensional surface fitting model of an optimal transmit power of RN. The proposed RN power control method is readily and locally implementable at the RN, and it can significantly improve EE of the RN compared to the fixed power control method and the spectral efficiency based method as verified by the rigorous numerical results.},
DOI = {10.3390/en12173234}
}



@Article{drones3030066,
AUTHOR = {Khoufi, Ines and Laouiti, Anis and Adjih, Cedric},
TITLE = {A Survey of Recent Extended Variants of the Traveling Salesman and Vehicle Routing Problems for Unmanned Aerial Vehicles},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/2504-446X/3/3/66},
ISSN = {2504-446X},
ABSTRACT = {The use of Unmanned Aerial Vehicles (UAVs) is rapidly growing in popularity. Initially introduced for military purposes, over the past few years, UAVs and related technologies have successfully transitioned to a whole new range of civilian applications such as delivery, logistics, surveillance, entertainment, and so forth. They have opened new possibilities such as allowing operation in otherwise difficult or hazardous areas, for instance. For all applications, one foremost concern is the selection of the paths and trajectories of UAVs, and at the same time, UAVs control comes with many challenges, as they have limited energy, limited load capacity and are vulnerable to difficult weather conditions. Generally, efficiently operating a drone can be mathematically formalized as a path optimization problem under some constraints. This shares some commonalities with similar problems that have been extensively studied in the context of urban vehicles and it is only natural that the recent literature has extended the latter to fit aerial vehicle constraints. The knowledge of such problems, their formulation, the resolution methods proposed—through the variants induced specifically by UAVs features—are of interest for practitioners for any UAV application. Hence, in this study, we propose a review of existing literature devoted to such UAV path optimization problems, focusing specifically on the sub-class of problems that consider the mobility on a macroscopic scale. These are related to the two existing general classic ones—the Traveling Salesman Problem and the Vehicle Routing Problem. We analyze the recent literature that adapted the problems to the UAV context, provide an extensive classification and taxonomy of their problems and their formulation and also give a synthetic overview of the resolution techniques, performance metrics and obtained numerical results.},
DOI = {10.3390/drones3030066}
}



@Article{rs11172008,
AUTHOR = {Yang, Qinchen and Liu, Man and Zhang, Zhitao and Yang, Shuqin and Ning, Jifeng and Han, Wenting},
TITLE = {Mapping Plastic Mulched Farmland for High Resolution Images of Unmanned Aerial Vehicle Using Deep Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2008},
URL = {https://www.mdpi.com/2072-4292/11/17/2008},
ISSN = {2072-4292},
ABSTRACT = {With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images.},
DOI = {10.3390/rs11172008}
}



@Article{rs11172011,
AUTHOR = {Wei, Lifei and Yu, Ming and Liang, Yajing and Yuan, Ziran and Huang, Can and Li, Rong and Yu, Yiwei},
TITLE = {Precise Crop Classification Using Spectral-Spatial-Location Fusion Based on Conditional Random Fields for UAV-Borne Hyperspectral Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2011},
URL = {https://www.mdpi.com/2072-4292/11/17/2011},
ISSN = {2072-4292},
ABSTRACT = {The precise classification of crop types is an important basis of agricultural monitoring and crop protection. With the rapid development of unmanned aerial vehicle (UAV) technology, UAV-borne hyperspectral remote sensing imagery with high spatial resolution has become the ideal data source for the precise classification of crops. For precise classification of crops with a wide variety of classes and varied spectra, the traditional spectral-based classification method has difficulty in mining large-scale spatial information and maintaining the detailed features of the classes. Therefore, a precise crop classification method using spectral-spatial-location fusion based on conditional random fields (SSLF-CRF) for UAV-borne hyperspectral remote sensing imagery is proposed in this paper. The proposed method integrates the spectral information, the spatial context, the spatial features, and the spatial location information in the conditional random field model by the probabilistic potentials, providing complementary information for the crop discrimination from different perspectives. The experimental results obtained with two UAV-borne high spatial resolution hyperspectral images confirm that the proposed method can solve the problems of large-scale spatial information modeling and spectral variability, improving the classification accuracy for each crop type. This method has important significance for the precise classification of crops in hyperspectral remote sensing imagery.},
DOI = {10.3390/rs11172011}
}



@Article{rs11172046,
AUTHOR = {Ghorbanzadeh, Omid and Meena, Sansar Raj and Blaschke, Thomas and Aryal, Jagannath},
TITLE = {UAV-Based Slope Failure Detection Using Deep-Learning Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2046},
URL = {https://www.mdpi.com/2072-4292/11/17/2046},
ISSN = {2072-4292},
ABSTRACT = {Slope failures occur when parts of a slope collapse abruptly under the influence of gravity, often triggered by a rainfall event or earthquake. The resulting slope failures often cause problems in mountainous or hilly regions, and the detection of slope failure is therefore an important topic for research. Most of the methods currently used for mapping and modelling slope failures rely on classification algorithms or feature extraction, but the spatial complexity of slope failures, the uncertainties inherent in expert knowledge, and problems in transferability, all combine to inhibit slope failure detection. In an attempt to overcome some of these problems we have analyzed the potential of deep learning convolutional neural networks (CNNs) for slope failure detection, in an area along a road section in the northern Himalayas, India. We used optical data from unmanned aerial vehicles (UAVs) over two separate study areas. Different CNN designs were used to produce eight different slope failure distribution maps, which were then compared with manually extracted slope failure polygons using different accuracy assessment metrics such as the precision, F-score, and mean intersection-over-union (mIOU). A slope failure inventory data set was produced for each of the study areas using a frequency-area distribution (FAD). The CNN approach that was found to perform best (precision accuracy assessment of almost 90% precision, F-score 85%, mIOU 74%) was one that used a window size of 64 &times; 64 pixels for the sample patches, and included slope data as an additional input layer. The additional information from the slope data helped to discriminate between slope failure areas and roads, which had similar spectral characteristics in the optical imagery. We concluded that the effectiveness of CNNs for slope failure detection was strongly dependent on their design (i.e., the window size selected for the sample patch, the data used, and the training strategies), but that CNNs are currently only designed by trial and error. While CNNs can be powerful tools, such trial and error strategies make it difficult to explain why a particular pooling or layer numbering works better than any other.},
DOI = {10.3390/rs11172046}
}



@Article{a12090183,
AUTHOR = {Li, Kexin and Wang, Jun and Qi, Dawei},
TITLE = {An Intelligent Warning Method for Diagnosing Underwater Structural Damage},
JOURNAL = {Algorithms},
VOLUME = {12},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {183},
URL = {https://www.mdpi.com/1999-4893/12/9/183},
ISSN = {1999-4893},
ABSTRACT = {A number of intelligent warning techniques have been implemented for detecting underwater infrastructure diagnosis to partially replace human-conducted on-site inspections. However, the extensively varying real-world situation (e.g., the adverse environmental conditions, the limited sample space, and the complex defect types) can lead to challenges to the wide adoption of intelligent warning techniques. To overcome these challenges, this paper proposed an intelligent algorithm combing gray level co-occurrence matrix (GLCM) with self-organization map (SOM) for accurate diagnosis of the underwater structural damage. In order to optimize the generative criterion for GLCM construction, a triangle algorithm was proposed based on orthogonal experiments. The constructed GLCM were utilized to evaluate the texture features of the regions of interest (ROI) of micro-injury images of underwater structures and extracted damage image texture characteristic parameters. The digital feature screening (DFS) method was used to obtain the most relevant features as the input for the SOM network. According to the unique topology information of the SOM network, the classification result, recognition efficiency, parameters, such as the network layer number, hidden layer node, and learning step, were optimized. The robustness and adaptability of the proposed approach were tested on underwater structure images through the DFS method. The results showed that the proposed method revealed quite better performances and can diagnose structure damage in underwater realistic situations.},
DOI = {10.3390/a12090183}
}



@Article{s19173752,
AUTHOR = {González-deSantos, L. M. and Martínez-Sánchez, J. and González-Jorge, H. and Ribeiro, M. and de Sousa, J. B. and Arias, P.},
TITLE = {Payload for Contact Inspection Tasks with UAV Systems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3752},
URL = {https://www.mdpi.com/1424-8220/19/17/3752},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a payload designed to perform semi-autonomous contact inspection tasks without any type of positioning system external to the UAV, such as a global navigation satellite system (GNSS) or motion capture system, making possible inspection in challenging GNSS- denied sites. This payload includes two LiDAR sensors which measure the distance between the UAV and the target structure and their inner orientation angle. The system uses this information to control the approaching of the UAV to the structure and the contact between both, actuating over the pitch and yaw signals. This control is performed using a hybrid automaton with different states that represent all the possible UAV status during the inspection tasks. It uses different control strategies in each state. An ultrasonic gauge has been used as the inspection sensor of the payload to measure the thickness of a metallic sheet. The sensor requires a stable contact in order to collect reliable measurements. Several tests have been performed on the system, reaching accurate results which show it is able to maintain a stable contact with the target structure.},
DOI = {10.3390/s19173752}
}



@Article{s19173754,
AUTHOR = {Stodola, Petr and Drozd, Jan and Mazal, Jan and Hodický, Jan and Procházka, Dalibor},
TITLE = {Cooperative Unmanned Aerial System Reconnaissance in a Complex Urban Environment and Uneven Terrain},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3754},
URL = {https://www.mdpi.com/1424-8220/19/17/3754},
ISSN = {1424-8220},
ABSTRACT = {Using unmanned robotic systems in military operations such as reconnaissance or surveillance, as well as in many civil applications, is common practice. In this article, the problem of monitoring the specified area of interest by a fleet of unmanned aerial systems is examined. The monitoring is planned via the Cooperative Aerial Model, which deploys a number of waypoints in the area; these waypoints are visited successively by unmanned systems. The original model proposed in the past assumed that the area to be explored is perfectly flat. A new formulation of this model is introduced in this article so that the model can be used in a complex environment with uneven terrain and/or with many obstacles, which may occlude some parts of the area of interest. The optimization algorithm based on the simulated annealing principles is proposed for positioning of waypoints to cover as large an area as possible. A set of scenarios has been designed to verify and evaluate the proposed approach. The key experiments are aimed at finding the minimum number of waypoints needed to explore at least the minimum requested portion of the area. Furthermore, the results are compared to the algorithm based on the lawnmower pattern.},
DOI = {10.3390/s19173754}
}



@Article{rs11172066,
AUTHOR = {Tilly, Nora and Bareth, Georg},
TITLE = {Estimating Nitrogen from Structural Crop Traits at Field Scale—A Novel Approach Versus Spectral Vegetation Indices},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2066},
URL = {https://www.mdpi.com/2072-4292/11/17/2066},
ISSN = {2072-4292},
ABSTRACT = {A sufficient nitrogen (N) supply is mandatory for healthy crop growth, but negative consequences of N losses into the environment are known. Hence, deeply understanding and monitoring crop growth for an optimized N management is advisable. In this context, remote sensing facilitates the capturing of crop traits. While several studies on estimating biomass from spectral and structural data can be found, N is so far only estimated from spectral features. It is well known that N is negatively related to dry biomass, which, in turn, can be estimated from crop height. Based on this indirect link, the present study aims at estimating N concentration at field scale in a two-step model: first, using crop height to estimate biomass, and second, using the modeled biomass to estimate N concentration. For comparison, N concentration was estimated from spectral data. The data was captured on a spring barley field experiment in two growing seasons. Crop surface height was measured with a terrestrial laser scanner, seven vegetation indices were calculated from field spectrometer measurements, and dry biomass and N concentration were destructively sampled. In the validation, better results were obtained with the models based on structural data (R2 &lt; 0.85) than on spectral data (R2 &lt; 0.70). A brief look at the N concentration of different plant organs showed stronger dependencies on structural data (R2: 0.40&ndash;0.81) than on spectral data (R2: 0.18&ndash;0.68). Overall, this first study shows the potential of crop-specific across‑season two-step models based on structural data for estimating crop N concentration at field scale. The validity of the models for in-season estimations requires further research.},
DOI = {10.3390/rs11172066}
}



@Article{rs11182082,
AUTHOR = {Marshall, Michael and Crommelinck, Sophie and Kohli, Divyani and Perger, Christoph and Yang, Michael Ying and Ghosh, Aniruddha and Fritz, Steffen and Bie, Kees de and Nelson, Andy},
TITLE = {Crowd-Driven and Automated Mapping of Field Boundaries in Highly Fragmented Agricultural Landscapes of Ethiopia with Very High Spatial Resolution Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2082},
URL = {https://www.mdpi.com/2072-4292/11/18/2082},
ISSN = {2072-4292},
ABSTRACT = {Mapping the extent and location of field boundaries is critical to food security analysis but remains problematic in the Global South where such information is needed the most. The difficulty is due primarily to fragmentation in the landscape, small farm sizes, and irregular farm boundaries. Very high-resolution satellite imagery affords an opportunity to delineate such fields, but the challenge remains of determining such boundaries in a systematic and accurate way. In this paper, we compare a new crowd-driven manual digitization tool (Crop Land Extent) with two semi-automated methods (contour detection and multi-resolution segmentation) to determine farm boundaries from WorldView imagery in highly fragmented agricultural landscapes of Ethiopia. More than 7000 one square-kilometer image tiles were used for the analysis. The three methods were assessed using quantitative completeness and spatial correctness. Contour detection tended to under-segment when compared to manual digitization, resulting in better performance for larger (approaching 1 ha) sized fields. Multi-resolution segmentation on the other hand, tended to over-segment, resulting in better performance for small fields. Neither semi-automated method in their current realizations however are suitable for field boundary mapping in highly fragmented landscapes. Crowd-driven manual digitization is promising, but requires more oversight, quality control, and training than the current workflow could allow.},
DOI = {10.3390/rs11182082}
}



@Article{s19183859,
AUTHOR = {Zhao, Xin and Yuan, Yitong and Song, Mengdie and Ding, Yang and Lin, Fenfang and Liang, Dong and Zhang, Dongyan},
TITLE = {Use of Unmanned Aerial Vehicle Imagery and Deep Learning UNet to Extract Rice Lodging},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3859},
URL = {https://www.mdpi.com/1424-8220/19/18/3859},
ISSN = {1424-8220},
ABSTRACT = {Rice lodging severely affects harvest yield. Traditional evaluation methods and manual on-site measurement are found to be time-consuming, labor-intensive, and cost-intensive. In this study, a new method for rice lodging assessment based on a deep learning UNet (U-shaped Network) architecture was proposed. The UAV (unmanned aerial vehicle) equipped with a high-resolution digital camera and a three-band multispectral camera synchronously was used to collect lodged and non-lodged rice images at an altitude of 100 m. After splicing and cropping the original images, the datasets with the lodged and non-lodged rice image samples were established by augmenting for building a UNet model. The research results showed that the dice coefficients in RGB (Red, Green and Blue) image and multispectral image test set were 0.9442 and 0.9284, respectively. The rice lodging recognition effect using the RGB images without feature extraction is better than that of multispectral images. The findings of this study are useful for rice lodging investigations by different optical sensors, which can provide an important method for large-area, high-efficiency, and low-cost rice lodging monitoring research.},
DOI = {10.3390/s19183859}
}



@Article{sym11091139,
AUTHOR = {Hu, Bo and Li, Jiaxi and Yang, Jie and Bai, Haitao and Li, Shuang and Sun, Youchang and Yang, Xiaoyu},
TITLE = {Reinforcement Learning Approach to Design Practical Adaptive Control for a Small-Scale Intelligent Vehicle},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1139},
URL = {https://www.mdpi.com/2073-8994/11/9/1139},
ISSN = {2073-8994},
ABSTRACT = {Reinforcement learning (RL) based techniques have been employed for the tracking and adaptive cruise control of a small-scale vehicle with the aim to transfer the obtained knowledge to a full-scale intelligent vehicle in the near future. Unlike most other control techniques, the purpose of this study is to seek a practical method that enables the vehicle, in the real environment and in real time, to learn the control behavior on its own while adapting to the changing circumstances. In this context, it is necessary to design an algorithm that symmetrically considers both time efficiency and accuracy. Meanwhile, in order to realize adaptive cruise control specifically, a set of symmetrical control actions consisting of steering angle and vehicle speed needs to be optimized simultaneously. In this paper, firstly, the experimental setup of the small-scale intelligent vehicle is introduced. Subsequently, three model-free RL algorithm are conducted to develop and finally form the strategy to keep the vehicle within its lanes at constant and top velocity. Furthermore, a model-based RL strategy is compared that incorporates learning from real experience and planning from simulated experience. Finally, a Q-learning based adaptive cruise control strategy is intermixed to the existing tracking control architecture to allow the vehicle slow-down in the curve and accelerate on straightaways. The experimental results show that the Q-learning and Sarsa (&lambda;) algorithms can achieve a better tracking behavior than the conventional Sarsa, and Q-learning outperform Sarsa (&lambda;) in terms of computational complexity. The Dyna-Q method performs similarly with the Sarsa (&lambda;) algorithms, but with a significant reduction of computational time. Compared with a fine-tuned proportion integration differentiation (PID) controller, the good-balanced Q-learning is seen to perform better and it can also be easily applied to control problems with over one control actions.},
DOI = {10.3390/sym11091139}
}



@Article{app9183789,
AUTHOR = {Moon, Jiyoun and Lee, Beom-Hee},
TITLE = {PDDL Planning with Natural Language-Based Scene Understanding for UAV-UGV Cooperation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2076-3417/9/18/3789},
ISSN = {2076-3417},
ABSTRACT = {Natural-language-based scene understanding can enable heterogeneous robots to cooperate efficiently in large and unconstructed environments. However, studies on symbolic planning rarely consider the semantic knowledge acquisition problem associated with the surrounding environments. Further, recent developments in deep learning methods show outstanding performance for semantic scene understanding using natural language. In this paper, a cooperation framework that connects deep learning techniques and a symbolic planner for heterogeneous robots is proposed. The framework is largely composed of the scene understanding engine, planning agent, and knowledge engine. We employ neural networks for natural-language-based scene understanding to share environmental information among robots. We then generate a sequence of actions for each robot using a planning domain definition language planner. JENA-TDB is used for knowledge acquisition storage. The proposed method is validated using simulation results obtained from one unmanned aerial and three ground vehicles.},
DOI = {10.3390/app9183789}
}



@Article{rs11182109,
AUTHOR = {Wang, Xiao-Hu and Zhang, Yi-Zhuo and Xu, Miao-Miao},
TITLE = {A Multi-Threshold Segmentation for Tree-Level Parameter Extraction in a Deciduous Forest Using Small-Footprint Airborne LiDAR Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2109},
URL = {https://www.mdpi.com/2072-4292/11/18/2109},
ISSN = {2072-4292},
ABSTRACT = {The development of new approaches to tree-level parameter extraction for forest resource inventory and management is an important area of ongoing research, which puts forward high requirements for the capabilities of single-tree segmentation and detection methods. Conventional methods implement segmenting routine with same resolution threshold for overstory and understory, ignoring that their lidar point densities are different, which leads to over-segmentation of the understory trees. To improve the segmentation accuracy of understory trees, this paper presents a multi-threshold segmentation approach for tree-level parameter extraction using small-footprint airborne LiDAR (Light Detection And Ranging) data. First, the point clouds are pre-processed and encoded to canopy layers according to the lidar return number, and multi-threshold segmentation using DSM-based (Digital Surface Model) method is implemented for each layer; tree segments are then combined across layers by merging criteria. Finally, individual trees are delineated, and tree parameters are extracted. The novelty of this method lies in its application of multi-resolution threshold segmentation strategy according to the variation of LiDAR point density in different canopy layers. We applied this approach to 271 permanent sample plots of the University of Kentucky&rsquo;s Robinson Forest, a deciduous canopy-closed forest with complex terrain and vegetation conditions. Experimental results show that a combination of multi-resolution threshold segmentation based on stratification and cross-layer tree segments merging method can provide a significant performance improvement in individual tree-level forest measurement. Compared with DSM-based method, the proposed multi-threshold segmentation approach strongly improved the average detection rate (from 52.3% to 73.4%) and average overall accuracy (from 65.2% to 76.9%) for understory trees. The overall accuracy increased from 75.1% to 82.6% for all trees, with an increase of the coefficient of determination R2 by 20 percentage points. The improvement of tree detection method brings the estimation of structural parameters for single trees up to an accuracy level: For tree height, R2 increased by 5.0 percentage points from 90% to 95%; and for tree location, the mean difference decreased by 23 cm from 105 cm to 82 cm.},
DOI = {10.3390/rs11182109}
}



@Article{s19183917,
AUTHOR = {Fan, Shurui and Li, Zirui and Xia, Kewen and Hao, Dongxia},
TITLE = {Quantitative and Qualitative Analysis of Multicomponent Gas Using Sensor Array},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3917},
URL = {https://www.mdpi.com/1424-8220/19/18/3917},
ISSN = {1424-8220},
ABSTRACT = {The gas sensor array has long been a major tool for measuring gas due to its high sensitivity, quick response, and low power consumption. This goal, however, faces a difficult challenge because of the cross-sensitivity of the gas sensor. This paper presents a novel gas mixture analysis method for gas sensor array applications. The features extracted from the raw data utilizing principal component analysis (PCA) were used to complete random forest (RF) modeling, which enabled qualitative identification. Support vector regression (SVR), optimized by the particle swarm optimization (PSO) algorithm, was used to select hyperparameters C and &gamma; to establish the optimal regression model for the purpose of quantitative analysis. Utilizing the dataset, we evaluated the effectiveness of our approach. Compared with logistic regression (LR) and support vector machine (SVM), the average recognition rate of PCA combined with RF was the highest (97%). The fitting effect of SVR optimized by PSO for gas concentration was better than that of SVR and solved the problem of hyperparameters selection.},
DOI = {10.3390/s19183917}
}



@Article{s19183935,
AUTHOR = {Liu, Xiaolei and Liu, Liansheng and Wang, Lulu and Guo, Qing and Peng, Xiyuan},
TITLE = {Performance Sensing Data Prediction for an Aircraft Auxiliary Power Unit Using the Optimized Extreme Learning Machine},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3935},
URL = {https://www.mdpi.com/1424-8220/19/18/3935},
ISSN = {1424-8220},
ABSTRACT = {The aircraft auxiliary power unit (APU) is responsible for environmental control in the cabin and the main engines starting the aircraft. The prediction of its performance sensing data is significant for condition-based maintenance. As a complex system, its performance sensing data have a typically nonlinear feature. In order to monitor this process, a model with strong nonlinear fitting ability needs to be formulated. A neural network has advantages of solving a nonlinear problem. Compared with the traditional back propagation neural network algorithm, an extreme learning machine (ELM) has features of a faster learning speed and better generalization performance. To enhance the training of the neural network with a back propagation algorithm, an ELM is employed to predict the performance sensing data of the APU in this study. However, the randomly generated weights and thresholds of the ELM often may result in unstable prediction results. To address this problem, a restricted Boltzmann machine (RBM) is utilized to optimize the ELM. In this way, a stable performance parameter prediction model of the APU can be obtained and better performance parameter prediction results can be achieved. The proposed method is evaluated by the real APU sensing data of China Southern Airlines Company Limited Shenyang Maintenance Base. Experimental results show that the optimized ELM with an RBM is more stable and can obtain more accurate prediction results.},
DOI = {10.3390/s19183935}
}



@Article{ijgi8090409,
AUTHOR = {Tan, Yumin and Li, Yunxin},
TITLE = {UAV Photogrammetry-Based 3D Road Distress Detection},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {409},
URL = {https://www.mdpi.com/2220-9964/8/9/409},
ISSN = {2220-9964},
ABSTRACT = {The timely and proper rehabilitation of damaged roads is essential for road maintenance, and an effective method to detect road surface distress with high efficiency and low cost is urgently needed. Meanwhile, unmanned aerial vehicles (UAVs), with the advantages of high flexibility, low cost, and easy maneuverability, are a new fascinating choice for road condition monitoring. In this paper, road images from UAV oblique photogrammetry are used to reconstruct road three-dimensional (3D) models, from which road pavement distress is automatically detected and the corresponding dimensions are extracted using the developed algorithm. Compared with a field survey, the detection result presents a high precision with an error of around 1 cm in the height dimension for most cases, demonstrating the potential of the proposed method for future engineering practice.},
DOI = {10.3390/ijgi8090409}
}



@Article{s19183958,
AUTHOR = {Han, Seongkyun and Yoo, Jisang and Kwon, Soonchul},
TITLE = {Real-Time Vehicle-Detection Method in Bird-View Unmanned-Aerial-Vehicle Imagery},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3958},
URL = {https://www.mdpi.com/1424-8220/19/18/3958},
ISSN = {1424-8220},
ABSTRACT = {Vehicle detection is an important research area that provides background information for the diversity of unmanned-aerial-vehicle (UAV) applications. In this paper, we propose a vehicle-detection method using a convolutional-neural-network (CNN)-based object detector. We design our method, DRFBNet300, with a Deeper Receptive Field Block (DRFB) module that enhances the expressiveness of feature maps to detect small objects in the UAV imagery. We also propose the UAV-cars dataset that includes the composition and angular distortion of vehicles in UAV imagery to train our DRFBNet300. Lastly, we propose a Split Image Processing (SIP) method to improve the accuracy of the detection model. Our DRFBNet300 achieves 21 mAP with 45 FPS in the MS COCO metric, which is the highest score compared to other lightweight single-stage methods running in real time. In addition, DRFBNet300, trained on the UAV-cars dataset, obtains the highest AP score at altitudes of 20&ndash;50 m. The gap of accuracy improvement by applying the SIP method became larger when the altitude increases. The DRFBNet300 trained on the UAV-cars dataset with SIP method operates at 33 FPS, enabling real-time vehicle detection.},
DOI = {10.3390/s19183958}
}



@Article{rs11182144,
AUTHOR = {Fraga-Lamas, Paula and Ramos, Lucía and Mondéjar-Guerra, Víctor and Fernández-Caramés, Tiago M.},
TITLE = {A Review on IoT Deep Learning UAV Systems for Autonomous Obstacle Detection and Collision Avoidance},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2144},
URL = {https://www.mdpi.com/2072-4292/11/18/2144},
ISSN = {2072-4292},
ABSTRACT = {Advances in Unmanned Aerial Vehicles (UAVs), also known as drones, offer unprecedented opportunities to boost a wide array of large-scale Internet of Things (IoT) applications. Nevertheless, UAV platforms still face important limitations mainly related to autonomy and weight that impact their remote sensing capabilities when capturing and processing the data required for developing autonomous and robust real-time obstacle detection and avoidance systems. In this regard, Deep Learning (DL) techniques have arisen as a promising alternative for improving real-time obstacle detection and collision avoidance for highly autonomous UAVs. This article reviews the most recent developments on DL Unmanned Aerial Systems (UASs) and provides a detailed explanation on the main DL techniques. Moreover, the latest DL-UAV communication architectures are studied and their most common hardware is analyzed. Furthermore, this article enumerates the most relevant open challenges for current DL-UAV solutions, thus allowing future researchers to define a roadmap for devising the new generation affordable autonomous DL-UAV IoT solutions.},
DOI = {10.3390/rs11182144}
}



@Article{rs11182155,
AUTHOR = {Wang, Jie and Simeonova, Sandra and Shahbazi, Mozhdeh},
TITLE = {Orientation- and Scale-Invariant Multi-Vehicle Detection and Tracking from Unmanned Aerial Videos},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2155},
URL = {https://www.mdpi.com/2072-4292/11/18/2155},
ISSN = {2072-4292},
ABSTRACT = {Along with the advancement of light-weight sensing and processing technologies, unmanned aerial vehicles (UAVs) have recently become popular platforms for intelligent traffic monitoring and control. UAV-mounted cameras can capture traffic-flow videos from various perspectives providing a comprehensive insight into road conditions. To analyze the traffic flow from remotely captured videos, a reliable and accurate vehicle detection-and-tracking approach is required. In this paper, we propose a deep-learning framework for vehicle detection and tracking from UAV videos for monitoring traffic flow in complex road structures. This approach is designed to be invariant to significant orientation and scale variations in the videos. The detection procedure is performed by fine-tuning a state-of-the-art object detector, You Only Look Once (YOLOv3), using several custom-labeled traffic datasets. Vehicle tracking is conducted following a tracking-by-detection paradigm, where deep appearance features are used for vehicle re-identification, and Kalman filtering is used for motion estimation. The proposed methodology is tested on a variety of real videos collected by UAVs under various conditions, e.g., in late afternoons with long vehicle shadows, in dawn with vehicles lights being on, over roundabouts and interchange roads where vehicle directions change considerably, and from various viewpoints where vehicles&rsquo; appearance undergo substantial perspective distortions. The proposed tracking-by-detection approach performs efficiently at 11 frames per second on color videos of 2720p resolution. Experiments demonstrated that high detection accuracy could be achieved with an average F1-score of 92.1%. Besides, the tracking technique performs accurately, with an average multiple-object tracking accuracy (MOTA) of 81.3%. The proposed approach also addressed the shortcomings of the state-of-the-art in multi-object tracking regarding frequent identity switching, resulting in a total of only one identity switch over every 305 tracked vehicles.},
DOI = {10.3390/rs11182155}
}



@Article{rs11182156,
AUTHOR = {Wang, Dezhi and Wan, Bo and Qiu, Penghua and Zuo, Zejun and Wang, Run and Wu, Xincai},
TITLE = {Mapping Height and Aboveground Biomass of Mangrove Forests on Hainan Island Using UAV-LiDAR Sampling},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2156},
URL = {https://www.mdpi.com/2072-4292/11/18/2156},
ISSN = {2072-4292},
ABSTRACT = {Hainan Island is the second-largest island in China and has the most species-diverse mangrove forests in the country. To date, the height and aboveground ground biomass (AGB) of the mangrove forests on Hainan Island are unknown, partly as a result of the challenges faced during extensive field sampling in mangrove habitats (intertidal mudflats inundated by periodic seawater). Therefore, this study used a low-cost UAV-LiDAR (light detection and ranging sensor mounted on an unmanned aerial vehicle) system as a sampling tool and Sentinel-2 imagery as auxiliary data to estimate and map the mangrove height and AGB on Hainan Island. Hainan Island has 3697.02 hectares of mangrove forests with an average patch area of approximately 1 ha. The results show that the mangroves on whole Hainan Island have an average height of 6.99 m, a total AGB of 474,199.31 Mg and an AGB density of 128.27 Mg ha&minus;1. The AGB hot spots are located in Qinglan Harbor and the south of Dongzhai Harbor. The proposed height model LiDAR-S2 performed well with an R2 of 0.67 and an RMSE (root mean square error) of 1.90 m; the proposed AGB model G~LiDAR~S2 performed better (an R2 of 0.62 and an RMSE of 50.36 Mg ha&minus;1) than the traditional AGB model G~S2 that directly related ground plots and Sentinel-2 data. The results also indicate that the LiDAR metrics describing the canopy&rsquo;s thickness and its top and bottom characteristics are the most important variables for mangrove AGB estimation. For the Sentinel-2 indices, the red-edge and shortwave infrared features, especially the red-edge 1 and shortwave infrared Band 11 features, play the most important roles in estimating mangrove AGB and height. In conclusion, this paper presents the first mangrove height and AGB maps of Hainan Island and demonstrates the feasibility of using UAV-LiDAR as a sampling tool for mangrove forests.},
DOI = {10.3390/rs11182156}
}



@Article{en12183569,
AUTHOR = {Mpfumali, Phathutshedzo and Sigauke, Caston and Bere, Alphonce and Mulaudzi, Sophie},
TITLE = {Day Ahead Hourly Global Horizontal Irradiance Forecasting—Application to South African Data},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3569},
URL = {https://www.mdpi.com/1996-1073/12/18/3569},
ISSN = {1996-1073},
ABSTRACT = {Due to its variability, solar power generation poses challenges to grid energy management. In order to ensure an economic operation of a national grid, including its stability, it is important to have accurate forecasts of solar power. The current paper discusses probabilistic forecasting of twenty-four hours ahead of global horizontal irradiance (GHI) using data from the Tellerie radiometric station in South Africa for the period August 2009 to April 2010. Variables are selected using a least absolute shrinkage and selection operator (Lasso) via hierarchical interactions and the parameters of the developed models are estimated using the Barrodale and Roberts&rsquo;s algorithm. Two forecast combination methods are used in this study. The first is a convex forecast combination algorithm where the average loss suffered by the models is based on the pinball loss function. A second forecast combination method, which is quantile regression averaging (QRA), is also used. The best set of forecasts is selected based on the prediction interval coverage probability (PICP), prediction interval normalised average width (PINAW) and prediction interval normalised average deviation (PINAD). The results demonstrate that QRA gives more robust prediction intervals than the other models. A comparative analysis is done with two machine learning methods&mdash;stochastic gradient boosting and support vector regression&mdash;which are used as benchmark models. Empirical results show that the QRA model yields the most accurate forecasts compared to the machine learning methods based on the probabilistic error measures. Results on combining prediction interval limits show that the PMis the best prediction limits combination method as it gives a hit rate of 0.955 which is very close to the target of 0.95. This modelling approach is expected to help in optimising the integration of solar power in the national grid.},
DOI = {10.3390/en12183569}
}



@Article{f10090815,
AUTHOR = {Zou, Xiaodan and Liang, Anjie and Wu, Bizhi and Su, Jun and Zheng, Renhua and Li, Jian},
TITLE = {UAV-Based High-Throughput Approach for Fast Growing Cunninghamia lanceolata (Lamb.) Cultivar Screening by Machine Learning},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {815},
URL = {https://www.mdpi.com/1999-4907/10/9/815},
ISSN = {1999-4907},
ABSTRACT = {Obtaining accurate measurements of tree height and diameter at breast height (DBH) in forests to evaluate the growth rate of cultivars is still a significant challenge, even when using light detection and ranging (LiDAR) and three-dimensional (3-D) modeling. As an alternative, we provide a novel high-throughput strategy for predicting the biomass of forests in the field by vegetation indices. This study proposes an integrated pipeline methodology to measure the biomass of different tree cultivars in plantation forests with high crown density, which combines unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Using a planation of Cunninghamia lanceolate, which is commonly known as Chinese fir, in Fujian, China, images were collected while using a hyperspectral camera. Vegetation indices and modeling were processed in Python using decision trees, random forests, support vector machine, and eXtreme Gradient Boosting (XGBoost) third-party libraries. The tree height and DBH of 2880 samples were manually measured and clustered into three groups&mdash;&ldquo;Fast&rdquo;, &ldquo;median&rdquo;, and &ldquo;normal&rdquo; growth groups&mdash;and 19 vegetation indices from 12,000 pixels were abstracted as the input of features for the modeling. After modeling and cross-validation, the classifier that was generated by random forests had the best prediction accuracy when compared to other algorithms (75%). This framework can be applied to other tree species to make management and business decisions.},
DOI = {10.3390/f10090815}
}



@Article{e21090912,
AUTHOR = {Mei, Wenjuan and Liu, Zhen and Su, Yuanzhang and Du, Li and Huang, Jianguo},
TITLE = {Evolved-Cooperative Correntropy-Based Extreme Learning Machine for Robust Prediction},
JOURNAL = {Entropy},
VOLUME = {21},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {912},
URL = {https://www.mdpi.com/1099-4300/21/9/912},
ISSN = {1099-4300},
ABSTRACT = {In recent years, the correntropy instead of the mean squared error has been widely taken as a powerful tool for enhancing the robustness against noise and outliers by forming the local similarity measurements. However, most correntropy-based models either have too simple descriptions of the correntropy or require too many parameters to adjust in advance, which is likely to cause poor performance since the correntropy fails to reflect the probability distributions of the signals. Therefore, in this paper, a novel correntropy-based extreme learning machine (ELM) called ECC-ELM has been proposed to provide a more robust training strategy based on the newly developed multi-kernel correntropy with the parameters that are generated using cooperative evolution. To achieve an accurate description of the correntropy, the method adopts a cooperative evolution which optimizes the bandwidths by switching delayed particle swarm optimization (SDPSO) and generates the corresponding influence coefficients that minimizes the minimum integrated error (MIE) to adaptively provide the best solution. The simulated experiments and real-world applications show that cooperative evolution can achieve the optimal solution which provides an accurate description on the probability distribution of the current error in the model. Therefore, the multi-kernel correntropy that is built with the optimal solution results in more robustness against the noise and outliers when training the model, which increases the accuracy of the predictions compared with other methods.},
DOI = {10.3390/e21090912}
}



@Article{s19194083,
AUTHOR = {Zhang, Xinxiang and Zeinali, Yasha and Story, Brett A. and Rajan, Dinesh},
TITLE = {Measurement of Three-Dimensional Structural Displacement Using a Hybrid Inertial Vision-Based System},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4083},
URL = {https://www.mdpi.com/1424-8220/19/19/4083},
ISSN = {1424-8220},
ABSTRACT = {Accurate three-dimensional displacement measurements of bridges and other structures have received significant attention in recent years. The main challenges of such measurements include the cost and the need for a scalable array of instrumentation. This paper presents a novel Hybrid Inertial Vision-Based Displacement Measurement (HIVBDM) system that can measure three-dimensional structural displacements by using a monocular charge-coupled device (CCD) camera, a stationary calibration target, and an attached tilt sensor. The HIVBDM system does not require the camera to be stationary during the measurements, while the camera movements, i.e., rotations and translations, during the measurement process are compensated by using a stationary calibration target in the field of view (FOV) of the camera. An attached tilt sensor is further used to refine the camera movement compensation, and better infers the global three-dimensional structural displacements. This HIVBDM system is evaluated on both short-term and long-term synthetic static structural displacements, which are conducted in an indoor simulated experimental environment. In the experiments, at a 9.75 m operating distance between the monitoring camera and the structure that is being monitored, the proposed HIVBDM system achieves an average of 1.440 mm Root Mean Square Error (RMSE) on the in-plane structural translations and an average of 2.904 mm RMSE on the out-of-plane structural translations.},
DOI = {10.3390/s19194083}
}



@Article{s19194091,
AUTHOR = {Guo, Qiwei and Chen, Yayong and Tang, Yu and Zhuang, Jiajun and He, Yong and Hou, Chaojun and Chu, Xuan and Zhong, Zhenyu and Luo, Shaoming},
TITLE = {Lychee Fruit Detection Based on Monocular Machine Vision in Orchard Environment},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4091},
URL = {https://www.mdpi.com/1424-8220/19/19/4091},
ISSN = {1424-8220},
ABSTRACT = {Due to the change of illumination environment and overlapping conditions caused by the neighboring fruits and other background objects, the simple application of the traditional machine vision method limits the detection accuracy of lychee fruits in natural orchard environments. Therefore, this research presented a detection method based on monocular machine vision to detect lychee fruits growing in overlapped conditions. Specifically, a combination of contrast limited adaptive histogram equalization (CLAHE), red/blue chromatic mapping, Otsu thresholding and morphology operations were adopted to segment the foreground regions of the lychees. A stepwise method was proposed for extracting individual lychee fruit from the lychee foreground region. The first step in this process was based on the relative position relation of the Hough circle and an equivalent area circle (equal to the area of the potential lychee foreground region) and was designed to distinguish lychee fruits growing in isolated or overlapped states. Then, a process based on the three-point definite circle theorem was performed to extract individual lychee fruits from the foreground regions of overlapped lychee fruit clusters. Finally, to enhance the robustness of the detection method, a local binary pattern support vector machine (LBP-SVM) was adopted to filter out the false positive detections generated by background chaff interferences. The performance of the presented method was evaluated using 485 images captured in a natural lychee orchard in Conghua (Area), Guangzhou. The detection results showed that the recall rate was 86.66%, the precision rate was greater than 87% and the F1-score was 87.07%.},
DOI = {10.3390/s19194091}
}



@Article{rs11192211,
AUTHOR = {Petliak, Helen and Cerovski-Darriau, Corina and Zaliva, Vadim and Stock, Jonathan},
TITLE = {Where’s the Rock: Using Convolutional Neural Networks to Improve Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2211},
URL = {https://www.mdpi.com/2072-4292/11/19/2211},
ISSN = {2072-4292},
ABSTRACT = {While machine learning techniques have been increasingly applied to land cover classification problems, these techniques have not focused on separating exposed bare rock from soil covered areas. Therefore, we built a convolutional neural network (CNN) to differentiate exposed bare rock (rock) from soil cover (other). We made a training dataset by mapping exposed rock at eight test sites across the Sierra Nevada Mountains (California, USA) using USDA’s 0.6 m National Aerial Inventory Program (NAIP) orthoimagery. These areas were then used to train and test the CNN. The resulting machine learning approach classifies bare rock in NAIP orthoimagery with a 0.95     F 1     score. Comparatively, the classical OBIA approach gives only a 0.84     F 1     score. This is an improvement over existing land cover maps, which underestimate rock by almost 90%. The resulting CNN approach is likely scalable but dependent on high-quality imagery and high-performance algorithms using representative training sets informed by expert mapping. As image quality and quantity continue to increase globally, machine learning models that incorporate high-quality training data informed by geologic, topographic, or other topical maps may be applied to more effectively identify exposed rock in large image collections.},
DOI = {10.3390/rs11192211}
}



@Article{robotics8040082,
AUTHOR = {Abouheaf, Mohammed and Gueaieb, Wail and Spinello, Davide},
TITLE = {Online Multi-Objective Model-Independent Adaptive Tracking Mechanism for Dynamical Systems},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {82},
URL = {https://www.mdpi.com/2218-6581/8/4/82},
ISSN = {2218-6581},
ABSTRACT = {The optimal tracking problem is addressed in the robotics literature by using a variety of robust and adaptive control approaches. However, these schemes are associated with implementation limitations such as applicability in uncertain dynamical environments with complete or partial model-based control structures, complexity and integrity in discrete-time environments, and scalability in complex coupled dynamical systems. An online adaptive learning mechanism is developed to tackle the above limitations and provide a generalized solution platform for a class of tracking control problems. This scheme minimizes the tracking errors and optimizes the overall dynamical behavior using simultaneous linear feedback control strategies. Reinforcement learning approaches based on value iteration processes are adopted to solve the underlying Bellman optimality equations. The resulting control strategies are updated in real time in an interactive manner without requiring any information about the dynamics of the underlying systems. Means of adaptive critics are employed to approximate the optimal solving value functions and the associated control strategies in real time. The proposed adaptive tracking mechanism is illustrated in simulation to control a flexible wing aircraft under uncertain aerodynamic learning environment.},
DOI = {10.3390/robotics8040082}
}



@Article{electronics8101079,
AUTHOR = {Phuc, Le Tran Huu and Jeon, HyeJun and Truong, Nguyen Tam Nguyen and Hak, Jung Jae},
TITLE = {Applying the Haar-cascade Algorithm for Detecting Safety Equipment in Safety Management Systems for Multiple Working Environments},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1079},
URL = {https://www.mdpi.com/2079-9292/8/10/1079},
ISSN = {2079-9292},
ABSTRACT = {There are many ways to maintain the safety of workers on a working site, such as using a human supervisor, computer supervisor, and smoke&ndash;flame detecting system. In order to create a safety warning system for the working site, the machine-learning algorithm&mdash;Haar-cascade classifier&mdash;was used to build four different classes for safety equipment recognition. Then a proposed algorithm was applied to calculate a score to determine the dangerousness of the current working environment based on the safety equipment and working environment. With this data, the system decides whether it is necessary to give a warning signal. For checking the efficiency of this project, three different situations were installed with this system. Generally, with the promising outcome, this application can be used in maintaining, supervising, and controlling the safety of a worker.},
DOI = {10.3390/electronics8101079}
}



@Article{s19194115,
AUTHOR = {Li, Yuxia and Peng, Bo and He, Lei and Fan, Kunlong and Li, Zhenxu and Tong, Ling},
TITLE = {Road Extraction from Unmanned Aerial Vehicle Remote Sensing Images Based on Improved Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4115},
URL = {https://www.mdpi.com/1424-8220/19/19/4115},
ISSN = {1424-8220},
ABSTRACT = {Roads are vital components of infrastructure, the extraction of which has become a topic of significant interest in the field of remote sensing. Because deep learning has been a popular method in image processing and information extraction, researchers have paid more attention to extracting road using neural networks. This article proposes the improvement of neural networks to extract roads from Unmanned Aerial Vehicle (UAV) remote sensing images. D-Linknet was first considered for its high performance; however, the huge scale of the net reduced computational efficiency. With a focus on the low computational efficiency problem of the popular D-LinkNet, this article made some improvements: (1) Replace the initial block with a stem block. (2) Rebuild the entire network based on ResNet units with a new structure, allowing for the construction of an improved neural network D-Linknetplus. (3) Add a 1 &times; 1 convolution layer before DBlock to reduce the input feature maps, reducing parameters and improving computational efficiency. Add another 1 &times; 1 convolution layer after DBlock to recover the required number of output channels. Accordingly, another improved neural network B-D-LinknetPlus was built. Comparisons were performed between the neural nets, and the verification were made with the Massachusetts Roads Dataset. The results show improved neural networks are helpful in reducing the network size and developing the precision needed for road extraction.},
DOI = {10.3390/s19194115}
}



@Article{rs11192225,
AUTHOR = {Agrafiotis, Panagiotis and Skarlatos, Dimitrios and Georgopoulos, Andreas and Karantzalos, Konstantinos},
TITLE = {DepthLearn: Learning to Correct the Refraction on Point Clouds Derived from Aerial Imagery for Accurate Dense Shallow Water Bathymetry Based on SVMs-Fusion with LiDAR Point Clouds},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2225},
URL = {https://www.mdpi.com/2072-4292/11/19/2225},
ISSN = {2072-4292},
ABSTRACT = {The determination of accurate bathymetric information is a key element for near offshore activities; hydrological studies, such as coastal engineering applications, sedimentary processes, hydrographic surveying, archaeological mapping and biological research. Through structure from motion (SfM) and multi-view-stereo (MVS) techniques, aerial imagery can provide a low-cost alternative compared to bathymetric LiDAR (Light Detection and Ranging) surveys, as it offers additional important visual information and higher spatial resolution. Nevertheless, water refraction poses significant challenges on depth determination. Till now, this problem has been addressed through customized image-based refraction correction algorithms or by modifying the collinearity equation. In this article, in order to overcome the water refraction errors in a massive and accurate way, we employ machine learning tools, which are able to learn the systematic underestimation of the estimated depths. In particular, an SVR (support vector regression) model was developed, based on known depth observations from bathymetric LiDAR surveys, which is able to accurately recover bathymetry from point clouds derived from SfM-MVS procedures. Experimental results and validation were based on datasets derived from different test-sites, and demonstrated the high potential of our approach. Moreover, we exploited the fusion of LiDAR and image-based point clouds towards addressing challenges of both modalities in problematic areas.},
DOI = {10.3390/rs11192225}
}



@Article{rs11192243,
AUTHOR = {Liu, Weiquan and Wang, Cheng and Bian, Xuesheng and Chen, Shuting and Li, Wei and Lin, Xiuhong and Li, Yongchuan and Weng, Dongdong and Lai, Shang-Hong and Li, Jonathan},
TITLE = {AE-GAN-Net: Learning Invariant Feature Descriptor to Match Ground Camera Images and a Large-Scale 3D Image-Based Point Cloud for Outdoor Augmented Reality},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2243},
URL = {https://www.mdpi.com/2072-4292/11/19/2243},
ISSN = {2072-4292},
ABSTRACT = {Establishing the spatial relationship between 2D images captured by real cameras and 3D models of the environment (2D and 3D space) is one way to achieve the virtual&ndash;real registration for Augmented Reality (AR) in outdoor environments. In this paper, we propose to match the 2D images captured by real cameras and the rendered images from the 3D image-based point cloud to indirectly establish the spatial relationship between 2D and 3D space. We call these two kinds of images as cross-domain images, because their imaging mechanisms and nature are quite different. However, unlike real camera images, the rendered images from the 3D image-based point cloud are inevitably contaminated with image distortion, blurred resolution, and obstructions, which makes image matching with the handcrafted descriptors or existing feature learning neural networks very challenging. Thus, we first propose a novel end-to-end network, AE-GAN-Net, consisting of two AutoEncoders (AEs) with Generative Adversarial Network (GAN) embedding, to learn invariant feature descriptors for cross-domain image matching. Second, a domain-consistent loss function, which balances image content and consistency of feature descriptors for cross-domain image pairs, is introduced to optimize AE-GAN-Net. AE-GAN-Net effectively captures domain-specific information, which is embedded into the learned feature descriptors, thus making the learned feature descriptors robust against image distortion, variations in viewpoints, spatial resolutions, rotation, and scaling. Experimental results show that AE-GAN-Net achieves state-of-the-art performance for image patch retrieval with the cross-domain image patch dataset, which is built from real camera images and the rendered images from 3D image-based point cloud. Finally, by evaluating virtual&ndash;real registration for AR on a campus by using the cross-domain image matching results, we demonstrate the feasibility of applying the proposed virtual&ndash;real registration to AR in outdoor environments.},
DOI = {10.3390/rs11192243}
}



@Article{s19194192,
AUTHOR = {Kim, Euiho and Shin, Yujin},
TITLE = {Feasibility Analysis of LTE-Based UAS Navigation in Deep Urban Areas and DSRC Augmentation},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4192},
URL = {https://www.mdpi.com/1424-8220/19/19/4192},
ISSN = {1424-8220},
ABSTRACT = {The current autonomous navigation of unmanned aircraft systems (UAS) heavily depends on Global Navigation Satellite Systems (GNSS). However, in challenging environments, such as deep urban areas, GNSS signals can be easily interrupted, so that UAS may lose navigation capability at any instant. For urban positioning and navigation, Long Term Evolution (LTE) has been considered a promising signal of opportunity due to its dense network in urban areas, and there has recently been great advancement in LTE positioning technology. However, the current LTE positioning accuracy is found to be insufficient for safe UAS navigation in deep urban areas. This paper evaluates the positioning performance of the current network of LTE base stations in a selected deep urban area and investigates the effectiveness of LTE augmentations using dedicated short range communication (DSRC) transceivers through the optimization of the ground LTE/DSRC network and cooperative positioning among UAS. The analysis results based on simulation using an urban canyon model and signal line of sight propagations show that the addition of four or five DSRC transceivers to the existing LTE base station network could provide better than 4&ndash;6 m horizontal positioning accuracy (95%) in the selected urban canyon at a position of 150 ft above the ground, while a dense LTE network alone may result in a 15&ndash;20 m horizontal positioning error. Additionally, the simulation results of cooperative positioning with inter-UAS ranging measurements in the DSRC augmented LTE network were shown to provide horizontal positioning accuracy better than 1 m in most flight space, assuming negligible time-synchronization errors in inter-UAS ranging measurements.},
DOI = {10.3390/s19194192}
}



@Article{jsan8040050,
AUTHOR = {Abdulkadir, Yusuf and Simpson, Oluyomi and Sun, Yichuang},
TITLE = {Interference Alignment for Cognitive Radio Communications and Networks: A Survey},
JOURNAL = {Journal of Sensor and Actuator Networks},
VOLUME = {8},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {50},
URL = {https://www.mdpi.com/2224-2708/8/4/50},
ISSN = {2224-2708},
ABSTRACT = {Interference alignment (IA) is an innovative wireless transmission strategy that has shown to be a promising technique for achieving optimal capacity scaling of a multiuser interference channel at asymptotically high-signal-to-noise ratio (SNR). Transmitters exploit the availability of multiple signaling dimensions in order to align their mutual interference at the receivers. Most of the research has focused on developing algorithms for determining alignment solutions as well as proving interference alignment&rsquo;s theoretical ability to achieve the maximum degrees of freedom in a wireless network. Cognitive radio, on the other hand, is a technique used to improve the utilization of the radio spectrum by opportunistically sensing and accessing unused licensed frequency spectrum, without causing harmful interference to the licensed users. With the increased deployment of wireless services, the possibility of detecting unused frequency spectrum becomes diminished. Thus, the concept of introducing interference alignment in cognitive radio has become a very attractive proposition. This paper provides a survey of the implementation of IA in cognitive radio under the main research paradigms, along with a summary and analysis of results under each system model.},
DOI = {10.3390/jsan8040050}
}



@Article{rs11192276,
AUTHOR = {Lee, Jae-Hun and Sull, Sanghoon},
TITLE = {Regression Tree CNN for Estimation of Ground Sampling Distance Based on Floating-Point Representation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2276},
URL = {https://www.mdpi.com/2072-4292/11/19/2276},
ISSN = {2072-4292},
ABSTRACT = {The estimation of ground sampling distance (GSD) from a remote sensing image enables measurement of the size of an object as well as more accurate segmentation in the image. In this paper, we propose a regression tree convolutional neural network (CNN) for estimating the value of GSD from an input image. The proposed regression tree CNN consists of a feature extraction CNN and a binomial tree layer. The proposed network first extracts features from an input image. Based on the extracted features, it predicts the GSD value that is represented by the floating-point number with the exponent and its mantissa. They are computed by coarse scale classification and finer scale regression, respectively, resulting in improved results. Experimental results with a Google Earth aerial image dataset and a mixed dataset consisting of eight remote sensing image public datasets with different GSDs show that the proposed network reduces the GSD prediction error rate by 25% compared to a baseline network that directly estimates the GSD.},
DOI = {10.3390/rs11192276}
}



@Article{rs11192278,
AUTHOR = {Yang, Tao and Li, Dongdong and Bai, Yi and Zhang, Fangbing and Li, Sen and Wang, Miao and Zhang, Zhuoyue and Li, Jing},
TITLE = {Multiple-Object-Tracking Algorithm Based on Dense Trajectory Voting in Aerial Videos},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2278},
URL = {https://www.mdpi.com/2072-4292/11/19/2278},
ISSN = {2072-4292},
ABSTRACT = {In recent years, UAV technology has developed rapidly. Due to the mobility, low cost, and variable monitoring altitude of UAVs, multiple-object detection and tracking in aerial videos has become a research hotspot in the field of computer vision. However, due to camera motion, small target size, target adhesion, and unpredictable target motion, it is still difficult to detect and track targets of interest in aerial videos, especially in the case of a low frame rate where the target position changes too much. In this paper, we propose a multiple-object-tracking algorithm based on dense-trajectory voting in aerial videos. The method models the multiple-target-tracking problem as a voting problem of the dense-optical-flow trajectory to the target ID, which can be applied to aerial-surveillance scenes and is robust to low-frame-rate videos. More specifically, we first built an aerial video dataset for vehicle targets, including a training dataset and a diverse test dataset. Based on this, we trained the neural network model by using a deep-learning method to detect vehicles in aerial videos. Thereafter, we calculated the dense optical flow in adjacent frames, and generated effective dense-optical-flow trajectories in each detection bounding box at the current time. When target IDs of optical-flow trajectories are known, the voting results of the optical-flow trajectories in each detection bounding box are counted. Finally, similarity between detection objects in adjacent frames was measured based on the voting results, and tracking results were obtained by data association. In order to evaluate the performance of this algorithm, we conducted experiments on self-built test datasets. A large number of experimental results showed that the proposed algorithm could obtain good target-tracking results in various complex scenarios, and performance was still robust at a low frame rate by changing the video frame rate. In addition, we carried out qualitative and quantitative comparison experiments between the algorithm and three state-of-the-art tracking algorithms, which further proved that this algorithm could not only obtain good tracking results in aerial videos with a normal frame rate, but also had excellent performance under low-frame-rate conditions.},
DOI = {10.3390/rs11192278}
}



@Article{en12193780,
AUTHOR = {Tang, Jinrui and Xiong, Binyu and Yang, Chen and Tang, Cuilan and Li, Yang and Su, Guoxing and Bian, Xinhao},
TITLE = {Development of an Integrated Power Distribution System Laboratory Platform Using Modular Miniature Physical Elements: A Case Study of Fault Location},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {3780},
URL = {https://www.mdpi.com/1996-1073/12/19/3780},
ISSN = {1996-1073},
ABSTRACT = {The main shortcomings of the software-based power engineering education are a lack of physical understanding of phenomena and hands-on experience. Existing scaled-down analogous educational power system platforms cannot be widely used for experiments in universities due to the high cost, complicated operation, and huge size. An integrated power distribution system laboratory platform (PDSLP) using modular miniature physical elements is proposed in this paper. The printed circuit board (PCB) and microelectronic technology are proposed to construct each physical element. Furthermore, the constructed physical elements are used to set up an integrated PDSLP based on modular assembly technology. The size of the proposed cost-efficient PDSLP is significantly reduced, and the reliability of the proposed PDSLP can be improved greatly because the signal transmission path is shortened and a number of welding points are reduced. A PDSLP for fault location in neutral non-effectively grounded distribution systems (NGDSs) is selected as a typical experimental scenario and one scaled-down distribution network with three feeders is subsequently implemented and discussed. The measured zero-sequence currents by our proposed PDSLP when a single-phase earth fault occurred can reveal the true features of the fault-generated signals, including steady-state and transient characteristics of zero-sequence currents. They can be readily observed and used for students to design corresponding fault location algorithms. Modular renewable energy sources and other elements can be designed, implemented and integrated into the proposed platform for the laboratory education of the active distribution networks in the future.},
DOI = {10.3390/en12193780}
}



@Article{rs11192326,
AUTHOR = {Fricker, Geoffrey A. and Ventura, Jonathan D. and Wolf, Jeffrey A. and North, Malcolm P. and Davis, Frank W. and Franklin, Janet},
TITLE = {A Convolutional Neural Network Classifier Identifies Tree Species in Mixed-Conifer Forest from Hyperspectral Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2326},
URL = {https://www.mdpi.com/2072-4292/11/19/2326},
ISSN = {2072-4292},
ABSTRACT = {In this study, we automate tree species classification and mapping using field-based training data, high spatial resolution airborne hyperspectral imagery, and a convolutional neural network classifier (CNN). We tested our methods by identifying seven dominant trees species as well as dead standing trees in a mixed-conifer forest in the Southern Sierra Nevada Mountains, CA (USA) using training, validation, and testing datasets composed of spatially-explicit transects and plots sampled across a single strip of imaging spectroscopy. We also used a three-band &lsquo;Red-Green-Blue&rsquo; pseudo true-color subset of the hyperspectral imagery strip to test the classification accuracy of a CNN model without the additional non-visible spectral data provided in the hyperspectral imagery. Our classifier is pixel-based rather than object based, although we use three-dimensional structural information from airborne Light Detection and Ranging (LiDAR) to identify trees (points &gt; 5 m above the ground) and the classifier was applied to image pixels that were thus identified as tree crowns. By training a CNN classifier using field data and hyperspectral imagery, we were able to accurately identify tree species and predict their distribution, as well as the distribution of tree mortality, across the landscape. Using a window size of 15 pixels and eight hidden convolutional layers, a CNN model classified the correct species of 713 individual trees from hyperspectral imagery with an average F-score of 0.87 and F-scores ranging from 0.67&ndash;0.95 depending on species. The CNN classification model performance increased from a combined F-score of 0.64 for the Red-Green-Blue model to a combined F-score of 0.87 for the hyperspectral model. The hyperspectral CNN model captures the species composition changes across ~700 meters (1935 to 2630 m) of elevation from a lower-elevation mixed oak conifer forest to a higher-elevation fir-dominated coniferous forest. High resolution tree species maps can support forest ecosystem monitoring and management, and identifying dead trees aids landscape assessment of forest mortality resulting from drought, insects and pathogens. We publicly provide our code to apply deep learning classifiers to tree species identification from geospatial imagery and field training data.},
DOI = {10.3390/rs11192326}
}



@Article{s19194332,
AUTHOR = {Opromolla, Roberto and Inchingolo, Giuseppe and Fasano, Giancarmine},
TITLE = {Airborne Visual Detection and Tracking of Cooperative UAVs Exploiting Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4332},
URL = {https://www.mdpi.com/1424-8220/19/19/4332},
ISSN = {1424-8220},
ABSTRACT = {The performance achievable by using Unmanned Aerial Vehicles (UAVs) for a large variety of civil and military applications, as well as the extent of applicable mission scenarios, can significantly benefit from the exploitation of formations of vehicles able to fly in a coordinated manner (swarms). In this respect, visual cameras represent a key instrument to enable coordination by giving each UAV the capability to visually monitor the other members of the formation. Hence, a related technological challenge is the development of robust solutions to detect and track cooperative targets through a sequence of frames. In this framework, this paper proposes an innovative approach to carry out this task based on deep learning. Specifically, the You Only Look Once (YOLO) object detection system is integrated within an original processing architecture in which the machine-vision algorithms are aided by navigation hints available thanks to the cooperative nature of the formation. An experimental flight test campaign, involving formations of two multirotor UAVs, is conducted to collect a database of images suitable to assess the performance of the proposed approach. Results demonstrate high-level accuracy, and robustness against challenging conditions in terms of illumination, background and target-range variability.},
DOI = {10.3390/s19194332}
}



@Article{agronomy9100618,
AUTHOR = {Hassler, Samuel C. and Baysal-Gurel, Fulya},
TITLE = {Unmanned Aircraft System (UAS) Technology and Applications in Agriculture},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {618},
URL = {https://www.mdpi.com/2073-4395/9/10/618},
ISSN = {2073-4395},
ABSTRACT = {Numerous sensors have been developed over time for precision agriculture; though, only recently have these sensors been incorporated into the new realm of unmanned aircraft systems (UAS). This UAS technology has allowed for a more integrated and optimized approach to various farming tasks such as field mapping, plant stress detection, biomass estimation, weed management, inventory counting, and chemical spraying, among others. These systems can be highly specialized depending on the particular goals of the researcher or farmer, yet many aspects of UAS are similar. All systems require an underlying platform&mdash;or unmanned aerial vehicle (UAV)&mdash;and one or more peripherals and sensing equipment such as imaging devices (RGB, multispectral, hyperspectral, near infra-red, RGB depth), gripping tools, or spraying equipment. Along with these wide-ranging peripherals and sensing equipment comes a great deal of data processing. Common tools to aid in this processing include vegetation indices, point clouds, machine learning models, and statistical methods. With any emerging technology, there are also a few considerations that need to be analyzed like legal constraints, economic trade-offs, and ease of use. This review then concludes with a discussion on the pros and cons of this technology, along with a brief outlook into future areas of research regarding UAS technology in agriculture.},
DOI = {10.3390/agronomy9100618}
}



@Article{s19204363,
AUTHOR = {Sun, Jie and Di, Liping and Sun, Ziheng and Shen, Yonglin and Lai, Zulong},
TITLE = {County-Level Soybean Yield Prediction Using Deep CNN-LSTM Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4363},
URL = {https://www.mdpi.com/1424-8220/19/20/4363},
ISSN = {1424-8220},
ABSTRACT = {Yield prediction is of great significance for yield mapping, crop market planning, crop insurance, and harvest management. Remote sensing is becoming increasingly important in crop yield prediction. Based on remote sensing data, great progress has been made in this field by using machine learning, especially the Deep Learning (DL) method, including Convolutional Neural Network (CNN) or Long Short-Term Memory (LSTM). Recent experiments in this area suggested that CNN can explore more spatial features and LSTM has the ability to reveal phenological characteristics, which both play an important role in crop yield prediction. However, very few experiments combining these two models for crop yield prediction have been reported. In this paper, we propose a deep CNN-LSTM model for both end-of-season and in-season soybean yield prediction in CONUS at the county-level. The model was trained by crop growth variables and environment variables, which include weather data, MODIS Land Surface Temperature (LST) data, and MODIS Surface Reflectance (SR) data; historical soybean yield data were employed as labels. Based on the Google Earth Engine (GEE), all these training data were combined and transformed into histogram-based tensors for deep learning. The results of the experiment indicate that the prediction performance of the proposed CNN-LSTM model can outperform the pure CNN or LSTM model in both end-of-season and in-season. The proposed method shows great potential in improving the accuracy of yield prediction for other crops like corn, wheat, and potatoes at fine scales in the future.},
DOI = {10.3390/s19204363}
}



@Article{rs11202343,
AUTHOR = {Zhang, Jianyong and Zhao, Yanling and Abbott, A. Lynn and Wynne, Randolph H. and Hu, Zhenqi and Zou, Yuzhu and Tian, Shuaishuai},
TITLE = {Automated Mapping of Typical Cropland Strips in the North China Plain Using Small Unmanned Aircraft Systems (sUAS) Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2343},
URL = {https://www.mdpi.com/2072-4292/11/20/2343},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of agricultural fields is needed for many purposes, including irrigation decisions and cadastral management. This paper is concerned with the automated mapping of cropland strips that are common in the North China Plain. These strips are commonly 3&ndash;8 m in width and 50&ndash;300 m in length, and are separated by small ridges that assist with irrigation. Conventional surveying methods are labor-intensive and time-consuming for this application, and only limited performance is possible with very high resolution satellite images. Small Unmanned Aircraft System (sUAS) images could provide an alternative approach to ridge detection and strip mapping. This paper presents a novel method for detecting cropland strips, utilizing centimeter spatial resolution imagery captured by sUAS flying at low altitude (60 m). Using digital surface models (DSM) and ortho-rectified imagery from sUAS data, this method extracts candidate ridge locations by surface roughness segmentation in combination with geometric constraints. This method then exploits vegetation removal and morphological operations to refine candidate ridge elements, leading to polyline-based representations of cropland strip boundaries. This procedure has been tested using sUAS data from four typical cropland plots located approximately 60 km west of Jinan, China. The plots contained early winter wheat. The results indicated an ability to detect ridges with comparatively high recall and precision (96.8% and 95.4%, respectively). Cropland strips were extracted with over 98.9% agreement relative to ground truth, with kappa coefficients over 97.4%. To our knowledge, this method is the first to attempt cropland strip mapping using centimeter spatial resolution sUAS images. These results have demonstrated that sUAS mapping is a viable approach for data collection to assist in agricultural land management in the North China Plain.},
DOI = {10.3390/rs11202343}
}



@Article{s19204378,
AUTHOR = {Franchetti, Benjamin and Ntouskos, Valsamis and Giuliani, Pierluigi and Herman, Tiara and Barnes, Luke and Pirri, Fiora},
TITLE = {Vision Based Modeling of Plants Phenotyping in Vertical Farming under Artificial Lighting},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4378},
URL = {https://www.mdpi.com/1424-8220/19/20/4378},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we present a novel method for vision based plants phenotyping in indoor vertical farming under artificial lighting. The method combines 3D plants modeling and deep segmentation of the higher leaves, during a period of 25&ndash;30 days, related to their growth. The novelty of our approach is in providing 3D reconstruction, leaf segmentation, geometric surface modeling, and deep network estimation for weight prediction to effectively measure plant growth, under three relevant phenotype features: height, weight and leaf area. Together with the vision based measurements, to verify the soundness of our proposed method, we also harvested the plants at specific time periods to take manual measurements, collecting a great amount of data. In particular, we manually collected 2592 data points related to the plant phenotype and 1728 images of the plants. This allowed us to show with a good number of experiments that the vision based methods ensure a quite accurate prediction of the considered features, providing a way to predict plant behavior, under specific conditions, without any need to resort to human measurements.},
DOI = {10.3390/s19204378}
}



@Article{app9204260,
AUTHOR = {Herbers, Patrick and König, Markus},
TITLE = {Indoor Localization for Augmented Reality Devices Using BIM, Point Clouds, and Template Matching},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4260},
URL = {https://www.mdpi.com/2076-3417/9/20/4260},
ISSN = {2076-3417},
ABSTRACT = {Mobile devices are a common target for augmented reality applications, especially for showing contextual information in buildings or construction sites. A prerequisite of contextual information display is the localization of objects and the device in the real world. In this paper, we present our approach to the problem of mobile indoor localization with a given building model. The approach does not use external sensors or input. Accurate external sensors such as stationary cameras may be expensive and difficult to set up and maintain. Relying on already existing external sources may also prove to be difficult, as especially inside buildings, Internet connections can be unreliable and GPS signals can be inaccurate. Therefore, we try to find a localization solution for augmented reality devices that can accurately localize itself only with data from internal sensors and preexisting information about the building. If a building has an accurate model of its geometry, we can use modern spatial mapping techniques and point-cloud matching to find a mapping between local device coordinates and global model coordinates. We use normal analysis and 2D template matching on an inverse distance map to determine this mapping. The proposed algorithm is designed to have a high speed and efficiency, as mobile devices are constrained by hardware limitations. We show an implementation of the algorithm on the Microsoft HoloLens, test the localization accuracy, and offer use cases for the technology.},
DOI = {10.3390/app9204260}
}



@Article{info10100312,
AUTHOR = {Alghamdi, Ibrahim and Anagnostopoulos, Christos and P. Pezaros, Dimitrios},
TITLE = {Delay-Tolerant Sequential Decision Making for Task Offloading in Mobile Edge Computing Environments},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {312},
URL = {https://www.mdpi.com/2078-2489/10/10/312},
ISSN = {2078-2489},
ABSTRACT = {In recent years, there has been a significant increase in the use of mobile devices and their applications. Meanwhile, cloud computing has been considered as the latest generation of computing infrastructure. There has also been a transformation in cloud computing ideas and their implementation so as to meet the demand for the latest applications. mobile edge computing (MEC) is a computing paradigm that provides cloud services near to the users at the edge of the network. Given the movement of mobile nodes between different MEC servers, the main aim would be the connection to the best server and at the right time in terms of the load of the server in order to optimize the quality of service (QoS) of the mobile nodes. We tackle the offloading decision making problem by adopting the principles of optimal stopping theory (OST) to minimize the execution delay in a sequential decision manner. A performance evaluation is provided using real world data sets with baseline deterministic and stochastic offloading models. The results show that our approach significantly minimizes the execution delay for task execution and the results are closer to the optimal solution than other offloading methods.},
DOI = {10.3390/info10100312}
}



@Article{rs11202375,
AUTHOR = {Zhang, Dongyan and Wang, Daoyong and Gu, Chunyan and Jin, Ning and Zhao, Haitao and Chen, Gao and Liang, Hongyi and Liang, Dong},
TITLE = {Using Neural Network to Identify the Severity of Wheat Fusarium Head Blight in the Field Environment},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2375},
URL = {https://www.mdpi.com/2072-4292/11/20/2375},
ISSN = {2072-4292},
ABSTRACT = {Fusarium head blight (FHB), one of the most important diseases of wheat, mainly occurs in the ear. Given that the severity of the disease cannot be accurately identified, the cost of pesticide application increases every year, and the agricultural ecological environment is also polluted. In this study, a neural network (NN) method was proposed based on the red-green-blue (RGB) image to segment wheat ear and disease spot in the field environment, and then to determine the disease grade. Firstly, a segmentation dataset of single wheat ear was constructed to provide a benchmark for the segmentation of the wheat ear. Secondly, a segmentation model of single wheat ear based on the fully convolutional network (FCN) was established to effectively realize the segmentation of the wheat ear in the field environment. An FHB segmentation algorithm was proposed based on a pulse-coupled neural network (PCNN) with K-means clustering of the improved artificial bee colony (IABC) to segment the diseased spot of wheat ear by automatic optimization of PCNN parameters. Finally, the disease grade was calculated using the ratio of the disease spot to the whole wheat ear. The experimental results show that: (1) the accuracy of the segmentation model for single wheat ear constructed in this study is 0.981. The segmentation time is less than 1 s, indicating that the model can quickly and accurately segment wheat ear in the field environment; (2) the segmentation method of the disease spot performed under each evaluation indicator is improved compared with the traditional segmentation methods, and the accuracy is 0.925 in the disease severity identification. These research results can provide important reference value for grading wheat FHB in the field environment, which also can be beneficial for real-time monitoring of other crops&rsquo; diseases under near-Earth remote sensing.},
DOI = {10.3390/rs11202375}
}



@Article{s19204446,
AUTHOR = {Shuai, Guanyuan and Martinez-Feria, Rafael A. and Zhang, Jinshui and Li, Shiming and Price, Richard and Basso, Bruno},
TITLE = {Capturing Maize Stand Heterogeneity Across Yield-Stability Zones Using Unmanned Aerial Vehicles (UAV)},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4446},
URL = {https://www.mdpi.com/1424-8220/19/20/4446},
ISSN = {1424-8220},
ABSTRACT = {Despite the new equipment capabilities, uneven crop stands are still common occurrences in crop fields, mainly due to spatial heterogeneity in soil conditions, seedling mortality due to herbivore predation and disease, or human error. Non-uniform plant stands may reduce grain yield in crops like maize. Thus, detecting signs of variability in crop stand density early in the season provides critical information for management decisions and crop yield forecasts. Processing techniques applied on images captured by unmanned aerial vehicles (UAVs) has been used successfully to identify crop rows and estimate stand density and, most recently, to estimate plant-to-plant interval distance. Here, we further test and apply an image processing algorithm on UAV images collected from yield-stability zones in a commercial crop field. Our objective was to implement the algorithm to compare variation of plant-spacing intervals to test whether yield differences within these zones are related to differences in crop stand characteristics. Our analysis indicates that the algorithm can be reliably used to estimate plant counts (precision &gt;95% and recall &gt;97%) and plant distance interval (R2 ~0.9 and relative error &lt;10%). Analysis of the collected data indicated that plant spacing variability differences were small among plots with large yield differences, suggesting that it was not a major cause of yield variability across zones with distinct yield history. This analysis provides an example of how plant-detection algorithms can be applied to improve the understanding of patterns of spatial and temporal yield variability.},
DOI = {10.3390/s19204446}
}



@Article{electronics8101167,
AUTHOR = {Lee, Yeunghak and Shim, Jaechang},
TITLE = {False Positive Decremented Research for Fire and Smoke Detection in Surveillance Camera using Spatial and Temporal Features Based on Deep Learning},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1167},
URL = {https://www.mdpi.com/2079-9292/8/10/1167},
ISSN = {2079-9292},
ABSTRACT = {Fire must be extinguished early, as it leads to economic losses and losses of precious lives. Vision-based methods have many difficulties in algorithm research due to the atypical nature fire flame and smoke. In this study, we introduce a novel smoke detection algorithm that reduces false positive detection using spatial and temporal features based on deep learning from factory installed surveillance cameras. First, we calculated the global frame similarity and mean square error (MSE) to detect the moving of fire flame and smoke from input surveillance cameras. Second, we extracted the fire flame and smoke candidate area using the deep learning algorithm (Faster Region-based Convolutional Network (R-CNN)). Third, the final fire flame and smoke area was decided by local spatial and temporal information: frame difference, color, similarity, wavelet transform, coefficient of variation, and MSE. This research proposed a new algorithm using global and local frame features, which is well presented object information to reduce false positive based on the deep learning method. Experimental results show that the false positive detection of the proposed algorithm was reduced to about 99.9% in maintaining the smoke and fire detection performance. It was confirmed that the proposed method has excellent false detection performance.},
DOI = {10.3390/electronics8101167}
}



@Article{s19204484,
AUTHOR = {García Rubio, Víctor and Rodrigo Ferrán, Juan Antonio and Menéndez García, Jose Manuel and Sánchez Almodóvar, Nuria and Lalueza Mayordomo, José María and Álvarez, Federico},
TITLE = {Automatic Change Detection System over Unmanned Aerial Vehicle Video Sequences Based on Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4484},
URL = {https://www.mdpi.com/1424-8220/19/20/4484},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the use of unmanned aerial vehicles (UAVs) for surveillance tasks has increased considerably. This technology provides a versatile and innovative approach to the field. However, the automation of tasks such as object recognition or change detection usually requires image processing techniques. In this paper we present a system for change detection in video sequences acquired by moving cameras. It is based on the combination of image alignment techniques with a deep learning model based on convolutional neural networks (CNNs). This approach covers two important topics. Firstly, the capability of our system to be adaptable to variations in the UAV flight. In particular, the difference of height between flights, and a slight modification of the camera&rsquo;s position or movement of the UAV because of natural conditions such as the effect of wind. These modifications can be produced by multiple factors, such as weather conditions, security requirements or human errors. Secondly, the precision of our model to detect changes in diverse environments, which has been compared with state-of-the-art methods in change detection. This has been measured using the Change Detection 2014 dataset, which provides a selection of labelled images from different scenarios for training change detection algorithms. We have used images from dynamic background, intermittent object motion and bad weather sections. These sections have been selected to test our algorithm&rsquo;s robustness to changes in the background, as in real flight conditions. Our system provides a precise solution for these scenarios, as the mean F-measure score from the image analysis surpasses 97%, and a significant precision in the intermittent object motion category, where the score is above 99%.},
DOI = {10.3390/s19204484}
}



@Article{s19204513,
AUTHOR = {Freimuth, Henk and König, Markus},
TITLE = {A Framework for Automated Acquisition and Processing of As-Built Data with Autonomous Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4513},
URL = {https://www.mdpi.com/1424-8220/19/20/4513},
ISSN = {1424-8220},
ABSTRACT = {Planning and scheduling in construction heavily depend on current information about the state of construction processes. However, the acquisition process for visual data requires human personnel to take photographs of construction objects. We propose using unmanned aerial vehicle (UAVs) for automated creation of images and point cloud data of particular construction objects. The method extracts locations of objects that require inspection from Four Dimensional Building Information Modelling (4D-BIM). With this information at hand viable flight missions around the known structures of the construction site are computed. During flight, the UAV uses stereo cameras to detect and avoid any obstacles that are not known to the model, for example moving humans or machinery. The combination of pre-computed waypoint missions and reactive avoidance ensures deterministic routing from takeoff to landing and operational safety for humans and machines. During flight, an additional software component compares the captured point cloud data with the model data, enabling automatic per-object completion checking or reconstruction. The prototype is developed in the Robot Operating System (ROS) and evaluated in Software-In-The-Loop (SITL) simulations for the sake of being executable on real UAVs.},
DOI = {10.3390/s19204513}
}



@Article{rs11202413,
AUTHOR = {Mesas-Carrascosa, Francisco Javier and Pérez Porras, Fernando and Triviño-Tarradas, Paula and Meroño de Larriva, Jose Emilio and García-Ferrer, Alfonso},
TITLE = {Project-Based Learning Applied to Unmanned Aerial Systems and Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2413},
URL = {https://www.mdpi.com/2072-4292/11/20/2413},
ISSN = {2072-4292},
ABSTRACT = {The development of unmanned aerial vehicle (UAV) technology and the miniaturization of sensors have changed the way remote sensing (RS) is used, popularizing this geoscientific discipline in other fields, such as precision agriculture. This makes it necessary to implement the use of these technologies in teaching RS alongside the classical platforms (satellite and manned aircraft). This manuscript describes how The Higher Technical School of Agricultural Engineering at the University of C&oacute;rdoba (Spain) has introduced UAV RS into the academic program by way of project-based learning (PBL). It also presents the basic characteristics of PBL, the design of the subject, the description of the teacher-guided and self-directed activities, as well as the degree of student satisfaction. The teaching and learning objectives of the subject are to learn how to determine the vigor, temperature, and water stress of a crop through the use of RGB, multispectral, and thermographic sensors onboard a UAV platform. From the onset, students are motivated, actively participate in the tasks related to the realization of UAV flights, and subsequent processing and analysis of the registered images. Students report that PBL is more engaging and allows them to develop a better understanding of RS.},
DOI = {10.3390/rs11202413}
}



@Article{rs11202415,
AUTHOR = {Woodget, Amy S. and Dietrich, James T. and Wilson, Robin T.},
TITLE = {Quantifying Below-Water Fluvial Geomorphic Change: The Implications of Refraction Correction, Water Surface Elevations, and Spatially Variable Error},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2415},
URL = {https://www.mdpi.com/2072-4292/11/20/2415},
ISSN = {2072-4292},
ABSTRACT = {Much of the geomorphic work of rivers occurs underwater. As a result, high resolutionquantification of geomorphic change in these submerged areas is important. Currently, to quantify thischange, multiple methods are required to get high resolution data for both the exposed and submergedareas. Remote sensing methods are often limited to the exposed areas due to the challenges imposedby the water, and those remote sensing methods for below the water surface require the collection ofextensive calibration data in-channel, which is time-consuming, labour-intensive, and sometimesprohibitive in dicult-to-access areas. Within this paper, we pioneer a novel approach for quantifyingabove- and below-water geomorphic change using Structure-from-Motion photogrammetry andinvestigate the implications of water surface elevations, refraction correction measures, and thespatial variability of topographic errors. We use two epochs of imagery from a site on the River Teme,Herefordshire, UK, collected using a remotely piloted aircraft system (RPAS) and processed usingStructure-from-Motion (SfM) photogrammetry. For the first time, we show that: (1) Quantification ofsubmerged geomorphic change to levels of accuracy commensurate with exposed areas is possiblewithout the need for calibration data or a dierent method from exposed areas; (2) there is minimaldierence in results produced by dierent refraction correction procedures using predominantlynadir imagery (small angle vs. multi-view), allowing users a choice of software packages/processingcomplexity; (3) improvements to our estimations of water surface elevations are critical for accuratetopographic estimation in submerged areas and can reduce mean elevation error by up to 73%;and (4) we can use machine learning, in the form of multiple linear regressions, and a Gaussian Na&iuml;veBayes classifier, based on the relationship between error and 11 independent variables, to generate ahigh resolution, spatially continuous model of geomorphic change in submerged areas, constrained byspatially variable error estimates. Our multiple regression model is capable of explaining up to 54%of magnitude and direction of topographic error, with accuracies of less than 0.04 m. With on-goingtesting and improvements, this machine learning approach has potential for routine application inspatially variable error estimation within the RPAS&ndash;SfM workflow.},
DOI = {10.3390/rs11202415}
}



@Article{app9204431,
AUTHOR = {Kwak, Jeonghoon and Sung, Yunsick},
TITLE = {End-To-End Controls Using K-Means Algorithm for 360-Degree Video Control Method on Omnidirectional Camera-Equipped Autonomous Micro Unmanned Aircraft Systems},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4431},
URL = {https://www.mdpi.com/2076-3417/9/20/4431},
ISSN = {2076-3417},
ABSTRACT = {Micro unmanned aircraft systems (micro UAS)-related technical research is important because micro UAS has the advantage of being able to perform missions remotely. When an omnidirectional camera is mounted, it captures all surrounding areas of the micro UAS. Normal field of view (NFoV) refers to a view presented as an image to a user in a 360-degree video. The 360-degree video is controlled using an end-to-end controls method to automatically provide the user with NFoVs without the user controlling the 360-degree video. When using the end-to-end controls method that controls 360-degree video, if there are various signals that control the 360-degree video, the training of the deep learning model requires a considerable amount of training data. Therefore, there is a need for a method of autonomously determining the signals to reduce the number of signals for controlling the 360-degree video. This paper proposes a method to autonomously determine the output to be used for end-to-end control-based deep learning model to control 360-degree video for micro UAS controllers. The output of the deep learning model to control 360-degree video is automatically determined using the K-means algorithm. Using a trained deep learning model, the user is presented with NFoVs in a 360-degree video. The proposed method was experimentally verified by providing NFoVs wherein the signals that control the 360-degree video were set by the proposed method and by user definition. The results of training the convolution neural network (CNN) model using the signals to provide NFoVs were compared, and the proposed method provided NFoVs similar to NFoVs of existing user with 24.4% more similarity compared to a user-defined approach.},
DOI = {10.3390/app9204431}
}



@Article{rs11202427,
AUTHOR = {Ghaffarian, Saman and Kerle, Norman and Pasolli, Edoardo and Jokar Arsanjani, Jamal},
TITLE = {Post-Disaster Building Database Updating Using Automated Deep Learning: An Integration of Pre-Disaster OpenStreetMap and Multi-Temporal Satellite Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2427},
URL = {https://www.mdpi.com/2072-4292/11/20/2427},
ISSN = {2072-4292},
ABSTRACT = {First responders and recovery planners need accurate and quickly derived information about the status of buildings as well as newly built ones to both help victims and to make decisions for reconstruction processes after a disaster. Deep learning and, in particular, convolutional neural network (CNN)-based approaches have recently become state-of-the-art methods to extract information from remote sensing images, in particular for image-based structural damage assessment. However, they are predominantly based on manually extracted training samples. In the present study, we use pre-disaster OpenStreetMap building data to automatically generate training samples to train the proposed deep learning approach after the co-registration of the map and the satellite images. The proposed deep learning framework is based on the U-net design with residual connections, which has been shown to be an effective method to increase the efficiency of CNN-based models. The ResUnet is followed by a Conditional Random Field (CRF) implementation to further refine the results. Experimental analysis was carried out on selected very high resolution (VHR) satellite images representing various scenarios after the 2013 Super Typhoon Haiyan in both the damage and the recovery phases in Tacloban, the Philippines. The results show the robustness of the proposed ResUnet-CRF framework in updating the building map after a disaster for both damage and recovery situations by producing an overall F1-score of 84.2%.},
DOI = {10.3390/rs11202427}
}



@Article{app9204444,
AUTHOR = {Kim, Byunghyun and Cho, Soojin},
TITLE = {Hyperspectral Super-Resolution Technique Using Histogram Matching and Endmember Optimization},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4444},
URL = {https://www.mdpi.com/2076-3417/9/20/4444},
ISSN = {2076-3417},
ABSTRACT = {In most hyperspectral super-resolution (HSR) methods, which are techniques used to improve the resolution of hyperspectral images (HSIs), the HSI and the target RGB image are assumed to have identical fields of view. However, because implementing these identical fields of view is difficult in practical applications, in this paper, we propose a HSR method that is applicable when an HSI and a target RGB image have different spatial information. The proposed HSR method first creates a low-resolution RGB image from a given HSI. Next, a histogram matching is performed on a high-resolution RGB image and a low-resolution RGB image obtained from an HSI. Finally, the proposed method optimizes endmember abundance of the high-resolution HSI towards the histogram-matched high-resolution RGB image. The entire procedure is evaluated using an open HSI dataset, the Harvard dataset, by adding spatial mismatch to the dataset. The spatial mismatch is implemented by shear transformation and cutting off the upper and left sides of the target RGB image. The proposed method achieved a lower error rate across the entire dataset, confirming its capability for super-resolution using images that have different fields of view.},
DOI = {10.3390/app9204444}
}



@Article{rs11202438,
AUTHOR = {Tao, Mingliang and Su, Jia and Huang, Yan and Wang, Ling},
TITLE = {Mitigation of Radio Frequency Interference in Synthetic Aperture Radar Data: Current Status and Future Trends},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2438},
URL = {https://www.mdpi.com/2072-4292/11/20/2438},
ISSN = {2072-4292},
ABSTRACT = {Radio frequency interference (RFI) is a major issue in accurate remote sensing by a synthetic aperture radar (SAR) system, which poses a great hindrance to raw data collection, image formation, and subsequent interpretation process. This paper provides a comprehensive study of the RFI mitigation techniques applicable for an SAR system. From the view of spectrum allocation, possible terrestrial and spaceborne RFI sources to SAR system and their geometry are analyzed. Typical signal models for various RFI types are provided, together with many illustrative examples from real measured data. Then, advanced signal processing techniques for removing RFI are reviewed. Advantages and drawbacks of each approach are discussed in terms of their applicability. Discussion on the future trends are provided from the perspective of cognitive, integrated, and adaptive. This review serves as a reference for future work on the implementation of the most suitable RFI mitigation scheme for an air-borne or space-borne SAR system.},
DOI = {10.3390/rs11202438}
}



@Article{rs11202440,
AUTHOR = {Almadhoun, Randa and Abduldayem, Abdullah and Taha, Tarek and Seneviratne, Lakmal and Zweiri, Yahya},
TITLE = {Guided Next Best View for 3D Reconstruction of Large Complex Structures},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2440},
URL = {https://www.mdpi.com/2072-4292/11/20/2440},
ISSN = {2072-4292},
ABSTRACT = {In this paper, a Next Best View (NBV) approach with a profiling stage and a novel utility function for 3D reconstruction using an Unmanned Aerial Vehicle (UAV) is proposed. The proposed approach performs an initial scan in order to build a rough model of the structure that is later used to improve coverage completeness and reduce flight time. Then, a more thorough NBV process is initiated, utilizing the rough model in order to create a dense 3D reconstruction of the structure of interest. The proposed approach exploits the reflectional symmetry feature if it exists in the initial scan of the structure. The proposed NBV approach is implemented with a novel utility function, which consists of four main components: information theory, model density, traveled distance, and predictive measures based on symmetries in the structure. This system outperforms classic information gain approaches with a higher density, entropy reduction and coverage completeness. Simulated and real experiments were conducted and the results show the effectiveness and applicability of the proposed approach.},
DOI = {10.3390/rs11202440}
}



@Article{rs11202455,
AUTHOR = {He, Zhi and He, Dan and Mei, Xiangqin and Hu, Saihan},
TITLE = {Wetland Classification Based on a New Efficient Generative Adversarial Network and Jilin-1 Satellite Image},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2455},
URL = {https://www.mdpi.com/2072-4292/11/20/2455},
ISSN = {2072-4292},
ABSTRACT = {Recent studies have shown that deep learning methods provide useful tools for wetland classification. However, it is difficult to perform species-level classification with limited labeled samples. In this paper, we propose a semi-supervised method for wetland species classification by using a new efficient generative adversarial network (GAN) and Jilin-1 satellite image. The main contributions of this paper are twofold. First, the proposed method, namely ShuffleGAN, requires only a small number of labeled samples. ShuffleGAN is composed of two neural networks (i.e., generator and discriminator), which perform an adversarial game in the training phase and ShuffleNet units are added in both generator and discriminator to obtain speed-accuracy tradeoff. Second, ShuffleGAN can perform species-level wetland classification. In addition to distinguishing the wetland areas from non-wetlands, different tree species located in the wetland are also identified, thus providing a more detailed distribution of the wetland land-covers. Experiments are conducted on the Haizhu Lake wetland data acquired by the Jilin-1 satellite. Compared with existing GAN, the improvement in overall accuracy (OA) of the proposed ShuffleGAN is more than 2%. This work can not only deepen the application of deep learning in wetland classification but also promote the study of fine classification of wetland land-covers.},
DOI = {10.3390/rs11202455}
}



@Article{ijgi8100464,
AUTHOR = {Fang, Shimin and Zhou, Xiaoguang and Zhang, Jing},
TITLE = {A Multilevel Mapping Strategy to Calculate the Information Content of Remotely Sensed Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {464},
URL = {https://www.mdpi.com/2220-9964/8/10/464},
ISSN = {2220-9964},
ABSTRACT = {Considering the multiscale characteristics of the human visual system and any natural scene, the spatial autocorrelation of remotely sensed imagery, and the multilevel spatial structure of ground targets in remote sensing images, an information-measurement approach based on a single-level geometrical mapping model can only reflect partial feature information at a single level (e.g., global statistical information and local spatial distribution information). The single mapping model cannot validly characterize the information of the multilevel and multiscale features of the spatial structures inherent in remotely sensed images. Additionally, the validity, practicability, and application range of the results of single-level mapping models are greatly limited in practical applications. In this paper, we present the multilevel geometrical mapping entropy (MGME) model to evaluate the information content of related attribute characteristics contained in remotely sensed images. Subsequently, experimental images with different types of objects, including reservoir area, farmland, water area (i.e., water and trees), and mountain area, were used to validate the performance of the proposed method. Experimental results show that the proposed method can not only reflect the difference in the information of images in terms of spectrum features, spatial structural features, and visual perception but also eliminates the inadequacy of a single-level mapping model. That is, the multilevel mapping strategy is feasible and valid. Additionally, the vector set of the MGME method and its standard deviation (Std) value can be used to further explore and study the spatial dependence of ground scenes and the difference in the spatial structural characteristics of different objects.},
DOI = {10.3390/ijgi8100464}
}



@Article{rs11212495,
AUTHOR = {Bohnenkamp, David and Behmann, Jan and Mahlein, Anne-Katrin},
TITLE = {In-Field Detection of Yellow Rust in Wheat on the Ground Canopy and UAV Scale},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2495},
URL = {https://www.mdpi.com/2072-4292/11/21/2495},
ISSN = {2072-4292},
ABSTRACT = {The application of hyperspectral imaging technology for plant disease detection in the field is still challenging. Existing equipment and analysis algorithms are adapted to highly controlled environmental conditions in the laboratory. However, only real time information from the field scale is able to guide plant protection measures and to optimize the use of resources. At the field scale, many parameters such as the optimal measurement distance, informative feature sets, and suitable algorithms have not been investigated. In this study, the hyperspectral detection and quantification of yellow rust in wheat was evaluated using two measurement platforms: a ground-based vehicle and an unmanned aerial vehicle (UAV). Different disease development stages and disease severities were provided in a plot-based field experiment. Measurements were performed weekly during the vegetation period. Data analysis was performed by three prediction algorithms with a focus on the selection of optimal feature sets. In this context, the across-scale application of optimized feature sets, an approach of information transfer between scales, was also evaluated. Relevant aspects for an on-line disease assessment in the field integrating affordable sensor technology, sensor spatial resolution, compact analysis models, and fast evaluation have been outlined and reflected upon. For the first time, a hyperspectral imaging observation experiment of a plant disease was comparatively performed at two scales, ground canopy and UAV.},
DOI = {10.3390/rs11212495}
}



@Article{rs11212499,
AUTHOR = {Xin, Jiang and Zhang, Xinchang and Zhang, Zhiqiang and Fang, Wu},
TITLE = {Road Extraction of High-Resolution Remote Sensing Images Derived from DenseUNet},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2499},
URL = {https://www.mdpi.com/2072-4292/11/21/2499},
ISSN = {2072-4292},
ABSTRACT = {Road network extraction is one of the significant assignments for disaster emergency response, intelligent transportation systems, and real-time updating road network. Road extraction base on high-resolution remote sensing images has become a hot topic. Presently, most of the researches are based on traditional machine learning algorithms, which are complex and computational because of impervious surfaces such as roads and buildings that are discernible in the images. Given the above problems, we propose a new method to extract the road network from remote sensing images using a DenseUNet model with few parameters and robust characteristics. DenseUNet consists of dense connection units and skips connections, which strengthens the fusion of different scales by connections at various network layers. The performance of the advanced method is validated on two datasets of high-resolution images by comparison with three classical semantic segmentation methods. The experimental results show that the method can be used for road extraction in complex scenes.},
DOI = {10.3390/rs11212499}
}



@Article{rs11212505,
AUTHOR = {Crommelinck, Sophie and Koeva, Mila and Yang, Michael Ying and Vosselman, George},
TITLE = {Application of Deep Learning for Delineation of Visible Cadastral Boundaries from Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2505},
URL = {https://www.mdpi.com/2072-4292/11/21/2505},
ISSN = {2072-4292},
ABSTRACT = {Cadastral boundaries are often demarcated by objects that are visible in remote sensing imagery. Indirect surveying relies on the delineation of visible parcel boundaries from such images. Despite advances in automated detection and localization of objects from images, indirect surveying is rarely automated and relies on manual on-screen delineation. We have previously introduced a boundary delineation workflow, comprising image segmentation, boundary classification and interactive delineation that we applied on Unmanned Aerial Vehicle (UAV) data to delineate roads. In this study, we improve each of these steps. For image segmentation, we remove the need to reduce the image resolution and we limit over-segmentation by reducing the number of segment lines by 80% through filtering. For boundary classification, we show how Convolutional Neural Networks (CNN) can be used for boundary line classification, thereby eliminating the previous need for Random Forest (RF) feature generation and thus achieving 71% accuracy. For interactive delineation, we develop additional and more intuitive delineation functionalities that cover more application cases. We test our approach on more varied and larger data sets by applying it to UAV and aerial imagery of 0.02&ndash;0.25 m resolution from Kenya, Rwanda and Ethiopia. We show that it is more effective in terms of clicks and time compared to manual delineation for parcels surrounded by visible boundaries. Strongest advantages are obtained for rural scenes delineated from aerial imagery, where the delineation effort per parcel requires 38% less time and 80% fewer clicks compared to manual delineation.},
DOI = {10.3390/rs11212505}
}



@Article{rs11212511,
AUTHOR = {Kerle, Norman and Ghaffarian, Saman and Nawrotzki, Raphael and Leppert, Gerald and Lech, Malte},
TITLE = {Evaluating Resilience-Centered Development Interventions with Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2511},
URL = {https://www.mdpi.com/2072-4292/11/21/2511},
ISSN = {2072-4292},
ABSTRACT = {Natural disasters are projected to increase in number and severity, in part due to climate change. At the same time a growing number of disaster risk reduction (DRR) and climate change adaptation measures are being implemented by governmental and non-governmental organizations, and substantial post-disaster donations are frequently pledged. At the same time there has been increasing demand for transparency and accountability, and thus evidence of those measures having a positive effect. We hypothesized that resilience-enhancing interventions should result in less damage during a hazard event, or at least quicker recovery. In this study we assessed recovery over a 3 year period of seven municipalities in the central Philippines devastated by Typhoon Haiyan in 2013. We used very high resolution optical images (&lt;1 m), and created detailed land cover and land use maps for four epochs before and after the event, using a machine learning approach with extreme gradient boosting. The spatially and temporally highly variable recovery maps were then statistically related to detailed questionnaire data acquired by DEval in 2012 and 2016, whose principal aim was to assess the impact of a 10 year land-planning intervention program by the German agency for technical cooperation (GIZ). The survey data allowed very detailed insights into DRR-related perspectives, motivations and drivers of the affected population. To some extent they also helped to overcome the principal limitation of remote sensing, which can effectively describe but not explain the reasons for differential recovery. However, while a number of causal links between intervention parameters and reconstruction was found, the common notion that a resilient community should recover better and more quickly could not be confirmed. The study also revealed a number of methodological limitations, such as the high cost for commercial image data not matching the spatially extensive but also detailed scale of field evaluations, the remote sensing analysis likely overestimating damage and thus providing incorrect recovery metrics, and image data catalogues especially for more remote communities often being incomplete. Nevertheless, the study provides a valuable proof of concept for the synergies resulting from an integration of socio-economic survey data and remote sensing imagery for recovery assessment.},
DOI = {10.3390/rs11212511}
}



@Article{rs11212523,
AUTHOR = {Xia, Wei and Ma, Caihong and Liu, Jianbo and Liu, Shibin and Chen, Fu and Yang, Zhi and Duan, Jianbo},
TITLE = {High-Resolution Remote Sensing Imagery Classification of Imbalanced Data Using Multistage Sampling Method and Deep Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2523},
URL = {https://www.mdpi.com/2072-4292/11/21/2523},
ISSN = {2072-4292},
ABSTRACT = {Class imbalance is a key issue for the application of deep learning for remote sensing image classification because a model generated by imbalanced samples training has low classification accuracy for minority classes. In this study, an accurate classification approach using the multistage sampling method and deep neural networks was proposed to classify imbalanced data. We first balance samples by multistage sampling to obtain the training sets. Then, a state-of-the-art model is adopted by combining the advantages of atrous spatial pyramid pooling (ASPP) and Encoder-Decoder for pixel-wise classification, which are two different types of fully convolutional networks (FCNs) that can obtain contextual information of multiple levels in the Encoder stage. The details and spatial dimensions of targets are restored using such information during the Decoder stage. We employ four deep learning-based classification algorithms (basic FCN, FCN-8S, ASPP, and Encoder-Decoder with ASPP of our approach) on multistage training sets (original, MUS1, and MUS2) of WorldView-3 images in southeastern Qinghai-Tibet Plateau and GF-2 images in northeastern Beijing for comparison. The experiments show that, compared with existing sets (original, MUS1, and identical) and existing method (cost weighting), the MUS2 training set of multistage sampling significantly enhance the classification performance for minority classes. Our approach shows distinct advantages for imbalanced data.},
DOI = {10.3390/rs11212523}
}



@Article{beverages5040062,
AUTHOR = {Gonzalez Viejo, Claudia and Torrico, Damir D. and Dunshea, Frank R. and Fuentes, Sigfredo},
TITLE = {Emerging Technologies Based on Artificial Intelligence to Assess the Quality and Consumer Preference of Beverages},
JOURNAL = {Beverages},
VOLUME = {5},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {62},
URL = {https://www.mdpi.com/2306-5710/5/4/62},
ISSN = {2306-5710},
ABSTRACT = {Beverages is a broad and important category within the food industry, which is comprised of a wide range of sub-categories and types of drinks with different levels of complexity for their manufacturing and quality assessment. Traditional methods to evaluate the quality traits of beverages consist of tedious, time-consuming, and costly techniques, which do not allow researchers to procure results in real-time. Therefore, there is a need to test and implement emerging technologies in order to automate and facilitate those analyses within this industry. This paper aimed to present the most recent publications and trends regarding the use of low-cost, reliable, and accurate, remote or non-contact techniques using robotics, machine learning, computer vision, biometrics and the application of artificial intelligence, as well as to identify the research gaps within the beverage industry. It was found that there is a wide opportunity in the development and use of robotics and biometrics for all types of beverages, but especially for hot and non-alcoholic drinks. Furthermore, there is a lack of knowledge and clarity within the industry, and research about the concepts of artificial intelligence and machine learning, as well as that concerning the correct design and interpretation of modeling related to the lack of inclusion of relevant data, additional to presenting over- or under-fitted models.},
DOI = {10.3390/beverages5040062}
}



@Article{app9214656,
AUTHOR = {Alhichri, Haikel and Bazi, Yakoub and Alajlan, Naif and Bin Jdira, Bilel},
TITLE = {Helping the Visually Impaired See via Image Multi-labeling Based on SqueezeNet CNN},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4656},
URL = {https://www.mdpi.com/2076-3417/9/21/4656},
ISSN = {2076-3417},
ABSTRACT = {This work presents a deep learning method for scene description. (1) Background: This method is part of a larger system, called BlindSys, that assists the visually impaired in an indoor environment. The method detects the presence of certain objects, regardless of their position in the scene. This problem is also known as image multi-labeling. (2) Methods: Our proposed deep learning solution is based on a light-weight pre-trained CNN called SqueezeNet. We improved the SqueezeNet architecture by resetting the last convolutional layer to free weights, replacing its activation function from a rectified linear unit (ReLU) to a LeakyReLU, and adding a BatchNormalization layer thereafter. We also replaced the activation functions at the output layer from softmax to linear functions. These adjustments make up the main contributions in this work. (3) Results: The proposed solution is tested on four image multi-labeling datasets representing different indoor environments. It has achieved results better than state-of-the-art solutions both in terms of accuracy and processing time. (4) Conclusions: The proposed deep CNN is an effective solution for predicting the presence of objects in a scene and can be successfully used as a module within BlindSys.},
DOI = {10.3390/app9214656}
}



@Article{s19214761,
AUTHOR = {Ahmad, Shabir and Malik, Sehrish and Park, Dong-Hwan and Kim, DoHyeun},
TITLE = {Design of Lightweight Driver-Assistance System for Safe Driving in Electric Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4761},
URL = {https://www.mdpi.com/1424-8220/19/21/4761},
ISSN = {1424-8220},
ABSTRACT = {Electric-vehicle technology is an emerging area offering several benefits such as economy due to low running costs. Electric vehicles can also help to significantly reduce CO2 emission, which is a vital factor for environmental pollution. Modern vehicles are equipped with driver-assistance systems that facilitate drivers by offloading some of the tasks a driver does while driving. Human beings are prone to errors. Therefore, accidents and fatalities can happen if the driver fails to perform a particular task within the deadline. In electric vehicles, the focus has always been to optimize the power and battery life, and thus, any additional hardware can affect their battery life significantly. In this paper, the design of driver-assistance systems has been introduced to automate and assist in some of the vital tasks, such as a braking system, in an optimized manner. We revamp the idea of the traditional driver-assistance system and propose a generic lightweight system based on the leading factors and their impact on accidents. We model tasks for these factors and simulate a low-cost driver-assistance system in a real-time context, where these scenarios are investigated and tasks schedulability is formally proved before deploying them in electric vehicles. The proposed driver-assistance system offers many advantages. It decreases the risk of accidents and monitors the safety of driving. If, at some point, the risk index is above a certain threshold, an automated control algorithm is triggered to reduce it by activating different actuators. At the same time, it is lightweight and does not require any dedicated hardware, which in turn has a significant advantage in terms of battery life. Results show that the proposed system not only is accurate but also has a very negligible effect on energy consumption and battery life.},
DOI = {10.3390/s19214761}
}



@Article{rs11212575,
AUTHOR = {Tavakkoli Piralilou, Sepideh and Shahabi, Hejar and Jarihani, Ben and Ghorbanzadeh, Omid and Blaschke, Thomas and Gholamnia, Khalil and Meena, Sansar Raj and Aryal, Jagannath},
TITLE = {Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2575},
URL = {https://www.mdpi.com/2072-4292/11/21/2575},
ISSN = {2072-4292},
ABSTRACT = {Landslides represent a severe hazard in many areas of the world. Accurate landslide maps are needed to document the occurrence and extent of landslides and to investigate their distribution, types, and the pattern of slope failures. Landslide maps are also crucial for determining landslide susceptibility and risk. Satellite data have been widely used for such investigations—next to data from airborne or unmanned aerial vehicle (UAV)-borne campaigns and Digital Elevation Models (DEMs). We have developed a methodology that incorporates object-based image analysis (OBIA) with three machine learning (ML) methods, namely, the multilayer perceptron neural network (MLP-NN) and random forest (RF), for landslide detection. We identified the optimal scale parameters (SP) and used them for multi-scale segmentation and further analysis. We evaluated the resulting objects using the object pureness index (OPI), object matching index (OMI), and object fitness index (OFI) measures. We then applied two different methods to optimize the landslide detection task: (a) an ensemble method of stacking that combines the different ML methods for improving the performance, and (b) Dempster–Shafer theory (DST), to combine the multi-scale segmentation and classification results. Through the combination of three ML methods and the multi-scale approach, the framework enhanced landslide detection when it was tested for detecting earthquake-triggered landslides in Rasuwa district, Nepal. PlanetScope optical satellite images and a DEM were used, along with the derived landslide conditioning factors. Different accuracy assessment measures were used to compare the results against a field-based landslide inventory. All ML methods yielded the highest overall accuracies ranging from 83.3% to 87.2% when using objects with the optimal SP compared to other SPs. However, applying DST to combine the multi-scale results of each ML method significantly increased the overall accuracies to almost 90%. Overall, the integration of OBIA with ML methods resulted in appropriate landslide detections, but using the optimal SP and ML method is crucial for success.},
DOI = {10.3390/rs11212575}
}



@Article{s19214794,
AUTHOR = {Rodriguez-Ramos, Alejandro and Alvarez-Fernandez, Adrian and Bavle, Hriday and Campoy, Pascual and How, Jonathan P.},
TITLE = {Vision-Based Multirotor Following Using Synthetic Learning Techniques},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4794},
URL = {https://www.mdpi.com/1424-8220/19/21/4794},
ISSN = {1424-8220},
ABSTRACT = {Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights).},
DOI = {10.3390/s19214794}
}



@Article{rs11212585,
AUTHOR = {Fromm, Michael and Schubert, Matthias and Castilla, Guillermo and Linke, Julia and McDermid, Greg},
TITLE = {Automated Detection of Conifer Seedlings in Drone Imagery Using Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2585},
URL = {https://www.mdpi.com/2072-4292/11/21/2585},
ISSN = {2072-4292},
ABSTRACT = {Monitoring tree regeneration in forest areas disturbed by resource extraction is a requirement for sustainably managing the boreal forest of Alberta, Canada. Small remotely piloted aircraft systems (sRPAS, a.k.a. drones) have the potential to decrease the cost of field surveys drastically, but produce large quantities of data that will require specialized processing techniques. In this study, we explored the possibility of using convolutional neural networks (CNNs) on this data for automatically detecting conifer seedlings along recovering seismic lines: a common legacy footprint from oil and gas exploration. We assessed three different CNN architectures, of which faster region-CNN (R-CNN) performed best (mean average precision 81%). Furthermore, we evaluated the effects of training-set size, season, seedling size, and spatial resolution on the detection performance. Our results indicate that drone imagery analyzed by artificial intelligence can be used to detect conifer seedling in regenerating sites with high accuracy, which increases with the size in pixels of the seedlings. By using a pre-trained network, the size of the training dataset can be reduced to a couple hundred seedlings without any significant loss of accuracy. Furthermore, we show that combining data from different seasons yields the best results. The proposed method is a first step towards automated monitoring of forest restoration/regeneration.},
DOI = {10.3390/rs11212585}
}



@Article{machines7040069,
AUTHOR = {Iannace, Gino and Ciaburro, Giuseppe and Trematerra, Amelia},
TITLE = {Wind Turbine Noise Prediction Using Random Forest Regression},
JOURNAL = {Machines},
VOLUME = {7},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/2075-1702/7/4/69},
ISSN = {2075-1702},
ABSTRACT = {Wind energy is one of the most widely used renewable energy sources in the world and has grown rapidly in recent years. However, the wind towers generate a noise that is perceived as an annoyance by the population living near the wind farms. It is therefore important to new tools that can help wind farm builders and the administrations. In this study, the measurements of the noise emitted by a wind farm and the data recorded by the supervisory control and data acquisition (SCADA) system were used to construct a prediction model. First, acoustic measurements and control system data have been analyzed to characterize the phenomenon. An appropriate number of observations were then extracted, and these data were pre-processed. Subsequently two models of prediction of sound pressure levels were built at the receiver: a model based on multiple linear regression, and a model based on Random Forest algorithm. As predictors wind speeds measured near the wind turbines and the active power of the turbines were selected. Both data were measured by the SCADA system of wind turbines. The model based on the Random Forest algorithm showed high values of the Pearson correlation coefficient (0.981), indicating a high number of correct predictions. This model can be extremely useful, both for the receiver and for the wind farm manager. Through the results of the model it will be possible to establish for which wind speed values the noise produced by wind turbines become dominant. Furthermore, the predictive model can give an overview of the noise produced by the receiver from the system in different operating conditions. Finally, the prediction model does not require the shutdown of the plant, a very expensive procedure due to the consequent loss of production.},
DOI = {10.3390/machines7040069}
}



@Article{ijgi8110501,
AUTHOR = {Ham, Sungil and Im, Junhyuck and Kim, Minjun and Cho, Kuk},
TITLE = {Construction and Verification of a High-Precision Base Map for an Autonomous Vehicle Monitoring System},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {501},
URL = {https://www.mdpi.com/2220-9964/8/11/501},
ISSN = {2220-9964},
ABSTRACT = {For autonomous driving, a control system that supports precise road maps is required to monitor the operation status of autonomous vehicles in the research stage. Such a system is also required for research related to automobile engineering, sensors, and artificial intelligence. The design of Google Maps and other map services is limited to the provision of map support at 20 levels of high-resolution precision. An ideal map should include information on roads, autonomous vehicles, and Internet of Things (IOT) facilities that support autonomous driving. The aim of this study was to design a map suitable for the control of autonomous vehicles in Gyeonggi Province in Korea. This work was part of the project &ldquo;Building a Testbed for Pilot Operations of Autonomous Vehicles&rdquo;. The map design scheme was redesigned for an autonomous vehicle control system based on the &ldquo;Easy Map&rdquo; developed by the National Geography Center, which provides free design schema. In addition, a vector-based precision map, including roads, sidewalks, and road markings, was produced to provide content suitable for 20 levels. A hybrid map that combines the vector layer of the road and an unmanned aerial vehicle (UAV) orthographic map was designed to facilitate vehicle identification. A control system that can display vehicle and sensor information based on the designed map was developed, and an environment to monitor the operation of autonomous vehicles was established. Finally, the high-precision map was verified through an accuracy test and driving data from autonomous vehicles.},
DOI = {10.3390/ijgi8110501}
}



@Article{s19224837,
AUTHOR = {Samaras, Stamatios and Diamantidou, Eleni and Ataloglou, Dimitrios and Sakellariou, Nikos and Vafeiadis, Anastasios and Magoulianitis, Vasilis and Lalas, Antonios and Dimou, Anastasios and Zarpalas, Dimitrios and Votis, Konstantinos and Daras, Petros and Tzovaras, Dimitrios},
TITLE = {Deep Learning on Multi Sensor Data for Counter UAV Applications—A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4837},
URL = {https://www.mdpi.com/1424-8220/19/22/4837},
ISSN = {1424-8220},
ABSTRACT = {Usage of Unmanned Aerial Vehicles (UAVs) is growing rapidly in a wide range of consumer applications, as they prove to be both autonomous and flexible in a variety of environments and tasks. However, this versatility and ease of use also brings a rapid evolution of threats by malicious actors that can use UAVs for criminal activities, converting them to passive or active threats. The need to protect critical infrastructures and important events from such threats has brought advances in counter UAV (c-UAV) applications. Nowadays, c-UAV applications offer systems that comprise a multi-sensory arsenal often including electro-optical, thermal, acoustic, radar and radio frequency sensors, whose information can be fused to increase the confidence of threat&rsquo;s identification. Nevertheless, real-time surveillance is a cumbersome process, but it is absolutely essential to detect promptly the occurrence of adverse events or conditions. To that end, many challenging tasks arise such as object detection, classification, multi-object tracking and multi-sensor information fusion. In recent years, researchers have utilized deep learning based methodologies to tackle these tasks for generic objects and made noteworthy progress, yet applying deep learning for UAV detection and classification is considered a novel concept. Therefore, the need to present a complete overview of deep learning technologies applied to c-UAV related tasks on multi-sensor data has emerged. The aim of this paper is to describe deep learning advances on c-UAV related tasks when applied to data originating from many different sensors as well as multi-sensor information fusion. This survey may help in making recommendations and improvements of c-UAV applications for the future.},
DOI = {10.3390/s19224837}
}



@Article{s19224851,
AUTHOR = {Zhou, Jun and Tian, Yichen and Yuan, Chao and Yin, Kai and Yang, Guang and Wen, Meiping},
TITLE = {Improved UAV Opium Poppy Detection Using an Updated YOLOv3 Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4851},
URL = {https://www.mdpi.com/1424-8220/19/22/4851},
ISSN = {1424-8220},
ABSTRACT = {Rapid detection of illicit opium poppy plants using UAV (unmanned aerial vehicle) imagery has become an important means to prevent and combat crimes related to drug cultivation. However, current methods rely on time-consuming visual image interpretation. Here, the You Only Look Once version 3 (YOLOv3) network structure was used to assess the influence that different backbone networks have on the average precision and detection speed of an UAV-derived dataset of poppy imagery, with MobileNetv2 (MN) selected as the most suitable backbone network. A Spatial Pyramid Pooling (SPP) unit was introduced and Generalized Intersection over Union (GIoU) was used to calculate the coordinate loss. The resulting SPP-GIoU-YOLOv3-MN model improved the average precision by 1.62% (from 94.75% to 96.37%) without decreasing speed and achieved an average precision of 96.37%, with a detection speed of 29 FPS using an RTX 2080Ti platform. The sliding window method was used for detection in complete UAV images, which took approximately 2.2 sec/image, approximately 10&times; faster than visual interpretation. The proposed technique significantly improved the efficiency of poppy detection in UAV images while also maintaining a high detection accuracy. The proposed method is thus suitable for the rapid detection of illicit opium poppy cultivation in residential areas and farmland where UAVs with ordinary visible light cameras can be operated at low altitudes (relative height &lt; 200 m).},
DOI = {10.3390/s19224851}
}



@Article{app9224826,
AUTHOR = {Tian, Furui and Zhao, Ying and Che, Xiangqian and Zhao, Yagebai and Xin, Dabo},
TITLE = {Concrete Crack Identification and Image Mosaic Based on Image Processing},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4826},
URL = {https://www.mdpi.com/2076-3417/9/22/4826},
ISSN = {2076-3417},
ABSTRACT = {Crack assessment is an essential process in bridge detection. In general, most non-contact crack detection techniques are not suitable for widespread use. The reason for this is that they all need to position the ruler at the inspection site in advance or calibrate the camera unit pixel size at a certain distance in a very intricate process. However, the object distance method in this paper can complete the calculation using only the crack image and the working distance, which are provided by an acquisition system equipped with a camera and laser range finder. First, the object distance method and the scale method are compared by calculating the crack width, and the results show that the object distance method is the more accurate method. Then, a double edge pixel statistical method is proposed to calculate the crack length, which solves the problem of redundant and missing pixels. In addition, the conventional mosaic algorithm is improved to realize an image mosaic for the more efficient splicing of crack images. Finally, a series of laboratory tests were conducted to verify the proposed approach. The experiments showed that the precision of crack length extraction can reach 92%, and the improved algorithm stitching precision can reach 98%.},
DOI = {10.3390/app9224826}
}



@Article{info10110349,
AUTHOR = {Tsouros, Dimosthenis C. and Bibi, Stamatia and Sarigiannidis, Panagiotis G.},
TITLE = {A Review on UAV-Based Applications for Precision Agriculture},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {349},
URL = {https://www.mdpi.com/2078-2489/10/11/349},
ISSN = {2078-2489},
ABSTRACT = {Emerging technologies such as Internet of Things (IoT) can provide significant potential in Smart Farming and Precision Agriculture applications, enabling the acquisition of real-time environmental data. IoT devices such as Unmanned Aerial Vehicles (UAVs) can be exploited in a variety of applications related to crops management, by capturing high spatial and temporal resolution images. These technologies are expected to revolutionize agriculture, enabling decision-making in days instead of weeks, promising significant reduction in cost and increase in the yield. Such decisions enable the effective application of farm inputs, supporting the four pillars of precision agriculture, i.e., apply the right practice, at the right place, at the right time and with the right quantity. However, the actual proliferation and exploitation of UAVs in Smart Farming has not been as robust as expected mainly due to the challenges confronted when selecting and deploying the relevant technologies, including the data acquisition and image processing methods. The main problem is that still there is no standardized workflow for the use of UAVs in such applications, as it is a relatively new area. In this article, we review the most recent applications of UAVs for Precision Agriculture. We discuss the most common applications, the types of UAVs exploited and then we focus on the data acquisition methods and technologies, appointing the benefits and drawbacks of each one. We also point out the most popular processing methods of aerial imagery and discuss the outcomes of each method and the potential applications of each one in the farming operations.},
DOI = {10.3390/info10110349}
}



@Article{rs11222641,
AUTHOR = {Zhao, Longcai and Li, Qiangzi and Zhang, Yuan and Wang, Hongyan and Du, Xin},
TITLE = {Integrating the Continuous Wavelet Transform and a Convolutional Neural Network to Identify Vineyard Using Time Series Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2641},
URL = {https://www.mdpi.com/2072-4292/11/22/2641},
ISSN = {2072-4292},
ABSTRACT = {Grape is an economic crop of great importance and is widely cultivated in China. With the development of remote sensing, abundant data sources strongly guarantee that researchers can identify crop types and map their spatial distributions. However, to date, only a few studies have been conducted to identify vineyards using satellite image data. In this study, a vineyard is identified using satellite images, and a new approach is proposed that integrates the continuous wavelet transform (CWT) and a convolutional neural network (CNN). Specifically, the original time series of the normalized difference vegetation index (NDVI), enhanced vegetation index (EVI), and green chlorophyll vegetation index (GCVI) are reconstructed by applying an iterated Savitzky-Golay (S-G) method to form a daily time series for a full year; then, the CWT is applied to three reconstructed time series to generate corresponding scalograms; and finally, CNN technology is used to identify vineyards based on the stacked scalograms. In addition to our approach, a traditional and common approach that uses a random forest (RF) to identify crop types based on multi-temporal images is selected as the control group. The experimental results demonstrated the following: (i) the proposed approach was comprehensively superior to the RF approach; it improved the overall accuracy by 9.87% (up to 89.66%); (ii) the CWT had a stable and effective influence on the reconstructed time series, and the scalograms fully represented the unique time-related frequency pattern of each of the planting conditions; and (iii) the convolution and max pooling processing of the CNN captured the unique and subtle distribution patterns of the scalograms to distinguish vineyards from other crops. Additionally, the proposed approach is considered as able to be applied to other practical scenarios, such as using time series data to identify crop types, map landcover/land use, and is recommended to be tested in future practical applications.},
DOI = {10.3390/rs11222641}
}



@Article{s19224945,
AUTHOR = {Liu, Wenlei and Wu, Sentang and Wu, Zhongbo and Wu, Xiaolong},
TITLE = {Incremental Pose Map Optimization for Monocular Vision SLAM Based on Similarity Transformation},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4945},
URL = {https://www.mdpi.com/1424-8220/19/22/4945},
ISSN = {1424-8220},
ABSTRACT = {The novel contribution of this paper is to propose an incremental pose map optimization for monocular vision simultaneous localization and mapping (SLAM) based on similarity transformation, which can effectively solve the scale drift problem of SLAM for monocular vision and eliminate the cumulative error by global optimization. With the method of mixed inverse depth estimation based on a probability graph, the problem of the uncertainty of depth estimation is effectively solved and the robustness of depth estimation is improved. Firstly, this paper proposes a method combining the sparse direct method based on histogram equalization and the feature point method for front-end processing, and the mixed inverse depth estimation method based on a probability graph is used to estimate the depth information. Then, a bag-of-words model based on the mean initialization K-means is proposed for closed-loop feature detection. Finally, the incremental pose map optimization method based on similarity transformation is proposed to process the back end to optimize the pose and depth information of the camera. When the closed loop is detected, global optimization is carried out to effectively eliminate the cumulative error of the system. In this paper, indoor and outdoor environmental experiments are carried out using open data sets, such as TUM and KITTI, which fully proves the effectiveness of this method. Closed-loop detection experiments using hand-held cameras verify the importance of closed-loop detection. This method can effectively solve the scale drift problem of monocular vision SLAM and has strong robustness.},
DOI = {10.3390/s19224945}
}



@Article{app9224871,
AUTHOR = {Liu, Quan and Feng, Chen and Song, Zida and Louis, Joseph and Zhou, Jian},
TITLE = {Deep Learning Model Comparison for Vision-Based Classification of Full/Empty-Load Trucks in Earthmoving Operations},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4871},
URL = {https://www.mdpi.com/2076-3417/9/22/4871},
ISSN = {2076-3417},
ABSTRACT = {Earthmoving is an integral civil engineering operation of significance, and tracking its productivity requires the statistics of loads moved by dump trucks. Since current truck loads&rsquo; statistics methods are laborious, costly, and limited in application, this paper presents the framework of a novel, automated, non-contact field earthmoving quantity statistics (FEQS) for projects with large earthmoving demands that use uniform and uncovered trucks. The proposed FEQS framework utilizes field surveillance systems and adopts vision-based deep learning for full/empty-load truck classification as the core work. Since convolutional neural network (CNN) and its transfer learning (TL) forms are popular vision-based deep learning models and numerous in type, a comparison study is conducted to test the framework&rsquo;s core work feasibility and evaluate the performance of different deep learning models in implementation. The comparison study involved 12 CNN or CNN-TL models in full/empty-load truck classification, and the results revealed that while several provided satisfactory performance, the VGG16-FineTune provided the optimal performance. This proved the core work feasibility of the proposed FEQS framework. Further discussion provides model choice suggestions that CNN-TL models are more feasible than CNN prototypes, and models that adopt different TL methods have advantages in either working accuracy or speed for different tasks.},
DOI = {10.3390/app9224871}
}



@Article{app9224886,
AUTHOR = {Tang, Jun and Ma, Xiang and Gu, Ren and Yang, Zhichao and Li, Shi and Yang, Chen and Yang, Bo},
TITLE = {Cost Consensus Algorithm Applications for EV Charging Station Participating in AGC of Interconnected Power Grid},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4886},
URL = {https://www.mdpi.com/2076-3417/9/22/4886},
ISSN = {2076-3417},
ABSTRACT = {In order to more effectively reduce the regulation costs of power grids and to improve the automatic generation control (AGC) performance, an optimization mathematical model of generation command dispatch for AGC with an electric vehicle (EV) charging station is proposed in this paper, in which a cost consensus algorithm for AGC is adopted. Particularly, virtual consensus variables are applied to exchange information among different AGC units. At the same time, the actual consensus variables are utilized to determine the generation command, upon which the flexibility of the proposed algorithm can be significantly enhanced. Furthermore, the implement feasibility of such an algorithm is verified through a series simulation experiments on the Hainan power grid in southern China, where the results demonstrate that the proposed algorithm can effectively realize an autonomous frequency regulation of EVs participating in AGC.},
DOI = {10.3390/app9224886}
}



@Article{f10111025,
AUTHOR = {Shin, Jung-il and Seo, Won-woo and Kim, Taejung and Park, Joowon and Woo, Choong-shik},
TITLE = {Using UAV Multispectral Images for Classification of Forest Burn Severity—A Case Study of the 2019 Gangneung Forest Fire},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1025},
URL = {https://www.mdpi.com/1999-4907/10/11/1025},
ISSN = {1999-4907},
ABSTRACT = {Unmanned aerial vehicle (UAV)-based remote sensing has limitations in acquiring images before a forest fire, although burn severity can be analyzed by comparing images before and after a fire. Determining the burned surface area is a challenging class in the analysis of burn area severity because it looks unburned in images from aircraft or satellites. This study analyzes the availability of multispectral UAV images that can be used to classify burn severity, including the burned surface class. RedEdge multispectral UAV image was acquired after a forest fire, which was then processed into a mosaic reflectance image. Hundreds of samples were collected for each burn severity class, and they were used as training and validation samples for classification. Maximum likelihood (MLH), spectral angle mapper (SAM), and thresholding of a normalized difference vegetation index (NDVI) were used as classifiers. In the results, all classifiers showed high overall accuracy. The classifiers also showed high accuracy for classification of the burned surface, even though there was some confusion among spectrally similar classes, unburned pine, and unburned deciduous. Therefore, multispectral UAV images can be used to analyze burn severity after a forest fire. Additionally, NDVI thresholding can also be an easy and accurate method, although thresholds should be generalized in the future.},
DOI = {10.3390/f10111025}
}



@Article{s19225012,
AUTHOR = {Arshad, Bilal and Ogie, Robert and Barthelemy, Johan and Pradhan, Biswajeet and Verstaevel, Nicolas and Perez, Pascal},
TITLE = {Computer Vision and IoT-Based Sensors in Flood Monitoring and Mapping: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {5012},
URL = {https://www.mdpi.com/1424-8220/19/22/5012},
ISSN = {1424-8220},
ABSTRACT = {Floods are amongst the most common and devastating of all natural hazards. The alarming number of flood-related deaths and financial losses suffered annually across the world call for improved response to flood risks. Interestingly, the last decade has presented great opportunities with a series of scholarly activities exploring how camera images and wireless sensor data from Internet-of-Things (IoT) networks can improve flood management. This paper presents a systematic review of the literature regarding IoT-based sensors and computer vision applications in flood monitoring and mapping. The paper contributes by highlighting the main computer vision techniques and IoT sensor approaches utilised in the literature for real-time flood monitoring, flood modelling, mapping and early warning systems including the estimation of water level. The paper further contributes by providing recommendations for future research. In particular, the study recommends ways in which computer vision and IoT sensor techniques can be harnessed to better monitor and manage coastal lagoons&mdash;an aspect that is under-explored in the literature.},
DOI = {10.3390/s19225012}
}



@Article{fi11110243,
AUTHOR = {Zhang, Wenjie and Wu, Pin and Peng, Yan and Liu, Dongke},
TITLE = {Roll Motion Prediction of Unmanned Surface Vehicle Based on Coupled CNN and LSTM},
JOURNAL = {Future Internet},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {243},
URL = {https://www.mdpi.com/1999-5903/11/11/243},
ISSN = {1999-5903},
ABSTRACT = {The prediction of roll motion in unmanned surface vehicles (USVs) is vital for marine safety and the efficiency of USV operations. However, the USV roll motion at sea is a complex time-varying nonlinear and non-stationary dynamic system, which varies with time-varying environmental disturbances as well as various sailing conditions. The conventional methods have the disadvantages of low accuracy, poor robustness, and insufficient practical application ability. The rise of deep learning provides new opportunities for USV motion modeling and prediction. In this paper, a data-driven neural network model is constructed by combining a convolution neural network (CNN) with long short-term memory (LSTM) for USV roll motion prediction. The CNN is used to extract spatially relevant and local time series features of the USV sensor data. The LSTM layer is exploited to reflect the long-term movement process of the USV and predict roll motion for the next moment. The fully connected layer is utilized to decode the LSTM output and calculate the final prediction results. The effectiveness of the proposed model was proved using USV roll motion prediction experiments based on two case studies from &ldquo;JingHai-VI&rdquo; and &ldquo;JingHai-III&rdquo; USVS of Shanghai University. Experimental results on a real data set indicated that our proposed model obviously outperformed the state-of-the-art methods.},
DOI = {10.3390/fi11110243}
}



@Article{rs11222700,
AUTHOR = {Wang, Wantian and Tang, Ziyue and Chen, Yichang and Zhang, Yuanpeng and Sun, Yongjian},
TITLE = {Aircraft Target Classification for Conventional Narrow-Band Radar with Multi-Wave Gates Sparse Echo Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2700},
URL = {https://www.mdpi.com/2072-4292/11/22/2700},
ISSN = {2072-4292},
ABSTRACT = {For a conventional narrow-band radar system, the detectable information of the target is limited, and it is difficult for the radar to accurately identify the target type. In particular, the classification probability will further decrease when part of the echo data is missed. By extracting the target features in time and frequency domains from multi-wave gates sparse echo data, this paper presents a classification algorithm in conventional narrow-band radar to identify three different types of aircraft target, i.e., helicopter, propeller and jet. Firstly, the classical sparse reconstruction algorithm is utilized to reconstruct the target frequency spectrum with single-wave gate sparse echo data. Then, the micro-Doppler effect caused by rotating parts of different targets is analyzed, and the micro-Doppler based features, such as amplitude deviation coefficient, time domain waveform entropy and frequency domain waveform entropy, are extracted from reconstructed echo data to identify targets. Thirdly, the target features extracted from multi-wave gates reconstructed echo data are weighted and fused to improve the accuracy of classification. Finally, the fused feature vectors are fed into a support vector machine (SVM) model for classification. By contrast with the conventional algorithm of aircraft target classification, the proposed algorithm can effectively process sparse echo data and achieve higher classification probability via weighted features fusion of multi-wave gates echo data. The experiments on synthetic data are carried out to validate the effectiveness of the proposed algorithm.},
DOI = {10.3390/rs11222700}
}



@Article{rs11222701,
AUTHOR = {Zheng, Yuhui and Song, Huihui and Sun, Le and Wu, Zebin and Jeon, Byeungwoo},
TITLE = {Spatiotemporal Fusion of Satellite Images via Very Deep Convolutional Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2701},
URL = {https://www.mdpi.com/2072-4292/11/22/2701},
ISSN = {2072-4292},
ABSTRACT = {Spatiotemporal fusion provides an effective way to fuse two types of remote sensing data featured by complementary spatial and temporal properties (typical representatives are Landsat and MODIS images) to generate fused data with both high spatial and temporal resolutions. This paper presents a very deep convolutional neural network (VDCN) based spatiotemporal fusion approach to effectively handle massive remote sensing data in practical applications. Compared with existing shallow learning methods, especially for the sparse representation based ones, the proposed VDCN-based model has the following merits: (1) explicitly correlating the MODIS and Landsat images by learning a non-linear mapping relationship; (2) automatically extracting effective image features; and (3) unifying the feature extraction, non-linear mapping, and image reconstruction into one optimization framework. In the training stage, we train a non-linear mapping between downsampled Landsat and MODIS data using VDCN, and then we train a multi-scale super-resolution (MSSR) VDCN between the original Landsat and downsampled Landsat data. The prediction procedure contains three layers, where each layer consists of a VDCN-based prediction and a fusion model. These layers achieve non-linear mapping from MODIS to downsampled Landsat data, the two-times SR of downsampled Landsat data, and the five-times SR of downsampled Landsat data, successively. Extensive evaluations are executed on two groups of commonly used Landsat&ndash;MODIS benchmark datasets. For the fusion results, the quantitative evaluations on all prediction dates and the visual effect on one key date demonstrate that the proposed approach achieves more accurate fusion results than sparse representation based methods.},
DOI = {10.3390/rs11222701}
}



@Article{rs11222704,
AUTHOR = {Goian, Abdulrahman and Ashour, Reem and Ahmad, Ubaid and Taha, Tarek and Almoosa, Nawaf and Seneviratne, Lakmal},
TITLE = {Victim Localization in USAR Scenario Exploiting Multi-Layer Mapping Structure},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2704},
URL = {https://www.mdpi.com/2072-4292/11/22/2704},
ISSN = {2072-4292},
ABSTRACT = {Urban search and rescue missions require rapid intervention to locate victims and survivors in the affected environments. To facilitate this activity, Unmanned Aerial Vehicles (UAVs) have been recently used to explore the environment and locate possible victims. In this paper, a UAV equipped with multiple complementary sensors is used to detect the presence of a human in an unknown environment. A novel human localization approach in unknown environments is proposed that merges information gathered from deep-learning-based human detection, wireless signal mapping, and thermal signature mapping to build an accurate global human location map. A next-best-view (NBV) approach with a proposed multi-objective utility function is used to iteratively evaluate the map to locate the presence of humans rapidly. Results demonstrate that the proposed strategy outperforms other methods in several performance measures such as the number of iterations, entropy reduction, and traveled distance.},
DOI = {10.3390/rs11222704}
}



@Article{s19225046,
AUTHOR = {Huang, Lvwen and Guo, Han and Rao, Qinqin and Hou, Zixia and Li, Shuqin and Qiu, Shicheng and Fan, Xinyun and Wang, Hongyan},
TITLE = {Body Dimension Measurements of Qinchuan Cattle with Transfer Learning from LiDAR Sensing},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {5046},
URL = {https://www.mdpi.com/1424-8220/19/22/5046},
ISSN = {1424-8220},
ABSTRACT = {For the time-consuming and stressful body measuring task of Qinchuan cattle and farmers, the demand for the automatic measurement of body dimensions has become more and more urgent. It is necessary to explore automatic measurements with deep learning to improve breeding efficiency and promote the development of industry. In this paper, a novel approach to measuring the body dimensions of live Qinchuan cattle with on transfer learning is proposed. Deep learning of the Kd-network was trained with classical three-dimensional (3D) point cloud datasets (PCD) of the ShapeNet datasets. After a series of processes of PCD sensed by the light detection and ranging (LiDAR) sensor, the cattle silhouettes could be extracted, which after augmentation could be applied as an input layer to the Kd-network. With the output of a convolutional layer of the trained deep model, the output layer of the deep model could be applied to pre-train the full connection network. The TrAdaBoost algorithm was employed to transfer the pre-trained convolutional layer and full connection of the deep model. To classify and recognize the PCD of the cattle silhouette, the average accuracy rate after training with transfer learning could reach up to 93.6%. On the basis of silhouette extraction, the candidate region of the feature surface shape could be extracted with mean curvature and Gaussian curvature. After the computation of the FPFH (fast point feature histogram) of the surface shape, the center of the feature surface could be recognized and the body dimensions of the cattle could finally be calculated. The experimental results showed that the comprehensive error of body dimensions was close to 2%, which could provide a feasible approach to the non-contact observations of the bodies of large physique livestock without any human intervention.},
DOI = {10.3390/s19225046}
}



@Article{f10111047,
AUTHOR = {Sun, Ying and Huang, Jianfeng and Ao, Zurui and Lao, Dazhao and Xin, Qinchuan},
TITLE = {Deep Learning Approaches for the Mapping of Tree Species Diversity in a Tropical Wetland Using Airborne LiDAR and High-Spatial-Resolution Remote Sensing Images},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1047},
URL = {https://www.mdpi.com/1999-4907/10/11/1047},
ISSN = {1999-4907},
ABSTRACT = {The monitoring of tree species diversity is important for forest or wetland ecosystem service maintenance or resource management. Remote sensing is an efficient alternative to traditional field work to map tree species diversity over large areas. Previous studies have used light detection and ranging (LiDAR) and imaging spectroscopy (hyperspectral or multispectral remote sensing) for species richness prediction. The recent development of very high spatial resolution (VHR) RGB images has enabled detailed characterization of canopies and forest structures. In this study, we developed a three-step workflow for mapping tree species diversity, the aim of which was to increase knowledge of tree species diversity assessment using deep learning in a tropical wetland (Haizhu Wetland) in South China based on VHR-RGB images and LiDAR points. Firstly, individual trees were detected based on a canopy height model (CHM, derived from LiDAR points) by the local-maxima-based method in the FUSION software (Version 3.70, Seattle, USA). Then, tree species at the individual tree level were identified via a patch-based image input method, which cropped the RGB images into small patches (the individually detected trees) based on the tree apexes detected. Three different deep learning methods (i.e., AlexNet, VGG16, and ResNet50) were modified to classify the tree species, as they can make good use of the spatial context information. Finally, four diversity indices, namely, the Margalef richness index, the Shannon&ndash;Wiener diversity index, the Simpson diversity index, and the Pielou evenness index, were calculated from the fixed subset with a size of 30 &times; 30 m for assessment. In the classification phase, VGG16 had the best performance, with an overall accuracy of 73.25% for 18 tree species. Based on the classification results, mapping of tree species diversity showed reasonable agreement with field survey data (R2Margalef = 0.4562, root-mean-square error RMSEMargalef = 0.5629; R2Shannon&ndash;Wiener = 0.7948, RMSEShannon&ndash;Wiener = 0.7202; R2Simpson = 0.7907, RMSESimpson = 0.1038; and R2Pielou = 0.5875, RMSEPielou = 0.3053). While challenges remain for individual tree detection and species classification, the deep-learning-based solution shows potential for mapping tree species diversity.},
DOI = {10.3390/f10111047}
}



@Article{s19235112,
AUTHOR = {Wu, Hao and Dai, Dahai and Wang, Xuesong},
TITLE = {A Novel Radar HRRP Recognition Method with Accelerated T-Distributed Stochastic Neighbor Embedding and Density-Based Clustering},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5112},
URL = {https://www.mdpi.com/1424-8220/19/23/5112},
ISSN = {1424-8220},
ABSTRACT = {High-resolution range profile (HRRP) has attracted intensive attention from radar community because it is easy to acquire and analyze. However, most of the conventional algorithms require the prior information of targets, and they cannot process a large number of samples in real time. In this paper, a novel HRRP recognition method is proposed to classify unlabeled samples automatically where the number of categories is unknown. Firstly, with the preprocessing of HRRPs, we adopt principal component analysis (PCA) for dimensionality reduction of data. Afterwards, t-distributed stochastic neighbor embedding (t-SNE) with Barnes&ndash;Hut approximation is conducted for the visualization of high-dimensional data. It proves to reduce the dimensionality, which has significantly improved the computation speed. Finally, it is exhibited that the recognition performance with density-based clustering is superior to conventional algorithms under the condition of large azimuth angle ranges and low signal-to-noise ratio (SNR).},
DOI = {10.3390/s19235112}
}



@Article{app9235062,
AUTHOR = {Bazi, Yakoub and Alhichri, Haikel and Alajlan, Naif and Melgani, Farid},
TITLE = {Scene Description for Visually Impaired People with Multi-Label Convolutional SVM Networks},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5062},
URL = {https://www.mdpi.com/2076-3417/9/23/5062},
ISSN = {2076-3417},
ABSTRACT = {In this paper, we present a portable camera-based method for helping visually impaired (VI) people to recognize multiple objects in images. This method relies on a novel multi-label convolutional support vector machine (CSVM) network for coarse description of images. The core idea of CSVM is to use a set of linear SVMs as filter banks for feature map generation. During the training phase, the weights of the SVM filters are obtained using a forward-supervised learning strategy unlike the backpropagation algorithm used in standard convolutional neural networks (CNNs). To handle multi-label detection, we introduce a multi-branch CSVM architecture, where each branch will be used for detecting one object in the image. This architecture exploits the correlation between the objects present in the image by means of an opportune fusion mechanism of the intermediate outputs provided by the convolution layers of each branch. The high-level reasoning of the network is done through binary classification SVMs for predicting the presence/absence of objects in the image. The experiments obtained on two indoor datasets and one outdoor dataset acquired from a portable camera mounted on a lightweight shield worn by the user, and connected via a USB wire to a laptop processing unit are reported and discussed.},
DOI = {10.3390/app9235062}
}



@Article{rs11232765,
AUTHOR = {Nex, Francesco and Duarte, Diogo and Tonolo, Fabio Giulio and Kerle, Norman},
TITLE = {Structural Building Damage Detection with Deep Learning: Assessment of a State-of-the-Art CNN in Operational Conditions},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2765},
URL = {https://www.mdpi.com/2072-4292/11/23/2765},
ISSN = {2072-4292},
ABSTRACT = {Remotely sensed data can provide the basis for timely and efficient building damage maps that are of fundamental importance to support the response activities following disaster events. However, the generation of these maps continues to be mainly based on the manual extraction of relevant information in operational frameworks. Considering the identification of visible structural damages caused by earthquakes and explosions, several recent works have shown that Convolutional Neural Networks (CNN) outperform traditional methods. However, the limited availability of publicly available image datasets depicting structural disaster damages, and the wide variety of sensors and spatial resolution used for these acquisitions (from space, aerial and UAV platforms), have limited the clarity of how these networks can effectively serve First Responder needs and emergency mapping service requirements. In this paper, an advanced CNN for visible structural damage detection is tested to shed some light on what deep learning networks can currently deliver, and its adoption in realistic operational conditions after earthquakes and explosions is critically discussed. The heterogeneous and large datasets collected by the authors covering different locations, spatial resolutions and platforms were used to assess the network performances in terms of transfer learning with specific regard to geographical transferability of the trained network to imagery acquired in different locations. The computational time needed to deliver these maps is also assessed. Results show that quality metrics are influenced by the composition of training samples used in the network. To promote their wider use, three pre-trained networks&mdash;optimized for satellite, airborne and UAV image spatial resolutions and viewing angles&mdash;are made freely available to the scientific community.},
DOI = {10.3390/rs11232765}
}



@Article{s19235149,
AUTHOR = {Gao, Bingbing and Hu, Gaoge and Zhu, Xinhe and Zhong, Yongmin},
TITLE = {A Robust Cubature Kalman Filter with Abnormal Observations Identification Using the Mahalanobis Distance Criterion for Vehicular INS/GNSS Integration},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5149},
URL = {https://www.mdpi.com/1424-8220/19/23/5149},
ISSN = {1424-8220},
ABSTRACT = {INS/GNSS (inertial navigation system/global navigation satellite system) integration is a promising solution of vehicle navigation for intelligent transportation systems. However, the observation of GNSS inevitably involves uncertainty due to the vulnerability to signal blockage in many urban/suburban areas, leading to the degraded navigation performance for INS/GNSS integration. This paper develops a novel robust CKF with scaling factor by combining the emerging cubature Kalman filter (CKF) with the concept of Mahalanobis distance criterion to address the above problem involved in nonlinear INS/GNSS integration. It establishes a theory of abnormal observations identification using the Mahalanobis distance criterion. Subsequently, a robust factor (scaling factor), which is calculated via the Mahalanobis distance criterion, is introduced into the standard CKF to inflate the observation noise covariance, resulting in a decreased filtering gain in the presence of abnormal observations. The proposed robust CKF can effectively resist the influence of abnormal observations on navigation solution and thus improves the robustness of CKF for vehicular INS/GNSS integration. Simulation and experimental results have demonstrated the effectiveness of the proposed robust CKF for vehicular navigation with INS/GNSS integration.},
DOI = {10.3390/s19235149}
}



@Article{rs11232787,
AUTHOR = {Zhang, Xiaokang and Shi, Wenzhong and Lv, Zhiyong and Peng, Feifei},
TITLE = {Land Cover Change Detection from High-Resolution Remote Sensing Imagery Using Multitemporal Deep Feature Collaborative Learning and a Semi-supervised Chan–Vese Model},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2787},
URL = {https://www.mdpi.com/2072-4292/11/23/2787},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a novel approach for automatically detecting land cover changes from multitemporal high-resolution remote sensing images in the deep feature space. This is accomplished by using multitemporal deep feature collaborative learning and a semi-supervised Chan&ndash;Vese (SCV) model. The multitemporal deep feature collaborative learning model is developed to obtain the multitemporal deep feature representations in the same high-level feature space and to improve the separability between changed and unchanged patterns. The deep difference feature map at the object-level is then extracted through a feature similarity measure. Based on the deep difference feature map, the SCV model is proposed to detect changes in which labeled patterns automatically derived from uncertainty analysis are integrated into the energy functional to efficiently drive the contour towards accurate boundaries of changed objects. The experimental results obtained on the four data sets acquired by different high-resolution sensors corroborate the effectiveness of the proposed approach.},
DOI = {10.3390/rs11232787}
}



@Article{s19235170,
AUTHOR = {Bithas, Petros S. and Michailidis, Emmanouel T. and Nomikos, Nikolaos and Vouyioukas, Demosthenes and Kanatas, Athanasios G.},
TITLE = {A Survey on Machine-Learning Techniques for UAV-Based Communications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5170},
URL = {https://www.mdpi.com/1424-8220/19/23/5170},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) will be an integral part of the next generation wireless communication networks. Their adoption in various communication-based applications is expected to improve coverage and spectral efficiency, as compared to traditional ground-based solutions. However, this new degree of freedom that will be included in the network will also add new challenges. In this context, the machine-learning (ML) framework is expected to provide solutions for the various problems that have already been identified when UAVs are used for communication purposes. In this article, we provide a detailed survey of all relevant research works, in which ML techniques have been used on UAV-based communications for improving various design and functional aspects such as channel modeling, resource management, positioning, and security.},
DOI = {10.3390/s19235170}
}



@Article{rs11232828,
AUTHOR = {Zhao, Nan and Ma, Ailong and Zhong, Yanfei and Zhao, Ji and Cao, Liqin},
TITLE = {Self-Training Classification Framework with Spatial-Contextual Information for Local Climate Zones},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2828},
URL = {https://www.mdpi.com/2072-4292/11/23/2828},
ISSN = {2072-4292},
ABSTRACT = {Local climate zones (LCZ) have become a generic criterion for climate analysis among global cities, as they can describe not only the urban climate but also the morphology inside the city. LCZ mapping based on the remote sensing classification method is a fundamental task, and the protocol proposed by the World Urban Database and Access Portal Tools (WUDAPT) project, which consists of random forest classification and filter-based spatial smoothing, is the most common approach. However, the classification and spatial smoothing lack a unified framework, which causes the appearance of small, isolated areas in the LCZ maps. In this paper, a spatial-contextual information-based self-training classification framework (SCSF) is proposed to solve this LCZ classification problem. In SCSF, conditional random field (CRF) is used to integrate the classification and spatial smoothing processing into one model and a self-training method is adopted, considering that the lack of sufficient expert-labeled training samples is always a big issue, especially for the complex LCZ scheme. Moreover, in the unary potentials of CRF modeling, pseudo-label selection using a self-training process is used to train the classifier, which fuses the regional spatial information through segmentation and the local neighborhood information through moving windows to provide a more reliable probabilistic classification map. In the pairwise potential function, SCSF can effectively improve the classification accuracy by integrating the spatial-contextual information through CRF. The experimental results prove that the proposed framework is efficient when compared to the traditional mapping product of WUDAPT in LCZ classification.},
DOI = {10.3390/rs11232828}
}



@Article{app9235184,
AUTHOR = {Xuan-Mung, Nguyen and Hong, Sung Kyung},
TITLE = {Robust Backstepping Trajectory Tracking Control of a Quadrotor with Input Saturation via Extended State Observer},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5184},
URL = {https://www.mdpi.com/2076-3417/9/23/5184},
ISSN = {2076-3417},
ABSTRACT = {Quadrotor unmanned aerial vehicles have become increasingly popular in several applications, and the improvement of their control performance has been documented in several studies. Nevertheless, the design of a high-performance tracking controller for aerial vehicles that reliably functions in the simultaneous presence of model uncertainties, external disturbances, and control input saturation still remains a challenge. In this paper, we present a robust backstepping trajectory tracking control of a quadrotor with input saturation. The controller design accounts for both parameterized uncertainties and external disturbances, whereas a new auxiliary system is proposed to cope with control input saturation. Taking into account that only the position and attitude of the quadrotor are measurable, we devise an extended state observer to supply the estimations of unmeasured states, model uncertainties, and external disturbances. We strictly prove the stability of the closed-loop system by using the Lyapunov theory and demonstrate the effectiveness of the proposed algorithm through numerical simulations.},
DOI = {10.3390/app9235184}
}



@Article{s19235270,
AUTHOR = {Wang, Yantian and Li, Haifeng and Jia, Peng and Zhang, Guo and Wang, Taoyang and Hao, Xiaoyun},
TITLE = {Multi-Scale DenseNets-Based Aircraft Detection from Remote Sensing Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5270},
URL = {https://www.mdpi.com/1424-8220/19/23/5270},
ISSN = {1424-8220},
ABSTRACT = {Deep learning-based aircraft detection methods have been increasingly implemented in recent years. However, due to the multi-resolution imaging modes, aircrafts in different images show very wide diversity on size, view and other visual features, which brings great challenges to detection. Although standard deep convolution neural networks (DCNN) can extract rich semantic features, they destroy the bottom-level location information. The features of small targets may also be submerged by redundant top-level features, resulting in poor detection. To address these problems, we proposed a compact multi-scale dense convolutional neural network (MS-DenseNet) for aircraft detection in remote sensing images. Herein, DenseNet was utilized for feature extraction, which enhances the propagation and reuse of the bottom-level high-resolution features. Subsequently, we combined feature pyramid network (FPN) with DenseNet to form a MS-DenseNet for learning multi-scale features, especially features of small objects. Finally, by compressing some of the unnecessary convolution layers of each dense block, we designed three new compact architectures: MS-DenseNet-41, MS-DenseNet-65, and MS-DenseNet-77. Comparative experiments showed that the compact MS-DenseNet-65 obtained a noticeable improvement in detecting small aircrafts and achieved state-of-the-art performance with a recall of 94% and an F1-score of 92.7% and cost less computational time. Furthermore, the experimental results on robustness of UCAS-AOD and RSOD datasets also indicate the good transferability of our method.},
DOI = {10.3390/s19235270}
}



@Article{s19235284,
AUTHOR = {Zhang, Heng and Wu, Jiayu and Liu, Yanli and Yu, Jia},
TITLE = {VaryBlock: A Novel Approach for Object Detection in Remote Sensed Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5284},
URL = {https://www.mdpi.com/1424-8220/19/23/5284},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the research on optical remote sensing images has received greater and greater attention. Object detection, as one of the most challenging tasks in the area of remote sensing, has been remarkably promoted by convolutional neural network (CNN)-based methods like You Only Look Once (YOLO) and Faster R-CNN. However, due to the complexity of backgrounds and the distinctive object distribution, directly applying these general object detection methods to the remote sensing object detection usually renders poor performance. To tackle this problem, a highly efficient and robust framework based on YOLO is proposed. We devise and integrate VaryBlock to the architecture which effectively offsets some of the information loss caused by downsampling. In addition, some techniques are utilized to facilitate the performance and to avoid overfitting. Experimental results show that our proposed method can enormously improve the mean average precision by a large margin on the NWPU VHR-10 dataset.},
DOI = {10.3390/s19235284}
}



@Article{s19235287,
AUTHOR = {Moreno-Armendáriz, Marco A. and Calvo, Hiram and Duchanoy, Carlos A. and López-Juárez, Anayantzin P. and Vargas-Monroy, Israel A. and Suarez-Castañon, Miguel Santiago},
TITLE = {Deep Green Diagnostics: Urban Green Space Analysis Using Deep Learning and Drone Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5287},
URL = {https://www.mdpi.com/1424-8220/19/23/5287},
ISSN = {1424-8220},
ABSTRACT = {Nowadays, more than half of the world’s population lives in urban areas, and this number continues increasing. Consequently, there are more and more scientific publications that analyze health problems of people associated with living in these highly urbanized locations. In particular, some of the recent work has focused on relating people’s health to the quality and quantity of urban green areas. In this context, and considering the huge amount of land area in large cities that must be supervised, our work seeks to develop a deep learning-based solution capable of determining the level of health of the land and to assess whether it is contaminated. The main purpose is to provide health institutions with software capable of creating updated maps that indicate where these phenomena are presented, as this information could be very useful to guide public health goals in large cities. Our software is released as open source code, and the data used for the experiments presented in this paper are also freely available.},
DOI = {10.3390/s19235287}
}



@Article{jmse7120438,
AUTHOR = {Wang, Le and Wu, Qing and Liu, Jialun and Li, Shijie and Negenborn, Rudy R.},
TITLE = {State-of-the-Art Research on Motion Control of Maritime Autonomous Surface Ships},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {7},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {438},
URL = {https://www.mdpi.com/2077-1312/7/12/438},
ISSN = {2077-1312},
ABSTRACT = {At present, with the development of waterborne transport vehicles, research on ship faces a new round of challenges in terms of intelligence and autonomy. The concept of maritime autonomous surface ships (MASS) has been put forward by the International Maritime Organization in 2017, in which MASS become the new focus of the waterborne transportation industry. This paper elaborates on the state-of-the-art research on motion control of MASS. Firstly, the characteristics and current research status of unmanned surface vessels in MASS and conventional ships are summarized, and the system composition of MASS is analyzed. In order to better realize the self-adaptability of the MASS motion control, the theory and algorithm of ship motion control-related systems are emphatically analyzed under the condition of classifying ship motion control. Especially, the application of intelligent algorithms in the ship control field is summarized and analyzed. Finally, this paper summarizes the challenges faced by MASS in the model establishment, motion control algorithms, and real ship experiments, and proposes the composition of MASS motion control system based on variable autonomous control strategy. Future researches on the accuracy and diversity of developments and applications to MASS motion control are suggested.},
DOI = {10.3390/jmse7120438}
}



@Article{rs11232858,
AUTHOR = {Ci, Tianyu and Liu, Zhen and Wang, Ying},
TITLE = {Assessment of the Degree of Building Damage Caused by Disaster Using Convolutional Neural Networks in Combination with Ordinal Regression},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2858},
URL = {https://www.mdpi.com/2072-4292/11/23/2858},
ISSN = {2072-4292},
ABSTRACT = {We propose a new convolutional neural networks method in combination with ordinal regression aiming at assessing the degree of building damage caused by earthquakes with aerial imagery. The ordinal regression model and a deep learning algorithm are incorporated to make full use of the information to improve the accuracy of the assessment. A new loss function was introduced in this paper to combine convolutional neural networks and ordinal regression. Assessing the level of damage to buildings can be considered as equivalent to predicting the ordered labels of buildings to be assessed. In the existing research, the problem has usually been simplified as a problem of pure classification to be further studied and discussed, which ignores the ordinal relationship between different levels of damage, resulting in a waste of information. Data accumulated throughout history are used to build network models for assessing the level of damage, and models for assessing levels of damage to buildings based on deep learning are described in detail, including model construction, implementation methods, and the selection of hyperparameters, and verification is conducted by experiments. When categorizing the damage to buildings into four types, we apply the method proposed in this paper to aerial images acquired from the 2014 Ludian earthquake and achieve an overall accuracy of 77.39%; when categorizing damage to buildings into two types, the overall accuracy of the model is 93.95%, exceeding such values in similar types of theories and methods.},
DOI = {10.3390/rs11232858}
}



@Article{rs11232873,
AUTHOR = {Kayad, Ahmed and Sozzi, Marco and Gatto, Simone and Marinello, Francesco and Pirotti, Francesco},
TITLE = {Monitoring Within-Field Variability of Corn Yield using Sentinel-2 and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2873},
URL = {https://www.mdpi.com/2072-4292/11/23/2873},
ISSN = {2072-4292},
ABSTRACT = {Monitoring and prediction of within-field crop variability can support farmers to make the right decisions in different situations. The current advances in remote sensing and the availability of high resolution, high frequency, and free Sentinel-2 images improve the implementation of Precision Agriculture (PA) for a wider range of farmers. This study investigated the possibility of using vegetation indices (VIs) derived from Sentinel-2 images and machine learning techniques to assess corn (Zea mays) grain yield spatial variability within the field scale. A 22-ha study field in North Italy was monitored between 2016 and 2018; corn yield was measured and recorded by a grain yield monitor mounted on the harvester machine recording more than 20,000 georeferenced yield observation points from the study field for each season. VIs from a total of 34 Sentinel-2 images at different crop ages were analyzed for correlation with the measured yield observations. Multiple regression and two different machine learning approaches were also tested to model corn grain yield. The three main results were the following: (i) the Green Normalized Difference Vegetation Index (GNDVI) provided the highest R2 value of 0.48 for monitoring within-field variability of corn grain yield; (ii) the most suitable period for corn yield monitoring was a crop age between 105 and 135 days from the planting date (R4&ndash;R6); (iii) Random Forests was the most accurate machine learning approach for predicting within-field variability of corn yield, with an R2 value of almost 0.6 over an independent validation set of half of the total observations. Based on the results, within-field variability of corn yield for previous seasons could be investigated from archived Sentinel-2 data with GNDVI at crop stage (R4&ndash;R6).},
DOI = {10.3390/rs11232873}
}



@Article{s19245364,
AUTHOR = {Nagy, Balázs and Botzheim, János and Korondi, Péter},
TITLE = {Magnetic Angular Rate and Gravity Sensor Based Supervised Learning for Positioning Tasks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5364},
URL = {https://www.mdpi.com/1424-8220/19/24/5364},
ISSN = {1424-8220},
ABSTRACT = {This paper deals with sensor fusion of magnetic, angular rate and gravity sensor (MARG). The main contribution of this paper is the sensor fusion performed by supervised learning, which means parallel processing of the different kinds of measured data and estimating the position in periodic and non-periodic cases. During the learning phase, the position estimated by sensor fusion is compared with position data of a motion capture system. The main challenge is avoiding the error caused by the implicit integral calculation of MARG. There are several filter based signal processing methods for disturbance and noise estimation, which are calculated for each sensor separately. These classical methods can be used for disturbance and noise reduction and extracting hidden information from it as well. This paper examines the different types of noises and proposes a machine learning-based method for calculation of position and orientation directly from nine separate sensors. This method includes the disturbance and noise reduction in addition to sensor fusion. The proposed method was validated by experiments which provided promising results on periodic and translational motion as well.},
DOI = {10.3390/s19245364}
}



@Article{rs11242906,
AUTHOR = {Bazine, Razika and Wu, Huayi and Boukhechba, Kamel},
TITLE = {Spectral DWT Multilevel Decomposition with Spatial Filtering Enhancement Preprocessing-Based Approaches for Hyperspectral Imagery Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2906},
URL = {https://www.mdpi.com/2072-4292/11/24/2906},
ISSN = {2072-4292},
ABSTRACT = {In this paper, spectral&ndash;spatial preprocessing using discrete wavelet transform (DWT) multilevel decomposition and spatial filtering is proposed for improving the accuracy of hyperspectral imagery classification. Specifically, spectral DWT multilevel decomposition (SDWT) is performed on the hyperspectral image to separate the approximation coefficients from the detail coefficients. For each level of decomposition, only the detail coefficients are spatially filtered instead of being discarded, as is often adopted by the wavelet-based approaches. Thus, three different spatial filters are explored, including two-dimensional DWT (2D-DWT), adaptive Wiener filter (AWF), and two-dimensional discrete cosine transform (2D-DCT). After the enhancement of the spectral information by performing the spatial filter on the detail coefficients, DWT reconstruction is carried out on both the approximation and the filtered detail coefficients. The final preprocessed image is fed into a linear support vector machine (SVM) classifier. Evaluation results on three widely used real hyperspectral datasets show that the proposed framework using spectral DWT multilevel decomposition with 2D-DCT filter (SDWT-2DCT_SVM) exhibits a significant performance and outperforms many state-of-the-art methods in terms of classification accuracy, even under the constraint of small training sample size, and execution time.},
DOI = {10.3390/rs11242906}
}



@Article{rs11242912,
AUTHOR = {Liu, Wei and Yang, MengYuan and Xie, Meng and Guo, Zihui and Li, ErZhu and Zhang, Lianpeng and Pei, Tao and Wang, Dong},
TITLE = {Accurate Building Extraction from Fused DSM and UAV Images Using a Chain Fully Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2912},
URL = {https://www.mdpi.com/2072-4292/11/24/2912},
ISSN = {2072-4292},
ABSTRACT = {Accurate extraction of buildings using high spatial resolution imagery is essential to a wide range of urban applications. However, it is difficult to extract semantic features from a variety of complex scenes (e.g., suburban, urban and urban village areas) because various complex man-made objects usually appear heterogeneous with large intra-class and low inter-class variations. The automatic extraction of buildings is thus extremely challenging. The fully convolutional neural networks (FCNs) developed in recent years have performed well in the extraction of urban man-made objects due to their ability to learn state-of-the-art features and to label pixels end-to-end. One of the most successful FCNs used in building extraction is U-net. However, the commonly used skip connection and feature fusion refinement modules in U-net often ignore the problem of feature selection, and the ability to extract smaller buildings and refine building boundaries needs to be improved. In this paper, we propose a trainable chain fully convolutional neural network (CFCN), which fuses high spatial resolution unmanned aerial vehicle (UAV) images and the digital surface model (DSM) for building extraction. Multilevel features are obtained from the fusion data, and an improved U-net is used for the coarse extraction of the building. To solve the problem of incomplete extraction of building boundaries, a U-net network is introduced by chain, which is used for the introduction of a coarse building boundary constraint, hole filling, and "speckle" removal. Typical areas such as suburban, urban, and urban villages were selected for building extraction experiments. The results show that the CFCN achieved recall of 98.67%, 98.62%, and 99.52% and intersection over union (IoU) of 96.23%, 96.43%, and 95.76% in suburban, urban, and urban village areas, respectively. Considering the IoU in conjunction with the CFCN and U-net resulted in improvements of 6.61%, 5.31%, and 6.45% in suburban, urban, and urban village areas, respectively. The proposed method can extract buildings with higher accuracy and with clearer and more complete boundaries.},
DOI = {10.3390/rs11242912}
}



@Article{s19245373,
AUTHOR = {Su, Jingxin and Miyazaki, Ryuji and Tamaki, Toru and Kaneda, Kazufumi},
TITLE = {High-Resolution Representation for Mobile Mapping Data in Curved Regular Grid Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5373},
URL = {https://www.mdpi.com/1424-8220/19/24/5373},
ISSN = {1424-8220},
ABSTRACT = {As mobile mapping systems become a mature technology, there are many applications for the process of the measured data. One interesting application is the use of driving simulators that can be used to analyze the data of tire vibration or vehicle simulations. In previous research, we presented our proposed method that can create a precise three-dimensional point cloud model of road surface regions and trajectory points. Our data sets were obtained by a vehicle-mounted mobile mapping system (MMS). The collected data were converted into point cloud data and color images. In this paper, we utilize the previous results as input data and present a solution that can generate an elevation grid for building an OpenCRG model. The OpenCRG project was originally developed to describe road surface elevation data, and also defined an open file format. As it can be difficult to generate a regular grid from point cloud directly, the road surface is first divided into straight lines, circular arcs, and and clothoids. Secondly, a non-regular grid which contains the elevation of road surface points is created for each road surface segment. Then, a regular grid is generated by accurately interpolating the elevation values from the non-regular grid. Finally, the curved regular grid (CRG) model files are created based on the above procedures, and can be visualized by OpenCRG tools. The experimental results on real-world data show that the proposed approach provided a very-high-resolution road surface elevation model.},
DOI = {10.3390/s19245373}
}



@Article{rs11242925,
AUTHOR = {Prado Osco, Lucas and Marques Ramos, Ana Paula and Roberto Pereira, Danilo and Akemi Saito Moriya, Érika and Nobuhiro Imai, Nilton and Takashi Matsubara, Edson and Estrabis, Nayara and de Souza, Maurício and Marcato Junior, José and Gonçalves, Wesley Nunes and Li, Jonathan and Liesenberg, Veraldo and Eduardo Creste, José},
TITLE = {Predicting Canopy Nitrogen Content in Citrus-Trees Using Random Forest Algorithm Associated to Spectral Vegetation Indices from UAV-Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2925},
URL = {https://www.mdpi.com/2072-4292/11/24/2925},
ISSN = {2072-4292},
ABSTRACT = {The traditional method of measuring nitrogen content in plants is a time-consuming and labor-intensive task. Spectral vegetation indices extracted from unmanned aerial vehicle (UAV) images and machine learning algorithms have been proved effective in assisting nutritional analysis in plants. Still, this analysis has not considered the combination of spectral indices and machine learning algorithms to predict nitrogen in tree-canopy structures. This paper proposes a new framework to infer the nitrogen content in citrus-tree at a canopy-level using spectral vegetation indices processed with the random forest algorithm. A total of 33 spectral indices were estimated from multispectral images acquired with a UAV-based sensor. Leaf samples were gathered from different planting-fields and the leaf nitrogen content (LNC) was measured in the laboratory, and later converted into the canopy nitrogen content (CNC). To evaluate the robustness of the proposed framework, we compared it with other machine learning algorithms. We used 33,600 citrus trees to evaluate the performance of the machine learning models. The random forest algorithm had higher performance in predicting CNC than all models tested, reaching an R2 of 0.90, MAE of 0.341 g&middot;kg&minus;1 and MSE of 0.307 g&middot;kg&minus;1. We demonstrated that our approach is able to reduce the need for chemical analysis of the leaf tissue and optimizes citrus orchard CNC monitoring.},
DOI = {10.3390/rs11242925}
}



@Article{electronics8121504,
AUTHOR = {Zhou, Yu and Wu, Chunxue and Wu, Qunhui and Eli, Zelda Makati and Xiong, Naixue and Zhang, Sheng},
TITLE = {Design and Analysis of Refined Inspection of Field Conditions of Oilfield Pumping Wells Based on Rotorcraft UAV Technology},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1504},
URL = {https://www.mdpi.com/2079-9292/8/12/1504},
ISSN = {2079-9292},
ABSTRACT = {The traditional oil well monitoring method relies on manual acquisition and various high-precision sensors. Using the indicator diagram to judge the working condition of the well is not only difficult to establish but also consumes huge manpower and financial resources. This paper proposes the use of computer vision in the detection of working conditions in oil extraction. Combined with the advantages of an unmanned aerial vehicle (UAV), UAV aerial photography images are used to realize real-time detection of on-site working conditions by real-time tracking of the working status of the head working and other related parts of the pumping unit. Considering the real-time performance of working condition detection, this paper proposes a framework that combines You only look once version 3 (YOLOv3) and a sort algorithm to complete multi-target tracking in the form of tracking by detection. The quality of the target detection in the framework is the key factor affecting the tracking effect. The experimental results show that a good detector makes the tracking speed achieve the real-time effect and provides help for the real-time detection of the working condition, which has a strong practical application.},
DOI = {10.3390/electronics8121504}
}



@Article{s19245436,
AUTHOR = {Barbedo, Jayme Garcia Arnal and Koenigkan, Luciano Vieira and Santos, Thiago Teixeira and Santos, Patrícia Menezes},
TITLE = {A Study on the Detection of Cattle in UAV Images Using Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/1424-8220/19/24/5436},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are being increasingly viewed as valuable tools to aid the management of farms. This kind of technology can be particularly useful in the context of extensive cattle farming, as production areas tend to be expansive and animals tend to be more loosely monitored. With the advent of deep learning, and convolutional neural networks (CNNs) in particular, extracting relevant information from aerial images has become more effective. Despite the technological advancements in drone, imaging and machine learning technologies, the application of UAVs for cattle monitoring is far from being thoroughly studied, with many research gaps still remaining. In this context, the objectives of this study were threefold: (1) to determine the highest possible accuracy that could be achieved in the detection of animals of the Canchim breed, which is visually similar to the Nelore breed (Bos taurus indicus); (2) to determine the ideal ground sample distance (GSD) for animal detection; (3) to determine the most accurate CNN architecture for this specific problem. The experiments involved 1853 images containing 8629 samples of animals, and 15 different CNN architectures were tested. A total of 900 models were trained (15 CNN architectures &times; 3 spacial resolutions &times; 2 datasets &times; 10-fold cross validation), allowing for a deep analysis of the several aspects that impact the detection of cattle using aerial images captured using UAVs. Results revealed that many CNN architectures are robust enough to reliably detect animals in aerial images even under far from ideal conditions, indicating the viability of using UAVs for cattle monitoring.},
DOI = {10.3390/s19245436}
}



@Article{en12244736,
AUTHOR = {Niccolai, Alessandro and Grimaccia, Francesco and Leva, Sonia},
TITLE = {Advanced Asset Management Tools in Photovoltaic Plant Monitoring: UAV-Based Digital Mapping},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {4736},
URL = {https://www.mdpi.com/1996-1073/12/24/4736},
ISSN = {1996-1073},
ABSTRACT = {Photovoltaic (PV) plant monitoring and maintenance has become an often critical activity: the high efficiency requirements of the new European policy have often been in contrast with the many low-quality plants installed in several countries over the past few years. In actual industrial practices, heterogeneous information is produced, and they are often managed in a fragmented way. Several software tools have been developed for obtaining reliable and valuable information from the PV plant&rsquo;s raw data. With the aim of gathering and managing all these data in a more complex and integrated manner, an information managing system is proposed in this work&mdash;it is composed of a structured database, called the Photovoltaic Indexed Database, and a user interface, called the Digital Map, that allows for easy access and completion of the information present in the database. This information managment system and PV plant digitalization process is able to analyze and properly index the IR in the database, as well as the visual images obtained in photovoltaic plant monitoring.},
DOI = {10.3390/en12244736}
}



