
@Article{app9112331,
AUTHOR = {Bote-Curiel, Luis and Muñoz-Romero, Sergio and Gerrero-Curieses, Alicia and Rojo-Álvarez, José Luis},
TITLE = {Deep Learning and Big Data in Healthcare: A Double Review for Critical Beginners},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2331},
URL = {https://www.mdpi.com/2076-3417/9/11/2331},
ISSN = {2076-3417},
ABSTRACT = {In the last few years, there has been a growing expectation created about the analysis of large amounts of data often available in organizations, which has been both scrutinized by the academic world and successfully exploited by industry. Nowadays, two of the most common terms heard in scientific circles are Big Data and Deep Learning. In this double review, we aim to shed some light on the current state of these different, yet somehow related branches of Data Science, in order to understand the current state and future evolution within the healthcare area. We start by giving a simple description of the technical elements of Big Data technologies, as well as an overview of the elements of Deep Learning techniques, according to their usual description in scientific literature. Then, we pay attention to the application fields that can be said to have delivered relevant real-world success stories, with emphasis on examples from large technology companies and financial institutions, among others. The academic effort that has been put into bringing these technologies to the healthcare sector are then summarized and analyzed from a twofold view as follows: first, the landscape of application examples is globally scrutinized according to the varying nature of medical data, including the data forms in electronic health recordings, medical time signals, and medical images; second, a specific application field is given special attention, in particular the electrocardiographic signal analysis, where a number of works have been published in the last two years. A set of toy application examples are provided with the publicly-available MIMIC dataset, aiming to help the beginners start with some principled, basic, and structured material and available code. Critical discussion is provided for current and forthcoming challenges on the use of both sets of techniques in our future healthcare.},
DOI = {10.3390/app9112331}
}



@Article{app9112335,
AUTHOR = {Ahmed, Sarfraz and Huda, M. Nazmul and Rajbhandari, Sujan and Saha, Chitta and Elshaw, Mark and Kanarachos, Stratis},
TITLE = {Pedestrian and Cyclist Detection and Intent Estimation for Autonomous Vehicles: A Survey},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2335},
URL = {https://www.mdpi.com/2076-3417/9/11/2335},
ISSN = {2076-3417},
ABSTRACT = {As autonomous vehicles become more common on the roads, their advancement draws on safety concerns for vulnerable road users, such as pedestrians and cyclists. This paper presents a review of recent developments in pedestrian and cyclist detection and intent estimation to increase the safety of autonomous vehicles, for both the driver and other road users. Understanding the intentions of the pedestrian/cyclist enables the self-driving vehicle to take actions to avoid incidents. To make this possible, development of methods/techniques, such as deep learning (DL), for the autonomous vehicle will be explored. For example, the development of pedestrian detection has been significantly advanced using DL approaches, such as; Fast Region-Convolutional Neural Network (R-CNN) , Faster R-CNN and Single Shot Detector (SSD). Although DL has been around for several decades, the hardware to realise the techniques have only recently become viable. Using these DL methods for pedestrian and cyclist detection and applying it for the tracking, motion modelling and pose estimation can allow for a successful and accurate method of intent estimation for the vulnerable road users. Although there has been a growth in research surrounding the study of pedestrian detection using vision-based approaches, further attention should include focus on cyclist detection. To further improve safety for these vulnerable road users (VRUs), approaches such as sensor fusion and intent estimation should be investigated.},
DOI = {10.3390/app9112335}
}



@Article{s19112596,
AUTHOR = {Jung, Dae-Hyun and Kim, Hak-Jin and Kim, Hyoung Seok and Choi, Jaeyoung and Kim, Jeong Do and Park, Soo Hyun},
TITLE = {Fusion of Spectroscopy and Cobalt Electrochemistry Data for Estimating Phosphate Concentration in Hydroponic Solution},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {2596},
URL = {https://www.mdpi.com/1424-8220/19/11/2596},
ISSN = {1424-8220},
ABSTRACT = {Phosphate is a key element affecting plant growth. Therefore, the accurate determination of phosphate concentration in hydroponic nutrient solutions is essential for providing a balanced set of nutrients to plants within a suitable range. This study aimed to develop a data fusion approach for determining phosphate concentrations in a paprika nutrient solution. As a conventional multivariate analysis approach using spectral data, partial least squares regression (PLSR) and principal components regression (PCR) models were developed using 56 samples for calibration and 24 samples for evaluation. The R2 values of estimation models using PCR and PLSR ranged from 0.44 to 0.64. Furthermore, an estimation model using raw electromotive force (EMF) data from cobalt electrodes gave R2 values of 0.58–0.71. To improve the model performance, a data fusion method was developed to estimate phosphate concentration using near infrared (NIR) spectral and cobalt electrochemical data. Raw EMF data from cobalt electrodes and principle component values from the spectral data were combined. Results of calibration and evaluation tests using an artificial neural network estimation model showed that R2 = 0.90 and 0.89 and root mean square error (RMSE) = 96.70 and 119.50 mg/L, respectively. These values are sufficiently high for application to measuring phosphate concentration in hydroponic solutions.},
DOI = {10.3390/s19112596}
}



@Article{rs11111371,
AUTHOR = {Wang, Yanyu and Zhang, Ke and Tang, Chunlan and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
TITLE = {Estimation of Rice Growth Parameters Based on Linear Mixed-Effect Model Using Multispectral Images from Fixed-Wing Unmanned Aerial Vehicles},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1371},
URL = {https://www.mdpi.com/2072-4292/11/11/1371},
ISSN = {2072-4292},
ABSTRACT = {The accurate estimation of aboveground biomass (AGB) and leaf area index (LAI) is critical to characterize crop growth status and predict grain yield. Unmanned aerial vehicle (UAV) -based remote sensing has attracted significant interest due to its high flexibility and easiness of operation. The mixed effect model introduced in this study can capture secondary factors that cannot be captured by standard empirical relationships. The objective of this study was to explore the potential benefit of using a linear mixed-effect (LME) model and multispectral images from a fixed-wing UAV to estimate both AGB and LAI of rice. Field experiments were conducted over two consecutive years (2017&ndash;2018), that involved different N rates, planting patterns and rice cultivars. Images were collected by a compact multispectral camera mounted on a fixed-wing UAV during key rice growth stages. LME, simple regression (SR), artificial neural networks (ANN) and random forests (RF) models were developed relating growth parameters (AGB and LAI) to spectral information. Cultivar (C), growth stage (S) and planting pattern (P) were selected as candidates of random effects for the LME models due to their significant effects on rice growth. Compared to other regression models (SR, ANN and RF), the LME model improved the AGB estimation accuracy for all stage groups to varying degrees: the R2 increased by 0.14&ndash;0.35 and the RMSE decreased by 0.88&ndash;1.80 t ha&minus;1 for the whole season, the R2 increased by 0.07&ndash;0.15 and the RMSE decreased by 0.31&ndash;0.61 t ha&minus;1 for pre-heading stages and the R2 increased by 0.21&ndash;0.53 and the RMSE decreased by 0.72&ndash;1.52 t ha&minus;1 for post-heading stages. Further analysis suggested that the LME model also successfully predicted within the groups when the number of groups was suitable. More importantly, depending on the availability of C, S, P or combinations thereof, mixed effects could lead to an outperformance of baseline retrieval methods (SR, ANN or RF) due to the inclusion of secondary effects. Satisfactory results were also obtained for the LAI estimation while the superiority of the LME model was not as significant as that for AGB estimation. This study demonstrates that the LME model could accurately estimate rice AGB and LAI and fixed-wing UAVs are promising for the monitoring of the crop growth status over large-scale farmland.},
DOI = {10.3390/rs11111371}
}



@Article{rs11111373,
AUTHOR = {Abdulridha, Jaafar and Batuman, Ozgur and Ampatzidis, Yiannis},
TITLE = {UAV-Based Remote Sensing Technique to Detect Citrus Canker Disease Utilizing Hyperspectral Imaging and Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1373},
URL = {https://www.mdpi.com/2072-4292/11/11/1373},
ISSN = {2072-4292},
ABSTRACT = {A remote sensing technique was developed to detect citrus canker in laboratory conditions and was verified in the grove by utilizing an unmanned aerial vehicle (UAV). In the laboratory, a hyperspectral (400&ndash;1000 nm) imaging system was utilized for the detection of citrus canker in several disease development stages (i.e., asymptomatic, early, and late symptoms) on Sugar Belle leaves and immature (green) fruit by using two classification methods: (i) radial basis function (RBF) and (ii) K nearest neighbor (KNN). The same imaging system mounted on an UAV was used to detect citrus canker on tree canopies in the orchard. The overall classification accuracy of the RBF was higher (94%, 96%, and 100%) than the KNN method (94%, 95%, and 96%) for detecting canker in leaves. Among the 31 studied vegetation indices, the water index (WI) and the Modified Chlorophyll Absorption in Reflectance Index (ARI and TCARI 1) more accurately detected canker in laboratory and in orchard conditions, respectively. Immature fruit was not a reliable tissue for early detection of canker. However, the proposed technique successfully distinguished the late stage canker-infected fruit with 92% classification accuracy. The UAV-based technique achieved 100% classification accuracy for identifying healthy and canker-infected trees.},
DOI = {10.3390/rs11111373}
}



@Article{rs11111374,
AUTHOR = {Rostami, Mohammad and Kolouri, Soheil and Eaton, Eric and Kim, Kyungnam},
TITLE = {Deep Transfer Learning for Few-Shot SAR Image Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1374},
URL = {https://www.mdpi.com/2072-4292/11/11/1374},
ISSN = {2072-4292},
ABSTRACT = {The reemergence of Deep Neural Networks (DNNs) has lead to high-performance supervised learning algorithms for the Electro-Optical (EO) domain classification and detection problems. This success is because generating huge labeled datasets has become possible using modern crowdsourcing labeling platforms such as Amazon&rsquo;s Mechanical Turk that recruit ordinary people to label data. Unlike the EO domain, labeling the Synthetic Aperture Radar (SAR) domain data can be much more challenging, and for various reasons, using crowdsourcing platforms is not feasible for labeling the SAR domain data. As a result, training deep networks using supervised learning is more challenging in the SAR domain. In the paper, we present a new framework to train a deep neural network for classifying Synthetic Aperture Radar (SAR) images by eliminating the need for a huge labeled dataset. Our idea is based on transferring knowledge from a related EO domain problem, where labeled data are easy to obtain. We transfer knowledge from the EO domain through learning a shared invariant cross-domain embedding space that is also discriminative for classification. To this end, we train two deep encoders that are coupled through their last year to map data points from the EO and the SAR domains to the shared embedding space such that the distance between the distributions of the two domains is minimized in the latent embedding space. We use the Sliced Wasserstein Distance (SWD) to measure and minimize the distance between these two distributions and use a limited number of SAR label data points to match the distributions class-conditionally. As a result of this training procedure, a classifier trained from the embedding space to the label space using mostly the EO data would generalize well on the SAR domain. We provide a theoretical analysis to demonstrate why our approach is effective and validate our algorithm on the problem of ship classification in the SAR domain by comparing against several other competing learning approaches.},
DOI = {10.3390/rs11111374}
}



@Article{rs11111380,
AUTHOR = {Abeysinghe, Tharindu and Simic Milas, Anita and Arend, Kristin and Hohman, Breann and Reil, Patrick and Gregory, Andrew and Vázquez-Ortega, Angélica},
TITLE = {Mapping Invasive Phragmites australis in the Old Woman Creek Estuary Using UAV Remote Sensing and Machine Learning Classifiers},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1380},
URL = {https://www.mdpi.com/2072-4292/11/11/1380},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicles (UAV) are increasingly used for spatiotemporal monitoring of invasive plants in coastal wetlands. Early identification of invasive species is necessary in planning, restoring, and managing wetlands. This study assessed the effectiveness of UAV technology to identify invasive Phragmites australis in the Old Woman Creek (OWC) estuary using machine learning (ML) algorithms: Neural network (NN), support vector machine (SVM), and k-nearest neighbor (kNN). The ML algorithms were compared with the parametric maximum likelihood classifier (MLC) using pixel- and object-based methods. Pixel-based NN was identified as the best classifier with an overall accuracy of 94.80% and the lowest error of omission of 1.59%, the outcome desirable for effective eradication of Phragmites. The results were reached combining Sequoia multispectral imagery (green, red, red edge, and near-infrared bands) combined with the canopy height model (CHM) acquired in the mid-growing season and normalized difference vegetation index (NDVI) acquired later in the season. The sensitivity analysis, using various vegetation indices, image texture, CHM, and principal components (PC), demonstrated the impact of various feature layers on the classifiers. The study emphasizes the necessity of a suitable sampling and cross-validation methods, as well as the importance of optimum classification parameters.},
DOI = {10.3390/rs11111380}
}



@Article{en12122274,
AUTHOR = {Xu, Jianzhong and Yan, Fu and Yun, Kumchol and Su, Lifei and Li, Fengshu and Guan, Jun},
TITLE = {Noninferior Solution Grey Wolf Optimizer with an Independent Local Search Mechanism for Solving Economic Load Dispatch Problems},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2274},
URL = {https://www.mdpi.com/1996-1073/12/12/2274},
ISSN = {1996-1073},
ABSTRACT = {The economic load dispatch (ELD) problem is a complex optimization problem in power systems. The main task for this optimization problem is to minimize the total fuel cost of generators while also meeting the conditional constraints of valve-point loading effects, prohibited operating zones, and nonsmooth cost functions. In this paper, a novel grey wolf optimization (GWO), abbreviated as NGWO, is proposed to solve the ELD problem by introducing an independent local search strategy and a noninferior solution neighborhood independent local search technique to the original GWO algorithm to achieve the best problem solution. A local search strategy is added to the standard GWO algorithm in the NGWO, which is called GWOI, to search the local neighborhood of the global optimal point in depth and to guarantee a better candidate. In addition, a noninferior solution neighborhood independent local search method is introduced into the GWOI algorithm to find a better solution in the noninferior solution neighborhood and ensure the high probability of jumping out of the local optimum. The feasibility of the proposed NGWO method is verified on five different power systems, and it is compared with other selected methods in terms of the solution quality, convergence rate, and robustness. The compared experimental results indicate that the proposed NGWO method can efficiently solve ELD problems with higher-quality solutions.},
DOI = {10.3390/en12122274}
}



@Article{rs11121443,
AUTHOR = {Yao, Huang and Qin, Rongjun and Chen, Xiaoyu},
TITLE = {Unmanned Aerial Vehicle for Remote Sensing Applications—A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1443},
URL = {https://www.mdpi.com/2072-4292/11/12/1443},
ISSN = {2072-4292},
ABSTRACT = {The unmanned aerial vehicle (UAV) sensors and platforms nowadays are being used in almost every application (e.g., agriculture, forestry, and mining) that needs observed information from the top or oblique views. While they intend to be a general remote sensing (RS) tool, the relevant RS data processing and analysis methods are still largely ad-hoc to applications. Although the obvious advantages of UAV data are their high spatial resolution and flexibility in acquisition and sensor integration, there is in general a lack of systematic analysis on how these characteristics alter solutions for typical RS tasks such as land-cover classification, change detection, and thematic mapping. For instance, the ultra-high-resolution data (less than 10 cm of Ground Sampling Distance (GSD)) bring more unwanted classes of objects (e.g., pedestrian and cars) in land-cover classification; the often available 3D data generated from photogrammetric images call for more advanced techniques for geometric and spectral analysis. In this paper, we perform a critical review on RS tasks that involve UAV data and their derived products as their main sources including raw perspective images, digital surface models, and orthophotos. In particular, we focus on solutions that address the &ldquo;new&rdquo; aspects of the UAV data including (1) ultra-high resolution; (2) availability of coherent geometric and spectral data; and (3) capability of simultaneously using multi-sensor data for fusion. Based on these solutions, we provide a brief summary of existing examples of UAV-based RS in agricultural, environmental, urban, and hazards assessment applications, etc., and by discussing their practical potentials, we share our views in their future research directions and draw conclusive remarks.},
DOI = {10.3390/rs11121443}
}



@Article{s19122775,
AUTHOR = {Munaye, Yirga Yayeh and Lin, Hsin-Piao and Adege, Abebe Belay and Tarekegn, Getaneh Berie},
TITLE = {UAV Positioning for Throughput Maximization Using Deep Learning Approaches},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2775},
URL = {https://www.mdpi.com/1424-8220/19/12/2775},
ISSN = {1424-8220},
ABSTRACT = {The use of unmanned aerial vehicles (UAVs) as a communication platform has great practical importance for future wireless networks, especially for on-demand deployment for temporary and emergency conditions. The user throughput estimation in a wireless system depends on the data traffic load and the available capacity to support that load. In UAV-assisted communication, the position of the UAV is one major factor that affects the capacity available to the data flows being served. This study applies multi-layer perceptron (MLP) and long short term memory (LSTM) approaches to determine the position of a UAV that maximizes the overall system performance and user throughput. To analyze and evaluate the system performance, we apply the hybrid of MLP-LSTM for classification regression tasks and K-means algorithms for automatic clustering of classes. The implementation of our work is done through TensorFlow packages. The performance of our proposed system is compared with other approaches to give accurate and novel results for both classification and regression tasks of the user throughput maximization and UAV positioning. According to the results, 98% of the user throughput maximization accuracy is correctly classified. Moreover, the UAV positioning provides accuracy levels of 94.73%, 98.33%, and 99.53% for original datasets (scenario 1), reduced features on the estimated values of user throughput at each grid point (scenario 2), and reduced feature datasets collected on different days and grid points achieved maximum throughput (scenario 3), respectively.},
DOI = {10.3390/s19122775}
}



@Article{rs11121468,
AUTHOR = {Vanbrabant, Yasmin and Tits, Laurent and Delalieux, Stephanie and Pauly, Klaas and Verjans, Wim and Somers, Ben},
TITLE = {Multitemporal Chlorophyll Mapping in Pome Fruit Orchards from Remotely Piloted Aircraft Systems},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1468},
URL = {https://www.mdpi.com/2072-4292/11/12/1468},
ISSN = {2072-4292},
ABSTRACT = {Early and precise spatio-temporal monitoring of tree vitality is key for steering management decisions in pome fruit orchards. Spaceborne remote sensing instruments face a tradeoff between spatial and spectral resolution, while manned aircraft sensor-platform systems are very expensive. In order to address the shortcomings of these platforms, this study investigates the potential of Remotely Piloted Aircraft Systems (RPAS) to facilitate rapid, low cost, and flexible chlorophyll monitoring. Due to the complexity of orchard scenery a robust chlorophyll retrieval model on RPAS level has not yet been developed. In this study, specific focus therefore lies on evaluating the sensitivity of retrieval models to confounding factors. For this study, multispectral and hyperspectral imagery was collected over pome fruit orchards. Sensitivities of both univariate and multivariate retrieval models were demonstrated under different species, phenology, shade, and illumination scenes. Results illustrate that multivariate models have a significantly higher accuracy than univariate models as the former provide accuracies for the canopy chlorophyll content retrieval of R2 = 0.80 and Relative Root Mean Square Error (RRMSE) = 12% for the hyperspectral sensor. Random forest regression on multispectral imagery (R2 &gt; 0.9 for May, June, July, and August, and R2 = 0.5 for October) and hyperspectral imagery (0.6 &lt; R2 &lt; 0.9) led to satisfactory high and consistent accuracies for all months.},
DOI = {10.3390/rs11121468}
}



@Article{s19122789,
AUTHOR = {Yang, Zhen and Yuan, Yongbo and Zhang, Mingyuan and Zhao, Xuefeng and Zhang, Yang and Tian, Boquan},
TITLE = {Safety Distance Identification for Crane Drivers Based on Mask R-CNN},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2789},
URL = {https://www.mdpi.com/1424-8220/19/12/2789},
ISSN = {1424-8220},
ABSTRACT = {Tower cranes are the most commonly used large-scale equipment on construction site. Because workers can&rsquo;t always pay attention to the environment at the top of the head, it is often difficult to avoid accidents when heavy objects fall. Therefore, safety construction accidents such as struck-by often occurs. In order to address crane issue, this research recorded video data by a tower crane camera, labeled the pictures, and operated image recognition with the MASK R-CNN method. Furthermore, The RGB color extraction was performed on the identified mask layer to obtain the pixel coordinates of workers and dangerous zone. At last, we used the pixel and actual distance conversion method to measure the safety distance. The contribution of this research to safety problem area is twofold: On one hand, without affecting the normal behavior of workers, an automatic collection, analysis, and early-warning system was established. On the other hand, the proposed automatic inspection system can help improve the safety operation of tower crane drivers.},
DOI = {10.3390/s19122789}
}



@Article{s19122823,
AUTHOR = {Stodola, Petr and Drozd, Jan and Nohel, Jan and Hodický, Jan and Procházka, Dalibor},
TITLE = {Trajectory Optimization in a Cooperative Aerial Reconnaissance Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2823},
URL = {https://www.mdpi.com/1424-8220/19/12/2823},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the use of modern technology in military operations has become standard practice. Unmanned systems play an important role in operations such as reconnaissance and surveillance. This article examines a model for planning aerial reconnaissance using a fleet of mutually cooperating unmanned aerial vehicles to increase the effectiveness of the task. The model deploys a number of waypoints such that, when every waypoint is visited by any vehicle in the fleet, the area of interest is fully explored. The deployment of waypoints must meet the conditions arising from the technical parameters of the sensory systems used and tactical requirements of the task at hand. This paper proposes an improvement of the model by optimizing the number and position of waypoints deployed in the area of interest, the effect of which is to improve the trajectories of individual unmanned systems, and thus increase the efficiency of the operation. To achieve this optimization, a modified simulated annealing algorithm is proposed. The improvement of the model is verified by several experiments. Two sets of benchmark problems were designed: (a) benchmark problems for verifying the proposed algorithm for optimizing waypoints, and (b) benchmark problems based on typical reconnaissance scenarios in the real environment to prove the increased effectiveness of the reconnaissance operation. Moreover, an experiment in the SteelBeast simulation system was also conducted.},
DOI = {10.3390/s19122823}
}



@Article{rs11121505,
AUTHOR = {Zhang, Heng and Eziz, Anwar and Xiao, Jian and Tao, Shengli and Wang, Shaopeng and Tang, Zhiyao and Zhu, Jiangling and Fang, Jingyun},
TITLE = {High-Resolution Vegetation Mapping Using eXtreme Gradient Boosting Based on Extensive Features},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1505},
URL = {https://www.mdpi.com/2072-4292/11/12/1505},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of vegetation is a premise for conserving, managing, and sustainably using vegetation resources, especially in conditions of intensive human activities and accelerating global changes. However, it is still challenging to produce high-resolution multiclass vegetation map in high accuracy, due to the incapacity of traditional mapping techniques in distinguishing mosaic vegetation classes with subtle differences and the paucity of fieldwork data. This study created a workflow by adopting a promising classifier, extreme gradient boosting (XGBoost), to produce accurate vegetation maps of two strikingly different cases (the Dzungarian Basin in China and New Zealand) based on extensive features and abundant vegetation data. For the Dzungarian Basin, a vegetation map with seven vegetation types, 17 subtypes, and 43 associations was produced with an overall accuracy of 0.907, 0.801, and 0.748, respectively. For New Zealand, a map of 10 habitats and a map of 41 vegetation classes were produced with 0.946, and 0.703 overall accuracy, respectively. The workflow incorporating simplified field survey procedures outperformed conventional field survey and remote sensing based methods in terms of accuracy and efficiency. In addition, it opens a possibility of building large-scale, high-resolution, and timely vegetation monitoring platforms for most terrestrial ecosystems worldwide with the aid of Google Earth Engine and citizen science programs.},
DOI = {10.3390/rs11121505}
}



@Article{rs11121507,
AUTHOR = {Cardenal, Javier and Fernández, Tomás and Pérez-García, José Luis and Gómez-López, José Miguel},
TITLE = {Measurement of Road Surface Deformation Using Images Captured from UAVs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1507},
URL = {https://www.mdpi.com/2072-4292/11/12/1507},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a methodology for measuring road surface deformation due to terrain instability processes. The methodology is based on ultra-high resolution images acquired from unmanned aerial vehicles (UAVs). Flights are georeferenced by means of Structure from Motion (SfM) techniques. Dense point clouds, obtained using the multiple-view stereo (MVS) approach, are used to generate digital surface models (DSM) and high resolution orthophotographs (0.02 m GSD). The methodology has been applied to an unstable area located in La Guardia (Jaen, Southern Spain), where an active landslide was identified. This landslide affected some roads and accesses to a highway at the landslide foot. The detailed road deformation was monitored between 2012 and 2015 by means of eleven UAV flights of ultrahigh resolution covering an area of about 260 m × 90 m. The accuracy of the analysis has been established in 0.02 ± 0.01 m in XY and 0.04 ± 0.02 m in Z. Large deformations in the order of two meters were registered in the total period analyzed that resulted in maximum average rates of 0.62 m/month in the unstable area. Some boundary conditions were considered because of the low required flying height (&lt;50 m above ground level) in order to achieve a suitable image GSD, the fast landslide dynamic, continuous maintenance works on the affected roads and dramatic seasonal vegetation changes throughout the monitoring period. Finally, we have analyzed the relation of displacements to rainfalls in the area, finding a significant correlation between the two variables, as well as two different reactivation episodes.},
DOI = {10.3390/rs11121507}
}



@Article{rs11131550,
AUTHOR = {Koch, Tobias and Körner, Marco and Fraundorfer, Friedrich},
TITLE = {Automatic and Semantically-Aware 3D UAV Flight Planning for Image-Based 3D Reconstruction},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1550},
URL = {https://www.mdpi.com/2072-4292/11/13/1550},
ISSN = {2072-4292},
ABSTRACT = {Small-scaled unmanned aerial vehicles (UAVs) emerge as ideal image acquisition platforms due to their high maneuverability even in complex and tightly built environments. The acquired images can be utilized to generate high-quality 3D models using current multi-view stereo approaches. However, the quality of the resulting 3D model highly depends on the preceding flight plan which still requires human expert knowledge, especially in complex urban and hazardous environments. In terms of safe flight plans, practical considerations often define prohibited and restricted airspaces to be accessed with the vehicle. We propose a 3D UAV path planning framework designed for detailed and complete small-scaled 3D reconstructions considering the semantic properties of the environment allowing for user-specified restrictions on the airspace. The generated trajectories account for the desired model resolution and the demands on a successful photogrammetric reconstruction. We exploit semantics from an initial flight to extract the target object and to define restricted and prohibited airspaces which have to be avoided during the path planning process to ensure a safe and short UAV path, while still aiming to maximize the object reconstruction quality. The path planning problem is formulated as an orienteering problem and solved via discrete optimization exploiting submodularity and photogrammetrical relevant heuristics. An evaluation of our method on a customized synthetic scene and on outdoor experiments suggests the real-world capability of our methodology by providing feasible, short and safe flight plans for the generation of detailed 3D reconstruction models.},
DOI = {10.3390/rs11131550}
}



@Article{rs11131554,
AUTHOR = {Zhang, Xin and Han, Liangxiu and Dong, Yingying and Shi, Yue and Huang, Wenjiang and Han, Lianghao and González-Moreno, Pablo and Ma, Huiqin and Ye, Huichun and Sobeih, Tam},
TITLE = {A Deep Learning-Based Approach for Automated Yellow Rust Disease Detection from High-Resolution Hyperspectral UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1554},
URL = {https://www.mdpi.com/2072-4292/11/13/1554},
ISSN = {2072-4292},
ABSTRACT = {Yellow rust in winter wheat is a widespread and serious fungal disease, resulting in significant yield losses globally. Effective monitoring and accurate detection of yellow rust are crucial to ensure stable and reliable wheat production and food security. The existing standard methods often rely on manual inspection of disease symptoms in a small crop area by agronomists or trained surveyors. This is costly, time consuming and prone to error due to the subjectivity of surveyors. Recent advances in unmanned aerial vehicles (UAVs) mounted with hyperspectral image sensors have the potential to address these issues with low cost and high efficiency. This work proposed a new deep convolutional neural network (DCNN) based approach for automated crop disease detection using very high spatial resolution hyperspectral images captured with UAVs. The proposed model introduced multiple Inception-Resnet layers for feature extraction and was optimized to establish the most suitable depth and width of the network. Benefiting from the ability of convolution layers to handle three-dimensional data, the model used both spatial and spectral information for yellow rust detection. The model was calibrated with hyperspectral imagery collected by UAVs in five different dates across a whole crop cycle over a well-controlled field experiment with healthy and rust infected wheat plots. Its performance was compared across sampling dates and with random forest, a representative of traditional classification methods in which only spectral information was used. It was found that the method has high performance across all the growing cycle, particularly at late stages of the disease spread. The overall accuracy of the proposed model (0.85) was higher than that of the random forest classifier (0.77). These results showed that combining both spectral and spatial information is a suitable approach to improving the accuracy of crop disease detection with high resolution UAV hyperspectral images.},
DOI = {10.3390/rs11131554}
}



@Article{app9132666,
AUTHOR = {Yin, Junnan and Zhu, Dequan and Liao, Juan and Zhu, Guangyue and Wang, Yao and Zhang, Shun},
TITLE = {Automatic Steering Control Algorithm Based on Compound Fuzzy PID for Rice Transplanter},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2666},
URL = {https://www.mdpi.com/2076-3417/9/13/2666},
ISSN = {2076-3417},
ABSTRACT = {In order to realize automatic steering controls of rice transplanters in paddy fields, an automatic steering control algorithm is essential. In this study, combining the fuzzy control with the proportional-integral-derivative (PID) control and the kinematics model, a compound fuzzy PID controller was proposed to adjust the real time data of the PID parameters for the automatic steering control. The Kubota SPU-68C rice transplanter was then modified with the new controller. Next, an automatic steering control experimental with the modified transplanter was carried out under two conditions of linear tracking and headland turning in verifying the automatic steering effect of the transplanter in different steering angle situations. The results showed that the deviation with the new controller and the modified transplanter was acceptable, with maximum deviation in linear tracking of 7.5 cm, the maximum headland turning a deviation of 11.5 cm, and the average a deviation of less than 5 cm. In conclusion, within the allowable deviation range of the field operation of the rice transplanter, the proposed algorithm successfully realized automatic steering controls of the transplanter under different steering angles.},
DOI = {10.3390/app9132666}
}



@Article{en12132530,
AUTHOR = {Kim, Dongil and Kang, Seokho},
TITLE = {Effect of Irrelevant Variables on Faulty Wafer Detection in Semiconductor Manufacturing},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2530},
URL = {https://www.mdpi.com/1996-1073/12/13/2530},
ISSN = {1996-1073},
ABSTRACT = {Machine learning has been applied successfully for faulty wafer detection tasks in semiconductor manufacturing. For the tasks, prediction models are built with prior data to predict the quality of future wafers as a function of their precedent process parameters and measurements. In real-world problems, it is common for the data to have a portion of input variables that are irrelevant to the prediction of an output variable. The inclusion of many irrelevant variables negatively affects the performance of prediction models. Typically, prediction models learned by different learning algorithms exhibit different sensitivities with regard to irrelevant variables. Algorithms with low sensitivities are preferred as a first trial for building prediction models, whereas a variable selection procedure is necessarily considered for highly sensitive algorithms. In this study, we investigate the effect of irrelevant variables on three well-known representative learning algorithms that can be applied to both classification and regression tasks: artificial neural network, decision tree (DT), and k-nearest neighbors (k-NN). We analyze the characteristics of these learning algorithms in the presence of irrelevant variables with different model complexity settings. An empirical analysis is performed using real-world datasets collected from a semiconductor manufacturer to examine how the number of irrelevant variables affects the behavior of prediction models trained with different learning algorithms and model complexity settings. The results indicate that the prediction accuracy of k-NN is highly degraded, whereas DT demonstrates the highest robustness in the presence of many irrelevant variables. In addition, a higher model complexity of learning algorithms leads to a higher sensitivity to irrelevant variables.},
DOI = {10.3390/en12132530}
}



@Article{app9132686,
AUTHOR = {Zhang, Jianming and Lu, Chaoquan and Wang, Jin and Wang, Lei and Yue, Xiao-Guang},
TITLE = {Concrete Cracks Detection Based on FCN with Dilated Convolution},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2686},
URL = {https://www.mdpi.com/2076-3417/9/13/2686},
ISSN = {2076-3417},
ABSTRACT = {In civil engineering, the stability of concrete is of great significance to safety of people&rsquo;s life and property, so it is necessary to detect concrete damage effectively. In this paper, we treat crack detection on concrete surface as a semantic segmentation task that distinguishes background from crack at the pixel level. Inspired by Fully Convolutional Networks (FCN), we propose a full convolution network based on dilated convolution for concrete crack detection, which consists of an encoder and a decoder. Specifically, we first used the residual network to extract the feature maps of the input image, designed the dilated convolutions with different dilation rates to extract the feature maps of different receptive fields, and fused the extracted features from multiple branches. Then, we exploited the stacked deconvolution to do up-sampling operator in the fused feature maps. Finally, we used the SoftMax function to classify the feature maps at the pixel level. In order to verify the validity of the model, we introduced the commonly used evaluation indicators of semantic segmentation: Pixel Accuracy (PA), Mean Pixel Accuracy (MPA), Mean Intersection over Union (MIoU), and Frequency Weighted Intersection over Union (FWIoU). The experimental results show that the proposed model converges faster and has better generalization performance on the test set by introducing dilated convolutions with different dilation rates and a multi-branch fusion strategy. Our model has a PA of 96.84%, MPA of 92.55%, MIoU of 86.05% and FWIoU of 94.22% on the test set, which is superior to other models.},
DOI = {10.3390/app9132686}
}



@Article{su11133637,
AUTHOR = {Lee, SangSik and Jeong, YiNa and Son, SuRak and Lee, ByungKwan},
TITLE = {A Self-Predictable Crop Yield Platform (SCYP) Based On Crop Diseases Using Deep Learning},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3637},
URL = {https://www.mdpi.com/2071-1050/11/13/3637},
ISSN = {2071-1050},
ABSTRACT = {This paper proposes a self-predictable crop yield platform (SCYP) based on crop diseases using deep learning that collects weather information (temperature, humidity, sunshine, precipitation, etc.) and farm status information (harvest date, disease information, crop status, ground temperature, etc.), diagnoses crop diseases by using convolutional neural network (CNN), and predicts crop yield based on factors such as climate change, crop diseases, and others by using artificial neural network (ANN). The SCYP consists of an image preprocessing module (IPM) to determine crop diseases through the Google Vision API and image resizing, a crop disease diagnosis module (CDDM) based on CNN to diagnose the types and extent of crop diseases through photographs, and a crop yield prediction module (CYPM) based on ANN by using information of crop diseases, remaining time until harvest (based on the date), current temperature, humidity and precipitation (amount of snowfall) in the area, sunshine amount, ground temperature, atmospheric pressure, moisture evaporation in the ground, etc. Four experiments were conducted to verify the efficiency of the SCYP. In the CDMM, the accuracy and operation time of each model were measured using three neural network models: CNN, region-CNN(R-CNN), and you only look once (YOLO). In the CYPM, rectified linear unit (ReLU), Sigmoid, and Step activation functions were compared to measure ANN accuracy. The accuracy of CNN was about 3.5% higher than that of R-CNN and about 5.4% higher than that of YOLO. The operation time of CNN was about 37 s less than that of R-CNN and about 72 s less than that of YOLO. The CDDM had slightly less operation time, but in this paper, we prefer accuracy over operation time to diagnose crop diseases efficiently and accurately. When the activation function of the ANN used in the CYPM was ReLU, the accuracy of the ANN was 2% higher than that of Sigmoid and 7% higher than that of Step. The CYPM prediction was about 34% more accurate when using multiple diseases than when not using them. Therefore, the SCYP can predict farm yields more accurately than traditional methods.},
DOI = {10.3390/su11133637}
}



@Article{rs11131584,
AUTHOR = {Chen, Yang and Lee, Won Suk and Gan, Hao and Peres, Natalia and Fraisse, Clyde and Zhang, Yanchao and He, Yong},
TITLE = {Strawberry Yield Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoimages},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {1584},
URL = {https://www.mdpi.com/2072-4292/11/13/1584},
ISSN = {2072-4292},
ABSTRACT = {Strawberry growers in Florida suffer from a lack of efficient and accurate yield forecasts for strawberries, which would allow them to allocate optimal labor and equipment, as well as other resources for harvesting, transportation, and marketing. Accurate estimation of the number of strawberry flowers and their distribution in a strawberry field is, therefore, imperative for predicting the coming strawberry yield. Usually, the number of flowers and their distribution are estimated manually, which is time-consuming, labor-intensive, and subjective. In this paper, we develop an automatic strawberry flower detection system for yield prediction with minimal labor and time costs. The system used a small unmanned aerial vehicle (UAV) (DJI Technology Co., Ltd., Shenzhen, China) equipped with an RGB (red, green, blue) camera to capture near-ground images of two varieties (Sensation and Radiance) at two different heights (2 m and 3 m) and built orthoimages of a 402 m2 strawberry field. The orthoimages were automatically processed using the Pix4D software and split into sequential pieces for deep learning detection. A faster region-based convolutional neural network (R-CNN), a state-of-the-art deep neural network model, was chosen for the detection and counting of the number of flowers, mature strawberries, and immature strawberries. The mean average precision (mAP) was 0.83 for all detected objects at 2 m heights and 0.72 for all detected objects at 3 m heights. We adopted this model to count strawberry flowers in November and December from 2 m aerial images and compared the results with a manual count. The average deep learning counting accuracy was 84.1% with average occlusion of 13.5%. Using this system could provide accurate counts of strawberry flowers, which can be used to forecast future yields and build distribution maps to help farmers observe the growth cycle of strawberry fields.},
DOI = {10.3390/rs11131584}
}



@Article{s19132955,
AUTHOR = {Fakhrulddin, Saif Saad and Gharghan, Sadik Kamel and Al-Naji, Ali and Chahl, Javaan},
TITLE = {An Advanced First Aid System Based on an Unmanned Aerial Vehicles and a Wireless Body Area Sensor Network for Elderly Persons in Outdoor Environments},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2955},
URL = {https://www.mdpi.com/1424-8220/19/13/2955},
ISSN = {1424-8220},
ABSTRACT = {For elderly persons, a fall can cause serious injuries such as a hip fracture or head injury. Here, an advanced first aid system is proposed for monitoring elderly patients with heart conditions that puts them at risk of falling and for providing first aid supplies using an unmanned aerial vehicle. A hybridized fall detection algorithm (FDB-HRT) is proposed based on a combination of acceleration and a heart rate threshold. Five volunteers were invited to evaluate the performance of the heartbeat sensor relative to a benchmark device, and the extracted data was validated using statistical analysis. In addition, the accuracy of fall detections and the recorded locations of fall incidents were validated. The proposed FDB-HRT algorithm was 99.16% and 99.2% accurate with regard to heart rate measurement and fall detection, respectively. In addition, the geolocation error of patient fall incidents based on a GPS module was evaluated by mean absolute error analysis for 17 different locations in three cities in Iraq. Mean absolute error was 1.08 &times; 10&minus;5&deg; and 2.01 &times; 10&minus;5&deg; for latitude and longitude data relative to data from the GPS Benchmark system. In addition, the results revealed that in urban areas, the UAV succeeded in all missions and arrived at the patient&rsquo;s locations before the ambulance, with an average time savings of 105 s. Moreover, a time saving of 31.81% was achieved when using the UAV to transport a first aid kit to the patient compared to an ambulance. As a result, we can conclude that when compared to delivering first aid via ambulance, our design greatly reduces delivery time. The proposed advanced first aid system outperformed previous systems presented in the literature in terms of accuracy of heart rate measurement, fall detection, and information messages and UAV arrival time.},
DOI = {10.3390/s19132955}
}



@Article{s19132965,
AUTHOR = {Contreras-Cruz, Marco Antonio and Ramirez-Paredes, Juan Pablo and Hernandez-Belmonte, Uriel Haile and Ayala-Ramirez, Victor},
TITLE = {Vision-Based Novelty Detection Using Deep Features and Evolved Novelty Filters for Specific Robotic Exploration and Inspection Tasks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {2965},
URL = {https://www.mdpi.com/1424-8220/19/13/2965},
ISSN = {1424-8220},
ABSTRACT = {One of the essential abilities in animals is to detect novelties within their environment. From the computational point of view, novelty detection consists of finding data that are different in some aspect to the known data. In robotics, researchers have incorporated novelty modules in robots to develop automatic exploration and inspection tasks. The visual sensor is one of the preferred sensors to perform this task. However, there exist problems as illumination changes, occlusion, and scale, among others. Besides, novelty detectors vary their performance depending on the specific application scenario. In this work, we propose a visual novelty detection framework for specific exploration and inspection tasks based on evolved novelty detectors. The system uses deep features to represent the visual information captured by the robots and applies a global optimization technique to design novelty detectors for specific robotics applications. We verified the performance of the proposed system against well-established state-of-the-art methods in a challenging scenario. This scenario was an outdoor environment covering typical problems in computer vision such as illumination changes, occlusion, and geometric transformations. The proposed framework presented high-novelty detection accuracy with competitive or even better results than the baseline methods.},
DOI = {10.3390/s19132965}
}



@Article{s19133014,
AUTHOR = {Jalil, Bushra and Leone, Giuseppe Riccardo and Martinelli, Massimo and Moroni, Davide and Pascali, Maria Antonietta and Berton, Andrea},
TITLE = {Fault Detection in Power Equipment via an Unmanned Aerial System Using Multi Modal Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {13},
ARTICLE-NUMBER = {3014},
URL = {https://www.mdpi.com/1424-8220/19/13/3014},
ISSN = {1424-8220},
ABSTRACT = {The power transmission lines are the link between power plants and the points of consumption, through substations. Most importantly, the assessment of damaged aerial power lines and rusted conductors is of extreme importance for public safety; hence, power lines and associated components must be periodically inspected to ensure a continuous supply and to identify any fault and defect. To achieve these objectives, recently, Unmanned Aerial Vehicles (UAVs) have been widely used; in fact, they provide a safe way to bring sensors close to the power transmission lines and their associated components without halting the equipment during the inspection, and reducing operational cost and risk. In this work, a drone, equipped with multi-modal sensors, captures images in the visible and infrared domain and transmits them to the ground station. We used state-of-the-art computer vision methods to highlight expected faults (i.e., hot spots) or damaged components of the electrical infrastructure (i.e., damaged insulators). Infrared imaging, which is invariant to large scale and illumination changes in the real operating environment, supported the identification of faults in power transmission lines; while a neural network is adapted and trained to detect and classify insulators from an optical video stream. We demonstrate our approach on data captured by a drone in Parma, Italy.},
DOI = {10.3390/s19133014}
}



@Article{app9142806,
AUTHOR = {Bui, Xuan-Nam and Lee, Chang Woo and Nguyen, Hoang and Bui, Hoang-Bac and Long, Nguyen Quoc and Le, Qui-Thao and Nguyen, Van-Duc and Nguyen, Ngoc-Bich and Moayedi, Hossein},
TITLE = {Estimating PM10 Concentration from Drilling Operations in Open-Pit Mines Using an Assembly of SVR and PSO},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2806},
URL = {https://www.mdpi.com/2076-3417/9/14/2806},
ISSN = {2076-3417},
ABSTRACT = {Dust is one of the components causing heavy environmental pollution in open-pit mines, especially PM10. Some pathologies related to the lung, respiratory system, and occupational diseases have been identified due to the effects of PM10 in open-pit mines. Therefore, the prediction and control of PM10 concentration in the production process are necessary for environmental and health protection. In this study, PM10 concentration from drilling operations in the Coc Sau open-pit coal mine (Vietnam) was investigated and considered through a database including 245 datasets collected. A novel hybrid artificial intelligence model was developed based on support vector regression (SVR) and a swarm optimization algorithm (i.e., particle swarm optimization (PSO)), namely PSO-SVR, for estimating PM10 concentration from drilling operations at the mine. Polynomial (P), radial basis function (RBF), and linear (L) kernel functions were considered and applied to the development of the PSO-SVR models in the present study, abbreviated as PSO-SVR-P, PSO-SVR-RBF, and PSO-SVR-L. Also, three benchmark artificial intelligence techniques, such as k-nearest neighbors (KNN), random forest (RF), and classification and regression trees (CART), were applied and developed for estimating PM10 concentration and then compared with the PSO-SVR models. Root-mean-squared error (RMSE) and determination coefficient (R2) were used as the statistical criteria for evaluating the performance of the developed models. The results exhibited that the PSO algorithm had an essential role in the optimization of the hyper-parameters of the SVR models. The PSO-SVR models (i.e., PSO-SVR-L, PSO-SVR-P, and PSO-SVR-RBF) had higher performance levels than the other models (i.e., RF, CART, and KNN) with an RMSE of 0.040, 0.042, and 0.043; and R2 of 0.954, 0.948, and 0.946; for the PSO-SVR-L, PSO-SVR-P, and PSO-SVR-RBF models, respectively. Of these PSO-SVR models, the PSO-SVR-L model was the most dominant model with an RMSE of 0.040 and R2 of 0.954. The remaining three benchmark models (i.e., RF, CART, and KNN) yielded a more unsatisfactory performance with an RMSE of 0.060, 0.052, and 0.067; and R2 of 0.894, 0.924, and 0.867, for the RF, CART, and KNN models, respectively. Furthermore, the findings of this study demonstrated that the density of rock mass, moisture content, and the penetration rate of the drill were essential parameters on the PM10 concentration caused by drilling operations in open-pit mines.},
DOI = {10.3390/app9142806}
}



@Article{s19143096,
AUTHOR = {Xin, Junfeng and Li, Shixin and Sheng, Jinlu and Zhang, Yongbo and Cui, Ying},
TITLE = {Application of Improved Particle Swarm Optimization for Navigation of Unmanned Surface Vehicles},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3096},
URL = {https://www.mdpi.com/1424-8220/19/14/3096},
ISSN = {1424-8220},
ABSTRACT = {Multi-sensor fusion for unmanned surface vehicles (USVs) is an important issue for autonomous navigation of USVs. In this paper, an improved particle swarm optimization (PSO) is proposed for real-time autonomous navigation of a USV in real maritime environment. To overcome the conventional PSO&rsquo;s inherent shortcomings, such as easy occurrence of premature convergence and human experience-determined parameters, and to enhance the precision and algorithm robustness of the solution, this work proposes three optimization strategies: linearly descending inertia weight, adaptively controlled acceleration coefficients, and random grouping inversion. Their respective or combinational effects on the effectiveness of path planning are investigated by Monte Carlo simulations for five TSPLIB instances and application tests for the navigation of a self-developed unmanned surface vehicle on the basis of multi-sensor data. Comparative results show that the adaptively controlled acceleration coefficients play a substantial role in reducing the path length and the linearly descending inertia weight help improve the algorithm robustness. Meanwhile, the random grouping inversion optimizes the capacity of local search and maintains the population diversity by stochastically dividing the single swarm into several subgroups. Moreover, the PSO combined with all three strategies shows the best performance with the shortest trajectory and the superior robustness, although retaining solution precision and avoiding being trapped in local optima require more time consumption. The experimental results of our USV demonstrate the effectiveness and efficiency of the proposed method for real-time navigation based on multi-sensor fusion.},
DOI = {10.3390/s19143096}
}



@Article{app9142808,
AUTHOR = {Peng, Yahui and Liu, Xiaochen and Shen, Chong and Huang, Haoqian and Zhao, Donghua and Cao, Huiliang and Guo, Xiaoting},
TITLE = {An Improved Optical Flow Algorithm Based on Mask-R-CNN and K-Means for Velocity Calculation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {2808},
URL = {https://www.mdpi.com/2076-3417/9/14/2808},
ISSN = {2076-3417},
ABSTRACT = {Aiming at enhancing the accuracy and reliability of velocity calculation in vision navigation, an improved method is proposed in this paper. The method integrates Mask-R-CNN (Mask Region-based Convolutional Neural Network) and K-Means with the pyramid Lucas Kanade algorithm in order to reduce the harmful effect of moving objects on velocity calculation. Firstly, Mask-R-CNN is used to recognize the objects which have motions relative to the ground and covers them with masks to enhance the similarity between pixels and to reduce the impacts of the noisy moving pixels. Then, the pyramid Lucas Kanade algorithm is used to calculate the optical flow value. Finally, the value is clustered by the K-Means algorithm to abandon the outliers, and vehicle velocity is calculated by the processed optical flow. The prominent advantages of the proposed algorithm are (i) decreasing the bad impacts to velocity calculation, due to the objects which have relative motions; (ii) obtaining the correct optical flow sets and velocity calculation outputs with less fluctuation; and (iii) the applicability enhancement of the optical flow algorithm in complex navigation environment. The proposed algorithm is tested by actual experiments. Results with superior precision and reliability show the feasibility and effectiveness of the proposed method for vehicle velocity calculation in vision navigation system.},
DOI = {10.3390/app9142808}
}



@Article{s19143106,
AUTHOR = {Zhou, Chengquan and Ye, Hongbao and Hu, Jun and Shi, Xiaoyan and Hua, Shan and Yue, Jibo and Xu, Zhifu and Yang, Guijun},
TITLE = {Automated Counting of Rice Panicle by Applying Deep Learning Model to Images from Unmanned Aerial Vehicle Platform},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3106},
URL = {https://www.mdpi.com/1424-8220/19/14/3106},
ISSN = {1424-8220},
ABSTRACT = {The number of panicles per unit area is a common indicator of rice yield and is of great significance to yield estimation, breeding, and phenotype analysis. Traditional counting methods have various drawbacks, such as long delay times and high subjectivity, and they are easily perturbed by noise. To improve the accuracy of rice detection and counting in the field, we developed and implemented a panicle detection and counting system that is based on improved region-based fully convolutional networks, and we use the system to automate rice-phenotype measurements. The field experiments were conducted in target areas to train and test the system and used a rotor light unmanned aerial vehicle equipped with a high-definition RGB camera to collect images. The trained model achieved a precision of 0.868 on a held-out test set, which demonstrates the feasibility of this approach. The algorithm can deal with the irregular edge of the rice panicle, the significantly different appearance between the different varieties and growing periods, the interference due to color overlapping between panicle and leaves, and the variations in illumination intensity and shading effects in the field. The result is more accurate and efficient recognition of rice-panicles, which facilitates rice breeding. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a global scale.},
DOI = {10.3390/s19143106}
}



@Article{rs11141678,
AUTHOR = {Fu, Yongyong and Ye, Ziran and Deng, Jinsong and Zheng, Xinyu and Huang, Yibo and Yang, Wu and Wang, Yaohua and Wang, Ke},
TITLE = {Finer Resolution Mapping of Marine Aquaculture Areas Using WorldView-2 Imagery and a Hierarchical Cascade Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1678},
URL = {https://www.mdpi.com/2072-4292/11/14/1678},
ISSN = {2072-4292},
ABSTRACT = {Marine aquaculture plays an important role in seafood supplement, economic development, and coastal ecosystem service provision. The precise delineation of marine aquaculture areas from high spatial resolution (HSR) imagery is vital for the sustainable development and management of coastal marine resources. However, various sizes and detailed structures of marine objects make it difficult for accurate mapping from HSR images by using conventional methods. Therefore, this study attempts to extract marine aquaculture areas by using an automatic labeling method based on the convolutional neural network (CNN), i.e., an end-to-end hierarchical cascade network (HCNet). Specifically, for marine objects of various sizes, we propose to improve the classification performance by utilizing multi-scale contextual information. Technically, based on the output of a CNN encoder, we employ atrous convolutions to capture multi-scale contextual information and aggregate them in a hierarchical cascade way. Meanwhile, for marine objects with detailed structures, we propose to refine the detailed information gradually by using a series of long-span connections with fine resolution features from the shallow layers. In addition, to decrease the semantic gaps between features in different levels, we propose to refine the feature space (i.e., channel and spatial dimensions) using an attention-based module. Experimental results show that our proposed HCNet can effectively identify and distinguish different kinds of marine aquaculture, with 98% of overall accuracy. It also achieves better classification performance compared with object-based support vector machine and state-of-the-art CNN-based methods, such as FCN-32s, U-Net, and DeeplabV2. Our developed method lays a solid foundation for the intelligent monitoring and management of coastal marine resources.},
DOI = {10.3390/rs11141678}
}



@Article{s19143121,
AUTHOR = {Guo, Jia and Gong, Xiangyang and Wang, Wendong and Que, Xirong and Liu, Jingyu},
TITLE = {SASRT: Semantic-Aware Super-Resolution Transmission for Adaptive Video Streaming over Wireless Multimedia Sensor Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3121},
URL = {https://www.mdpi.com/1424-8220/19/14/3121},
ISSN = {1424-8220},
ABSTRACT = {There are few network resources in wireless multimedia sensor networks (WMSNs). Compressing media data can reduce the reliance of user&rsquo;s Quality of Experience (QoE) on network resources. Existing video coding software, such as H.264 and H.265, focuses only on spatial and short-term information redundancy. However, video usually contains redundancy over a long period of time. Therefore, compressing video information redundancy with a long period of time without compromising the user experience and adaptive delivery is a challenge in WMSNs. In this paper, a semantic-aware super-resolution transmission for adaptive video streaming system (SASRT) for WMSNs is presented. In the SASRT, some deep learning algorithms are used to extract video semantic information and enrich the video quality. On the multimedia sensor, different bit-rate semantic information and video data are encoded and uploaded to user. Semantic information can also be identified on the user side, further reducing the amount of data that needs to be transferred. However, identifying semantic information on the user side may increase the computational cost of the user side. On the user side, video quality is enriched with super-resolution technologies. The major challenges faced by SASRT include where the semantic information is identified, how to choose the bit rates of semantic and video information, and how network resources should be allocated to video and semantic information. The optimization problem is formulated as a complexity-constrained nonlinear NP-hard problem. Three adaptive strategies and a heuristic algorithm are proposed to solve the optimization problem. Simulation results demonstrate that SASRT can compress video information redundancy with a long period of time effectively and enrich the user experience with limited network resources while simultaneously improving the utilization of these network resources.},
DOI = {10.3390/s19143121}
}



@Article{make1030046,
AUTHOR = {Manzo, Mario},
TITLE = {Graph-Based Image Matching for Indoor Localization},
JOURNAL = {Machine Learning and Knowledge Extraction},
VOLUME = {1},
YEAR = {2019},
NUMBER = {3},
PAGES = {785--804},
URL = {https://www.mdpi.com/2504-4990/1/3/46},
ISSN = {2504-4990},
ABSTRACT = {Graphs are a very useful framework for representing information. In general, these data structures are used in different application domains where data of interest are described in terms of local and spatial relations. In this context, the aim is to propose an alternative graph-based image representation. An image is encoded by a Region Adjacency Graph (RAG), based on Multicolored Neighborhood (MCN) clustering. This representation is integrated into a Content-Based Image Retrieval (CBIR) system, designed for the vision-based positioning task. The image matching phase, in the CBIR system, is managed with an approach of attributed graph matching, named the extended-VF algorithm. Evaluated in a context of indoor localization, the proposed system reports remarkable performance.},
DOI = {10.3390/make1030046}
}



@Article{rs11141692,
AUTHOR = {Farooq, Adnan and Jia, Xiuping and Hu, Jiankun and Zhou, Jun},
TITLE = {Multi-Resolution Weed Classification via Convolutional Neural Network and Superpixel Based Local Binary Pattern Using Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1692},
URL = {https://www.mdpi.com/2072-4292/11/14/1692},
ISSN = {2072-4292},
ABSTRACT = {Automatic weed detection and classification faces the challenges of large intraclass variation and high spectral similarity to other vegetation. With the availability of new high-resolution remote sensing data from various platforms and sensors, it is possible to capture both spectral and spatial characteristics of weed species at multiple scales. Effective multi-resolution feature learning is then desirable to extract distinctive intensity, texture and shape features of each category of weed to enhance the weed separability. We propose a feature extraction method using a Convolutional Neural Network (CNN) and superpixel based Local Binary Pattern (LBP). Both middle and high level spatial features are learned using the CNN. Local texture features from superpixel-based LBP are extracted, and are also used as input to Support Vector Machines (SVM) for weed classification. Experimental results on the hyperspectral and remote sensing datasets verify the effectiveness of the proposed method, and show that it outperforms several feature extraction approaches.},
DOI = {10.3390/rs11141692}
}



@Article{rs11141694,
AUTHOR = {Mekhalfi, Mohamed Lamine and Bejiga, Mesay Belete and Soresina, Davide and Melgani, Farid and Demir, Begüm},
TITLE = {Capsule Networks for Object Detection in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1694},
URL = {https://www.mdpi.com/2072-4292/11/14/1694},
ISSN = {2072-4292},
ABSTRACT = {Recent advances in Convolutional Neural Networks (CNNs) have attracted great attention in remote sensing due to their high capability to model high-level semantic content of Remote Sensing (RS) images. However, CNNs do not explicitly retain the relative position of objects in an image and, thus, the effectiveness of the obtained features is limited in the framework of the complex object detection problems. To address this problem, in this paper we introduce Capsule Networks (CapsNets) for object detection in Unmanned Aerial Vehicle-acquired images. Unlike CNNs, CapsNets extract and exploit the information content about objects&rsquo; relative position across several layers, which enables parsing crowded scenes with overlapping objects. Experimental results obtained on two datasets for car and solar panel detection problems show that CapsNets provide similar object detection accuracies when compared to state-of-the-art deep models with significantly reduced computational time. This is due to the fact that CapsNets emphasize dynamic routine instead of the depth.},
DOI = {10.3390/rs11141694}
}



@Article{rs11141708,
AUTHOR = {Cao, Shuang and Yu, Yongtao and Guan, Haiyan and Peng, Daifeng and Yan, Wanqian},
TITLE = {Affine-Function Transformation-Based Object Matching for Vehicle Detection from Unmanned Aerial Vehicle Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1708},
URL = {https://www.mdpi.com/2072-4292/11/14/1708},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection from remote sensing images plays a significant role in transportation related applications. However, the scale variations, orientation variations, illumination variations, and partial occlusions of vehicles, as well as the image qualities, bring great challenges for accurate vehicle detection. In this paper, we present an affine-function transformation-based object matching framework for vehicle detection from unmanned aerial vehicle (UAV) images. First, meaningful and non-redundant patches are generated through a superpixel segmentation strategy. Then, the affine-function transformation-based object matching framework is applied to a vehicle template and each of the patches for vehicle existence estimation. Finally, vehicles are detected and located after matching cost thresholding, vehicle location estimation, and multiple response elimination. Quantitative evaluations on two UAV image datasets show that the proposed method achieves an average completeness, correctness, quality, and F1-measure of 0.909, 0.969, 0.883, and 0.938, respectively. Comparative studies also demonstrate that the proposed method achieves compatible performance with the Faster R-CNN and outperforms the other eight existing methods in accurately detecting vehicles of various conditions.},
DOI = {10.3390/rs11141708}
}



@Article{rs11141713,
AUTHOR = {Jozdani, Shahab Eddin and Johnson, Brian Alan and Chen, Dongmei},
TITLE = {Comparing Deep Neural Networks, Ensemble Classifiers, and Support Vector Machine Algorithms for Object-Based Urban Land Use/Land Cover Classification},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {1713},
URL = {https://www.mdpi.com/2072-4292/11/14/1713},
ISSN = {2072-4292},
ABSTRACT = {With the advent of high-spatial resolution (HSR) satellite imagery, urban land use/land cover (LULC) mapping has become one of the most popular applications in remote sensing. Due to the importance of context information (e.g., size/shape/texture) for classifying urban LULC features, Geographic Object-Based Image Analysis (GEOBIA) techniques are commonly employed for mapping urban areas. Regardless of adopting a pixel- or object-based framework, the selection of a suitable classifier is of critical importance for urban mapping. The popularity of deep learning (DL) (or deep neural networks (DNNs)) for image classification has recently skyrocketed, but it is still arguable if, or to what extent, DL methods can outperform other state-of-the art ensemble and/or Support Vector Machines (SVM) algorithms in the context of urban LULC classification using GEOBIA. In this study, we carried out an experimental comparison among different architectures of DNNs (i.e., regular deep multilayer perceptron (MLP), regular autoencoder (RAE), sparse, autoencoder (SAE), variational autoencoder (AE), convolutional neural networks (CNN)), common ensemble algorithms (Random Forests (RF), Bagging Trees (BT), Gradient Boosting Trees (GB), and Extreme Gradient Boosting (XGB)), and SVM to investigate their potential for urban mapping using a GEOBIA approach. We tested the classifiers on two RS images (with spatial resolutions of 30 cm and 50 cm). Based on our experiments, we drew three main conclusions: First, we found that the MLP model was the most accurate classifier. Second, unsupervised pretraining with the use of autoencoders led to no improvement in the classification result. In addition, the small difference in the classification accuracies of MLP from those of other models like SVM, GB, and XGB classifiers demonstrated that other state-of-the-art machine learning classifiers are still versatile enough to handle mapping of complex landscapes. Finally, the experiments showed that the integration of CNN and GEOBIA could not lead to more accurate results than the other classifiers applied.},
DOI = {10.3390/rs11141713}
}



@Article{pr7070464,
AUTHOR = {Gong, Qingwu and Tan, Si and Wang, Yubo and Liu, Dong and Qiao, Hui and Wu, Liuchuang},
TITLE = {Online Operation Risk Assessment of the Wind Power System of the Convolution Neural Network (CNN) Considering Multiple Random Factors},
JOURNAL = {Processes},
VOLUME = {7},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {464},
URL = {https://www.mdpi.com/2227-9717/7/7/464},
ISSN = {2227-9717},
ABSTRACT = {In order to solve the problem of the inaccuracy of the traditional online operation risk assessment model based on a physical mechanism and the inability to adapt to the actual operation of massive online operation monitoring data, this paper proposes an online operation risk assessment of the wind power system of the convolution neural network (CNN) considering multiple random factors. This paper analyzes multiple random factors of the wind power system, including uncertain wind power output, load fluctuations, frequent changes in operation patterns, and the electrical equipment failure rate, and generates the sample data based on multi-random factors. It uses the CNN algorithm network, offline training to obtain the risk assessment model, and online application to obtain the real-time online operation risk state of the wind power system. Finally, the online operation risk assessment model is verified by simulation using the standard network of 39 nodes of 10 machines New England system. The results prove that the risk assessment model presented in this paper is more rapid and suitable for online application.},
DOI = {10.3390/pr7070464}
}



@Article{s19143200,
AUTHOR = {Kim, Whui and Jung, Woo-Sung and Choi, Hyun Kyun},
TITLE = {Lightweight Driver Monitoring System Based on Multi-Task Mobilenets},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3200},
URL = {https://www.mdpi.com/1424-8220/19/14/3200},
ISSN = {1424-8220},
ABSTRACT = {Research on driver status recognition has been actively conducted to reduce fatal crashes caused by the driver&rsquo;s distraction and drowsiness. As in many other research areas, deep-learning-based algorithms are showing excellent performance for driver status recognition. However, despite decades of research in the driver status recognition area, the visual image-based driver monitoring system has not been widely used in the automobile industry. This is because the system requires high-performance processors, as well as has a hierarchical structure in which each procedure is affected by an inaccuracy from the previous procedure. To avoid using a hierarchical structure, we propose a method using Mobilenets without the functions of face detection and tracking and show this method is enabled to recognize facial behaviors that indicate the driver&rsquo;s distraction. However, frames per second processed by Mobilenets with a Raspberry pi, one of the single-board computers, is not enough to recognize the driver status. To alleviate this problem, we propose a lightweight driver monitoring system using a resource sharing device in a vehicle (e.g., a driver&rsquo;s mobile phone). The proposed system is based on Multi-Task Mobilenets (MT-Mobilenets), which consists of the Mobilenets&rsquo; base and multi-task classifier. The three Softmax regressions of the multi-task classifier help one Mobilenets base recognize facial behaviors related to the driver status, such as distraction, fatigue, and drowsiness. The proposed system based on MT-Mobilenets improved the accuracy of the driver status recognition with Raspberry Pi by using one additional device.},
DOI = {10.3390/s19143200}
}



@Article{sym11070944,
AUTHOR = {Tao, Jiadong and Yin, Zhong and Liu, Lei and Tian, Ying and Sun, Zhanquan and Zhang, Jianhua},
TITLE = {Individual-Specific Classification of Mental Workload Levels Via an Ensemble Heterogeneous Extreme Learning Machine for EEG Modeling},
JOURNAL = {Symmetry},
VOLUME = {11},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {944},
URL = {https://www.mdpi.com/2073-8994/11/7/944},
ISSN = {2073-8994},
ABSTRACT = {In a human&ndash;machine cooperation system, assessing the mental workload (MW) of the human operator is quite crucial to maintaining safe operation conditions. Among various MW indicators, electroencephalography (EEG) signals are particularly attractive because of their high temporal resolution and sensitivity to the occupation of working memory. However, the individual difference of the EEG feature distribution may impair the machine-learning based MW classifier. In this paper, we employed a fast-training neural network, extreme learning machine (ELM), as the basis to build an individual-specific classifier ensemble to recognize binary MW. To improve the diversity of the classification committee, heterogeneous member classifiers were adopted by fusing multiple ELMs and Bayesian models. Specifically, a deep network structure was applied in each weak model aiming at finding informative EEG feature representations. The structure of hyper-parameters of the proposed heterogeneous ensemble ELM (HE-ELM) was then identified and then its performance was compared against several competitive MW classifiers. We found that the HE-ELM model was superior for improving the individual-specific accuracy of MW assessments.},
DOI = {10.3390/sym11070944}
}



@Article{robotics8030059,
AUTHOR = {Iannace, Gino and Ciaburro, Giuseppe and Trematerra, Amelia},
TITLE = {Fault Diagnosis for UAV Blades Using Artificial Neural Network},
JOURNAL = {Robotics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2218-6581/8/3/59},
ISSN = {2218-6581},
ABSTRACT = {In recent years, unmanned aerial vehicles (UAVs) have been used in several fields including, for example, archaeology, cargo transport, conservation, healthcare, filmmaking, hobbies and recreational use. UAVs are aircraft characterized by the absence of a human pilot on board. The extensive use of these devices has highlighted maintenance problems with regard to the propellers, which represent the source of propulsion of the aircraft. A defect in the propellers of a drone can cause the aircraft to fall to the ground and its consequent destruction, and it also constitutes a safety problem for objects and people that are in the range of action of the aircraft. In this study, the measurements of the noise emitted by a UAV were used to build a classification model to detect unbalanced blades in a UAV propeller. To simulate the fault condition, two strips of paper tape were applied to the upper surface of a blade. The paper tape created a substantial modification of the aerodynamics of the blade, and this modification characterized the noise produced by the blade in its rotation. Then, a model based on artificial neural network algorithms was built to detect unbalanced blades in a UAV propeller. This model showed high accuracy (0.9763), indicating a high number of correct detections and suggests the adoption of this tool to verify the operating conditions of a UAV. The test must be performed indoors; from the measurements of the noise produced by the UAV it is possible to identify an imbalance in the propeller blade.},
DOI = {10.3390/robotics8030059}
}



@Article{s19143217,
AUTHOR = {Cho, Jaechan and Jung, Yongchul and Kim, Dong-Sun and Lee, Seongjoo and Jung, Yunho},
TITLE = {Moving Object Detection Based on Optical Flow Estimation and a Gaussian Mixture Model for Advanced Driver Assistance Systems},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {14},
ARTICLE-NUMBER = {3217},
URL = {https://www.mdpi.com/1424-8220/19/14/3217},
ISSN = {1424-8220},
ABSTRACT = {Most approaches for moving object detection (MOD) based on computer vision are limited to stationary camera environments. In advanced driver assistance systems (ADAS), however, ego-motion is added to image frames owing to the use of a moving camera. This results in mixed motion in the image frames and makes it difficult to classify target objects and background. In this paper, we propose an efficient MOD algorithm that can cope with moving camera environments. In addition, we present a hardware design and implementation results for the real-time processing of the proposed algorithm. The proposed moving object detector was designed using hardware description language (HDL) and its real-time performance was evaluated using an FPGA based test system. Experimental results demonstrate that our design achieves better detection performance than existing MOD systems. The proposed moving object detector was implemented with 13.2K logic slices, 104 DSP48s, and 163 BRAM and can support real-time processing of 30 fps at an operating frequency of 200 MHz.},
DOI = {10.3390/s19143217}
}



@Article{geosciences9070323,
AUTHOR = {Jakovljevic, Gordana and Govedarica, Miro and Alvarez-Taboada, Flor and Pajic, Vladimir},
TITLE = {Accuracy Assessment of Deep Learning Based Classification of LiDAR and UAV Points Clouds for DTM Creation and Flood Risk Mapping},
JOURNAL = {Geosciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {7},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2076-3263/9/7/323},
ISSN = {2076-3263},
ABSTRACT = {Digital elevation model (DEM) has been frequently used for the reduction and management of flood risk. Various classification methods have been developed to extract DEM from point clouds. However, the accuracy and computational efficiency need to be improved. The objectives of this study were as follows: (1) to determine the suitability of a new method to produce DEM from unmanned aerial vehicle (UAV) and light detection and ranging (LiDAR) data, using a raw point cloud classification and ground point filtering based on deep learning and neural networks (NN); (2) to test the convenience of rebalancing datasets for point cloud classification; (3) to evaluate the effect of the land cover class on the algorithm performance and the elevation accuracy; and (4) to assess the usability of the LiDAR and UAV structure from motion (SfM) DEM in flood risk mapping. In this paper, a new method of raw point cloud classification and ground point filtering based on deep learning using NN is proposed and tested on LiDAR and UAV data. The NN was trained on approximately 6 million points from which local and global geometric features and intensity data were extracted. Pixel-by-pixel accuracy assessment and visual inspection confirmed that filtering point clouds based on deep learning using NN is an appropriate technique for ground classification and producing DEM, as for the test and validation areas, both ground and non-ground classes achieved high recall (&gt;0.70) and high precision values (&gt;0.85), which showed that the two classes were well handled by the model. The type of method used for balancing the original dataset did not have a significant influence in the algorithm accuracy, and it was suggested not to use any of them unless the distribution of the generated and real data set will remain the same. Furthermore, the comparisons between true data and LiDAR and a UAV structure from motion (UAV SfM) point clouds were analyzed, as well as the derived DEM. The root mean square error (RMSE) and the mean average error (MAE) of the DEM were 0.25 m and 0.05 m, respectively, for LiDAR data, and 0.59 m and &ndash;0.28 m, respectively, for UAV data. For all land cover classes, the UAV DEM overestimated the elevation, whereas the LIDAR DEM underestimated it. The accuracy was not significantly different in the LiDAR DEM for the different vegetation classes, while for the UAV DEM, the RMSE increased with the height of the vegetation class. The comparison of the inundation areas derived from true LiDAR and UAV data for different water levels showed that in all cases, the largest differences were obtained for the lowest water level tested, while they performed best for very high water levels. Overall, the approach presented in this work produced DEM from LiDAR and UAV data with the required accuracy for flood mapping according to European Flood Directive standards. Although LiDAR is the recommended technology for point cloud acquisition, a suitable alternative is also UAV SfM in hilly areas.},
DOI = {10.3390/geosciences9070323}
}



@Article{app9152961,
AUTHOR = {Cao, Mingwei and Jia, Wei and Lv, Zhihan and Zheng, Liping and Liu, Xiaoping},
TITLE = {Superpixel-Based Feature Tracking for Structure from Motion},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2961},
URL = {https://www.mdpi.com/2076-3417/9/15/2961},
ISSN = {2076-3417},
ABSTRACT = {Feature tracking in image collections significantly affects the efficiency and accuracy of Structure from Motion (SFM). Insufficient correspondences may result in disconnected structures and incomplete components, while the redundant correspondences containing incorrect ones may yield to folded and superimposed structures. In this paper, we present a Superpixel-based feature tracking method for structure from motion. In the proposed method, we first propose to use a joint approach to detect local keypoints and compute descriptors. Second, the superpixel-based approach is used to generate labels for the input image. Third, we combine the Speed Up Robust Feature and binary test in the generated label regions to produce a set of combined descriptors for the detected keypoints. Fourth, the locality-sensitive hash (LSH)-based k nearest neighboring matching (KNN) is utilized to produce feature correspondences, and then the ratio test approach is used to remove outliers from the previous matching collection. Finally, we conduct comprehensive experiments on several challenging benchmarking datasets including highly ambiguous and duplicated scenes. Experimental results show that the proposed method gets better performances with respect to the state of the art methods.},
DOI = {10.3390/app9152961}
}



@Article{app9152976,
AUTHOR = {Luo, Cai and Zhao, Weikang and Du, Zhenpeng and Yu, Leijian},
TITLE = {A Neural Network Based Landing Method for an Unmanned Aerial Vehicle with Soft Landing Gears},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2976},
URL = {https://www.mdpi.com/2076-3417/9/15/2976},
ISSN = {2076-3417},
ABSTRACT = {This paper presents the design, implementation, and testing of a soft landing gear together with a neural network-based control method for replicating avian landing behavior on non-flat surfaces. With full consideration of unmanned aerial vehicles and landing gear requirements, a quadrotor helicopter, comprised of one flying unit and one landing assistance unit, is employed. Considering the touchdown speed and posture, a novel design of a soft mechanism for non-flat surfaces is proposed, in order to absorb the remaining landing impact. The framework of the control strategy is designed based on a derived dynamic model. A neural network-based backstepping controller is applied to achieve the desired trajectory. The simulation and outdoor testing results attest to the effectiveness and reliability of the proposed control method.},
DOI = {10.3390/app9152976}
}



@Article{drones3030059,
AUTHOR = {Hildmann, Hanno and Kovacs, Ernö},
TITLE = {Review: Using Unmanned Aerial Vehicles (UAVs) as Mobile Sensing Platforms (MSPs) for Disaster Response, Civil Security and Public Safety},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {59},
URL = {https://www.mdpi.com/2504-446X/3/3/59},
ISSN = {2504-446X},
ABSTRACT = {The use of UAVs in areas ranging from agriculture over urban services to entertainment or simply as a hobby has rapidly grown over the last years. Regarding serious/commercial applications, UAVs have been considered in the literature, especially as mobile sensing/actuation platforms (i.e., as a delivery platform for an increasingly wide range of sensors and actuators). With regard to timely, cost-effective and very rich data acquisition, both, NEC Research as well as TNO are pursuing investigations into the use of UAVs and swarms of UAVs for scenarios where high-resolution requirements, prohibiting environments or tight time constraints render traditional approaches ineffective. In this review article, we provide a brief overview of safety and security-focused application areas that we identified as main targets for industrial and commercial projects, especially in the context of intelligent autonomous systems and autonomous/semi-autonomously operating swarms. We discuss a number of challenges related to the deployment of UAVs in general and to their deployment within the identified application areas in particular. As such, this article is meant to serve as a review and overview of the literature and the state-of-the-art, but also to offer an outlook over our possible (near-term) future work and the challenges that we will face there.},
DOI = {10.3390/drones3030059}
}



@Article{app9153015,
AUTHOR = {Yun, Sungmin and Kim, Sungho},
TITLE = {TIR-MS: Thermal Infrared Mean-Shift for Robust Pedestrian Head Tracking in Dynamic Target and Background Variations},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3015},
URL = {https://www.mdpi.com/2076-3417/9/15/3015},
ISSN = {2076-3417},
ABSTRACT = {Thermal infrared (TIR) pedestrian tracking is one of the major issues in computer vision. Mean-shift is a powerful and versatile non-parametric iterative algorithm for finding local maxima in probability distributions. In existing infrared data, and mean-shift-based tracking is generally based on the brightness feature values. Unfortunately, the brightness is distorted by the target and background variations. This paper proposes a novel pedestrian tracking algorithm, thermal infrared mean-shift (TIR-MS), by introducing radiometric temperature data in mean-shift tracking. The thermal brightness image (eight-bits) was distorted by the automatic contrast enhancement of the scene such as hot objects in the background. On the other hand, the temperature data was unaffected directly by the background change, except for variations by the seasonal effect, which is more stable than the brightness. The experimental results showed that the TIR-MS outperformed the original mean-shift-based brightness when tracking a pedestrian head with successive background variations.},
DOI = {10.3390/app9153015}
}



@Article{s19153316,
AUTHOR = {Salhaoui, Marouane and Guerrero-González, Antonio and Arioua, Mounir and Ortiz, Francisco J. and El Oualkadi, Ahmed and Torregrosa, Carlos Luis},
TITLE = {Smart Industrial IoT Monitoring and Control System Based on UAV and Cloud Computing Applied to a Concrete Plant},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {3316},
URL = {https://www.mdpi.com/1424-8220/19/15/3316},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are now considered one of the best remote sensing techniques for gathering data over large areas. They are now being used in the industry sector as sensing tools for proactively solving or preventing many issues, besides quantifying production and helping to make decisions. UAVs are a highly consistent technological platform for efficient and cost-effective data collection and event monitoring. The industrial Internet of things (IIoT) sends data from systems that monitor and control the physical world to data processing systems that cloud computing has shown to be important tools for meeting processing requirements. In fog computing, the IoT gateway links different objects to the internet. It can operate as a joint interface for different networks and support different communication protocols. A great deal of effort has been put into developing UAVs and multi-UAV systems. This paper introduces a smart IIoT monitoring and control system based on an unmanned aerial vehicle that uses cloud computing services and exploits fog computing as the bridge between IIoT layers. Its novelty lies in the fact that the UAV is automatically integrated into an industrial control system through an IoT gateway platform, while UAV photos are systematically and instantly computed and analyzed in the cloud. Visual supervision of the plant by drones and cloud services is integrated in real-time into the control loop of the industrial control system. As a proof of concept, the platform was used in a case study in an industrial concrete plant. The results obtained clearly illustrate the feasibility of the proposed platform in providing a reliable and efficient system for UAV remote control to improve product quality and reduce waste. For this, we studied the communication latency between the different IIoT layers in different IoT gateways.},
DOI = {10.3390/s19153316}
}



@Article{buildings9080176,
AUTHOR = {Ghaychi Afrouz, Setareh and Razavi, Mohammad Reza and Pourkand, Ashkan and Mara Dias Wilson, Claudia},
TITLE = {Dynamic Displacement of an Aluminum Frame Using Close Range Photogrammetry},
JOURNAL = {Buildings},
VOLUME = {9},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {176},
URL = {https://www.mdpi.com/2075-5309/9/8/176},
ISSN = {2075-5309},
ABSTRACT = {Dynamic displacement measurement of objects can be challenging due to the limitations of conventional methods and pricey instrumentation of unconventional methods, such as laser scanners. In this research, Close Range Photogrammetry (CRP) is used as an affordable non-contact method to measure 3D dynamic displacements. It is proposed as a reliable alternative to traditional dynamic deformation measurement methods such as displacement sensors or accelerometers. For this purpose, dynamic displacements of a three-dimensional one-story building frame model on a one-dimensional shake table are determined by using the traditional method of attached accelerometer and CRP. The results of the CRP method are compared with the results of the traditional methods as well as numerical models. The results show a good agreement which evidences the reliability of the CRP with regular cameras.},
DOI = {10.3390/buildings9080176}
}



@Article{f10080643,
AUTHOR = {Fan, Guangpeng and Chen, Feixiang and Li, Yan and Liu, Binbin and Fan, Xu},
TITLE = {Development and Testing of a New Ground Measurement Tool to Assist in Forest GIS Surveys},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {643},
URL = {https://www.mdpi.com/1999-4907/10/8/643},
ISSN = {1999-4907},
ABSTRACT = {In present forest surveys, some problems occur because of the cost and time required when using external tools to acquire tree measurement. Therefore, it is of great importance to develop a new cost-saving and time-saving ground measurement method implemented in a forest geographic information system (GIS) survey. To obtain a better solution, this paper presents the design and implementation of a new ground measurement tool in which mobile devices play a very important role. Based on terrestrial photogrammetry, location-based services (LBS), and computer vision, the tool assists forest GIS surveys in obtaining important forest structure factors such as tree position, diameter at breast height (DBH), tree height, and tree species. This paper selected two plots to verify the accuracy of the ground measurement tool. Experiments show that the root mean square error (RMSE) of the position coordinates of the trees was 0.222 m and 0.229 m, respectively, and the relative root mean square error (rRMSE) was close to 0. The rRMSE of the DBH measurement was 10.17% and 13.38%, and the relative Bias (rBias) of the DBH measurement was &minus;0.88% and &minus;2.41%. The rRMSE of tree height measurement was 6.74% and 6.69%, and the rBias of tree height measurement was &minus;1.69% and &minus;1.27%, which conforms to the forest investigation requirements. In addition, workers usually make visual observations of trees and then combine their personal knowledge or experience to identify tree species, which may lead to the situations when they cannot distinguish tree species due to insufficient knowledge or experience. Based on MobileNets, a lightweight convolutional neural network designed for mobile phone, a model was trained to assist workers in identifying tree species. The dataset was collected from some forest parks in Beijing. The accuracy of the tree species recognition model was 94.02% on a test dataset and 93.21% on a test dataset in the mobile phone. This provides an effective reference for workers to identify tree species and can assist in artificial identification of tree species. Experiments show that this solution using the ground measurement tool saves time and cost for forest resources GIS surveys.},
DOI = {10.3390/f10080643}
}



@Article{electronics8080856,
AUTHOR = {Konecny, Jaromir and Kromer, Pavel and Prauzek, Michal and Musilek, Petr},
TITLE = {Scan Matching by Cross-Correlation and Differential Evolution},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {856},
URL = {https://www.mdpi.com/2079-9292/8/8/856},
ISSN = {2079-9292},
ABSTRACT = {Scan matching is an important task, solved in the context of many high-level problems including pose estimation, indoor localization, simultaneous localization and mapping and others. Methods that are accurate and adaptive and at the same time computationally efficient are required to enable location-based services in autonomous mobile devices. Such devices usually have a wide range of high-resolution sensors but only a limited processing power and constrained energy supply. This work introduces a novel high-level scan matching strategy that uses a combination of two advanced algorithms recently used in this field: cross-correlation and differential evolution. The cross-correlation between two laser range scans is used as an efficient measure of scan alignment and the differential evolution algorithm is used to search for the parameters of a transformation that aligns the scans. The proposed method was experimentally validated and showed good ability to match laser range scans taken shortly after each other and an excellent ability to match laser range scans taken with longer time intervals between them.},
DOI = {10.3390/electronics8080856}
}



@Article{rs11151816,
AUTHOR = {Iizuka, Kotaro and Kato, Tsuyoshi and Silsigia, Sisva and Soufiningrum, Alifia Yuni and Kozan, Osamu},
TITLE = {Estimating and Examining the Sensitivity of Different Vegetation Indices to Fractions of Vegetation Cover at Different Scaling Grids for Early Stage Acacia Plantation Forests Using a Fixed-Wing UAS},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1816},
URL = {https://www.mdpi.com/2072-4292/11/15/1816},
ISSN = {2072-4292},
ABSTRACT = {Understanding the information on land conditions and especially green vegetation cover is important for monitoring ecosystem dynamics. The fraction of vegetation cover (FVC) is a key variable that can be used to observe vegetation cover trends. Conventionally, satellite data are utilized to compute these variables, although computations in regions such as the tropics can limit the amount of available observation information due to frequent cloud coverage. Unmanned aerial systems (UASs) have become increasingly prominent in recent research and can remotely sense using the same methods as satellites but at a lower altitude. UASs are not limited by clouds and have a much higher resolution. This study utilizes a UAS to determine the emerging trends for FVC estimates at an industrial plantation site in Indonesia, which utilizes fast-growing Acacia trees that can rapidly change the land conditions. First, the UAS was utilized to collect high-resolution RGB imagery and multispectral images for the study area. The data were used to develop general land use/land cover (LULC) information for the site. Multispectral data were converted to various vegetation indices, and within the determined resolution grid (5, 10, 30 and 60 m), the fraction of each LULC type was analyzed for its correlation between the different vegetation indices (Vis). Finally, a simple empirical model was developed to estimate the FVC from the UAS data. The results show the correlation between the FVC (acacias) and different Vis ranging from R2 = 0.66&ndash;0.74, 0.76&ndash;0.8, 0.84&ndash;0.89 and 0.93&ndash;0.94 for 5, 10, 30 and 60 m grid resolutions, respectively. This study indicates that UAS-based FVC estimations can be used for observing fast-growing acacia trees at a fine scale resolution, which may assist current restoration programs in Indonesia.},
DOI = {10.3390/rs11151816}
}



@Article{ijerph16152801,
AUTHOR = {Cao, Chen and Chen, Jianping and Zhang, Wen and Xu, Peihua and Zheng, Lianjing and Zhu, Chun},
TITLE = {Geospatial Analysis of Mass-Wasting Susceptibility of Four Small Catchments in Mountainous Area of Miyun County, Beijing},
JOURNAL = {International Journal of Environmental Research and Public Health},
VOLUME = {16},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {2801},
URL = {https://www.mdpi.com/1660-4601/16/15/2801},
PubMedID = {31390774},
ISSN = {1660-4601},
ABSTRACT = {Driven by the pull of gravity, mass-wasting comprises all of the sedimentary processes related to remobilization of sediments deposited on slopes, including creep, sliding, slumping, flow, and fall. It is vital to conduct mass-wasting susceptibility mapping, with the aim of providing decision makers with management advice. The current study presents two individual data mining methods&mdash;the frequency ratio (FR) and information value model (IVM) methods&mdash;to map mass-wasting susceptibility in four catchments in Miyun County, Beijing, China. To achieve this goal, nine influence factors and a mass-wasting inventory map were used and produced, respectively. In this study, 71 mass-wasting locations were investigated in the field. Of these hazard locations, 70% of them were randomly selected to build the model, and the remaining 30% of the hazard locations were used for validation. Finally, a receiver operating characteristic (ROC) curve was used to assess the mass-wasting susceptibility maps produced by the above-mentioned models. Results show that the FR had a higher concordance and spatial differentiation, with respective values of 0.902 (area under the success rate) and 0.883 (area under the prediction rate), while the IVM had lower values of 0.865 (area under the success rate) and 0.855 (area under the prediction rate). Both proposed methodologies are useful for general planning and evaluation purposes, and they are shown to be reasonable models. Slopes of 6&ndash;21&deg; were the most common thresholds that controlled occurrence of mass-wasting. Farmland terraces were mainly composed of gravel, mud, and clay, which are more prone to mass-wasting. Mass-wasting susceptibility mapping is feasible and potentially highly valuable. It could provide useful information in support of environmental health policies.},
DOI = {10.3390/ijerph16152801}
}



@Article{rs11151837,
AUTHOR = {Brinkhoff, James and Dunn, Brian W. and Robson, Andrew J. and Dunn, Tina S. and Dehaan, Remy L.},
TITLE = {Modeling Mid-Season Rice Nitrogen Uptake Using Multispectral Satellite Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {15},
ARTICLE-NUMBER = {1837},
URL = {https://www.mdpi.com/2072-4292/11/15/1837},
ISSN = {2072-4292},
ABSTRACT = {Mid-season nitrogen (N) application in rice crops can maximize yield and profitability. This requires accurate and efficient methods of determining rice N uptake in order to prescribe optimal N amounts for topdressing. This study aims to determine the accuracy of using remotely sensed multispectral data from satellites to predict N uptake of rice at the panicle initiation (PI) growth stage, with a view to providing optimum variable-rate N topdressing prescriptions without needing physical sampling. Field experiments over 4 years, 4&ndash;6 N rates, 4 varieties and 2 sites were conducted, with at least 3 replicates of each plot. One WorldView satellite image for each year was acquired, close to the date of PI. Numerous single- and multi-variable models were investigated. Among single-variable models, the square of the NDRE vegetation index was shown to be a good predictor of N uptake (R     2     = 0.75, RMSE = 22.8 kg/ha for data pooled from all years and experiments). For multi-variable models, Lasso regularization was used to ensure an interpretable and compact model was chosen and to avoid over fitting. Combinations of remotely sensed reflectances and spectral indexes as well as variety, climate and management data as input variables for model training achieved R     2    &lt; 0.9 and RMSE &lt; 15 kg/ha for the pooled data set. The ability of remotely sensed data to predict N uptake in new seasons where no physical sample data has yet been obtained was tested. A methodology to extract models that generalize well to new seasons was developed, avoiding model overfitting. Lasso regularization selected four or less input variables, and yielded R     2     of better than 0.67 and RMSE better than 27.4 kg/ha over four test seasons that weren&rsquo;t used to train the models.},
DOI = {10.3390/rs11151837}
}



@Article{s19163451,
AUTHOR = {Lay, Usman Salihu and Pradhan, Biswajeet and Yusoff, Zainuddin Bin Md and Abdallah, Ahmad Fikri Bin and Aryal, Jagannath and Park, Hyuck-Jin},
TITLE = {Data Mining and Statistical Approaches in Debris-Flow Susceptibility Modelling Using Airborne LiDAR Data},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3451},
URL = {https://www.mdpi.com/1424-8220/19/16/3451},
ISSN = {1424-8220},
ABSTRACT = {Cameron Highland is a popular tourist hub in the mountainous area of Peninsular Malaysia. Most communities in this area suffer frequent incidence of debris flow, especially during monsoon seasons. Despite the loss of lives and properties recorded annually from debris flow, most studies in the region concentrate on landslides and flood susceptibilities. In this study, debris-flow susceptibility prediction was carried out using two data mining techniques; Multivariate Adaptive Regression Splines (MARS) and Support Vector Regression (SVR) models. The existing inventory of debris-flow events (640 points) were selected for training 70% (448) and validation 30% (192). Twelve conditioning factors namely; elevation, plan-curvature, slope angle, total curvature, slope aspect, Stream Transport Index (STI), profile curvature, roughness index, Stream Catchment Area (SCA), Stream Power Index (SPI), Topographic Wetness Index (TWI) and Topographic Position Index (TPI) were selected from Light Detection and Ranging (LiDAR)-derived Digital Elevation Model (DEM) data. Multi-collinearity was checked using Information Factor, Cramer&rsquo;s V, and Gini Index to identify the relative importance of conditioning factors. The susceptibility models were produced and categorized into five classes; not-susceptible, low, moderate, high and very-high classes. Models performances were evaluated using success and prediction rates where the area under the curve (AUC) showed a higher performance of MARS (93% and 83%) over SVR (76% and 72%). The result of this study will be important in contingency hazards and risks management plans to reduce the loss of lives and properties in the area.},
DOI = {10.3390/s19163451}
}



@Article{rs11161856,
AUTHOR = {Ghoussein, Youssra and Nicolas, Hervé and Haury, Jacques and Fadel, Ali and Pichelin, Pascal and Abou Hamdan, Hussein and Faour, Ghaleb},
TITLE = {Multitemporal Remote Sensing Based on an FVC Reference Period Using Sentinel-2 for Monitoring Eichhornia crassipes on a Mediterranean River},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {1856},
URL = {https://www.mdpi.com/2072-4292/11/16/1856},
ISSN = {2072-4292},
ABSTRACT = {Invasive aquatic plants are a serious global ecological and socio-economic problem because they can cause local extinction of native species and alter navigation and fishing. Eichhornia crassipes (water hyacinth) is a dangerous invasive floating plant that is widely distributed throughout the world. In Lebanon, it has spread since 2006 in the Al Kabir River. Remote sensing techniques have been widely developed to detect and monitor dynamics and extents of invasive plants such as water hyacinth over large areas. However, they become challenging to use in narrow areas such as the Al Kabir River and we developed a new image-analysis method to extract water hyacinth areas on the river. The method is based on a time series of a biophysical variable obtained from Sentinel-2 images. After defining a reference period between two growing cycles, we used the fractional vegetation cover (FVC) to estimate the water hyacinth surface area in the river. This method makes it possible to monitor water hyacinth development and estimate the total area it colonizes in the river corridor. This method can help ecologists and other stakeholders to map invasive plants in rivers and improve their control.},
DOI = {10.3390/rs11161856}
}



@Article{s19163542,
AUTHOR = {Lygouras, Eleftherios and Santavas, Nicholas and Taitzoglou, Anastasios and Tarchanidis, Konstantinos and Mitropoulos, Athanasios and Gasteratos, Antonios},
TITLE = {Unsupervised Human Detection with an Embedded Vision System on a Fully Autonomous UAV for Search and Rescue Operations},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3542},
URL = {https://www.mdpi.com/1424-8220/19/16/3542},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) play a primary role in a plethora of technical and scientific fields owing to their wide range of applications. In particular, the provision of emergency services during the occurrence of a crisis event is a vital application domain where such aerial robots can contribute, sending out valuable assistance to both distressed humans and rescue teams. Bearing in mind that time constraints constitute a crucial parameter in search and rescue (SAR) missions, the punctual and precise detection of humans in peril is of paramount importance. The paper in hand deals with real-time human detection onboard a fully autonomous rescue UAV. Using deep learning techniques, the implemented embedded system was capable of detecting open water swimmers. This allowed the UAV to provide assistance accurately in a fully unsupervised manner, thus enhancing first responder operational capabilities. The novelty of the proposed system is the combination of global navigation satellite system (GNSS) techniques and computer vision algorithms for both precise human detection and rescue apparatus release. Details about hardware configuration as well as the system&rsquo;s performance evaluation are fully discussed.},
DOI = {10.3390/s19163542}
}



@Article{electronics8080904,
AUTHOR = {Li, Qingyu and Dai, Keren and Wang, Xiaofeng and Zhang, Yu and Zhang, He and Jiang, Defu},
TITLE = {Low-Complexity Failed Element Diagnosis for Radar-Communication mmWave Antenna Array with Low SNR},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {904},
URL = {https://www.mdpi.com/2079-9292/8/8/904},
ISSN = {2079-9292},
ABSTRACT = {The millimeter-wave (mmWave) antenna array plays an important role in the excellent performance of wireless sensors networks (WSN) or unmanned aerial vehicle (UAV) clusters. However, the array elements are easily damaged in its harsh working environment but hard to be repaired or exchanged timely, resulting in a serious decline in the beamforming performance. Thus, accurate self-diagnosis of the failed elements is of great importance. In previous studies, there are still significant difficulties for large-scale arrays under extremely low SNR. In this paper, a diagnosis algorithm with low complexity and high reliability for the failed elements is proposed, which is based on a joint decision of communication signal and sensing echoes. Compared with the previous studies, the complexity of the algorithm is reduced by the construction of low-dimensional feature vectors for classification, the decoupling of the degree of arrival (DOA) estimation and the failed pattern diagnosis, with the help of the sub-array division. Simulation results show that, under an ultra-low SNR of &minus;12.5 dB for communication signals and &minus;16 dB for sensing echoes, an accurate self-diagnosis with a block error rate lower than 8% can be realized. The study in this paper will effectively promote the long-term and reliable operation of the mmWave antenna array in WSN, UAV clusters and other similar fields.},
DOI = {10.3390/electronics8080904}
}



@Article{s19163595,
AUTHOR = {Santos, Anderson Aparecido dos and Marcato Junior, José and Araújo, Márcio Santos and Di Martini, David Robledo and Tetila, Everton Castelão and Siqueira, Henrique Lopes and Aoki, Camila and Eltner, Anette and Matsubara, Edson Takashi and Pistori, Hemerson and Feitosa, Raul Queiroz and Liesenberg, Veraldo and Gonçalves, Wesley Nunes},
TITLE = {Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {16},
ARTICLE-NUMBER = {3595},
URL = {https://www.mdpi.com/1424-8220/19/16/3595},
ISSN = {1424-8220},
ABSTRACT = {Detection and classification of tree species from remote sensing data were performed using mainly multispectral and hyperspectral images and Light Detection And Ranging (LiDAR) data. Despite the comparatively lower cost and higher spatial resolution, few studies focused on images captured by Red-Green-Blue (RGB) sensors. Besides, the recent years have witnessed an impressive progress of deep learning methods for object detection. Motivated by this scenario, we proposed and evaluated the usage of Convolutional Neural Network (CNN)-based methods combined with Unmanned Aerial Vehicle (UAV) high spatial resolution RGB imagery for the detection of law protected tree species. Three state-of-the-art object detection methods were evaluated: Faster Region-based Convolutional Neural Network (Faster R-CNN), YOLOv3 and RetinaNet. A dataset was built to assess the selected methods, comprising 392 RBG images captured from August 2018 to February 2019, over a forested urban area in midwest Brazil. The target object is an important tree species threatened by extinction known as Dipteryx alata Vogel (Fabaceae). The experimental analysis delivered average precision around 92% with an associated processing times below 30 miliseconds.},
DOI = {10.3390/s19163595}
}



@Article{su11174557,
AUTHOR = {Liu, Chunting and Jia, Guozhu},
TITLE = {Industrial Big Data and Computational Sustainability: Multi-Method Comparison Driven by High-Dimensional Data for Improving Reliability and Sustainability of Complex Systems},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {4557},
URL = {https://www.mdpi.com/2071-1050/11/17/4557},
ISSN = {2071-1050},
ABSTRACT = {Sustainable development is of great significance. The emerging research on data-driven computational sustainability has become an effective way to solve this problem. This paper presents a fault diagnosis and prediction framework for complex systems based on multi-dimensional data and multi-method comparison, aimed at improving the reliability and sustainability of the system by selecting methods with relatively superior performance. This study took the avionics system in the industrial field as an example. Based on the literature research on typical fault modes and fault diagnosis requirements of avionics systems, three popular high-dimensional data-driven fault diagnosis methods&mdash;support vector machine, convolutional neural network, and long- and short-term memory neural network&mdash;were comprehensively analyzed and compared. Finally, the actual bearing failure data were used for programming in order to verify and compare various methods and the process of selecting the superior method driven by high-dimensional data was fully demonstrated. We attempt to provide a sustainable development idea that continuously explores multi-method integration and comparison, aimed at improving the calculation efficiency and accuracy of reliability assessments, optimizing system performance, and ultimately achieving the goal of long-term improvement of system reliability and sustainability.},
DOI = {10.3390/su11174557}
}



@Article{electronics8090919,
AUTHOR = {Wu, Ruidong and Liu, Bing and Fu, Jiafeng and Xu, Mingzhu and Fu, Ping and Li, Junbao},
TITLE = {Research and Implementation of ε-SVR Training Method Based on FPGA},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {919},
URL = {https://www.mdpi.com/2079-9292/8/9/919},
ISSN = {2079-9292},
ABSTRACT = {Online training of Support Vector Regression (SVR) in the field of machine learning is a computationally complex algorithm. Due to the need for multiple iterative processing in training, SVR training is usually implemented on computer, and the existing training methods cannot be directly implemented on Field-Programmable Gate Array (FPGA), which restricts the application range. This paper reconstructs the training framework and implementation without precision loss to reduce the total latency required for matrix update, reducing time consumption by 90%. A general &epsilon;-SVR training system with low latency is implemented on Zynq platform. Taking the regression of samples in two-dimensional as an example, the maximum acceleration ratio is 27.014&times; compared with microcontroller platform and the energy consumption is 12.449% of microcontroller. From the experiments for the University of California, Riverside (UCR) time series data set. The regression results obtain excellent regression effects. The minimum coefficient of determination is 0.996, and running time is less than 30 ms, which can meet the requirements of different applications for real-time regression.},
DOI = {10.3390/electronics8090919}
}



@Article{rs11171979,
AUTHOR = {Lu, Bing and He, Yuhong},
TITLE = {Evaluating Empirical Regression, Machine Learning, and Radiative Transfer Modelling for Estimating Vegetation Chlorophyll Content Using Bi-Seasonal Hyperspectral Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1979},
URL = {https://www.mdpi.com/2072-4292/11/17/1979},
ISSN = {2072-4292},
ABSTRACT = {Different types of methods have been developed to retrieve vegetation attributes from remote sensing data, including conventional empirical regressions (i.e., linear regression (LR)), advanced empirical regressions (e.g., multivariable linear regression (MLR), partial least square regression (PLSR)), machine learning (e.g., random forest regression (RFR), decision tree regression (DTR)), and radiative transfer modelling (RTM, e.g., PROSAIL). Given that each algorithm has its own strengths and weaknesses, it is essential to compare them and evaluate their effectiveness. Previous studies have mainly used single-date multispectral imagery or ground-based hyperspectral reflectance data for evaluating the models, while multi-seasonal hyperspectral images have been rarely used. Extensive spectral and spatial information in hyperspectral images, as well as temporal variations of landscapes, potentially influence the model performance. In this research, LR, PLSR, RFR, and PROSAIL, representing different types of methods, were evaluated for estimating vegetation chlorophyll content from bi-seasonal hyperspectral images (i.e., a middle- and a late-growing season image, respectively). Results show that the PLSR and RFR generally performed better than LR and PROSAIL. RFR achieved the highest accuracy for both images. This research provides insights on the effectiveness of different models for estimating vegetation chlorophyll content using hyperspectral images, aiming to support future vegetation monitoring research.},
DOI = {10.3390/rs11171979}
}



@Article{app9173488,
AUTHOR = {Ćwiąkała, Paweł},
TITLE = {Testing Procedure of Unmanned Aerial Vehicles (UAVs) Trajectory in Automatic Missions},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3488},
URL = {https://www.mdpi.com/2076-3417/9/17/3488},
ISSN = {2076-3417},
ABSTRACT = {This paper describes an experimental test campaign while using an Unmanned Aerial Vehicle (UAV) and measuring the obtained UAV positions during different flight tasks and in different operative conditions. A new test procedure has been presented and tested for different devices in various weather conditions. This paper describes and analyses the measurements of the flight trajectory of the UAV that was performed with the use of a robotic total station (RTS), as compared to the design data and the data recorded in the internal memory of the UAV. Five different test tasks have been conducted. The obtained results have allowed for the assessment of the correctness of task performance as compared to the design and to determine the flying accuracy of the entire UAV set. The proposed set of tasks can be successfully utilised to control the correctness of operation of various types of UAVs and it may be implemented as a universal test to verify the algorithms optimising take-offs and landings, test flights of the objects, as well as flight planning in various terrain and weather conditions, which will increase the safety of the flights while using UAVs.},
DOI = {10.3390/app9173488}
}



@Article{rs11171997,
AUTHOR = {Jeziorska, Justyna},
TITLE = {UAS for Wetland Mapping and Hydrological Modeling},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {1997},
URL = {https://www.mdpi.com/2072-4292/11/17/1997},
ISSN = {2072-4292},
ABSTRACT = {The miniaturization and affordable production of integrated microelectronics have improved in recent years, making unmanned aerial systems (UAS) accessible to consumers and igniting their interest. Researchers have proposed UAS-based solutions for almost any conceivable problem, but the greatest impact will likely be in applications that exploit the unique advantages of the technology: work in dangerous or difficult-to-access areas, high spatial resolution and/or frequent measurements of environmental phenomena, and deployment of novel sensing technology over small to moderate spatial scales. Examples of such applications may be the identification of wetland areas and use of high-resolution spatial data for hydrological modeling. However, because of the large&mdash;and growing&mdash;assortment of aircraft and sensors available on the market, an evolving regulatory environment, and limited practical guidance or examples of wetland mapping with UAS, it has been difficult to confidently devise or recommend UAS-based monitoring strategies for these applications. This paper provides a comprehensive review of UAS hardware, software, regulations, scientific applications, and data collection/post-processing procedures that are relevant for wetland monitoring and hydrological modeling.},
DOI = {10.3390/rs11171997}
}



@Article{drones3030066,
AUTHOR = {Khoufi, Ines and Laouiti, Anis and Adjih, Cedric},
TITLE = {A Survey of Recent Extended Variants of the Traveling Salesman and Vehicle Routing Problems for Unmanned Aerial Vehicles},
JOURNAL = {Drones},
VOLUME = {3},
YEAR = {2019},
NUMBER = {3},
ARTICLE-NUMBER = {66},
URL = {https://www.mdpi.com/2504-446X/3/3/66},
ISSN = {2504-446X},
ABSTRACT = {The use of Unmanned Aerial Vehicles (UAVs) is rapidly growing in popularity. Initially introduced for military purposes, over the past few years, UAVs and related technologies have successfully transitioned to a whole new range of civilian applications such as delivery, logistics, surveillance, entertainment, and so forth. They have opened new possibilities such as allowing operation in otherwise difficult or hazardous areas, for instance. For all applications, one foremost concern is the selection of the paths and trajectories of UAVs, and at the same time, UAVs control comes with many challenges, as they have limited energy, limited load capacity and are vulnerable to difficult weather conditions. Generally, efficiently operating a drone can be mathematically formalized as a path optimization problem under some constraints. This shares some commonalities with similar problems that have been extensively studied in the context of urban vehicles and it is only natural that the recent literature has extended the latter to fit aerial vehicle constraints. The knowledge of such problems, their formulation, the resolution methods proposed—through the variants induced specifically by UAVs features—are of interest for practitioners for any UAV application. Hence, in this study, we propose a review of existing literature devoted to such UAV path optimization problems, focusing specifically on the sub-class of problems that consider the mobility on a macroscopic scale. These are related to the two existing general classic ones—the Traveling Salesman Problem and the Vehicle Routing Problem. We analyze the recent literature that adapted the problems to the UAV context, provide an extensive classification and taxonomy of their problems and their formulation and also give a synthetic overview of the resolution techniques, performance metrics and obtained numerical results.},
DOI = {10.3390/drones3030066}
}



@Article{rs11172008,
AUTHOR = {Yang, Qinchen and Liu, Man and Zhang, Zhitao and Yang, Shuqin and Ning, Jifeng and Han, Wenting},
TITLE = {Mapping Plastic Mulched Farmland for High Resolution Images of Unmanned Aerial Vehicle Using Deep Semantic Segmentation},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2008},
URL = {https://www.mdpi.com/2072-4292/11/17/2008},
ISSN = {2072-4292},
ABSTRACT = {With increasing consumption, plastic mulch benefits agriculture by promoting crop quality and yield, but the environmental and soil pollution is becoming increasingly serious. Therefore, research on the monitoring of plastic mulched farmland (PMF) has received increasing attention. Plastic mulched farmland in unmanned aerial vehicle (UAV) remote images due to the high resolution, shows a prominent spatial pattern, which brings difficulties to the task of monitoring PMF. In this paper, through a comparison between two deep semantic segmentation methods, SegNet and fully convolutional networks (FCN), and a traditional classification method, Support Vector Machine (SVM), we propose an end-to-end deep-learning method aimed at accurately recognizing PMF for UAV remote sensing images from Hetao Irrigation District, Inner Mongolia, China. After experiments with single-band, three-band and six-band image data, we found that deep semantic segmentation models built via single-band data which only use the texture pattern of PMF can identify it well; for example, SegNet reaching the highest accuracy of 88.68% in a 900 nm band. Furthermore, with three visual bands and six-band data (3 visible bands and 3 near-infrared bands), deep semantic segmentation models combining the texture and spectral features further improve the accuracy of PMF identification, whereas six-band data obtains an optimal performance for FCN and SegNet. In addition, deep semantic segmentation methods, FCN and SegNet, due to their strong feature extraction capability and direct pixel classification, clearly outperform the traditional SVM method in precision and speed. Among three classification methods, SegNet model built on three-band and six-band data obtains the optimal average accuracy of 89.62% and 90.6%, respectively. Therefore, the proposed deep semantic segmentation model, when tested against the traditional classification method, provides a promising path for mapping PMF in UAV remote sensing images.},
DOI = {10.3390/rs11172008}
}



@Article{rs11172046,
AUTHOR = {Ghorbanzadeh, Omid and Meena, Sansar Raj and Blaschke, Thomas and Aryal, Jagannath},
TITLE = {UAV-Based Slope Failure Detection Using Deep-Learning Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2046},
URL = {https://www.mdpi.com/2072-4292/11/17/2046},
ISSN = {2072-4292},
ABSTRACT = {Slope failures occur when parts of a slope collapse abruptly under the influence of gravity, often triggered by a rainfall event or earthquake. The resulting slope failures often cause problems in mountainous or hilly regions, and the detection of slope failure is therefore an important topic for research. Most of the methods currently used for mapping and modelling slope failures rely on classification algorithms or feature extraction, but the spatial complexity of slope failures, the uncertainties inherent in expert knowledge, and problems in transferability, all combine to inhibit slope failure detection. In an attempt to overcome some of these problems we have analyzed the potential of deep learning convolutional neural networks (CNNs) for slope failure detection, in an area along a road section in the northern Himalayas, India. We used optical data from unmanned aerial vehicles (UAVs) over two separate study areas. Different CNN designs were used to produce eight different slope failure distribution maps, which were then compared with manually extracted slope failure polygons using different accuracy assessment metrics such as the precision, F-score, and mean intersection-over-union (mIOU). A slope failure inventory data set was produced for each of the study areas using a frequency-area distribution (FAD). The CNN approach that was found to perform best (precision accuracy assessment of almost 90% precision, F-score 85%, mIOU 74%) was one that used a window size of 64 &times; 64 pixels for the sample patches, and included slope data as an additional input layer. The additional information from the slope data helped to discriminate between slope failure areas and roads, which had similar spectral characteristics in the optical imagery. We concluded that the effectiveness of CNNs for slope failure detection was strongly dependent on their design (i.e., the window size selected for the sample patch, the data used, and the training strategies), but that CNNs are currently only designed by trial and error. While CNNs can be powerful tools, such trial and error strategies make it difficult to explain why a particular pooling or layer numbering works better than any other.},
DOI = {10.3390/rs11172046}
}



@Article{a12090183,
AUTHOR = {Li, Kexin and Wang, Jun and Qi, Dawei},
TITLE = {An Intelligent Warning Method for Diagnosing Underwater Structural Damage},
JOURNAL = {Algorithms},
VOLUME = {12},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {183},
URL = {https://www.mdpi.com/1999-4893/12/9/183},
ISSN = {1999-4893},
ABSTRACT = {A number of intelligent warning techniques have been implemented for detecting underwater infrastructure diagnosis to partially replace human-conducted on-site inspections. However, the extensively varying real-world situation (e.g., the adverse environmental conditions, the limited sample space, and the complex defect types) can lead to challenges to the wide adoption of intelligent warning techniques. To overcome these challenges, this paper proposed an intelligent algorithm combing gray level co-occurrence matrix (GLCM) with self-organization map (SOM) for accurate diagnosis of the underwater structural damage. In order to optimize the generative criterion for GLCM construction, a triangle algorithm was proposed based on orthogonal experiments. The constructed GLCM were utilized to evaluate the texture features of the regions of interest (ROI) of micro-injury images of underwater structures and extracted damage image texture characteristic parameters. The digital feature screening (DFS) method was used to obtain the most relevant features as the input for the SOM network. According to the unique topology information of the SOM network, the classification result, recognition efficiency, parameters, such as the network layer number, hidden layer node, and learning step, were optimized. The robustness and adaptability of the proposed approach were tested on underwater structure images through the DFS method. The results showed that the proposed method revealed quite better performances and can diagnose structure damage in underwater realistic situations.},
DOI = {10.3390/a12090183}
}



@Article{s19173754,
AUTHOR = {Stodola, Petr and Drozd, Jan and Mazal, Jan and Hodický, Jan and Procházka, Dalibor},
TITLE = {Cooperative Unmanned Aerial System Reconnaissance in a Complex Urban Environment and Uneven Terrain},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3754},
URL = {https://www.mdpi.com/1424-8220/19/17/3754},
ISSN = {1424-8220},
ABSTRACT = {Using unmanned robotic systems in military operations such as reconnaissance or surveillance, as well as in many civil applications, is common practice. In this article, the problem of monitoring the specified area of interest by a fleet of unmanned aerial systems is examined. The monitoring is planned via the Cooperative Aerial Model, which deploys a number of waypoints in the area; these waypoints are visited successively by unmanned systems. The original model proposed in the past assumed that the area to be explored is perfectly flat. A new formulation of this model is introduced in this article so that the model can be used in a complex environment with uneven terrain and/or with many obstacles, which may occlude some parts of the area of interest. The optimization algorithm based on the simulated annealing principles is proposed for positioning of waypoints to cover as large an area as possible. A set of scenarios has been designed to verify and evaluate the proposed approach. The key experiments are aimed at finding the minimum number of waypoints needed to explore at least the minimum requested portion of the area. Furthermore, the results are compared to the algorithm based on the lawnmower pattern.},
DOI = {10.3390/s19173754}
}



@Article{rs11172049,
AUTHOR = {Moeini Rad, Amir and Abkar, Ali Akbar and Mojaradi, Barat},
TITLE = {Supervised Distance-Based Feature Selection for Hyperspectral Target Detection},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {2049},
URL = {https://www.mdpi.com/2072-4292/11/17/2049},
ISSN = {2072-4292},
ABSTRACT = {Feature/band selection (FS/BS) for target detection (TD) attempts to select features/bands that increase the discrimination between the target and the image background. Moreover, TD usually suffers from background interference. Therefore, bands that help detectors to effectively suppress the background and magnify the target signal are considered to be more useful. In this regard, three supervised distance-based filter FS methods are proposed in this paper. The first method is based on the TD concept. It uses the image autocorrelation matrix and the target signature in the detection space (DS) for FS. Features that increase the first-norm distance between the target energy and the mean energy of the background in DS are selected as optimal. The other two methods use background modeling via image clustering. The cluster mean spectra, along with the target spectrum, are then transferred into DS. Orthogonal subspace projection distance (OSPD) and first-norm distance (FND) are used as two FS criteria to select optimal features. Two datasets, HyMap RIT and SIM.GA, are used for the experiments. Several measures, i.e., true positives (TPs), false alarms (FAs), target detection accuracy (TDA), total negative score (TNS), and the receiver operating characteristics (ROC) area under the curve (AUC) are employed to evaluate the proposed methods and to investigate the impact of FS on the TD performance. The experimental results show that our proposed FS methods, as compared with five existing FS methods, have improving impacts on common target detectors and help them to yield better results.},
DOI = {10.3390/rs11172049}
}



@Article{s19173796,
AUTHOR = {Shafi, Uferah and Mumtaz, Rafia and García-Nieto, José and Hassan, Syed Ali and Zaidi, Syed Ali Raza and Iqbal, Naveed},
TITLE = {Precision Agriculture Techniques and Practices: From Considerations to Applications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3796},
URL = {https://www.mdpi.com/1424-8220/19/17/3796},
ISSN = {1424-8220},
ABSTRACT = {Internet of Things (IoT)-based automation of agricultural events can change the agriculture sector from being static and manual to dynamic and smart, leading to enhanced production with reduced human efforts. Precision Agriculture (PA) along with Wireless Sensor Network (WSN) are the main drivers of automation in the agriculture domain. PA uses specific sensors and software to ensure that the crops receive exactly what they need to optimize productivity and sustainability. PA includes retrieving real data about the conditions of soil, crops and weather from the sensors deployed in the fields. High-resolution images of crops are obtained from satellite or air-borne platforms (manned or unmanned), which are further processed to extract information used to provide future decisions. In this paper, a review of near and remote sensor networks in the agriculture domain is presented along with several considerations and challenges. This survey includes wireless communication technologies, sensors, and wireless nodes used to assess the environmental behaviour, the platforms used to obtain spectral images of crops, the common vegetation indices used to analyse spectral images and applications of WSN in agriculture. As a proof of concept, we present a case study showing how WSN-based PA system can be implemented. We propose an IoT-based smart solution for crop health monitoring, which is comprised of two modules. The first module is a wireless sensor network-based system to monitor real-time crop health status. The second module uses a low altitude remote sensing platform to obtain multi-spectral imagery, which is further processed to classify healthy and unhealthy crops. We also highlight the results obtained using a case study and list the challenges and future directions based on our work.},
DOI = {10.3390/s19173796}
}



@Article{en12173383,
AUTHOR = {Kim, Woo-Yong and Lee, Pyeong-Yeon and Kim, Jonghoon and Kim, Kyung-Soo},
TITLE = {A Nonlinear-Model-Based Observer for a State-of-Charge Estimation of a Lithium-Ion Battery in Electric Vehicles},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {17},
ARTICLE-NUMBER = {3383},
URL = {https://www.mdpi.com/1996-1073/12/17/3383},
ISSN = {1996-1073},
ABSTRACT = {This paper presents a nonlinear-model-based observer for the state of charge estimation of a lithium-ion battery cell that always exhibits a nonlinear relationship between the state of charge and the open-circuit voltage. The proposed nonlinear model for the battery cell and its observer can estimate the state of charge without the linearization technique commonly adopted by previous studies. The proposed method has the following advantages: (1) The observability condition of the proposed nonlinear-model-based observer is derived regardless of the shape of the open circuit voltage curve, and (2) because the terminal voltage is contained in the state vector, the proposed model and its observer are insensitive to sensor noise. A series of experiments using an INR 18650 25R battery cell are performed, and it is shown that the proposed method produces convincing results for the state of charge estimation compared to conventional SOC estimation methods.},
DOI = {10.3390/en12173383}
}



@Article{app9183708,
AUTHOR = {Tan, Liguo and Wu, Juncheng and Yang, Xiaoyan and Song, Senmin},
TITLE = {Research on Optimal Landing Trajectory Planning Method between an UAV and a Moving Vessel},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3708},
URL = {https://www.mdpi.com/2076-3417/9/18/3708},
ISSN = {2076-3417},
ABSTRACT = {The location, velocity, and flight path angle of an autonomous unmanned aerial vehicle (UAV) landing on a moving vessel are key factors for an optimal landing trajectory. To tackle this challenge, this paper proposes a method for calculating the optimal approach landing trajectory between an UAV and a small vessel. A numerical approach (iterative method) is used to calculate the optimal approach landing trajectory, and the initial lead is introduced in the calculation process of the UAV trajectory for the inclination and heading angle for accuracy improvement, so that the UAV can track and calculate the optimal landing trajectory with high precision. Compared with the variational method, the proposed method can calculate an optimal turning direction angle for the UAV during the landing. Simulation experiments verify the effectiveness of the proposed algorithm and give optimal initialization values.},
DOI = {10.3390/app9183708}
}



@Article{rs11182104,
AUTHOR = {Bhardwaj, Anshuman and Sam, Lydia and Martín-Torres, F. Javier and Zorzano, María-Paz and Ramírez Luque, Juan Antonio},
TITLE = {UAV Imaging of a Martian Brine Analogue Environment in a Fluvio-Aeolian Setting},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2104},
URL = {https://www.mdpi.com/2072-4292/11/18/2104},
ISSN = {2072-4292},
ABSTRACT = {Understanding extraterrestrial environments and landforms through remote sensing and terrestrial analogy has gained momentum in recent years due to advances in remote sensing platforms, sensors, and computing efficiency. The seasonal brines of the largest salt plateau on Earth in Salar de Uyuni (Bolivian Altiplano) have been inadequately studied for their localized hydrodynamics and the regolith volume transport across the freshwater-brine mixing zones. These brines have recently been projected as a new analogue site for the proposed Martian brines, such as recurring slope lineae (RSL) and slope streaks. The Martian brines have been postulated to be the result of ongoing deliquescence-based salt-hydrology processes on contemporary Mars, similar to the studied Salar de Uyuni brines. As part of a field-site campaign during the cold and dry season in the latter half of August 2017, we deployed an unmanned aerial vehicle (UAV) at two sites of the Salar de Uyuni to perform detailed terrain mapping and geomorphometry. We generated high-resolution (2 cm/pixel) photogrammetric digital elevation models (DEMs) for observing and quantifying short-term terrain changes within the brines and their surroundings. The achieved co-registration for the temporal DEMs was considerably high, from which precise inferences regarding the terrain dynamics were derived. The observed average rate of bottom surface elevation change for brines was ~1.02 mm/day, with localized signs of erosion and deposition. Additionally, we observed short-term changes in the adjacent geomorphology and salt cracks. We conclude that the transferred regolith volume via such brines can be extremely low, well within the resolution limits of the remote sensors that are currently orbiting Mars, thereby making it difficult to resolve the topographic relief and terrain perturbations that are produced by such flows on Mars. Thus, the absence of observable erosion and deposition features within or around most of the proposed Martian RSL and slope streaks cannot be used to dismiss the possibility of fluidized flow within these features.},
DOI = {10.3390/rs11182104}
}



@Article{app9183789,
AUTHOR = {Moon, Jiyoun and Lee, Beom-Hee},
TITLE = {PDDL Planning with Natural Language-Based Scene Understanding for UAV-UGV Cooperation},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3789},
URL = {https://www.mdpi.com/2076-3417/9/18/3789},
ISSN = {2076-3417},
ABSTRACT = {Natural-language-based scene understanding can enable heterogeneous robots to cooperate efficiently in large and unconstructed environments. However, studies on symbolic planning rarely consider the semantic knowledge acquisition problem associated with the surrounding environments. Further, recent developments in deep learning methods show outstanding performance for semantic scene understanding using natural language. In this paper, a cooperation framework that connects deep learning techniques and a symbolic planner for heterogeneous robots is proposed. The framework is largely composed of the scene understanding engine, planning agent, and knowledge engine. We employ neural networks for natural-language-based scene understanding to share environmental information among robots. We then generate a sequence of actions for each robot using a planning domain definition language planner. JENA-TDB is used for knowledge acquisition storage. The proposed method is validated using simulation results obtained from one unmanned aerial and three ground vehicles.},
DOI = {10.3390/app9183789}
}



@Article{a12090193,
AUTHOR = {Torres-Sospedra, Joaquín and Nebot, Patricio},
TITLE = {Combining Satellite Images and Cadastral Information for Outdoor Autonomous Mapping and Navigation: A Proof-of-Concept Study in Citric Groves},
JOURNAL = {Algorithms},
VOLUME = {12},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {193},
URL = {https://www.mdpi.com/1999-4893/12/9/193},
ISSN = {1999-4893},
ABSTRACT = {The development of robotic applications for agricultural environments has several problems which are not present in the robotic systems used for indoor environments. Some of these problems can be solved with an efficient navigation system. In this paper, a new system is introduced to improve the navigation tasks for those robots which operate in agricultural environments. Concretely, the paper focuses on the problem related to the autonomous mapping of agricultural parcels (i.e., an orange grove). The map created by the system will be used to help the robots navigate into the parcel to perform maintenance tasks such as weed removal, harvest, or pest inspection. The proposed system connects to a satellite positioning service to obtain the real coordinates where the robotic system is placed. With these coordinates, the parcel information is downloaded from an online map service in order to autonomously obtain a map of the parcel in a readable format for the robot. Finally, path planning is performed by means of Fast Marching techniques using the robot or a team of two robots. This paper introduces the proof-of-concept and describes all the necessary steps and algorithms to obtain the path planning just from the initial coordinates of the robot.},
DOI = {10.3390/a12090193}
}



@Article{s19183917,
AUTHOR = {Fan, Shurui and Li, Zirui and Xia, Kewen and Hao, Dongxia},
TITLE = {Quantitative and Qualitative Analysis of Multicomponent Gas Using Sensor Array},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3917},
URL = {https://www.mdpi.com/1424-8220/19/18/3917},
ISSN = {1424-8220},
ABSTRACT = {The gas sensor array has long been a major tool for measuring gas due to its high sensitivity, quick response, and low power consumption. This goal, however, faces a difficult challenge because of the cross-sensitivity of the gas sensor. This paper presents a novel gas mixture analysis method for gas sensor array applications. The features extracted from the raw data utilizing principal component analysis (PCA) were used to complete random forest (RF) modeling, which enabled qualitative identification. Support vector regression (SVR), optimized by the particle swarm optimization (PSO) algorithm, was used to select hyperparameters C and &gamma; to establish the optimal regression model for the purpose of quantitative analysis. Utilizing the dataset, we evaluated the effectiveness of our approach. Compared with logistic regression (LR) and support vector machine (SVM), the average recognition rate of PCA combined with RF was the highest (97%). The fitting effect of SVR optimized by PSO for gas concentration was better than that of SVR and solved the problem of hyperparameters selection.},
DOI = {10.3390/s19183917}
}



@Article{s19183935,
AUTHOR = {Liu, Xiaolei and Liu, Liansheng and Wang, Lulu and Guo, Qing and Peng, Xiyuan},
TITLE = {Performance Sensing Data Prediction for an Aircraft Auxiliary Power Unit Using the Optimized Extreme Learning Machine},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3935},
URL = {https://www.mdpi.com/1424-8220/19/18/3935},
ISSN = {1424-8220},
ABSTRACT = {The aircraft auxiliary power unit (APU) is responsible for environmental control in the cabin and the main engines starting the aircraft. The prediction of its performance sensing data is significant for condition-based maintenance. As a complex system, its performance sensing data have a typically nonlinear feature. In order to monitor this process, a model with strong nonlinear fitting ability needs to be formulated. A neural network has advantages of solving a nonlinear problem. Compared with the traditional back propagation neural network algorithm, an extreme learning machine (ELM) has features of a faster learning speed and better generalization performance. To enhance the training of the neural network with a back propagation algorithm, an ELM is employed to predict the performance sensing data of the APU in this study. However, the randomly generated weights and thresholds of the ELM often may result in unstable prediction results. To address this problem, a restricted Boltzmann machine (RBM) is utilized to optimize the ELM. In this way, a stable performance parameter prediction model of the APU can be obtained and better performance parameter prediction results can be achieved. The proposed method is evaluated by the real APU sensing data of China Southern Airlines Company Limited Shenyang Maintenance Base. Experimental results show that the optimized ELM with an RBM is more stable and can obtain more accurate prediction results.},
DOI = {10.3390/s19183935}
}



@Article{ijgi8090409,
AUTHOR = {Tan, Yumin and Li, Yunxin},
TITLE = {UAV Photogrammetry-Based 3D Road Distress Detection},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {409},
URL = {https://www.mdpi.com/2220-9964/8/9/409},
ISSN = {2220-9964},
ABSTRACT = {The timely and proper rehabilitation of damaged roads is essential for road maintenance, and an effective method to detect road surface distress with high efficiency and low cost is urgently needed. Meanwhile, unmanned aerial vehicles (UAVs), with the advantages of high flexibility, low cost, and easy maneuverability, are a new fascinating choice for road condition monitoring. In this paper, road images from UAV oblique photogrammetry are used to reconstruct road three-dimensional (3D) models, from which road pavement distress is automatically detected and the corresponding dimensions are extracted using the developed algorithm. Compared with a field survey, the detection result presents a high precision with an error of around 1 cm in the height dimension for most cases, demonstrating the potential of the proposed method for future engineering practice.},
DOI = {10.3390/ijgi8090409}
}



@Article{s19183958,
AUTHOR = {Han, Seongkyun and Yoo, Jisang and Kwon, Soonchul},
TITLE = {Real-Time Vehicle-Detection Method in Bird-View Unmanned-Aerial-Vehicle Imagery},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3958},
URL = {https://www.mdpi.com/1424-8220/19/18/3958},
ISSN = {1424-8220},
ABSTRACT = {Vehicle detection is an important research area that provides background information for the diversity of unmanned-aerial-vehicle (UAV) applications. In this paper, we propose a vehicle-detection method using a convolutional-neural-network (CNN)-based object detector. We design our method, DRFBNet300, with a Deeper Receptive Field Block (DRFB) module that enhances the expressiveness of feature maps to detect small objects in the UAV imagery. We also propose the UAV-cars dataset that includes the composition and angular distortion of vehicles in UAV imagery to train our DRFBNet300. Lastly, we propose a Split Image Processing (SIP) method to improve the accuracy of the detection model. Our DRFBNet300 achieves 21 mAP with 45 FPS in the MS COCO metric, which is the highest score compared to other lightweight single-stage methods running in real time. In addition, DRFBNet300, trained on the UAV-cars dataset, obtains the highest AP score at altitudes of 20&ndash;50 m. The gap of accuracy improvement by applying the SIP method became larger when the altitude increases. The DRFBNet300 trained on the UAV-cars dataset with SIP method operates at 33 FPS, enabling real-time vehicle detection.},
DOI = {10.3390/s19183958}
}



@Article{en12183539,
AUTHOR = {Bjaoui, Marwen and Khiari, Brahim and Benadli, Ridha and Memni, Mouad and Sellami, Anis},
TITLE = {Practical Implementation of the Backstepping Sliding Mode Controller MPPT for a PV-Storage Application},
JOURNAL = {Energies},
VOLUME = {12},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {3539},
URL = {https://www.mdpi.com/1996-1073/12/18/3539},
ISSN = {1996-1073},
ABSTRACT = {This study presents a design and an implementation of a robust Maximum Power Point Tracking (MPPT) for a stand-alone photovoltaic (PV) system with battery storage. A new control scheme is applied for the boost converter based on the combination of the adaptive perturb and observe fuzzy logic controller (P&amp;O-FLC) MPPT technique and the backstepping sliding mode control (BS-SMC) approach. The MPPT controller design was used to accurately track the PV operating point to its maximum power point (MPP) under changing climatic conditions. The presented MPPT based on the P&amp;O-FLC technique generates the reference PV voltage and then a cascade control loop type, based on the BS-SMC approach is used. The aims of this approach are applied to regulate the inductor current and then the PV voltage to its reference values. In order to reduce system costs and complexity, a high gain observer (HGO) was designed, based on the model of the PV system, to estimate online the real value of the boost converter&rsquo;s inductor current. The performance and the robustness of the BS-SMC approach are evaluated using a comparative simulation with a conventional proportional integral (PI) controller implemented in the MATLAB/Simulink environment. The obtained results demonstrate that the proposed approach not only provides a near-perfect tracking performance (dynamic response, overshoot, steady-state error), but also offers greater robustness and stability than the conventional PI controller. Experimental results fitted with dSPACE software reveal that the PV module could reach the MPP and achieve the performance and robustness of the designed BS-SMC MPPT controller.},
DOI = {10.3390/en12183539}
}



@Article{rs11182155,
AUTHOR = {Wang, Jie and Simeonova, Sandra and Shahbazi, Mozhdeh},
TITLE = {Orientation- and Scale-Invariant Multi-Vehicle Detection and Tracking from Unmanned Aerial Videos},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {18},
ARTICLE-NUMBER = {2155},
URL = {https://www.mdpi.com/2072-4292/11/18/2155},
ISSN = {2072-4292},
ABSTRACT = {Along with the advancement of light-weight sensing and processing technologies, unmanned aerial vehicles (UAVs) have recently become popular platforms for intelligent traffic monitoring and control. UAV-mounted cameras can capture traffic-flow videos from various perspectives providing a comprehensive insight into road conditions. To analyze the traffic flow from remotely captured videos, a reliable and accurate vehicle detection-and-tracking approach is required. In this paper, we propose a deep-learning framework for vehicle detection and tracking from UAV videos for monitoring traffic flow in complex road structures. This approach is designed to be invariant to significant orientation and scale variations in the videos. The detection procedure is performed by fine-tuning a state-of-the-art object detector, You Only Look Once (YOLOv3), using several custom-labeled traffic datasets. Vehicle tracking is conducted following a tracking-by-detection paradigm, where deep appearance features are used for vehicle re-identification, and Kalman filtering is used for motion estimation. The proposed methodology is tested on a variety of real videos collected by UAVs under various conditions, e.g., in late afternoons with long vehicle shadows, in dawn with vehicles lights being on, over roundabouts and interchange roads where vehicle directions change considerably, and from various viewpoints where vehicles&rsquo; appearance undergo substantial perspective distortions. The proposed tracking-by-detection approach performs efficiently at 11 frames per second on color videos of 2720p resolution. Experiments demonstrated that high detection accuracy could be achieved with an average F1-score of 92.1%. Besides, the tracking technique performs accurately, with an average multiple-object tracking accuracy (MOTA) of 81.3%. The proposed approach also addressed the shortcomings of the state-of-the-art in multi-object tracking regarding frequent identity switching, resulting in a total of only one identity switch over every 305 tracked vehicles.},
DOI = {10.3390/rs11182155}
}



@Article{rs11192212,
AUTHOR = {Salameh, Edward and Frappart, Frédéric and Almar, Rafael and Baptista, Paulo and Heygster, Georg and Lubac, Bertrand and Raucoules, Daniel and Almeida, Luis Pedro and Bergsma, Erwin W. J. and Capo, Sylvain and De Michele, Marcello and Idier, Deborah and Li, Zhen and Marieu, Vincent and Poupardin, Adrien and Silva, Paulo A. and Turki, Imen and Laignel, Benoit},
TITLE = {Monitoring Beach Topography and Nearshore Bathymetry Using Spaceborne Remote Sensing: A Review},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2212},
URL = {https://www.mdpi.com/2072-4292/11/19/2212},
ISSN = {2072-4292},
ABSTRACT = {With high anthropogenic pressure and the effects of climate change (e.g., sea level rise) on coastal regions, there is a greater need for accurate and up-to-date information about the topography of these systems. Reliable topography and bathymetry information are fundamental parameters for modelling the morpho-hydrodynamics of coastal areas, for flood forecasting, and for coastal management. Traditional methods such as ground, ship-borne, and airborne surveys suffer from limited spatial coverage and temporal sampling due to logistical constraints and high costs which limit their ability to provide the needed information. The recent advancements of spaceborne remote sensing techniques, along with their ability to acquire data over large spatial areas and to provide high frequency temporal monitoring, has made them very attractive for topography and bathymetry mapping. In this review, we present an overview of the current state of spaceborne-based remote sensing techniques used to estimate the topography and bathymetry of beaches, intertidal, and nearshore areas. We also provide some insights about the potential of these techniques when using data provided by new and future satellite missions.},
DOI = {10.3390/rs11192212}
}



@Article{electronics8101077,
AUTHOR = {Bai, Guoxing and Meng, Yu and Liu, Li and Luo, Weidong and Gu, Qing and Liu, Li},
TITLE = {Review and Comparison of Path Tracking Based on Model Predictive Control},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1077},
URL = {https://www.mdpi.com/2079-9292/8/10/1077},
ISSN = {2079-9292},
ABSTRACT = {Recently, model predictive control (MPC) is increasingly applied to path tracking of mobile devices, such as mobile robots. The characteristics of these MPC-based controllers are not identical due to the different approaches taken during design. According to the differences in the prediction models, we believe that the existing MPC-based path tracking controllers can be divided into four categories. We named them linear model predictive control (LMPC), linear error model predictive control (LEMPC), nonlinear model predictive control (NMPC), and nonlinear error model predictive control (NEMPC). Subsequently, we built these four controllers for the same mobile robot and compared them. By comparison, we got some conclusions. The real-time performance of LMPC and LEMPC is good, but they are less robust to reference paths and positioning errors. NMPC performs well when the reference velocity is high and the radius of the reference path is small. It is also robust to positioning errors. However, the real-time performance of NMPC is slightly worse. NEMPC has many disadvantages. Like LMPC and LEMPC, it performs poorly when the reference velocity is high and the radius of the reference path is small. Its real-time performance is also not good enough.},
DOI = {10.3390/electronics8101077}
}



@Article{electronics8101079,
AUTHOR = {Phuc, Le Tran Huu and Jeon, HyeJun and Truong, Nguyen Tam Nguyen and Hak, Jung Jae},
TITLE = {Applying the Haar-cascade Algorithm for Detecting Safety Equipment in Safety Management Systems for Multiple Working Environments},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {1079},
URL = {https://www.mdpi.com/2079-9292/8/10/1079},
ISSN = {2079-9292},
ABSTRACT = {There are many ways to maintain the safety of workers on a working site, such as using a human supervisor, computer supervisor, and smoke&ndash;flame detecting system. In order to create a safety warning system for the working site, the machine-learning algorithm&mdash;Haar-cascade classifier&mdash;was used to build four different classes for safety equipment recognition. Then a proposed algorithm was applied to calculate a score to determine the dangerousness of the current working environment based on the safety equipment and working environment. With this data, the system decides whether it is necessary to give a warning signal. For checking the efficiency of this project, three different situations were installed with this system. Generally, with the promising outcome, this application can be used in maintaining, supervising, and controlling the safety of a worker.},
DOI = {10.3390/electronics8101079}
}



@Article{s19194115,
AUTHOR = {Li, Yuxia and Peng, Bo and He, Lei and Fan, Kunlong and Li, Zhenxu and Tong, Ling},
TITLE = {Road Extraction from Unmanned Aerial Vehicle Remote Sensing Images Based on Improved Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4115},
URL = {https://www.mdpi.com/1424-8220/19/19/4115},
ISSN = {1424-8220},
ABSTRACT = {Roads are vital components of infrastructure, the extraction of which has become a topic of significant interest in the field of remote sensing. Because deep learning has been a popular method in image processing and information extraction, researchers have paid more attention to extracting road using neural networks. This article proposes the improvement of neural networks to extract roads from Unmanned Aerial Vehicle (UAV) remote sensing images. D-Linknet was first considered for its high performance; however, the huge scale of the net reduced computational efficiency. With a focus on the low computational efficiency problem of the popular D-LinkNet, this article made some improvements: (1) Replace the initial block with a stem block. (2) Rebuild the entire network based on ResNet units with a new structure, allowing for the construction of an improved neural network D-Linknetplus. (3) Add a 1 &times; 1 convolution layer before DBlock to reduce the input feature maps, reducing parameters and improving computational efficiency. Add another 1 &times; 1 convolution layer after DBlock to recover the required number of output channels. Accordingly, another improved neural network B-D-LinknetPlus was built. Comparisons were performed between the neural nets, and the verification were made with the Massachusetts Roads Dataset. The results show improved neural networks are helpful in reducing the network size and developing the precision needed for road extraction.},
DOI = {10.3390/s19194115}
}



@Article{app9194010,
AUTHOR = {Nguyen, Ngoc Phi and Hong, Sung Kyung},
TITLE = {Active Fault-Tolerant Control of a Quadcopter against Time-Varying Actuator Faults and Saturations Using Sliding Mode Backstepping Approach},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4010},
URL = {https://www.mdpi.com/2076-3417/9/19/4010},
ISSN = {2076-3417},
ABSTRACT = {Fault-tolerant control is becoming an interesting topic because of its reliability and safety. This paper reports an active fault-tolerant control method for a quadcopter unmanned aerial vehicle (UAV) to handle actuator faults, disturbances, and input constraints. A robust fault diagnosis based on the      H &infin;      scheme was designed to estimate the magnitude of a time-varying fault in the presence of disturbances with unknown upper bounds. Once the fault estimation was complete, a fault-tolerant control scheme was proposed for the attitude system, using adaptive sliding mode backstepping control to accommodate the actuator faults, despite actuator saturation limitation and disturbances. The Lyapunov theory was applied to prove the robustness and stability of the closed-loop system under faulty operation. Simulation results show the effectiveness of the fault diagnosis scheme and proposed controller for handling actuator faults.},
DOI = {10.3390/app9194010}
}



@Article{rs11192238,
AUTHOR = {Jiao, Leilei and Sun, Weiwei and Yang, Gang and Ren, Guangbo and Liu, Yinnian},
TITLE = {A Hierarchical Classification Framework of Satellite Multispectral/Hyperspectral Images for Mapping Coastal Wetlands},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {2238},
URL = {https://www.mdpi.com/2072-4292/11/19/2238},
ISSN = {2072-4292},
ABSTRACT = {Mapping different land cover types with satellite remote sensing data is significant for restoring and protecting natural resources and ecological services in coastal wetlands. In this paper, we propose a hierarchical classification framework (HCF) that implements two levels of classification scheme to identify different land cover types of coastal wetlands. The first level utilizes the designed decision tree to roughly group land covers into four rough classes and the second level combines multiple features (i.e., spectral feature, texture feature and geometric feature) of each class to distinguish different subtypes of land covers in each rough class. Two groups of classification experiments on Landsat and Sentinel multispectral data and China Gaofen (GF)-5 hyperspectral data are carried out in order to testify the classification behaviors of two famous coastal wetlands of China, that is, Yellow River Estuary and Yancheng coastal wetland. Experimental results on Landsat data show that the proposed HCF performs better than support vector machine and random forest in classifying land covers of coastal wetlands. Moreover, HCF is suitable for both multispectral data and hyperspectral data and the GF-5 data is superior to Landsat-8 and Sentinel-2 multispectral data in obtaining fine classification results of coastal wetlands.},
DOI = {10.3390/rs11192238}
}



@Article{s19194196,
AUTHOR = {Zhang, Lingling and Tang, Chengkai and Zhang, Yi and Song, Houbing},
TITLE = {Inertial-Navigation-Aided Single-Satellite Highly Dynamic Positioning Algorithm},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {19},
ARTICLE-NUMBER = {4196},
URL = {https://www.mdpi.com/1424-8220/19/19/4196},
ISSN = {1424-8220},
ABSTRACT = {Nowadays, research on global navigation satellite systems (GNSS) has reached a certain level of maturity to provide high-precision positioning services in many applications. Nonetheless, there are challenging GNSS-denial environments where a temporarily deployed single-satellite positioning system is a promising choice. To further meet the emergency call of highly dynamic targets in such situations, an augmented single-satellite positioning algorithm is proposed in this paper. First, the initial location of the highly dynamic target is found by real-time displacement feedback from the inertial navigation system (INS). Then, considering the continuity of position change, and taking advantage of the high accuracy and robustness of the unscented Kalman filter (UKF), target location is through iteration and fusion. Comparing this proposed method with the least-squares Newton-iterative Doppler single-satellite positioning system and the pseudorange rate-assisted method under synthetic error conditions, the positioning error of our algorithm was     10 %     less than the other two algorithms. This verified the validation of our algorithm in the single-satellite system with highly dynamic targets.},
DOI = {10.3390/s19194196}
}



@Article{agronomy9100618,
AUTHOR = {Hassler, Samuel C. and Baysal-Gurel, Fulya},
TITLE = {Unmanned Aircraft System (UAS) Technology and Applications in Agriculture},
JOURNAL = {Agronomy},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {618},
URL = {https://www.mdpi.com/2073-4395/9/10/618},
ISSN = {2073-4395},
ABSTRACT = {Numerous sensors have been developed over time for precision agriculture; though, only recently have these sensors been incorporated into the new realm of unmanned aircraft systems (UAS). This UAS technology has allowed for a more integrated and optimized approach to various farming tasks such as field mapping, plant stress detection, biomass estimation, weed management, inventory counting, and chemical spraying, among others. These systems can be highly specialized depending on the particular goals of the researcher or farmer, yet many aspects of UAS are similar. All systems require an underlying platform&mdash;or unmanned aerial vehicle (UAV)&mdash;and one or more peripherals and sensing equipment such as imaging devices (RGB, multispectral, hyperspectral, near infra-red, RGB depth), gripping tools, or spraying equipment. Along with these wide-ranging peripherals and sensing equipment comes a great deal of data processing. Common tools to aid in this processing include vegetation indices, point clouds, machine learning models, and statistical methods. With any emerging technology, there are also a few considerations that need to be analyzed like legal constraints, economic trade-offs, and ease of use. This review then concludes with a discussion on the pros and cons of this technology, along with a brief outlook into future areas of research regarding UAS technology in agriculture.},
DOI = {10.3390/agronomy9100618}
}



@Article{rs11202343,
AUTHOR = {Zhang, Jianyong and Zhao, Yanling and Abbott, A. Lynn and Wynne, Randolph H. and Hu, Zhenqi and Zou, Yuzhu and Tian, Shuaishuai},
TITLE = {Automated Mapping of Typical Cropland Strips in the North China Plain Using Small Unmanned Aircraft Systems (sUAS) Photogrammetry},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2343},
URL = {https://www.mdpi.com/2072-4292/11/20/2343},
ISSN = {2072-4292},
ABSTRACT = {Accurate mapping of agricultural fields is needed for many purposes, including irrigation decisions and cadastral management. This paper is concerned with the automated mapping of cropland strips that are common in the North China Plain. These strips are commonly 3&ndash;8 m in width and 50&ndash;300 m in length, and are separated by small ridges that assist with irrigation. Conventional surveying methods are labor-intensive and time-consuming for this application, and only limited performance is possible with very high resolution satellite images. Small Unmanned Aircraft System (sUAS) images could provide an alternative approach to ridge detection and strip mapping. This paper presents a novel method for detecting cropland strips, utilizing centimeter spatial resolution imagery captured by sUAS flying at low altitude (60 m). Using digital surface models (DSM) and ortho-rectified imagery from sUAS data, this method extracts candidate ridge locations by surface roughness segmentation in combination with geometric constraints. This method then exploits vegetation removal and morphological operations to refine candidate ridge elements, leading to polyline-based representations of cropland strip boundaries. This procedure has been tested using sUAS data from four typical cropland plots located approximately 60 km west of Jinan, China. The plots contained early winter wheat. The results indicated an ability to detect ridges with comparatively high recall and precision (96.8% and 95.4%, respectively). Cropland strips were extracted with over 98.9% agreement relative to ground truth, with kappa coefficients over 97.4%. To our knowledge, this method is the first to attempt cropland strip mapping using centimeter spatial resolution sUAS images. These results have demonstrated that sUAS mapping is a viable approach for data collection to assist in agricultural land management in the North China Plain.},
DOI = {10.3390/rs11202343}
}



@Article{rs11202375,
AUTHOR = {Zhang, Dongyan and Wang, Daoyong and Gu, Chunyan and Jin, Ning and Zhao, Haitao and Chen, Gao and Liang, Hongyi and Liang, Dong},
TITLE = {Using Neural Network to Identify the Severity of Wheat Fusarium Head Blight in the Field Environment},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2375},
URL = {https://www.mdpi.com/2072-4292/11/20/2375},
ISSN = {2072-4292},
ABSTRACT = {Fusarium head blight (FHB), one of the most important diseases of wheat, mainly occurs in the ear. Given that the severity of the disease cannot be accurately identified, the cost of pesticide application increases every year, and the agricultural ecological environment is also polluted. In this study, a neural network (NN) method was proposed based on the red-green-blue (RGB) image to segment wheat ear and disease spot in the field environment, and then to determine the disease grade. Firstly, a segmentation dataset of single wheat ear was constructed to provide a benchmark for the segmentation of the wheat ear. Secondly, a segmentation model of single wheat ear based on the fully convolutional network (FCN) was established to effectively realize the segmentation of the wheat ear in the field environment. An FHB segmentation algorithm was proposed based on a pulse-coupled neural network (PCNN) with K-means clustering of the improved artificial bee colony (IABC) to segment the diseased spot of wheat ear by automatic optimization of PCNN parameters. Finally, the disease grade was calculated using the ratio of the disease spot to the whole wheat ear. The experimental results show that: (1) the accuracy of the segmentation model for single wheat ear constructed in this study is 0.981. The segmentation time is less than 1 s, indicating that the model can quickly and accurately segment wheat ear in the field environment; (2) the segmentation method of the disease spot performed under each evaluation indicator is improved compared with the traditional segmentation methods, and the accuracy is 0.925 in the disease severity identification. These research results can provide important reference value for grading wheat FHB in the field environment, which also can be beneficial for real-time monitoring of other crops&rsquo; diseases under near-Earth remote sensing.},
DOI = {10.3390/rs11202375}
}



@Article{ijgi8100454,
AUTHOR = {Kang, Junfeng and Fang, Lei and Li, Shuang and Wang, Xiangrong},
TITLE = {Parallel Cellular Automata Markov Model for Land Use Change Prediction over MapReduce Framework},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {454},
URL = {https://www.mdpi.com/2220-9964/8/10/454},
ISSN = {2220-9964},
ABSTRACT = {The Cellular Automata Markov model combines the cellular automata (CA) model&rsquo;s ability to simulate the spatial variation of complex systems and the long-term prediction of the Markov model. In this research, we designed a parallel CA-Markov model based on the MapReduce framework. The model was divided into two main parts: A parallel Markov model based on MapReduce (Cloud-Markov), and comprehensive evaluation method of land-use changes based on cellular automata and MapReduce (Cloud-CELUC). Choosing Hangzhou as the study area and using Landsat remote-sensing images from 2006 and 2013 as the experiment data, we conducted three experiments to evaluate the parallel CA-Markov model on the Hadoop environment. Efficiency evaluations were conducted to compare Cloud-Markov and Cloud-CELUC with different numbers of data. The results showed that the accelerated ratios of Cloud-Markov and Cloud-CELUC were 3.43 and 1.86, respectively, compared with their serial algorithms. The validity test of the prediction algorithm was performed using the parallel CA-Markov model to simulate land-use changes in Hangzhou in 2013 and to analyze the relationship between the simulation results and the interpretation results of the remote-sensing images. The Kappa coefficients of construction land, natural-reserve land, and agricultural land were 0.86, 0.68, and 0.66, respectively, which demonstrates the validity of the parallel model. Hangzhou land-use changes in 2020 were predicted and analyzed. The results show that the central area of construction land is rapidly increasing due to a developed transportation system and is mainly transferred from agricultural land.},
DOI = {10.3390/ijgi8100454}
}



@Article{app9204312,
AUTHOR = {Xia, Lang and Zhang, Ruirui and Chen, Liping and Huang, Yanbo and Xu, Gang and Wen, Yao and Yi, Tongchuan},
TITLE = {Monitor Cotton Budding Using SVM and UAV Images},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4312},
URL = {https://www.mdpi.com/2076-3417/9/20/4312},
ISSN = {2076-3417},
ABSTRACT = {Monitoring the cotton budding rate is important for growers so that they can replant cotton in a timely fashion at locations at which cotton density is sparse. In this study, a true-color camera was mounted on an unmanned aerial vehicle (UAV) and used to collect images of young cotton plants to estimate the germination of cotton plants. The collected images were preprocessed by stitching them together to obtain the single orthomosaic image. The support-vector machine method and maximum likelihood classification method were conducted to identify the cotton plants in the image. The accuracy evaluation indicated the overall accuracy of the classification for SVM is 96.65% with the Kappa coefficient of 93.99%, while for maximum likelihood classification, the accuracy is 87.85% with a Kappa coefficient of 80.67%. A method based on the morphological characteristics of cotton plants was proposed to identify and count the overlapping cotton plants in this study. The analysis showed that the method can improve the detection accuracy by 6.3% when compared to without it. The validation based on visual interpretation indicated that the method presented an accuracy of 91.13%. The study showed that the minimal resolution of no less than 1.2 cm/pixel in practice for image collection is necessary in order to recognize cotton plants accurately.},
DOI = {10.3390/app9204312}
}



@Article{s19204484,
AUTHOR = {García Rubio, Víctor and Rodrigo Ferrán, Juan Antonio and Menéndez García, Jose Manuel and Sánchez Almodóvar, Nuria and Lalueza Mayordomo, José María and Álvarez, Federico},
TITLE = {Automatic Change Detection System over Unmanned Aerial Vehicle Video Sequences Based on Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {4484},
URL = {https://www.mdpi.com/1424-8220/19/20/4484},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the use of unmanned aerial vehicles (UAVs) for surveillance tasks has increased considerably. This technology provides a versatile and innovative approach to the field. However, the automation of tasks such as object recognition or change detection usually requires image processing techniques. In this paper we present a system for change detection in video sequences acquired by moving cameras. It is based on the combination of image alignment techniques with a deep learning model based on convolutional neural networks (CNNs). This approach covers two important topics. Firstly, the capability of our system to be adaptable to variations in the UAV flight. In particular, the difference of height between flights, and a slight modification of the camera&rsquo;s position or movement of the UAV because of natural conditions such as the effect of wind. These modifications can be produced by multiple factors, such as weather conditions, security requirements or human errors. Secondly, the precision of our model to detect changes in diverse environments, which has been compared with state-of-the-art methods in change detection. This has been measured using the Change Detection 2014 dataset, which provides a selection of labelled images from different scenarios for training change detection algorithms. We have used images from dynamic background, intermittent object motion and bad weather sections. These sections have been selected to test our algorithm&rsquo;s robustness to changes in the background, as in real flight conditions. Our system provides a precise solution for these scenarios, as the mean F-measure score from the image analysis surpasses 97%, and a significant precision in the intermittent object motion category, where the score is above 99%.},
DOI = {10.3390/s19204484}
}



@Article{rs11202415,
AUTHOR = {Woodget, Amy S. and Dietrich, James T. and Wilson, Robin T.},
TITLE = {Quantifying Below-Water Fluvial Geomorphic Change: The Implications of Refraction Correction, Water Surface Elevations, and Spatially Variable Error},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2415},
URL = {https://www.mdpi.com/2072-4292/11/20/2415},
ISSN = {2072-4292},
ABSTRACT = {Much of the geomorphic work of rivers occurs underwater. As a result, high resolutionquantification of geomorphic change in these submerged areas is important. Currently, to quantify thischange, multiple methods are required to get high resolution data for both the exposed and submergedareas. Remote sensing methods are often limited to the exposed areas due to the challenges imposedby the water, and those remote sensing methods for below the water surface require the collection ofextensive calibration data in-channel, which is time-consuming, labour-intensive, and sometimesprohibitive in dicult-to-access areas. Within this paper, we pioneer a novel approach for quantifyingabove- and below-water geomorphic change using Structure-from-Motion photogrammetry andinvestigate the implications of water surface elevations, refraction correction measures, and thespatial variability of topographic errors. We use two epochs of imagery from a site on the River Teme,Herefordshire, UK, collected using a remotely piloted aircraft system (RPAS) and processed usingStructure-from-Motion (SfM) photogrammetry. For the first time, we show that: (1) Quantification ofsubmerged geomorphic change to levels of accuracy commensurate with exposed areas is possiblewithout the need for calibration data or a dierent method from exposed areas; (2) there is minimaldierence in results produced by dierent refraction correction procedures using predominantlynadir imagery (small angle vs. multi-view), allowing users a choice of software packages/processingcomplexity; (3) improvements to our estimations of water surface elevations are critical for accuratetopographic estimation in submerged areas and can reduce mean elevation error by up to 73%;and (4) we can use machine learning, in the form of multiple linear regressions, and a Gaussian Na&iuml;veBayes classifier, based on the relationship between error and 11 independent variables, to generate ahigh resolution, spatially continuous model of geomorphic change in submerged areas, constrained byspatially variable error estimates. Our multiple regression model is capable of explaining up to 54%of magnitude and direction of topographic error, with accuracies of less than 0.04 m. With on-goingtesting and improvements, this machine learning approach has potential for routine application inspatially variable error estimation within the RPAS&ndash;SfM workflow.},
DOI = {10.3390/rs11202415}
}



@Article{rs11202427,
AUTHOR = {Ghaffarian, Saman and Kerle, Norman and Pasolli, Edoardo and Jokar Arsanjani, Jamal},
TITLE = {Post-Disaster Building Database Updating Using Automated Deep Learning: An Integration of Pre-Disaster OpenStreetMap and Multi-Temporal Satellite Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2427},
URL = {https://www.mdpi.com/2072-4292/11/20/2427},
ISSN = {2072-4292},
ABSTRACT = {First responders and recovery planners need accurate and quickly derived information about the status of buildings as well as newly built ones to both help victims and to make decisions for reconstruction processes after a disaster. Deep learning and, in particular, convolutional neural network (CNN)-based approaches have recently become state-of-the-art methods to extract information from remote sensing images, in particular for image-based structural damage assessment. However, they are predominantly based on manually extracted training samples. In the present study, we use pre-disaster OpenStreetMap building data to automatically generate training samples to train the proposed deep learning approach after the co-registration of the map and the satellite images. The proposed deep learning framework is based on the U-net design with residual connections, which has been shown to be an effective method to increase the efficiency of CNN-based models. The ResUnet is followed by a Conditional Random Field (CRF) implementation to further refine the results. Experimental analysis was carried out on selected very high resolution (VHR) satellite images representing various scenarios after the 2013 Super Typhoon Haiyan in both the damage and the recovery phases in Tacloban, the Philippines. The results show the robustness of the proposed ResUnet-CRF framework in updating the building map after a disaster for both damage and recovery situations by producing an overall F1-score of 84.2%.},
DOI = {10.3390/rs11202427}
}



@Article{rs11202455,
AUTHOR = {He, Zhi and He, Dan and Mei, Xiangqin and Hu, Saihan},
TITLE = {Wetland Classification Based on a New Efficient Generative Adversarial Network and Jilin-1 Satellite Image},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2455},
URL = {https://www.mdpi.com/2072-4292/11/20/2455},
ISSN = {2072-4292},
ABSTRACT = {Recent studies have shown that deep learning methods provide useful tools for wetland classification. However, it is difficult to perform species-level classification with limited labeled samples. In this paper, we propose a semi-supervised method for wetland species classification by using a new efficient generative adversarial network (GAN) and Jilin-1 satellite image. The main contributions of this paper are twofold. First, the proposed method, namely ShuffleGAN, requires only a small number of labeled samples. ShuffleGAN is composed of two neural networks (i.e., generator and discriminator), which perform an adversarial game in the training phase and ShuffleNet units are added in both generator and discriminator to obtain speed-accuracy tradeoff. Second, ShuffleGAN can perform species-level wetland classification. In addition to distinguishing the wetland areas from non-wetlands, different tree species located in the wetland are also identified, thus providing a more detailed distribution of the wetland land-covers. Experiments are conducted on the Haizhu Lake wetland data acquired by the Jilin-1 satellite. Compared with existing GAN, the improvement in overall accuracy (OA) of the proposed ShuffleGAN is more than 2%. This work can not only deepen the application of deep learning in wetland classification but also promote the study of fine classification of wetland land-covers.},
DOI = {10.3390/rs11202455}
}



@Article{rs11202456,
AUTHOR = {Zhu, Wanxue and Sun, Zhigang and Huang, Yaohuan and Lai, Jianbin and Li, Jing and Zhang, Junqiang and Yang, Bin and Li, Binbin and Li, Shiji and Zhu, Kangying and Li, Yang and Liao, Xiaohan},
TITLE = {Improving Field-Scale Wheat LAI Retrieval Based on UAV Remote-Sensing Observations and Optimized VI-LUTs},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {20},
ARTICLE-NUMBER = {2456},
URL = {https://www.mdpi.com/2072-4292/11/20/2456},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) is a key biophysical parameter for monitoring crop growth status, predicting crop yield, and quantifying crop variability in agronomic applications. Mapping the LAI at the field scale using multispectral cameras onboard unmanned aerial vehicles (UAVs) is a promising precision-agriculture application with specific requirements: The LAI retrieval method should be (1) robust so that crop LAI can be estimated with similar accuracy and (2) easy to use so that it can be applied to the adjustment of field management practices. In this study, three UAV remote-sensing missions (UAVs with Micasense RedEdge-M and Cubert S185 cameras) were carried out over six experimental plots from 2018 to 2019 to investigate the performance of reflectance-based lookup tables (LUTs) and vegetation index (VI)-based LUTs generated from the PROSAIL model for wheat LAI retrieval. The effects of the central wavelengths and bandwidths for the VI calculations on the LAI retrieval were further examined. We found that the VI-LUT strategy was more robust and accurate than the reflectance-LUT strategy. The differences in the LAI retrieval accuracy among the four VI-LUTs were small, although the improved modified chlorophyll absorption ratio index-lookup table (MCARI2-LUT) and normalized difference vegetation index-lookup table (NDVI-LUT) performed slightly better. We also found that both of the central wavelengths and bandwidths of the VIs had effects on the LAI retrieval. The VI-LUTs with optimized central wavelengths (red = 612 nm, near-infrared (NIR) = 756 nm) and narrow bandwidths (~4 nm) improved the wheat LAI retrieval accuracy (R2 &ge; 0.75). The results of this study provide an alternative method for retrieving crop LAI, which is robust and easy use for precision-agriculture applications and may be helpful for designing UAV multispectral cameras for agricultural monitoring.},
DOI = {10.3390/rs11202456}
}



@Article{rs11212495,
AUTHOR = {Bohnenkamp, David and Behmann, Jan and Mahlein, Anne-Katrin},
TITLE = {In-Field Detection of Yellow Rust in Wheat on the Ground Canopy and UAV Scale},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2495},
URL = {https://www.mdpi.com/2072-4292/11/21/2495},
ISSN = {2072-4292},
ABSTRACT = {The application of hyperspectral imaging technology for plant disease detection in the field is still challenging. Existing equipment and analysis algorithms are adapted to highly controlled environmental conditions in the laboratory. However, only real time information from the field scale is able to guide plant protection measures and to optimize the use of resources. At the field scale, many parameters such as the optimal measurement distance, informative feature sets, and suitable algorithms have not been investigated. In this study, the hyperspectral detection and quantification of yellow rust in wheat was evaluated using two measurement platforms: a ground-based vehicle and an unmanned aerial vehicle (UAV). Different disease development stages and disease severities were provided in a plot-based field experiment. Measurements were performed weekly during the vegetation period. Data analysis was performed by three prediction algorithms with a focus on the selection of optimal feature sets. In this context, the across-scale application of optimized feature sets, an approach of information transfer between scales, was also evaluated. Relevant aspects for an on-line disease assessment in the field integrating affordable sensor technology, sensor spatial resolution, compact analysis models, and fast evaluation have been outlined and reflected upon. For the first time, a hyperspectral imaging observation experiment of a plant disease was comparatively performed at two scales, ground canopy and UAV.},
DOI = {10.3390/rs11212495}
}



@Article{rs11212499,
AUTHOR = {Xin, Jiang and Zhang, Xinchang and Zhang, Zhiqiang and Fang, Wu},
TITLE = {Road Extraction of High-Resolution Remote Sensing Images Derived from DenseUNet},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2499},
URL = {https://www.mdpi.com/2072-4292/11/21/2499},
ISSN = {2072-4292},
ABSTRACT = {Road network extraction is one of the significant assignments for disaster emergency response, intelligent transportation systems, and real-time updating road network. Road extraction base on high-resolution remote sensing images has become a hot topic. Presently, most of the researches are based on traditional machine learning algorithms, which are complex and computational because of impervious surfaces such as roads and buildings that are discernible in the images. Given the above problems, we propose a new method to extract the road network from remote sensing images using a DenseUNet model with few parameters and robust characteristics. DenseUNet consists of dense connection units and skips connections, which strengthens the fusion of different scales by connections at various network layers. The performance of the advanced method is validated on two datasets of high-resolution images by comparison with three classical semantic segmentation methods. The experimental results show that the method can be used for road extraction in complex scenes.},
DOI = {10.3390/rs11212499}
}



@Article{rs11212505,
AUTHOR = {Crommelinck, Sophie and Koeva, Mila and Yang, Michael Ying and Vosselman, George},
TITLE = {Application of Deep Learning for Delineation of Visible Cadastral Boundaries from Remote Sensing Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2505},
URL = {https://www.mdpi.com/2072-4292/11/21/2505},
ISSN = {2072-4292},
ABSTRACT = {Cadastral boundaries are often demarcated by objects that are visible in remote sensing imagery. Indirect surveying relies on the delineation of visible parcel boundaries from such images. Despite advances in automated detection and localization of objects from images, indirect surveying is rarely automated and relies on manual on-screen delineation. We have previously introduced a boundary delineation workflow, comprising image segmentation, boundary classification and interactive delineation that we applied on Unmanned Aerial Vehicle (UAV) data to delineate roads. In this study, we improve each of these steps. For image segmentation, we remove the need to reduce the image resolution and we limit over-segmentation by reducing the number of segment lines by 80% through filtering. For boundary classification, we show how Convolutional Neural Networks (CNN) can be used for boundary line classification, thereby eliminating the previous need for Random Forest (RF) feature generation and thus achieving 71% accuracy. For interactive delineation, we develop additional and more intuitive delineation functionalities that cover more application cases. We test our approach on more varied and larger data sets by applying it to UAV and aerial imagery of 0.02&ndash;0.25 m resolution from Kenya, Rwanda and Ethiopia. We show that it is more effective in terms of clicks and time compared to manual delineation for parcels surrounded by visible boundaries. Strongest advantages are obtained for rural scenes delineated from aerial imagery, where the delineation effort per parcel requires 38% less time and 80% fewer clicks compared to manual delineation.},
DOI = {10.3390/rs11212505}
}



@Article{rs11212511,
AUTHOR = {Kerle, Norman and Ghaffarian, Saman and Nawrotzki, Raphael and Leppert, Gerald and Lech, Malte},
TITLE = {Evaluating Resilience-Centered Development Interventions with Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2511},
URL = {https://www.mdpi.com/2072-4292/11/21/2511},
ISSN = {2072-4292},
ABSTRACT = {Natural disasters are projected to increase in number and severity, in part due to climate change. At the same time a growing number of disaster risk reduction (DRR) and climate change adaptation measures are being implemented by governmental and non-governmental organizations, and substantial post-disaster donations are frequently pledged. At the same time there has been increasing demand for transparency and accountability, and thus evidence of those measures having a positive effect. We hypothesized that resilience-enhancing interventions should result in less damage during a hazard event, or at least quicker recovery. In this study we assessed recovery over a 3 year period of seven municipalities in the central Philippines devastated by Typhoon Haiyan in 2013. We used very high resolution optical images (&lt;1 m), and created detailed land cover and land use maps for four epochs before and after the event, using a machine learning approach with extreme gradient boosting. The spatially and temporally highly variable recovery maps were then statistically related to detailed questionnaire data acquired by DEval in 2012 and 2016, whose principal aim was to assess the impact of a 10 year land-planning intervention program by the German agency for technical cooperation (GIZ). The survey data allowed very detailed insights into DRR-related perspectives, motivations and drivers of the affected population. To some extent they also helped to overcome the principal limitation of remote sensing, which can effectively describe but not explain the reasons for differential recovery. However, while a number of causal links between intervention parameters and reconstruction was found, the common notion that a resilient community should recover better and more quickly could not be confirmed. The study also revealed a number of methodological limitations, such as the high cost for commercial image data not matching the spatially extensive but also detailed scale of field evaluations, the remote sensing analysis likely overestimating damage and thus providing incorrect recovery metrics, and image data catalogues especially for more remote communities often being incomplete. Nevertheless, the study provides a valuable proof of concept for the synergies resulting from an integration of socio-economic survey data and remote sensing imagery for recovery assessment.},
DOI = {10.3390/rs11212511}
}



@Article{rs11212523,
AUTHOR = {Xia, Wei and Ma, Caihong and Liu, Jianbo and Liu, Shibin and Chen, Fu and Yang, Zhi and Duan, Jianbo},
TITLE = {High-Resolution Remote Sensing Imagery Classification of Imbalanced Data Using Multistage Sampling Method and Deep Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2523},
URL = {https://www.mdpi.com/2072-4292/11/21/2523},
ISSN = {2072-4292},
ABSTRACT = {Class imbalance is a key issue for the application of deep learning for remote sensing image classification because a model generated by imbalanced samples training has low classification accuracy for minority classes. In this study, an accurate classification approach using the multistage sampling method and deep neural networks was proposed to classify imbalanced data. We first balance samples by multistage sampling to obtain the training sets. Then, a state-of-the-art model is adopted by combining the advantages of atrous spatial pyramid pooling (ASPP) and Encoder-Decoder for pixel-wise classification, which are two different types of fully convolutional networks (FCNs) that can obtain contextual information of multiple levels in the Encoder stage. The details and spatial dimensions of targets are restored using such information during the Decoder stage. We employ four deep learning-based classification algorithms (basic FCN, FCN-8S, ASPP, and Encoder-Decoder with ASPP of our approach) on multistage training sets (original, MUS1, and MUS2) of WorldView-3 images in southeastern Qinghai-Tibet Plateau and GF-2 images in northeastern Beijing for comparison. The experiments show that, compared with existing sets (original, MUS1, and identical) and existing method (cost weighting), the MUS2 training set of multistage sampling significantly enhance the classification performance for minority classes. Our approach shows distinct advantages for imbalanced data.},
DOI = {10.3390/rs11212523}
}



@Article{rs11212575,
AUTHOR = {Tavakkoli Piralilou, Sepideh and Shahabi, Hejar and Jarihani, Ben and Ghorbanzadeh, Omid and Blaschke, Thomas and Gholamnia, Khalil and Meena, Sansar Raj and Aryal, Jagannath},
TITLE = {Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2575},
URL = {https://www.mdpi.com/2072-4292/11/21/2575},
ISSN = {2072-4292},
ABSTRACT = {Landslides represent a severe hazard in many areas of the world. Accurate landslide maps are needed to document the occurrence and extent of landslides and to investigate their distribution, types, and the pattern of slope failures. Landslide maps are also crucial for determining landslide susceptibility and risk. Satellite data have been widely used for such investigations—next to data from airborne or unmanned aerial vehicle (UAV)-borne campaigns and Digital Elevation Models (DEMs). We have developed a methodology that incorporates object-based image analysis (OBIA) with three machine learning (ML) methods, namely, the multilayer perceptron neural network (MLP-NN) and random forest (RF), for landslide detection. We identified the optimal scale parameters (SP) and used them for multi-scale segmentation and further analysis. We evaluated the resulting objects using the object pureness index (OPI), object matching index (OMI), and object fitness index (OFI) measures. We then applied two different methods to optimize the landslide detection task: (a) an ensemble method of stacking that combines the different ML methods for improving the performance, and (b) Dempster–Shafer theory (DST), to combine the multi-scale segmentation and classification results. Through the combination of three ML methods and the multi-scale approach, the framework enhanced landslide detection when it was tested for detecting earthquake-triggered landslides in Rasuwa district, Nepal. PlanetScope optical satellite images and a DEM were used, along with the derived landslide conditioning factors. Different accuracy assessment measures were used to compare the results against a field-based landslide inventory. All ML methods yielded the highest overall accuracies ranging from 83.3% to 87.2% when using objects with the optimal SP compared to other SPs. However, applying DST to combine the multi-scale results of each ML method significantly increased the overall accuracies to almost 90%. Overall, the integration of OBIA with ML methods resulted in appropriate landslide detections, but using the optimal SP and ML method is crucial for success.},
DOI = {10.3390/rs11212575}
}



@Article{su11216116,
AUTHOR = {Mangewa, Lazaro J. and Ndakidemi, Patrick A. and Munishi, Linus K.},
TITLE = {Integrating UAV Technology in an Ecological Monitoring System for Community Wildlife Management Areas in Tanzania},
JOURNAL = {Sustainability},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {6116},
URL = {https://www.mdpi.com/2071-1050/11/21/6116},
ISSN = {2071-1050},
ABSTRACT = {Unmanned aerial vehicles (UAV) have recently emerged as a new remote sensing aerial platform, and they are seemingly advancing real-time data generation. Nonetheless, considerable uncertainties remain in the extent to which wildlife managers can integrate UAVs into ecological monitoring systems for wildlife and their habitats. In this review, we discuss the recent progress and gaps in UAV use in wildlife conservation and management. The review notes that there is scanty information on UAV use in ecological monitoring of medium-to-large mammals found in groups in heterogeneous habitats. We also explore the need and extent to which the technology can be integrated into ecological monitoring systems for mammals in heterogeneous habitats and in topographically-challenging community wildlife-management areas, as a complementary platform to the traditional techniques. Based on its ability to provide high-resolution images in real-time, further experiments on its wider use in the ecological monitoring of wildlife on a spatiotemporal scale are important. The experimentation outputs will make the UAV a very reliable remote sensing platform that addresses the challenges facing conventional techniques.},
DOI = {10.3390/su11216116}
}



@Article{s19214794,
AUTHOR = {Rodriguez-Ramos, Alejandro and Alvarez-Fernandez, Adrian and Bavle, Hriday and Campoy, Pascual and How, Jonathan P.},
TITLE = {Vision-Based Multirotor Following Using Synthetic Learning Techniques},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {4794},
URL = {https://www.mdpi.com/1424-8220/19/21/4794},
ISSN = {1424-8220},
ABSTRACT = {Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights).},
DOI = {10.3390/s19214794}
}



@Article{rs11212585,
AUTHOR = {Fromm, Michael and Schubert, Matthias and Castilla, Guillermo and Linke, Julia and McDermid, Greg},
TITLE = {Automated Detection of Conifer Seedlings in Drone Imagery Using Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {21},
ARTICLE-NUMBER = {2585},
URL = {https://www.mdpi.com/2072-4292/11/21/2585},
ISSN = {2072-4292},
ABSTRACT = {Monitoring tree regeneration in forest areas disturbed by resource extraction is a requirement for sustainably managing the boreal forest of Alberta, Canada. Small remotely piloted aircraft systems (sRPAS, a.k.a. drones) have the potential to decrease the cost of field surveys drastically, but produce large quantities of data that will require specialized processing techniques. In this study, we explored the possibility of using convolutional neural networks (CNNs) on this data for automatically detecting conifer seedlings along recovering seismic lines: a common legacy footprint from oil and gas exploration. We assessed three different CNN architectures, of which faster region-CNN (R-CNN) performed best (mean average precision 81%). Furthermore, we evaluated the effects of training-set size, season, seedling size, and spatial resolution on the detection performance. Our results indicate that drone imagery analyzed by artificial intelligence can be used to detect conifer seedling in regenerating sites with high accuracy, which increases with the size in pixels of the seedlings. By using a pre-trained network, the size of the training dataset can be reduced to a couple hundred seedlings without any significant loss of accuracy. Furthermore, we show that combining data from different seasons yields the best results. The proposed method is a first step towards automated monitoring of forest restoration/regeneration.},
DOI = {10.3390/rs11212585}
}



@Article{ijgi8110501,
AUTHOR = {Ham, Sungil and Im, Junhyuck and Kim, Minjun and Cho, Kuk},
TITLE = {Construction and Verification of a High-Precision Base Map for an Autonomous Vehicle Monitoring System},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {501},
URL = {https://www.mdpi.com/2220-9964/8/11/501},
ISSN = {2220-9964},
ABSTRACT = {For autonomous driving, a control system that supports precise road maps is required to monitor the operation status of autonomous vehicles in the research stage. Such a system is also required for research related to automobile engineering, sensors, and artificial intelligence. The design of Google Maps and other map services is limited to the provision of map support at 20 levels of high-resolution precision. An ideal map should include information on roads, autonomous vehicles, and Internet of Things (IOT) facilities that support autonomous driving. The aim of this study was to design a map suitable for the control of autonomous vehicles in Gyeonggi Province in Korea. This work was part of the project &ldquo;Building a Testbed for Pilot Operations of Autonomous Vehicles&rdquo;. The map design scheme was redesigned for an autonomous vehicle control system based on the &ldquo;Easy Map&rdquo; developed by the National Geography Center, which provides free design schema. In addition, a vector-based precision map, including roads, sidewalks, and road markings, was produced to provide content suitable for 20 levels. A hybrid map that combines the vector layer of the road and an unmanned aerial vehicle (UAV) orthographic map was designed to facilitate vehicle identification. A control system that can display vehicle and sensor information based on the designed map was developed, and an environment to monitor the operation of autonomous vehicles was established. Finally, the high-precision map was verified through an accuracy test and driving data from autonomous vehicles.},
DOI = {10.3390/ijgi8110501}
}



@Article{app9224756,
AUTHOR = {Yu, Lanbing and Cao, Ying and Zhou, Chao and Wang, Yang and Huo, Zhitao},
TITLE = {Landslide Susceptibility Mapping Combining Information Gain Ratio and Support Vector Machines: A Case Study from Wushan Segment in the Three Gorges Reservoir Area, China},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4756},
URL = {https://www.mdpi.com/2076-3417/9/22/4756},
ISSN = {2076-3417},
ABSTRACT = {Landslides are destructive geological hazards that occur all over the world. Due to the periodic regulation of reservoir water level, a large number of landslides occur in the Three Gorges Reservoir area (TGRA). The main objective of this study was to explore the preference of machine learning models for landslide susceptibility mapping in the TGRA. The Wushan segment of TGRA was selected as a case study. At first, 165 landslides were identified and a total of 14 landslide causal factors were constructed from different data sources. Multicollinearity analysis and information gain ratio (IGR) model were applied to select landslide causal factors. Subsequently, the landslide susceptibility mapping using the calculated results of four models, namely, support vector machines (SVM), artificial neural networks (ANN), classification and regression tree (CART), and logistic regression (LR). The accuracy of these four maps were evaluated using the receive operating characteristic (ROC) and the accuracy statistic. Results revealed that eliminating the inconsequential factors can perhaps improve the accuracy of landslide susceptibility modelling, and the SVM model had the best performance in this study, providing strong technical support for landslide susceptibility modelling in TGRA.},
DOI = {10.3390/app9224756}
}



@Article{s19224851,
AUTHOR = {Zhou, Jun and Tian, Yichen and Yuan, Chao and Yin, Kai and Yang, Guang and Wen, Meiping},
TITLE = {Improved UAV Opium Poppy Detection Using an Updated YOLOv3 Model},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4851},
URL = {https://www.mdpi.com/1424-8220/19/22/4851},
ISSN = {1424-8220},
ABSTRACT = {Rapid detection of illicit opium poppy plants using UAV (unmanned aerial vehicle) imagery has become an important means to prevent and combat crimes related to drug cultivation. However, current methods rely on time-consuming visual image interpretation. Here, the You Only Look Once version 3 (YOLOv3) network structure was used to assess the influence that different backbone networks have on the average precision and detection speed of an UAV-derived dataset of poppy imagery, with MobileNetv2 (MN) selected as the most suitable backbone network. A Spatial Pyramid Pooling (SPP) unit was introduced and Generalized Intersection over Union (GIoU) was used to calculate the coordinate loss. The resulting SPP-GIoU-YOLOv3-MN model improved the average precision by 1.62% (from 94.75% to 96.37%) without decreasing speed and achieved an average precision of 96.37%, with a detection speed of 29 FPS using an RTX 2080Ti platform. The sliding window method was used for detection in complete UAV images, which took approximately 2.2 sec/image, approximately 10&times; faster than visual interpretation. The proposed technique significantly improved the efficiency of poppy detection in UAV images while also maintaining a high detection accuracy. The proposed method is thus suitable for the rapid detection of illicit opium poppy cultivation in residential areas and farmland where UAVs with ordinary visible light cameras can be operated at low altitudes (relative height &lt; 200 m).},
DOI = {10.3390/s19224851}
}



@Article{s19224893,
AUTHOR = {Shahabi, Hejar and Jarihani, Ben and Tavakkoli Piralilou, Sepideh and Chittleborough, David and Avand, Mohammadtaghi and Ghorbanzadeh, Omid},
TITLE = {A Semi-Automated Object-Based Gully Networks Detection Using Different Machine Learning Models: A Case Study of Bowen Catchment, Queensland, Australia},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4893},
URL = {https://www.mdpi.com/1424-8220/19/22/4893},
ISSN = {1424-8220},
ABSTRACT = {Gully erosion is a dominant source of sediment and particulates to the Great Barrier Reef (GBR) World Heritage area. We selected the Bowen catchment, a tributary of the Burdekin Basin, as our area of study; the region is associated with a high density of gully networks. We aimed to use a semi-automated object-based gully networks detection process using a combination of multi-source and multi-scale remote sensing and ground-based data. An advanced approach was employed by integrating geographic object-based image analysis (GEOBIA) with current machine learning (ML) models. These included artificial neural networks (ANN), support vector machines (SVM), and random forests (RF), and an ensemble ML model of stacking to deal with the spatial scaling problem in gully networks detection. Spectral indices such as the normalized difference vegetation index (NDVI) and topographic conditioning factors, such as elevation, slope, aspect, topographic wetness index (TWI), slope length (SL), and curvature, were generated from Sentinel 2A images and the ALOS 12-m digital elevation model (DEM), respectively. For image segmentation, the ESP2 tool was used to obtain three optimal scale factors. On using object pureness index (OPI), object matching index (OMI), and object fitness index (OFI), the accuracy of each scale in image segmentation was evaluated. The scale parameter of 45 with OFI of 0.94, which is a combination of OPI and OMI indices, proved to be the optimal scale parameter for image segmentation. Furthermore, segmented objects based on scale 45 were overlaid with 70% and 30% of a prepared gully inventory map to select the ML models&rsquo; training and testing objects, respectively. The quantitative accuracy assessment methods of Precision, Recall, and an F1 measure were used to evaluate the model&rsquo;s performance. Integration of GEOBIA with the stacking model using a scale of 45 resulted in the highest accuracy in detection of gully networks with an F1 measure value of 0.89. Here, we conclude that the adoption of optimal scale object definition in the GEOBIA and application of the ensemble stacking of ML models resulted in higher accuracy in the detection of gully networks.},
DOI = {10.3390/s19224893}
}



@Article{s19224895,
AUTHOR = {Silva, Maurício R. and Souza, Elitelma S. and Alsina, Pablo J. and Leite, Deyvid L. and Morais, Mateus R. and Pereira, Diego S. and Nascimento, Luís B. P. and Medeiros, Adelardo A. D. and Junior, Francisco H. Cunha and Nogueira, Marcelo B. and Albuquerque, Glauberto L. A. and Dantas, João B. D.},
TITLE = {Performance Evaluation of Multi-UAV Network Applied to Scanning Rocket Impact Area},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4895},
URL = {https://www.mdpi.com/1424-8220/19/22/4895},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a communication network for a squadron of unmanned aerial vehicles (UAVs) to be used in the scanning rocket impact area for Barreira do Inferno Launch Center&mdash;CLBI (Rio Grande do Norte, Brazil), aiming at detecting intruder boats. The main features of communication networks associated with multi-UAV systems are presented. This system sends information through Wireless Sensor Networks (WSN). After comparing and analyzing area scanning strategies, it presents the specification of a data communication network architecture for a squadron of UAVs within a sensor network using XBee Pro 900HP S3B modules. A brief description is made about the initial information from the construction of the system. The embedded hardware and the design procedure of a dedicated communication antenna to the XBee modules are presented. In order to evaluate the performance of the proposed architecture in terms of robustness and reliability, a set of experimental tests in different communication scenarios is carried out. Network management software is employed to measure the throughput, packet loss and other performance indicators in the communication links between the different network nodes. Experimental results allow verifying the quality and performance of the network nodes, as well as the reliability of the communication links, assessing signal received quality, range and latency.},
DOI = {10.3390/s19224895}
}



@Article{info10110349,
AUTHOR = {Tsouros, Dimosthenis C. and Bibi, Stamatia and Sarigiannidis, Panagiotis G.},
TITLE = {A Review on UAV-Based Applications for Precision Agriculture},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {349},
URL = {https://www.mdpi.com/2078-2489/10/11/349},
ISSN = {2078-2489},
ABSTRACT = {Emerging technologies such as Internet of Things (IoT) can provide significant potential in Smart Farming and Precision Agriculture applications, enabling the acquisition of real-time environmental data. IoT devices such as Unmanned Aerial Vehicles (UAVs) can be exploited in a variety of applications related to crops management, by capturing high spatial and temporal resolution images. These technologies are expected to revolutionize agriculture, enabling decision-making in days instead of weeks, promising significant reduction in cost and increase in the yield. Such decisions enable the effective application of farm inputs, supporting the four pillars of precision agriculture, i.e., apply the right practice, at the right place, at the right time and with the right quantity. However, the actual proliferation and exploitation of UAVs in Smart Farming has not been as robust as expected mainly due to the challenges confronted when selecting and deploying the relevant technologies, including the data acquisition and image processing methods. The main problem is that still there is no standardized workflow for the use of UAVs in such applications, as it is a relatively new area. In this article, we review the most recent applications of UAVs for Precision Agriculture. We discuss the most common applications, the types of UAVs exploited and then we focus on the data acquisition methods and technologies, appointing the benefits and drawbacks of each one. We also point out the most popular processing methods of aerial imagery and discuss the outcomes of each method and the potential applications of each one in the farming operations.},
DOI = {10.3390/info10110349}
}



@Article{app9224829,
AUTHOR = {Riid, Andri and Lõuk, Roland and Pihlak, Rene and Tepljakov, Aleksei and Vassiljeva, Kristina},
TITLE = {Pavement Distress Detection with Deep Learning Using the Orthoframes Acquired by a Mobile Mapping System},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4829},
URL = {https://www.mdpi.com/2076-3417/9/22/4829},
ISSN = {2076-3417},
ABSTRACT = {The subject matter of this research article is automatic detection of pavement distress on highway roads using computer vision algorithms. Specifically, deep learning convolutional neural network models are employed towards the implementation of the detector. Source data for training the detector come in the form of orthoframes acquired by a mobile mapping system. Compared to our previous work, the orthoframes are generally of better quality, but more importantly, in this work, we introduce a manual preprocessing step: sets of orthoframes are carefully selected for training and manually digitized to ensure adequate performance of the detector. Pretrained convolutional neural networks are then fine-tuned for the problem of pavement distress detection. Corresponding experimental results are provided and analyzed and indicate a successful implementation of the detector.},
DOI = {10.3390/app9224829}
}



@Article{ijgi8110511,
AUTHOR = {Yu, Hao and Wang, Lei and Wang, Zongming and Ren, Chunying and Zhang, Bai},
TITLE = {Using Landsat OLI and Random Forest to Assess Grassland Degradation with Aboveground Net Primary Production and Electrical Conductivity Data},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {511},
URL = {https://www.mdpi.com/2220-9964/8/11/511},
ISSN = {2220-9964},
ABSTRACT = {Grassland coverage, aboveground net primary production (ANPP), and species composition are used as indicators of grassland degradation. However, soil salinization deficiency, which is also a factor of grassland degradation, is rarely used in grassland degradation assessment in semiarid regions. We assessed grassland degradation by its quality, quantity, and spatial pattern over semiarid west Jilin, China. Considering soil salinization in west Jilin, electrical conductivity (EC) is used as an index with ANPP to assess grassland degradation. First, the spatial distribution of the grassland was measured with information mined from multi-temporal remote sensing images using an object-based image analysis combined with classification and decision tree methods. Second, with 166 field samples, we utilized the random forest (RF) algorithm as the variable selection and regression method for predicting EC and ANPP. Finally, we created a new grassland degradation model (GDM) based on ANPP and EC. The results showed the R2 (0.91) and RMSE (0.057 mS/cm) of the EC model were generally highest and lowest when the ntree was 400; the ANPP model was optimal (R2 = 0.85 and RMSE = 15.81 gC/m2) when the ntree was 600. Grassland area of west Jilin was 609.67 &times; 103 ha in 2017, there were 373.79 &times; 103 ha of degraded grassland, with 210.47 &times; 103 ha being intensively degraded. This paper surpasses past limitations of excessive reliance on vegetation index to construct a grassland degradation model which considers the characteristics of the study area and soil salinity. The results confirm the positive influence of the ecological conservation projects sponsored by the government. The research outcome could offer supporting data for decision making to help alleviate grassland degradation and promote the rehabilitation of grassland vegetation.},
DOI = {10.3390/ijgi8110511}
}



@Article{rs11222641,
AUTHOR = {Zhao, Longcai and Li, Qiangzi and Zhang, Yuan and Wang, Hongyan and Du, Xin},
TITLE = {Integrating the Continuous Wavelet Transform and a Convolutional Neural Network to Identify Vineyard Using Time Series Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2641},
URL = {https://www.mdpi.com/2072-4292/11/22/2641},
ISSN = {2072-4292},
ABSTRACT = {Grape is an economic crop of great importance and is widely cultivated in China. With the development of remote sensing, abundant data sources strongly guarantee that researchers can identify crop types and map their spatial distributions. However, to date, only a few studies have been conducted to identify vineyards using satellite image data. In this study, a vineyard is identified using satellite images, and a new approach is proposed that integrates the continuous wavelet transform (CWT) and a convolutional neural network (CNN). Specifically, the original time series of the normalized difference vegetation index (NDVI), enhanced vegetation index (EVI), and green chlorophyll vegetation index (GCVI) are reconstructed by applying an iterated Savitzky-Golay (S-G) method to form a daily time series for a full year; then, the CWT is applied to three reconstructed time series to generate corresponding scalograms; and finally, CNN technology is used to identify vineyards based on the stacked scalograms. In addition to our approach, a traditional and common approach that uses a random forest (RF) to identify crop types based on multi-temporal images is selected as the control group. The experimental results demonstrated the following: (i) the proposed approach was comprehensively superior to the RF approach; it improved the overall accuracy by 9.87% (up to 89.66%); (ii) the CWT had a stable and effective influence on the reconstructed time series, and the scalograms fully represented the unique time-related frequency pattern of each of the planting conditions; and (iii) the convolution and max pooling processing of the CNN captured the unique and subtle distribution patterns of the scalograms to distinguish vineyards from other crops. Additionally, the proposed approach is considered as able to be applied to other practical scenarios, such as using time series data to identify crop types, map landcover/land use, and is recommended to be tested in future practical applications.},
DOI = {10.3390/rs11222641}
}



@Article{app9224868,
AUTHOR = {Bui, Hoang-Bac and Nguyen, Hoang and Choi, Yosoon and Bui, Xuan-Nam and Nguyen-Thoi, Trung and Zandi, Yousef},
TITLE = {A Novel Artificial Intelligence Technique to Estimate the Gross Calorific Value of Coal Based on Meta-Heuristic and Support Vector Regression Algorithms},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {4868},
URL = {https://www.mdpi.com/2076-3417/9/22/4868},
ISSN = {2076-3417},
ABSTRACT = {Gross calorific value (GCV) is one of the essential parameters for evaluating coal quality. Therefore, accurate GCV prediction is one of the primary ways to improve heating value as well as coal production. A novel evolutionary-based predictive system was proposed in this study for predicting GCV with high accuracy, namely the particle swarm optimization (PSO)-support vector regression (SVR) model. It was developed based on the SVR and PSO algorithms. Three different kernel functions were employed to establish the PSO-SVR models, including radial basis function, linear, and polynomial functions. Besides, three benchmark machine learning models including classification and regression trees (CART), multiple linear regression (MLR), and principle component analysis (PCA) were also developed to estimate GCV and then compared with the proposed PSO-SVR model; 2583 coal samples were used to analyze the proximate components and GCV for this study. Then, they were used to develop the mentioned models as well as check their performance in experimental results. Root-mean-squared error (RMSE), correlation coefficient (R2), ranking, and intensity color criteria were used and computed to evaluate the GCV predictive models developed. The results revealed that the proposed PSO-SVR model with radial basis function had better accuracy than the other models. The PSO algorithm was optimized in the SVR model with high efficiency. These should be used as a supporting tool in practical engineering to determine the heating value of coal seams in complex geological conditions.},
DOI = {10.3390/app9224868}
}



@Article{rs11222681,
AUTHOR = {Abdelbaki, Asmaa and Schlerf, Martin and Verhoef, Wout and Udelhoven, Thomas},
TITLE = {Introduction of Variable Correlation for the Improved Retrieval of Crop Traits Using Canopy Reflectance Model Inversion},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2681},
URL = {https://www.mdpi.com/2072-4292/11/22/2681},
ISSN = {2072-4292},
ABSTRACT = {Look-up table (LUT)-based canopy reflectance models are considered robust methods to estimate vegetation attributes from remotely sensed data. However, the LUT inversion approach is sensitive to measurements and model uncertainties, which raise the ill-posed inverse problem. Therefore, regularization options are needed to mitigate this problem and reduce the uncertainties of estimates. In this study, we introduce a new method to regularize the LUT inversion approach to improve the accuracy of biophysical parameters (leaf area index (LAI) and fractional vegetation cover (fCover)). This was achieved by incorporating known variable correlations that existed at the test site into the LUT approach to correlate the model variables of the Soil–Leaf–Canopy (SLC) model using the Cholesky decomposition algorithm. The retrievals of 27 potato plots obtained from the regularized LUT (LUTreg) were compared with the standard LUT (LUTstd), which did not consider variable correlations. Different solutions from both types of LUTs (LUTreg and LUTstd) were utilized to improve the quality of the model outputs. Results indicate that the present method improved the accuracy of LAI estimation, with the coefficient of determination R2 = 0.74 and normalized root-mean-square error NRMSE = 24.45% in LUTreg, compared with R2 = 0.71 and NRMSE = 25.57% in LUTstd. In addition, the variability of LAI decreased in LUTreg (5.10) compared with that in LUTstd (12.10). Hence, our results give new insight into the impact of adding the correlation between variables to the LUT inversion approach to improve the accuracy of estimations. In this study, only two correlated variables (LAI and fCover) were examined; in subsequent studies, the full correlation matrix based on the Cholesky algorithm should be explored.},
DOI = {10.3390/rs11222681}
}



@Article{s19225012,
AUTHOR = {Arshad, Bilal and Ogie, Robert and Barthelemy, Johan and Pradhan, Biswajeet and Verstaevel, Nicolas and Perez, Pascal},
TITLE = {Computer Vision and IoT-Based Sensors in Flood Monitoring and Mapping: A Systematic Review},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {5012},
URL = {https://www.mdpi.com/1424-8220/19/22/5012},
ISSN = {1424-8220},
ABSTRACT = {Floods are amongst the most common and devastating of all natural hazards. The alarming number of flood-related deaths and financial losses suffered annually across the world call for improved response to flood risks. Interestingly, the last decade has presented great opportunities with a series of scholarly activities exploring how camera images and wireless sensor data from Internet-of-Things (IoT) networks can improve flood management. This paper presents a systematic review of the literature regarding IoT-based sensors and computer vision applications in flood monitoring and mapping. The paper contributes by highlighting the main computer vision techniques and IoT sensor approaches utilised in the literature for real-time flood monitoring, flood modelling, mapping and early warning systems including the estimation of water level. The paper further contributes by providing recommendations for future research. In particular, the study recommends ways in which computer vision and IoT sensor techniques can be harnessed to better monitor and manage coastal lagoons&mdash;an aspect that is under-explored in the literature.},
DOI = {10.3390/s19225012}
}



@Article{rs11222700,
AUTHOR = {Wang, Wantian and Tang, Ziyue and Chen, Yichang and Zhang, Yuanpeng and Sun, Yongjian},
TITLE = {Aircraft Target Classification for Conventional Narrow-Band Radar with Multi-Wave Gates Sparse Echo Data},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {22},
ARTICLE-NUMBER = {2700},
URL = {https://www.mdpi.com/2072-4292/11/22/2700},
ISSN = {2072-4292},
ABSTRACT = {For a conventional narrow-band radar system, the detectable information of the target is limited, and it is difficult for the radar to accurately identify the target type. In particular, the classification probability will further decrease when part of the echo data is missed. By extracting the target features in time and frequency domains from multi-wave gates sparse echo data, this paper presents a classification algorithm in conventional narrow-band radar to identify three different types of aircraft target, i.e., helicopter, propeller and jet. Firstly, the classical sparse reconstruction algorithm is utilized to reconstruct the target frequency spectrum with single-wave gate sparse echo data. Then, the micro-Doppler effect caused by rotating parts of different targets is analyzed, and the micro-Doppler based features, such as amplitude deviation coefficient, time domain waveform entropy and frequency domain waveform entropy, are extracted from reconstructed echo data to identify targets. Thirdly, the target features extracted from multi-wave gates reconstructed echo data are weighted and fused to improve the accuracy of classification. Finally, the fused feature vectors are fed into a support vector machine (SVM) model for classification. By contrast with the conventional algorithm of aircraft target classification, the proposed algorithm can effectively process sparse echo data and achieve higher classification probability via weighted features fusion of multi-wave gates echo data. The experiments on synthetic data are carried out to validate the effectiveness of the proposed algorithm.},
DOI = {10.3390/rs11222700}
}



@Article{f10111047,
AUTHOR = {Sun, Ying and Huang, Jianfeng and Ao, Zurui and Lao, Dazhao and Xin, Qinchuan},
TITLE = {Deep Learning Approaches for the Mapping of Tree Species Diversity in a Tropical Wetland Using Airborne LiDAR and High-Spatial-Resolution Remote Sensing Images},
JOURNAL = {Forests},
VOLUME = {10},
YEAR = {2019},
NUMBER = {11},
ARTICLE-NUMBER = {1047},
URL = {https://www.mdpi.com/1999-4907/10/11/1047},
ISSN = {1999-4907},
ABSTRACT = {The monitoring of tree species diversity is important for forest or wetland ecosystem service maintenance or resource management. Remote sensing is an efficient alternative to traditional field work to map tree species diversity over large areas. Previous studies have used light detection and ranging (LiDAR) and imaging spectroscopy (hyperspectral or multispectral remote sensing) for species richness prediction. The recent development of very high spatial resolution (VHR) RGB images has enabled detailed characterization of canopies and forest structures. In this study, we developed a three-step workflow for mapping tree species diversity, the aim of which was to increase knowledge of tree species diversity assessment using deep learning in a tropical wetland (Haizhu Wetland) in South China based on VHR-RGB images and LiDAR points. Firstly, individual trees were detected based on a canopy height model (CHM, derived from LiDAR points) by the local-maxima-based method in the FUSION software (Version 3.70, Seattle, USA). Then, tree species at the individual tree level were identified via a patch-based image input method, which cropped the RGB images into small patches (the individually detected trees) based on the tree apexes detected. Three different deep learning methods (i.e., AlexNet, VGG16, and ResNet50) were modified to classify the tree species, as they can make good use of the spatial context information. Finally, four diversity indices, namely, the Margalef richness index, the Shannon&ndash;Wiener diversity index, the Simpson diversity index, and the Pielou evenness index, were calculated from the fixed subset with a size of 30 &times; 30 m for assessment. In the classification phase, VGG16 had the best performance, with an overall accuracy of 73.25% for 18 tree species. Based on the classification results, mapping of tree species diversity showed reasonable agreement with field survey data (R2Margalef = 0.4562, root-mean-square error RMSEMargalef = 0.5629; R2Shannon&ndash;Wiener = 0.7948, RMSEShannon&ndash;Wiener = 0.7202; R2Simpson = 0.7907, RMSESimpson = 0.1038; and R2Pielou = 0.5875, RMSEPielou = 0.3053). While challenges remain for individual tree detection and species classification, the deep-learning-based solution shows potential for mapping tree species diversity.},
DOI = {10.3390/f10111047}
}



@Article{rs11232752,
AUTHOR = {Zhang, Xiaoyan and Zhao, Jinming and Yang, Guijun and Liu, Jiangang and Cao, Jiqiu and Li, Chunyan and Zhao, Xiaoqing and Gai, Junyi},
TITLE = {Establishment of Plot-Yield Prediction Models in Soybean Breeding Programs Using UAV-Based Hyperspectral Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2752},
URL = {https://www.mdpi.com/2072-4292/11/23/2752},
ISSN = {2072-4292},
ABSTRACT = {Yield evaluation of breeding lines is the key to successful release of cultivars, which is becoming a serious issue due to soil heterogeneity in enlarged field tests. This study aimed at establishing plot-yield prediction models using unmanned aerial vehicle (UAV)-based hyperspectral remote sensing for yield-selection in large-scale soybean breeding programs. Three sets of soybean breeding lines (1103 in total) were tested in blocks-in-replication experiments for plot yield and canopy spectral reflectance on 454~950 nm bands at different growth stages using a UAV-based hyperspectral spectrometer (Cubert UHD185 Firefly). The four elements for plot-yield prediction model construction were studied respectively and concluded as: the suitable reflectance-sampling unit-size in a plot was its 20%–80% central part; normalized difference vegetation index (NDVI) and ration vegetation index (RVI) were the best combination of vegetation indices; the initial seed-filling stage (R5) was the best for a single stage prediction, while another was the best combination for a two growth-stage prediction; and multi-variate linear regression was suitable for plot-yield prediction. In model establishment for each material-set, a random half was used for modelling and another half for verification. Twenty-one two growth-stage two vegetation-index prediction models were established and compared for their modelling coefficient of determination (RM2) and root mean square error of the model (RMSEM), verification RV2 and RMSEV, and their sum RS2 and RMSES. Integrated with the coincidence rate between the model predicted and the practical yield-selection results, the models, MA1-2, MA4-2 and MA6-2, with coincidence rates of 56.8%, 58.5% and 52.4%, respectively, were chosen for yield-prediction in yield-test nurseries. The established model construction elements and methods can be used as local models for pre-harvest yield-selection and post-harvest integrated yield-selection in advanced breeding nurseries as well as yield potential prediction in plant-derived-line nurseries. Furthermore, multiple models can be used jointly for plot-yield prediction in soybean breeding programs.},
DOI = {10.3390/rs11232752}
}



@Article{s19235170,
AUTHOR = {Bithas, Petros S. and Michailidis, Emmanouel T. and Nomikos, Nikolaos and Vouyioukas, Demosthenes and Kanatas, Athanasios G.},
TITLE = {A Survey on Machine-Learning Techniques for UAV-Based Communications},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5170},
URL = {https://www.mdpi.com/1424-8220/19/23/5170},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) will be an integral part of the next generation wireless communication networks. Their adoption in various communication-based applications is expected to improve coverage and spectral efficiency, as compared to traditional ground-based solutions. However, this new degree of freedom that will be included in the network will also add new challenges. In this context, the machine-learning (ML) framework is expected to provide solutions for the various problems that have already been identified when UAVs are used for communication purposes. In this article, we provide a detailed survey of all relevant research works, in which ML techniques have been used on UAV-based communications for improving various design and functional aspects such as channel modeling, resource management, positioning, and security.},
DOI = {10.3390/s19235170}
}



@Article{rs11232794,
AUTHOR = {Röll, Georg and Hartung, Jens and Graeff-Hönninger, Simone},
TITLE = {Determination of Plant Nitrogen Content in Wheat Plants via Spectral Reflectance Measurements: Impact of Leaf Number and Leaf Position},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2794},
URL = {https://www.mdpi.com/2072-4292/11/23/2794},
ISSN = {2072-4292},
ABSTRACT = {The determination of plant nitrogen (N) content (%) in wheat via destructive lab analysis is expensive and inadequate for precision farming applications. Vegetation indices (VI) based on spectral reflectance can be used to predict plant N content indirectly. For these VI, reflectance from space-borne, airborne, or ground-borne sensors is captured. Measurements are often taken at the canopy level for practical reasons. Hence, translocation processes of nutrients that take place within the plant might be ignored or measurements might be less accurate if nutrient deficiency symptoms occur on the older leaves. This study investigated the impact of leaf number and measurement position on the leaf itself on the determination of plant N content (%) via reflectance measurements. Two hydroponic experiments were carried out. In the first experiment, the N fertilizer amount and growth stage for the determination of N content was varied, while the second experiment focused on a secondary induction of N deficiency due to drought stress. For each plant, reflectance measurements were taken from three leaves (L1, L2, L3) and at three positions on the leaf (P1, P2, P3). In addition, the N content (%) of the whole plant was determined by chemical lab analysis. Reflectance spectrometer measurements (400&ndash;1650 nm) were used to calculate 16 VI for each combination of leaf and position. N content (%) was predicted using each VI for each leaf and each position. Significant lower mean residual error variance (MREV) was found for leaves L1 and L3 and for measurement position on P3 in the N trial, but the difference of MREV between the leaves was very low and therefore considered as not relevant. The drought stress trial also led to no significant differences in MREV between leaves and positions. Neither the position on the leaf nor the leaf number had an impact on the accuracy of plant nitrogen determination via spectral reflectance measurements, wherefore measurements taken at the canopy level seem to be a valid approach.},
DOI = {10.3390/rs11232794}
}



@Article{s19235284,
AUTHOR = {Zhang, Heng and Wu, Jiayu and Liu, Yanli and Yu, Jia},
TITLE = {VaryBlock: A Novel Approach for Object Detection in Remote Sensed Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5284},
URL = {https://www.mdpi.com/1424-8220/19/23/5284},
ISSN = {1424-8220},
ABSTRACT = {In recent years, the research on optical remote sensing images has received greater and greater attention. Object detection, as one of the most challenging tasks in the area of remote sensing, has been remarkably promoted by convolutional neural network (CNN)-based methods like You Only Look Once (YOLO) and Faster R-CNN. However, due to the complexity of backgrounds and the distinctive object distribution, directly applying these general object detection methods to the remote sensing object detection usually renders poor performance. To tackle this problem, a highly efficient and robust framework based on YOLO is proposed. We devise and integrate VaryBlock to the architecture which effectively offsets some of the information loss caused by downsampling. In addition, some techniques are utilized to facilitate the performance and to avoid overfitting. Experimental results show that our proposed method can enormously improve the mean average precision by a large margin on the NWPU VHR-10 dataset.},
DOI = {10.3390/s19235284}
}



@Article{s19235287,
AUTHOR = {Moreno-Armendáriz, Marco A. and Calvo, Hiram and Duchanoy, Carlos A. and López-Juárez, Anayantzin P. and Vargas-Monroy, Israel A. and Suarez-Castañon, Miguel Santiago},
TITLE = {Deep Green Diagnostics: Urban Green Space Analysis Using Deep Learning and Drone Images},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {5287},
URL = {https://www.mdpi.com/1424-8220/19/23/5287},
ISSN = {1424-8220},
ABSTRACT = {Nowadays, more than half of the world’s population lives in urban areas, and this number continues increasing. Consequently, there are more and more scientific publications that analyze health problems of people associated with living in these highly urbanized locations. In particular, some of the recent work has focused on relating people’s health to the quality and quantity of urban green areas. In this context, and considering the huge amount of land area in large cities that must be supervised, our work seeks to develop a deep learning-based solution capable of determining the level of health of the land and to assess whether it is contaminated. The main purpose is to provide health institutions with software capable of creating updated maps that indicate where these phenomena are presented, as this information could be very useful to guide public health goals in large cities. Our software is released as open source code, and the data used for the experiments presented in this paper are also freely available.},
DOI = {10.3390/s19235287}
}



@Article{biomimetics4040076,
AUTHOR = {Communier, David and Le Besnerais, Franck and Botez, Ruxandra Mihaela and Wong, Tony},
TITLE = {Design, Manufacturing, and Testing of a New Concept for a Morphing Leading Edge using a Subsonic Blow Down Wind Tunnel},
JOURNAL = {Biomimetics},
VOLUME = {4},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {76},
URL = {https://www.mdpi.com/2313-7673/4/4/76},
PubMedID = {31810231},
ISSN = {2313-7673},
ABSTRACT = {This paper presents the design and wind tunnel test results of a wing including a morphing leading edge for a medium unmanned aerial vehicle with a maximum wingspan of 5 m. The design of the morphing leading edge system is part of research on the design of a morphing camber system. The concept presented here has the advantage of being simple to manufacture (wooden construction) and light for the structure of the wing (compliance mechanism). The morphing leading edge prototype demonstrates the possibility of modifying the stall angle of the wing. In addition, the modification of the stall angle is performed without affecting the slope of the lift coefficient. This prototype is designed to validate the functionality of the deformation method applied to the leading edge of the wing. The mechanism can be further optimized in terms of shape and material to obtain a greater deformation of the leading edge, and, thus, to have a higher impact on the increase of the stall angle than the first prototype of the morphing leading edge presented in this paper.},
DOI = {10.3390/biomimetics4040076}
}



@Article{rs11232873,
AUTHOR = {Kayad, Ahmed and Sozzi, Marco and Gatto, Simone and Marinello, Francesco and Pirotti, Francesco},
TITLE = {Monitoring Within-Field Variability of Corn Yield using Sentinel-2 and Machine Learning Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {23},
ARTICLE-NUMBER = {2873},
URL = {https://www.mdpi.com/2072-4292/11/23/2873},
ISSN = {2072-4292},
ABSTRACT = {Monitoring and prediction of within-field crop variability can support farmers to make the right decisions in different situations. The current advances in remote sensing and the availability of high resolution, high frequency, and free Sentinel-2 images improve the implementation of Precision Agriculture (PA) for a wider range of farmers. This study investigated the possibility of using vegetation indices (VIs) derived from Sentinel-2 images and machine learning techniques to assess corn (Zea mays) grain yield spatial variability within the field scale. A 22-ha study field in North Italy was monitored between 2016 and 2018; corn yield was measured and recorded by a grain yield monitor mounted on the harvester machine recording more than 20,000 georeferenced yield observation points from the study field for each season. VIs from a total of 34 Sentinel-2 images at different crop ages were analyzed for correlation with the measured yield observations. Multiple regression and two different machine learning approaches were also tested to model corn grain yield. The three main results were the following: (i) the Green Normalized Difference Vegetation Index (GNDVI) provided the highest R2 value of 0.48 for monitoring within-field variability of corn grain yield; (ii) the most suitable period for corn yield monitoring was a crop age between 105 and 135 days from the planting date (R4&ndash;R6); (iii) Random Forests was the most accurate machine learning approach for predicting within-field variability of corn yield, with an R2 value of almost 0.6 over an independent validation set of half of the total observations. Based on the results, within-field variability of corn yield for previous seasons could be investigated from archived Sentinel-2 data with GNDVI at crop stage (R4&ndash;R6).},
DOI = {10.3390/rs11232873}
}



@Article{rs11242912,
AUTHOR = {Liu, Wei and Yang, MengYuan and Xie, Meng and Guo, Zihui and Li, ErZhu and Zhang, Lianpeng and Pei, Tao and Wang, Dong},
TITLE = {Accurate Building Extraction from Fused DSM and UAV Images Using a Chain Fully Convolutional Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2912},
URL = {https://www.mdpi.com/2072-4292/11/24/2912},
ISSN = {2072-4292},
ABSTRACT = {Accurate extraction of buildings using high spatial resolution imagery is essential to a wide range of urban applications. However, it is difficult to extract semantic features from a variety of complex scenes (e.g., suburban, urban and urban village areas) because various complex man-made objects usually appear heterogeneous with large intra-class and low inter-class variations. The automatic extraction of buildings is thus extremely challenging. The fully convolutional neural networks (FCNs) developed in recent years have performed well in the extraction of urban man-made objects due to their ability to learn state-of-the-art features and to label pixels end-to-end. One of the most successful FCNs used in building extraction is U-net. However, the commonly used skip connection and feature fusion refinement modules in U-net often ignore the problem of feature selection, and the ability to extract smaller buildings and refine building boundaries needs to be improved. In this paper, we propose a trainable chain fully convolutional neural network (CFCN), which fuses high spatial resolution unmanned aerial vehicle (UAV) images and the digital surface model (DSM) for building extraction. Multilevel features are obtained from the fusion data, and an improved U-net is used for the coarse extraction of the building. To solve the problem of incomplete extraction of building boundaries, a U-net network is introduced by chain, which is used for the introduction of a coarse building boundary constraint, hole filling, and "speckle" removal. Typical areas such as suburban, urban, and urban villages were selected for building extraction experiments. The results show that the CFCN achieved recall of 98.67%, 98.62%, and 99.52% and intersection over union (IoU) of 96.23%, 96.43%, and 95.76% in suburban, urban, and urban village areas, respectively. Considering the IoU in conjunction with the CFCN and U-net resulted in improvements of 6.61%, 5.31%, and 6.45% in suburban, urban, and urban village areas, respectively. The proposed method can extract buildings with higher accuracy and with clearer and more complete boundaries.},
DOI = {10.3390/rs11242912}
}



@Article{rs11242925,
AUTHOR = {Prado Osco, Lucas and Marques Ramos, Ana Paula and Roberto Pereira, Danilo and Akemi Saito Moriya, Érika and Nobuhiro Imai, Nilton and Takashi Matsubara, Edson and Estrabis, Nayara and de Souza, Maurício and Marcato Junior, José and Gonçalves, Wesley Nunes and Li, Jonathan and Liesenberg, Veraldo and Eduardo Creste, José},
TITLE = {Predicting Canopy Nitrogen Content in Citrus-Trees Using Random Forest Algorithm Associated to Spectral Vegetation Indices from UAV-Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {2925},
URL = {https://www.mdpi.com/2072-4292/11/24/2925},
ISSN = {2072-4292},
ABSTRACT = {The traditional method of measuring nitrogen content in plants is a time-consuming and labor-intensive task. Spectral vegetation indices extracted from unmanned aerial vehicle (UAV) images and machine learning algorithms have been proved effective in assisting nutritional analysis in plants. Still, this analysis has not considered the combination of spectral indices and machine learning algorithms to predict nitrogen in tree-canopy structures. This paper proposes a new framework to infer the nitrogen content in citrus-tree at a canopy-level using spectral vegetation indices processed with the random forest algorithm. A total of 33 spectral indices were estimated from multispectral images acquired with a UAV-based sensor. Leaf samples were gathered from different planting-fields and the leaf nitrogen content (LNC) was measured in the laboratory, and later converted into the canopy nitrogen content (CNC). To evaluate the robustness of the proposed framework, we compared it with other machine learning algorithms. We used 33,600 citrus trees to evaluate the performance of the machine learning models. The random forest algorithm had higher performance in predicting CNC than all models tested, reaching an R2 of 0.90, MAE of 0.341 g&middot;kg&minus;1 and MSE of 0.307 g&middot;kg&minus;1. We demonstrated that our approach is able to reduce the need for chemical analysis of the leaf tissue and optimizes citrus orchard CNC monitoring.},
DOI = {10.3390/rs11242925}
}



@Article{electronics8121504,
AUTHOR = {Zhou, Yu and Wu, Chunxue and Wu, Qunhui and Eli, Zelda Makati and Xiong, Naixue and Zhang, Sheng},
TITLE = {Design and Analysis of Refined Inspection of Field Conditions of Oilfield Pumping Wells Based on Rotorcraft UAV Technology},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1504},
URL = {https://www.mdpi.com/2079-9292/8/12/1504},
ISSN = {2079-9292},
ABSTRACT = {The traditional oil well monitoring method relies on manual acquisition and various high-precision sensors. Using the indicator diagram to judge the working condition of the well is not only difficult to establish but also consumes huge manpower and financial resources. This paper proposes the use of computer vision in the detection of working conditions in oil extraction. Combined with the advantages of an unmanned aerial vehicle (UAV), UAV aerial photography images are used to realize real-time detection of on-site working conditions by real-time tracking of the working status of the head working and other related parts of the pumping unit. Considering the real-time performance of working condition detection, this paper proposes a framework that combines You only look once version 3 (YOLOv3) and a sort algorithm to complete multi-target tracking in the form of tracking by detection. The quality of the target detection in the framework is the key factor affecting the tracking effect. The experimental results show that a good detector makes the tracking speed achieve the real-time effect and provides help for the real-time detection of the working condition, which has a strong practical application.},
DOI = {10.3390/electronics8121504}
}



@Article{s19245436,
AUTHOR = {Barbedo, Jayme Garcia Arnal and Koenigkan, Luciano Vieira and Santos, Thiago Teixeira and Santos, Patrícia Menezes},
TITLE = {A Study on the Detection of Cattle in UAV Images Using Deep Learning},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5436},
URL = {https://www.mdpi.com/1424-8220/19/24/5436},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are being increasingly viewed as valuable tools to aid the management of farms. This kind of technology can be particularly useful in the context of extensive cattle farming, as production areas tend to be expansive and animals tend to be more loosely monitored. With the advent of deep learning, and convolutional neural networks (CNNs) in particular, extracting relevant information from aerial images has become more effective. Despite the technological advancements in drone, imaging and machine learning technologies, the application of UAVs for cattle monitoring is far from being thoroughly studied, with many research gaps still remaining. In this context, the objectives of this study were threefold: (1) to determine the highest possible accuracy that could be achieved in the detection of animals of the Canchim breed, which is visually similar to the Nelore breed (Bos taurus indicus); (2) to determine the ideal ground sample distance (GSD) for animal detection; (3) to determine the most accurate CNN architecture for this specific problem. The experiments involved 1853 images containing 8629 samples of animals, and 15 different CNN architectures were tested. A total of 900 models were trained (15 CNN architectures &times; 3 spacial resolutions &times; 2 datasets &times; 10-fold cross validation), allowing for a deep analysis of the several aspects that impact the detection of cattle using aerial images captured using UAVs. Results revealed that many CNN architectures are robust enough to reliably detect animals in aerial images even under far from ideal conditions, indicating the viability of using UAVs for cattle monitoring.},
DOI = {10.3390/s19245436}
}



@Article{ijgi8120585,
AUTHOR = {Hadavandsiri, Zahra and Lichti, Derek D. and Jahraus, Adam and Jarron, David},
TITLE = {Concrete Preliminary Damage Inspection by Classification of Terrestrial Laser Scanner Point Clouds through Systematic Threshold Definition},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {8},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {585},
URL = {https://www.mdpi.com/2220-9964/8/12/585},
ISSN = {2220-9964},
ABSTRACT = {This paper presents a novel approach for automatic, preliminary detection of damage in concrete structures using ground-based terrestrial laser scanners. The method is based on computation of defect-sensitive features such as the surface curvature, since the surface roughness changes strongly if an area is affected by damage. A robust version of principal component analysis (PCA) classification is proposed to distinguish between structural damage and outliers present in the laser scanning data. Numerical simulations were conducted to develop a systematic point-wise defect classifier that automatically diagnoses the location of superficial damage on the investigated region. The method provides a complete picture of the surface health of concrete structures. It has been tested on two real datasets: a concrete heritage aqueduct in Brooks, Alberta, Canada; and a civil pedestrian concrete structure. The experiment results demonstrate the validity and accuracy of the proposed systematic framework for detecting and localizing areas of damage as small as 1 cm or less.},
DOI = {10.3390/ijgi8120585}
}



@Article{app9245477,
AUTHOR = {Ahn, Hyojung and Choi, Han-Lim and Kang, Minguk and Moon, SungTae},
TITLE = {Learning-Based Anomaly Detection and Monitoring for Swarm Drone Flights},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5477},
URL = {https://www.mdpi.com/2076-3417/9/24/5477},
ISSN = {2076-3417},
ABSTRACT = {This paper addresses anomaly detection and monitoring for swarm drone flights. While the current practice of swarm flight typically relies on the operator&rsquo;s naked eyes to monitor health of the multiple vehicles, this work proposes a machine learning-based framework to enable detection of abnormal behavior of a large number of flying drones on the fly. The method works in two steps: a sequence of two unsupervised learning procedures reduces the dimensionality of the real flight test data and labels them as normal and abnormal cases; then, a deep neural network classifier with one-dimensional convolution layers followed by fully connected multi-layer perceptron extracts the associated features and distinguishes the anomaly from normal conditions. The proposed anomaly detection scheme is validated on the real flight test data, highlighting its capability of online implementation.},
DOI = {10.3390/app9245477}
}



@Article{s19245506,
AUTHOR = {Lucas, Carlos and Hernández-Sosa, Daniel and Greiner, David and Zamuda, Aleš and Caldeira, Rui},
TITLE = {An Approach to Multi-Objective Path Planning Optimization for Underwater Gliders},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5506},
URL = {https://www.mdpi.com/1424-8220/19/24/5506},
ISSN = {1424-8220},
ABSTRACT = {Underwater gliders are energy-efficient vehicles that rely on changes in buoyancy in order to convert up and down movement into forward displacement. These vehicles are conceived as multi-sensor platforms, and can be used to collect ocean data for long periods in wide range areas. This endurance is achieved at the cost of low speed, which requires extensive planning to ensure vehicle safety and mission success, particularly when dealing with strong ocean currents. As gliders are often involved on missions that pursue multiple objectives (track events, reach a target point, avoid obstacles, sample specified areas, save energy), path planning requires a way to deal with several constraints at the same time; this makes glider path planning a multi-objective (MO) optimization problem. In this work, we analyse the usage of the non-dominated sorting genetic algorithm II (NSGA-II) to tackle a MO glider path planning application on a complex environment integrating 3D and time varying ocean currents. Multiple experiments using a glider kinematic simulator coupled with NSGA-II, combining different control parameters were carried out, to find the best parameter configuration that provided suitable paths for the desired mission. Ultimately, the system described in this work was able to optimize multi-objective trajectories, providing non dominated solutions. Such a planning tool could be of great interest in real mission planning, to assist glider pilots in selecting the most convenient paths for the vehicle, taking into account ocean forecasts and particular characteristics of the deployment location.},
DOI = {10.3390/s19245506}
}



@Article{w11122633,
AUTHOR = {Yang, Shengtian and Wang, Juan and Wang, Pengfei and Gong, Tongliang and Liu, Huiping},
TITLE = {Low Altitude Unmanned Aerial Vehicles (UAVs) and Satellite Remote Sensing Are Used to Calculated River Discharge Attenuation Coefficients of Ungauged Catchments in Arid Desert},
JOURNAL = {Water},
VOLUME = {11},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {2633},
URL = {https://www.mdpi.com/2073-4441/11/12/2633},
ISSN = {2073-4441},
ABSTRACT = {The arid desert ecosystem is very fragile, and the change of its river discharge has a direct impact on irrigation and natural environment. River discharge attenuation coefficients is a key index to reveal the stability of desert river ecosystem. However, due to the harsh conditions in desert areas, it is difficult to establish a hydrological station to obtain data and calculate the attenuation coefficients, so it is urgent to develop new methods to master the attenuation coefficients of rivers. In this study, Taklamakan desert river was selected as the research area, and the river discharge of the desert river were estimated by combining low-altitude UAV and satellite remote sensing technology, so as to calculate the attenuation status of the river in its natural state. Combined with satellite remote sensing, the surface runoff in the desert reaches of the Hotan River from 1993 to 2017 were estimated. The results showed that the base of runoff attenuation in the lower reaches of the Hotan River is 40%. Coupled UAV and satellite remote sensing technology can provide technical support for the study of surface runoff in desert rivers within ungauged basins. Using UAV and satellite remote sensing can monitor surface runoff effectively providing important reference for river discharge monitoring in ungauged catchments.},
DOI = {10.3390/w11122633}
}



@Article{s19245558,
AUTHOR = {Chen, Yayong and Hou, Chaojun and Tang, Yu and Zhuang, Jiajun and Lin, Jintian and He, Yong and Guo, Qiwei and Zhong, Zhenyu and Lei, Huan and Luo, Shaoming},
TITLE = {Citrus Tree Segmentation from UAV Images Based on Monocular Machine Vision in a Natural Orchard Environment},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {5558},
URL = {https://www.mdpi.com/1424-8220/19/24/5558},
ISSN = {1424-8220},
ABSTRACT = {The segmentation of citrus trees in a natural orchard environment is a key technology for achieving the fully autonomous operation of agricultural unmanned aerial vehicles (UAVs). Therefore, a tree segmentation method based on monocular machine vision technology and a support vector machine (SVM) algorithm are proposed in this paper to segment citrus trees precisely under different brightness and weed coverage conditions. To reduce the sensitivity to environmental brightness, a selective illumination histogram equalization method was developed to compensate for the illumination, thereby improving the brightness contrast for the foreground without changing its hue and saturation. To accurately differentiate fruit trees from different weed coverage backgrounds, a chromatic aberration segmentation algorithm and the Otsu threshold method were combined to extract potential fruit tree regions. Then, 14 color features, five statistical texture features, and local binary pattern features of those regions were calculated to establish an SVM segmentation model. The proposed method was verified on a dataset with different brightness and weed coverage conditions, and the results show that the citrus tree segmentation accuracy reached 85.27% &plusmn; 9.43%; thus, the proposed method achieved better performance than two similar methods.},
DOI = {10.3390/s19245558}
}



@Article{rs11243053,
AUTHOR = {Kocur-Bera, Katarzyna and Dawidowicz, Agnieszka},
TITLE = {Land Use versus Land Cover: Geo-Analysis of National Roads and Synchronisation Algorithms},
JOURNAL = {Remote Sensing},
VOLUME = {11},
YEAR = {2019},
NUMBER = {24},
ARTICLE-NUMBER = {3053},
URL = {https://www.mdpi.com/2072-4292/11/24/3053},
ISSN = {2072-4292},
ABSTRACT = {Technological progress in Earth surface observation provides a vast range of information on the land and methods of its use. This enables property owners, users and administrators to monitor the state of the boundaries of the land they own/administer. The land cover, monitored directly on the ground, is not always consistent with the land use entered in the Land and Property Registry (LPR). Discrepancies between these data are often found in former communist countries. One of the reasons for this was the rapid process of land privatisation, which took place in Poland, without updating information on the plot geodetic boundaries. The study examined and compared the land use (entered in the LPR) with the land cover (on the ground) for national roads (acr. LU-LC). The most frequent discrepancies were selected, using CLC2018, digital orthophotomaps (using the Web Map Service (WMS) browsing service compliant with Open Geospatial Consortium (OGC) standards), cadastral data, statistical modelling and an updated survey of the right-of-way. Subsequently, six algorithms were proposed to synchronise the land use and land cover when the right-of-way was used by unauthorised persons, and two algorithms for cases of unauthorised use of land by the road administrator. Currently, it is difficult to synchronise the land cover with the land use from the administrative, legal and social points of view. The results of analyses show that full synchronisation of land use and land cover is complicated and time-consuming, although desired.},
DOI = {10.3390/rs11243053}
}



@Article{s20010038,
AUTHOR = {Khan, Muhammad Fahad and Yau, Kok-Lim Alvin and Noor, Rafidah Md and Imran, Muhammad Ali},
TITLE = {Routing Schemes in FANETs: A Survey},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {38},
URL = {https://www.mdpi.com/1424-8220/20/1/38},
ISSN = {1424-8220},
ABSTRACT = {Flying ad hoc network (FANET) is a self-organizing wireless network that enables inexpensive, flexible, and easy-to-deploy flying nodes, such as unmanned aerial vehicles (UAVs), to communicate among themselves in the absence of fixed network infrastructure. FANET is one of the emerging networks that has an extensive range of next-generation applications. Hence, FANET plays a significant role in achieving application-based goals. Routing enables the flying nodes to collaborate and coordinate among themselves and to establish routes to radio access infrastructure, particularly FANET base station (BS). With a longer route lifetime, the effects of link disconnections and network partitions reduce. Routing must cater to two main characteristics of FANETs that reduce the route lifetime. Firstly, the collaboration nature requires the flying nodes to exchange messages and to coordinate among themselves, causing high energy consumption. Secondly, the mobility pattern of the flying nodes is highly dynamic in a three-dimensional space and they may be spaced far apart, causing link disconnection. In this paper, we present a comprehensive survey of the limited research work of routing schemes in FANETs. Different aspects, including objectives, challenges, routing metrics, characteristics, and performance measures, are covered. Furthermore, we present open issues.},
DOI = {10.3390/s20010038}
}



@Article{rs12010056,
AUTHOR = {de Castro, Ana I. and Peña, José M. and Torres-Sánchez, Jorge and Jiménez-Brenes, Francisco M. and Valencia-Gredilla, Francisco and Recasens, Jordi and López-Granados, Francisca},
TITLE = {Mapping Cynodon Dactylon Infesting Cover Crops with an Automatic Decision Tree-OBIA Procedure and UAV Imagery for Precision Viticulture},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {56},
URL = {https://www.mdpi.com/2072-4292/12/1/56},
ISSN = {2072-4292},
ABSTRACT = {The establishment and management of cover crops are common practices widely used in irrigated viticulture around the world, as they bring great benefits not only to protect and improve the soil, but also to control vine vigor and improve the yield quality, among others. However, these benefits are often reduced when cover crops are infested by Cynodon dactylon (bermudagrass), which impacts crop production due to its competition for water and nutrients and causes important economic losses for the winegrowers. Therefore, the discrimination of Cynodon dactylon in cover crops would enable site-specific control to be applied and thus drastically mitigate damage to the vineyard. In this context, this research proposes a novel, automatic and robust image analysis algorithm for the quick and accurate mapping of Cynodon dactylon growing in vineyard cover crops. The algorithm was developed using aerial images taken with an Unmanned Aerial Vehicle (UAV) and combined decision tree (DT) and object-based image analysis (OBIA) approaches. The relevance of this work consisted in dealing with the constraint caused by the spectral similarity of these complex scenarios formed by vines, cover crops, Cynodon dactylon, and bare soil. The incorporation of height information from the Digital Surface Model and several features selected by machine learning tools in the DT-OBIA algorithm solved this spectral similarity limitation and allowed the precise design of Cynodon dactylon maps. Another contribution of this work is the short time needed to apply the full process from UAV flights to image analysis, which can enable useful maps to be created on demand (within two days of the farmer&acute;s request) and is thus timely for controlling Cynodon dactylon in the herbicide application window. Therefore, this combination of UAV imagery and a DT-OBIA algorithm would allow winegrowers to apply site-specific control of Cynodon dactylon and maintain cover crop-based management systems and their consequent benefits in the vineyards, and also comply with the European legal framework for the sustainable use of agricultural inputs and implementation of integrated crop management.},
DOI = {10.3390/rs12010056}
}



@Article{jmse8010009,
AUTHOR = {Zollini, Sara and Alicandro, Maria and Cuevas-González, María and Baiocchi, Valerio and Dominici, Donatella and Buscema, Paolo Massimo},
TITLE = {Shoreline Extraction Based on an Active Connection Matrix (ACM) Image Enhancement Strategy},
JOURNAL = {Journal of Marine Science and Engineering},
VOLUME = {8},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {9},
URL = {https://www.mdpi.com/2077-1312/8/1/9},
ISSN = {2077-1312},
ABSTRACT = {Coastal environments are facing constant changes over time due to their dynamic nature and geological, geomorphological, hydrodynamic, biological, climatic and anthropogenic factors. For these reasons, the monitoring of these areas is crucial for the safeguarding of the cultural heritage and the populations living there. The focus of this paper is shoreline extraction by means of an experimental algorithm, called J-Net Dynamic (Semeion Research Center of Sciences of Communication, Rome, Italy). It was tested on two types of image: a very high resolution (VHR) multispectral image (WorldView-2) and a high resolution (HR) radar synthetic aperture radar (SAR) image (Sentinel-1). The extracted shorelines were compared with those manually digitized for both images independently. The results obtained with the J-Net Dynamic algorithm were also compared with common algorithms, widely used in the literature, including the WorldView water index and the Canny edge detector. The results show that the experimental algorithm is more effective than the others, as it improves shoreline extraction accuracy both in the optical and SAR images.},
DOI = {10.3390/jmse8010009}
}



@Article{rs12010095,
AUTHOR = {Li, Hongjun and Zhang, Yuming and Lei, Yuping and Antoniuk, Vita and Hu, Chunsheng},
TITLE = {Evaluating Different Non-Destructive Estimation Methods for Winter Wheat (Triticum aestivum L.) Nitrogen Status Based on Canopy Spectrum},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {95},
URL = {https://www.mdpi.com/2072-4292/12/1/95},
ISSN = {2072-4292},
ABSTRACT = {Compared to conventional laboratory testing methods, crop nitrogen estimation methods based on canopy spectral characteristics have advantages in terms of timeliness, cost, and practicality. A variety of rapid and non-destructive estimation methods based on the canopy spectrum have been developed on the scale of space, sky, and ground. In order to understand the differences in estimation accuracy and applicability of these methods, as well as for the convenience of users to select the suitable technology, models for estimation of nitrogen status of winter wheat were developed and compared for three methods: drone equipped with a multispectral camera, soil plant analysis development (SPAD) chlorophyll meter, and smartphone photography. Based on the correlations between observed nitrogen status in winter wheat and related vegetation indices, green normalized difference vegetation index (GNDVI) and visible atmospherically resistant index (VARI) were selected as the sensitive vegetation indices for the drone equipped with a multispectral camera and smartphone photography methods, respectively. The correlation coefficients between GNDVI, SPAD, and VARI were 0.92 ** and 0.89 **, and that between SPAD and VARI was 0.90 **, which indicated that three vegetation indices for these three estimation methods were significantly related to each other. The determination coefficients of the 0&ndash;90 cm soil nitrate nitrogen content estimation models for the drone equipped with a multispectral camera, SPAD, and smartphone photography methods were 0.63, 0.54, and 0.81, respectively. In the estimation accuracy evaluation, the method of smartphone photography had the smallest root mean square error (RMSE = 9.80 mg/kg). The accuracy of the smartphone photography method was slightly higher than the other two methods. Due to the limitations of these models, it was found that the crop nitrogen estimation methods based on canopy spectrum were not suitable for the crops under severe phosphate deficiency. In addition, in estimation of soil nitrate nitrogen content, there were saturation responses in the estimation indicators of the three methods. In order to introduce these three methods in the precise management of nitrogen fertilizer, it is necessary to further improve their estimation models.},
DOI = {10.3390/rs12010095}
}



@Article{rs12010104,
AUTHOR = {He, Qimin and Zhang, Kefei and Wu, Suqin and Zhao, Qingzhi and Wang, Xiaoming and Shen, Zhen and Li, Longjiang and Wan, Moufeng and Liu, Xiaoyang},
TITLE = {Real-Time GNSS-Derived PWV for Typhoon Characterizations: A Case Study for Super Typhoon Mangkhut in Hong Kong},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {104},
URL = {https://www.mdpi.com/2072-4292/12/1/104},
ISSN = {2072-4292},
ABSTRACT = {Typhoons can be serious natural disasters for the sustainability and development of society. The development of a typhoon usually involves a pre-existing weather disturbance, warm tropical oceans, and a large amount of moisture. This implies that a large variation in the atmospheric water vapor over the path of a typhoon can be used to study the characteristics of the typhoon. This is the reason that the variation in precipitable water vapor (PWV) is often used to capture the signature of a typhoon in meteorology. This study investigates the usability of real-time PWV retrieved from global navigation satellite systems (GNSS) for typhoons&rsquo; characterizations, and especially, the following aspects were investigated: (1) The correlation between PWV and atmospheric parameters including pressure, temperature, precipitation, and wind speed; (2) water vapor transportation during a typhoon period; and (3) the correlation between the movement of a typhoon and the transportation of water vapor. The case study selected for this research was Super Typhoon Mangkhut that occurred in mid-September 2018 in Hong Kong. The PWV time series were obtained from a conversion of GNSS-derived zenith total delays (ZTDs) using observations at 10 stations selected from the Hong Kong GNSS continuously operating reference stations (CORS) network, which are also located along the path of the typhoon. The Bernese GNSS Software (ver. 5.2) was used to obtain the ZTDs; and the root mean square (RMS) of the differences between the GNSS-ZTDs and International GNSS Service post-processed ZTDs time series was less than 8 mm. The RMS of the differences between the GNSS-PWVs (i.e., the ZTDs converted PWVs) and radiosonde-derived PWVs (RS-PWVs) time series was less than 2 mm. The changes in PWV reflect the variation in wind speed during the typhoon period to a certain degree, and their correlation coefficient was 0.76, meaning a significant positive correlation. In addition, a new approach was proposed to estimate the direction and speed of a typhoon&rsquo;s movement using the time difference of PWV arrival at different sites. The direction and speed estimated agreed well with the ones published by the China Meteorological Administration. These results suggest that GNSS-derived PWV has a great potential for the monitoring and even prediction of typhoon events, especially for near real-time warnings.},
DOI = {10.3390/rs12010104}
}



@Article{rs12010126,
AUTHOR = {Wijesingha, Jayan and Astor, Thomas and Schulze-Brüninghoff, Damian and Wengert, Matthias and Wachendorf, Michael},
TITLE = {Predicting Forage Quality of Grasslands Using UAV-Borne Imaging Spectroscopy},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {126},
URL = {https://www.mdpi.com/2072-4292/12/1/126},
ISSN = {2072-4292},
ABSTRACT = {The timely knowledge of forage quality of grasslands is vital for matching the demands in animal feeding. Remote sensing (RS) is a promising tool for estimating field-scale forage quality compared with traditional methods, which usually do not provide equally detailed information. However, the applicability of RS prediction models depends on the variability of the underlying calibration data, which can be brought about by the inclusion of a multitude of grassland types and management practices in the model development. Major aims of this study were (i) to build forage quality estimation models for multiple grassland types based on an unmanned aerial vehicle (UAV)-borne imaging spectroscopy and (ii) to generate forage quality distribution maps using the best models obtained. The study examined data from eight grasslands in northern Hesse, Germany, which largely differed in terms of vegetation type and cutting regime. The UAV with a hyperspectral camera on board was utilised to acquire spectral images from the grasslands, and crude protein (CP) and acid detergent fibre (ADF) concentration of the forage was assessed at each cut. Five predictive modelling regression algorithms were applied to develop quality estimation models. Further, grassland forage quality distribution maps were created using the best models developed. The normalised spectral reflectance data showed the strongest relationship with both CP and ADF concentration. From all predictive algorithms, support vector regression provided the highest precision and accuracy for CP estimation (median normalised root mean square error prediction (nRMSEp) = 10.6%), while cubist regression model proved best for ADF estimation (median nRMSEp = 13.4%). The maps generated for both CP and ADF showed a distinct spatial variation in forage quality values for the different grasslands and cutting regimes. Overall, the results disclose that UAV-borne imaging spectroscopy, in combination with predictive modelling, provides a promising tool for accurate forage quality estimation of multiple grasslands.},
DOI = {10.3390/rs12010126}
}



@Article{rs12010133,
AUTHOR = {Dong, Xinyu and Zhang, Zhichao and Yu, Ruiyang and Tian, Qingjiu and Zhu, Xicun},
TITLE = {Extraction of Information about Individual Trees from High-Spatial-Resolution UAV-Acquired Images of an Orchard},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {133},
URL = {https://www.mdpi.com/2072-4292/12/1/133},
ISSN = {2072-4292},
ABSTRACT = {The extraction of information about individual trees is essential to supporting the growing of fruit in orchard management. Data acquired from spectral sensors mounted on unmanned aerial vehicles (UAVs) have very high spatial and temporal resolution. However, an efficient and reliable method for extracting information about individual trees with irregular tree-crown shapes and a complicated background is lacking. In this study, we developed and tested the performance of an approach, based on UAV imagery, to extracting information about individual trees in an orchard with a complicated background that includes apple trees (Plot 1) and pear trees (Plot 2). The workflow involves the construction of a digital orthophoto map (DOM), digital surface models (DSMs), and digital terrain models (DTMs) using the Structure from Motion (SfM) and Multi-View Stereo (MVS) approaches, as well as the calculation of the Excess Green minus Excess Red Index (ExGR) and the selection of various thresholds. Furthermore, a local-maxima filter method and marker-controlled watershed segmentation were used for the detection and delineation, respectively, of individual trees. The accuracy of the proposed method was evaluated by comparing its results with manual estimates of the numbers of trees and the areas and diameters of tree-crowns, all three of which parameters were obtained from the DOM. The results of the proposed method are in good agreement with these manual estimates: The F-scores for the estimated numbers of individual trees were 99.0% and 99.3% in Plot 1 and Plot 2, respectively, while the Producer&rsquo;s Accuracy (PA) and User&rsquo;s Accuracy (UA) for the delineation of individual tree-crowns were above 95% for both of the plots. For the area of individual tree-crowns, root-mean-square error (RMSE) values of 0.72 m2 and 0.48 m2 were obtained for Plot 1 and Plot 2, respectively, while for the diameter of individual tree-crowns, RMSE values of 0.39 m and 0.26 m were obtained for Plot 1 (339 trees correctly identified) and Plot 2 (203 trees correctly identified), respectively. Both the areas and diameters of individual tree-crowns were overestimated to varying degrees.},
DOI = {10.3390/rs12010133}
}



@Article{s20010293,
AUTHOR = {Li, Yaxin and Li, Wenbin and Tang, Shengjun and Darwish, Walid and Hu, Yuling and Chen, Wu},
TITLE = {Automatic Indoor as-Built Building Information Models Generation by Using Low-Cost RGB-D Sensors},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {293},
URL = {https://www.mdpi.com/1424-8220/20/1/293},
ISSN = {1424-8220},
ABSTRACT = {To generate indoor as-built building information models (AB BIMs) automatically and economically is a great technological challenge. Many approaches have been developed to address this problem in recent years, but it is far from being settled, particularly for the point cloud segmentation and the extraction of the relationship among different elements due to the complicated indoor environment. This is even more difficult for the low-quality point cloud generated by low-cost scanning equipment. This paper proposes an automatic as-built BIMs generation framework that transforms the noisy 3D point cloud produced by a low-cost RGB-D sensor (about 708 USD for data collection equipment, 379 USD for the Structure sensor and 329 USD for iPad) to the as-built BIMs, without any manual intervention. The experiment results show that the proposed method has competitive robustness and accuracy, compared to the high-quality Terrestrial Lidar System (TLS), with the element extraction accuracy of 100%, mean dimension reconstruction accuracy of 98.6% and mean area reconstruction accuracy of 93.6%. Also, the proposed framework makes the BIM generation workflows more efficient in both data collection and data processing. In the experiments, the time consumption of data collection for a typical room, with an area of 45&ndash;67      m 2     , is reduced to 4&ndash;6 min with an RGB-D sensor from 50&ndash;60 min with TLS. The processing time to generate BIM models is about half minutes automatically, from around 10 min with a conventional semi-manual method.},
DOI = {10.3390/s20010293}
}



@Article{s20020345,
AUTHOR = {Tang, Wei and Wang, Lijian and Gu, Jiawei and Gu, Yunfeng},
TITLE = {Single Neural Adaptive PID Control for Small UAV Micro-Turbojet Engine},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {345},
URL = {https://www.mdpi.com/1424-8220/20/2/345},
ISSN = {1424-8220},
ABSTRACT = {The micro-turbojet engine (MTE) is especially suitable for unmanned aerial vehicles (UAVs). Because the rotor speed is proportional to the thrust force, the accurate speed tracking control is indispensable for MTE. Thanks to its simplicity, the proportional&ndash;integral&ndash;derivative (PID) controller is commonly used for rotor speed regulation. However, the PID controller cannot guarantee superior performance over the entire operation range due to the time-variance and strong nonlinearity of MTE. The gain scheduling approach using a family of linear controllers is recognized as an efficient alternative, but such a solution heavily relies on the model sets and pre-knowledge. To tackle such challenges, a single neural adaptive PID (SNA-PID) controller is proposed herein for rotor speed control. The new controller featuring with a single-neuron network is able to adaptively tune the gains (weights) online. The simple structure of the controller reduces the computational load and facilitates the algorithm implementation on low-cost hardware. Finally, the proposed controller is validated by numerical simulations and experiments on the MTE in laboratory conditions, and the results show that the proposed controller achieves remarkable effectiveness for speed tracking control. In comparison with the PID controller, the proposed controller yields 54% and 66% reductions on static tracking error under two typical cases.},
DOI = {10.3390/s20020345}
}



@Article{rs12020215,
AUTHOR = {Zha, Hainie and Miao, Yuxin and Wang, Tiantian and Li, Yue and Zhang, Jing and Sun, Weichao and Feng, Zhengqi and Kusnierek, Krzysztof},
TITLE = {Improving Unmanned Aerial Vehicle Remote Sensing-Based Rice Nitrogen Nutrition Index Prediction with Machine Learning},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {215},
URL = {https://www.mdpi.com/2072-4292/12/2/215},
ISSN = {2072-4292},
ABSTRACT = {Optimizing nitrogen (N) management in rice is crucial for China&rsquo;s food security and sustainable agricultural development. Nondestructive crop growth monitoring based on remote sensing technologies can accurately assess crop N status, which may be used to guide the in-season site-specific N recommendations. The fixed-wing unmanned aerial vehicle (UAV)-based remote sensing is a low-cost, easy-to-operate technology for collecting spectral reflectance imagery, an important data source for precision N management. The relationships between many vegetation indices (VIs) derived from spectral reflectance data and crop parameters are known to be nonlinear. As a result, nonlinear machine learning methods have the potential to improve the estimation accuracy. The objective of this study was to evaluate five different approaches for estimating rice (Oryza sativa L.) aboveground biomass (AGB), plant N uptake (PNU), and N nutrition index (NNI) at stem elongation (SE) and heading (HD) stages in Northeast China: (1) single VI (SVI); (2) stepwise multiple linear regression (SMLR); (3) random forest (RF); (4) support vector machine (SVM); and (5) artificial neural networks (ANN) regression. The results indicated that machine learning methods improved the NNI estimation compared to VI-SLR and SMLR methods. The RF algorithm performed the best for estimating NNI (R2 = 0.94 (SE) and 0.96 (HD) for calibration and 0.61 (SE) and 0.79 (HD) for validation). The root mean square errors (RMSEs) were 0.09, and the relative errors were &lt;10% in all the models. It is concluded that the RF machine learning regression can significantly improve the estimation of rice N status using UAV remote sensing. The application machine learning methods offers a new opportunity to better use remote sensing data for monitoring crop growth conditions and guiding precision crop management. More studies are needed to further improve these machine learning-based models by combining both remote sensing data and other related soil, weather, and management information for applications in precision N and crop management.},
DOI = {10.3390/rs12020215}
}



@Article{rs12020221,
AUTHOR = {Zhang, Xiuwei and Jin, Jiaojiao and Lan, Zeze and Li, Chunjiang and Fan, Minhao and Wang, Yafei and Yu, Xin and Zhang, Yanning},
TITLE = {ICENET: A Semantic Segmentation Deep Network for River Ice by Fusing Positional and Channel-Wise Attentive Features},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {221},
URL = {https://www.mdpi.com/2072-4292/12/2/221},
ISSN = {2072-4292},
ABSTRACT = {River ice monitoring is of great significance for river management, ship navigation and ice hazard forecasting in cold-regions. Accurate ice segmentation is one most important pieces of technology in ice monitoring research. It can provide the prerequisite information for the calculation of ice cover density, drift ice speed, ice cover distribution, change detection and so on. Unmanned aerial vehicle (UAV) aerial photography has the advantages of higher spatial and temporal resolution. As UAV technology has become more popular and cheaper, it has been widely used in ice monitoring. So, we focused on river ice segmentation based on UAV remote sensing images. In this study, the NWPU_YRCC dataset was built for river ice segmentation, in which all images were captured by different UAVs in the region of the Yellow River, the most difficult river to manage in the world. To the best of our knowledge, this is the first public UAV image dataset for river ice segmentation. Meanwhile, a semantic segmentation deep convolution neural network by fusing positional and channel-wise attentive features is proposed for river ice semantic segmentation, named ICENET. Experiments demonstrated that the proposed ICENET outperforms the state-of-the-art methods, achieving a superior result on the NWPU_YRCC dataset.},
DOI = {10.3390/rs12020221}
}



@Article{en13020326,
AUTHOR = {Wang, Ju and Wang, Guoqiang and Hu, Xiaoxuan and Luo, He and Xu, Haiqing},
TITLE = {Cooperative Transmission Tower Inspection with a Vehicle and a UAV in Urban Areas},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {326},
URL = {https://www.mdpi.com/1996-1073/13/2/326},
ISSN = {1996-1073},
ABSTRACT = {To reduce the workload of inspectors and improve the inspection efficiency of urban transmission towers, a new inspection method is proposed in this paper, in which an unmanned aerial vehicle (UAV) and vehicle cooperate with each other. We investigate the cooperative path planning problem of a UAV and a vehicle for transmission tower inspection and develop a new 0&ndash;1 integer programming model to address the problem. An odd-even layered genetic algorithm (O-ELGA) is proposed to efficiently solve the model. Finally, the effectiveness of the algorithm is further verified by simulation experiments.},
DOI = {10.3390/en13020326}
}



@Article{rs12020245,
AUTHOR = {Senthilnath, J. and Varia, Neelanshi and Dokania, Akanksha and Anand, Gaotham and Benediktsson, Jón Atli},
TITLE = {Deep TEC: Deep Transfer Learning with Ensemble Classifier for Road Extraction from UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {245},
URL = {https://www.mdpi.com/2072-4292/12/2/245},
ISSN = {2072-4292},
ABSTRACT = {Unmanned aerial vehicle (UAV) remote sensing has a wide area of applications and in this paper, we attempt to address one such problem&mdash;road extraction from UAV-captured RGB images. The key challenge here is to solve the road extraction problem using the UAV multiple remote sensing scene datasets that are acquired with different sensors over different locations. We aim to extract the knowledge from a dataset that is available in the literature and apply this extracted knowledge on our dataset. The paper focuses on a novel method which consists of deep TEC (deep transfer learning with ensemble classifier) for road extraction using UAV imagery. The proposed deep TEC performs road extraction on UAV imagery in two stages, namely, deep transfer learning and ensemble classifier. In the first stage, with the help of deep learning methods, namely, the conditional generative adversarial network, the cycle generative adversarial network and the fully convolutional network, the model is pre-trained on the benchmark UAV road extraction dataset that is available in the literature. With this extracted knowledge (based on the pre-trained model) the road regions are then extracted on our UAV acquired images. Finally, for the road classified images, ensemble classification is carried out. In particular, the deep TEC method has an average quality of 71%, which is 10% higher than the next best standard deep learning methods. Deep TEC also shows a higher level of performance measures such as completeness, correctness and F1 score measures. Therefore, the obtained results show that the deep TEC is efficient in extracting road networks in an urban region.},
DOI = {10.3390/rs12020245}
}



@Article{app10020515,
AUTHOR = {González-Patiño, David and Villuendas-Rey, Yenny and Argüelles-Cruz, Amadeo José and Camacho-Nieto, Oscar and Yáñez-Márquez, Cornelio},
TITLE = {AISAC: An Artificial Immune System for Associative Classification Applied to Breast Cancer Detection},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {515},
URL = {https://www.mdpi.com/2076-3417/10/2/515},
ISSN = {2076-3417},
ABSTRACT = {Early breast cancer diagnosis is crucial, as it can prevent further complications and save the life of the patient by treating the disease at its most curable stage. In this paper, we propose a new artificial immune system model for associative classification with competitive performance for breast cancer detection. The proposed model has its foundations in the biological immune system; it mimics the detection skills of the immune system to provide correct identification of antigens. The Wilcoxon test was used to identify the statistically significant differences between our proposal and other classification algorithms based on the same bio-inspired model. These statistical tests evidenced the enhanced performance shown by the proposed model by outperforming other immune-based algorithms. The proposed model proved to be competitive with respect to other well-known classification models. In addition, the model benefits from a low computational cost. The success of this model for classification tasks shows that swarm intelligence is useful for this kind of problem, and that it is not limited to optimization tasks.},
DOI = {10.3390/app10020515}
}



@Article{app10020636,
AUTHOR = {Rokhsaritalemi, Somaiieh and Sadeghi-Niaraki, Abolghasem and Choi, Soo-Mi},
TITLE = {A Review on Mixed Reality: Current Trends, Challenges and Prospects},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {636},
URL = {https://www.mdpi.com/2076-3417/10/2/636},
ISSN = {2076-3417},
ABSTRACT = {Currently, new technologies have enabled the design of smart applications that are used as decision-making tools in the problems of daily life. The key issue in designing such an application is the increasing level of user interaction. Mixed reality (MR) is an emerging technology that deals with maximum user interaction in the real world compared to other similar technologies. Developing an MR application is complicated, and depends on the different components that have been addressed in previous literature. In addition to the extraction of such components, a comprehensive study that presents a generic framework comprising all components required to develop MR applications needs to be performed. This review studies intensive research to obtain a comprehensive framework for MR applications. The suggested framework comprises five layers: the first layer considers system components; the second and third layers focus on architectural issues for component integration; the fourth layer is the application layer that executes the architecture; and the fifth layer is the user interface layer that enables user interaction. The merits of this study are as follows: this review can act as a proper resource for MR basic concepts, and it introduces MR development steps and analytical models, a simulation toolkit, system types, and architecture types, in addition to practical issues for stakeholders such as considering MR different domains.},
DOI = {10.3390/app10020636}
}



@Article{app10020666,
AUTHOR = {Jung, Daekyo and Tran Tuan, Vu and Quoc Tran, Dai and Park, Minsoo and Park, Seunghee},
TITLE = {Conceptual Framework of an Intelligent Decision Support System for Smart City Disaster Management},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {666},
URL = {https://www.mdpi.com/2076-3417/10/2/666},
ISSN = {2076-3417},
ABSTRACT = {In order to protect human lives and infrastructure, as well as to minimize the risk of damage, it is important to predict and respond to natural disasters in advance. However, currently, the standardized disaster response system in South Korea still needs further advancement, and the response phase systems need to be improved to ensure that they are properly equipped to cope with natural disasters. Existing studies on intelligent disaster management systems (IDSSs) in South Korea have focused only on storms, floods, and earthquakes, and they have not used past data. This research proposes a new conceptual framework of an IDSS for disaster management, with particular attention paid to wildfires and cold/heat waves. The IDSS uses big data collected from open application programming interface (API) and artificial intelligence (AI) algorithms to help decision-makers make faster and more accurate decisions. In addition, a simple example of the use of a convolutional neural network (CNN) to detect fire in surveillance video has been developed, which can be used for automatic fire detection and provide an appropriate response. The system will also consider connecting to open source intelligence (OSINT) to identify vulnerabilities, mitigate risks, and develop more robust security policies than those currently in place to prevent cyber-attacks.},
DOI = {10.3390/app10020666}
}



@Article{rs12020325,
AUTHOR = {Zha, Yufei and Wu, Min and Qiu, Zhuling and Sun, Jingxian and Zhang, Peng and Huang, Wei},
TITLE = {Online Semantic Subspace Learning with Siamese Network for UAV Tracking},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {325},
URL = {https://www.mdpi.com/2072-4292/12/2/325},
ISSN = {2072-4292},
ABSTRACT = {In urban environment monitoring, visual tracking on unmanned aerial vehicles (UAVs) can produce more applications owing to the inherent advantages, but it also brings new challenges for existing visual tracking approaches (such as complex background clutters, rotation, fast motion, small objects, and realtime issues due to camera motion and viewpoint changes). Based on the Siamese network, tracking can be conducted efficiently in recent UAV datasets. Unfortunately, the learned convolutional neural network (CNN) features are not discriminative when identifying the target from the background/clutter, In particular for the distractor, and cannot capture the appearance variations temporally. Additionally, occlusion and disappearance are also reasons for tracking failure. In this paper, a semantic subspace module is designed to be integrated into the Siamese network tracker to encode the local fine-grained details of the target for UAV tracking. More specifically, the target&rsquo;s semantic subspace is learned online to adapt to the target in the temporal domain. Additionally, the pixel-wise response of the semantic subspace can be used to detect occlusion and disappearance of the target, and this enables reasonable updating to relieve model drifting. Substantial experiments conducted on challenging UAV benchmarks illustrate that the proposed method can obtain competitive results in both accuracy and efficiency when they are applied to UAV videos.},
DOI = {10.3390/rs12020325}
}



@Article{s20020563,
AUTHOR = {Lobo Torres, Daliana and Queiroz Feitosa, Raul and Nigri Happ, Patrick and Elena Cué La Rosa, Laura and Marcato Junior, José and Martins, José and Olã Bressan, Patrik and Gonçalves, Wesley Nunes and Liesenberg, Veraldo},
TITLE = {Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {563},
URL = {https://www.mdpi.com/1424-8220/20/2/563},
ISSN = {1424-8220},
ABSTRACT = {This study proposes and evaluates five deep fully convolutional networks (FCNs) for the semantic segmentation of a single tree species: SegNet, U-Net, FC-DenseNet, and two DeepLabv3+ variants. The performance of the FCN designs is evaluated experimentally in terms of classification accuracy and computational load. We also verify the benefits of fully connected conditional random fields (CRFs) as a post-processing step to improve the segmentation maps. The analysis is conducted on a set of images captured by an RGB camera aboard a UAV flying over an urban area. The dataset also contains a mask that indicates the occurrence of an endangered species called Dipteryx alata Vogel, also known as cumbaru, taken as the species to be identified. The experimental analysis shows the effectiveness of each design and reports average overall accuracy ranging from 88.9% to 96.7%, an F1-score between 87.0% and 96.1%, and IoU from 77.1% to 92.5%. We also realize that CRF consistently improves the performance, but at a high computational cost.},
DOI = {10.3390/s20020563}
}



@Article{rs12020336,
AUTHOR = {Zhang, Yishan and Wu, Lun and Ren, Huazhong and Liu, Yu and Zheng, Yongqian and Liu, Yaowen and Dong, Jiaji},
TITLE = {Mapping Water Quality Parameters in Urban Rivers from Hyperspectral Images Using a New Self-Adapting Selection of Multiple Artificial Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {336},
URL = {https://www.mdpi.com/2072-4292/12/2/336},
ISSN = {2072-4292},
ABSTRACT = {Protection of water environments is an important part of overall environmental protection; hence, many people devote their efforts to monitoring and improving water quality. In this study, a self-adapting selection method of multiple artificial neural networks (ANNs) using hyperspectral remote sensing and ground-measured water quality data is proposed to quantitatively predict water quality parameters, including phosphorus, nitrogen, biochemical oxygen demand (BOD), chemical oxygen demand (COD), and chlorophyll a. Seventy-nine ground measured data samples are used as training data in the establishment of the proposed model, and 30 samples are used as testing data. The proposed method based on traditional ANNs of numerical prediction involves feature selection of bands, self-adapting selection based on multiple selection criteria, stepwise backtracking, and combined weighted correlation. Water quality parameters are estimated with coefficient of determination      R 2      ranging from 0.93 (phosphorus) to 0.98 (nitrogen), which is higher than the value (0.7 to 0.8) obtained by traditional ANNs. MPAE (mean percent of absolute error) values ranging from 5% to 11% are used rather than root mean square error to evaluate the predicting precision of the proposed model because the magnitude of each water quality parameter considerably differs, thereby providing reasonable and interpretable results. Compared with other ANNs with backpropagation, this study proposes an auto-adapting method assisted by the above-mentioned methods to select the best model with all settings, such as the number of hidden layers, number of neurons in each hidden layer, choice of optimizer, and activation function. Different settings for ANNS with backpropagation are important to improve precision and compatibility for different data. Furthermore, the proposed method is applied to hyperspectral remote sensing images collected using an unmanned aerial vehicle for monitoring the water quality in the Shiqi River, Zhongshan City, Guangdong Province, China. Obtained results indicate the locations of pollution sources.},
DOI = {10.3390/rs12020336}
}



@Article{su12030767,
AUTHOR = {Tian, Ai-Qing and Chu, Shu-Chuan and Pan, Jeng-Shyang and Cui, Huanqing and Zheng, Wei-Min},
TITLE = {A Compact Pigeon-Inspired Optimization for Maximum Short-Term Generation Mode in Cascade Hydroelectric Power Station},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {767},
URL = {https://www.mdpi.com/2071-1050/12/3/767},
ISSN = {2071-1050},
ABSTRACT = {Pigeon-inspired optimization (PIO) is a new type of intelligent algorithm. It is proposed that the algorithm simulates the movement of pigeons going home. In this paper, a new pigeon herding algorithm called compact pigeon-inspired optimization (CPIO) is proposed. The challenging task for multiple algorithms is not only combining operations, but also constraining existing devices. The proposed algorithm aims to solve complex scientific and industrial problems with many data packets, including the use of classical optimization problems and the ability to find optimal solutions in many solution spaces with limited hardware resources. A real-valued prototype vector performs probability and statistical calculations, and then generates optimal candidate solutions for CPIO optimization algorithms. The CPIO algorithm was used to evaluate a variety of continuous multi-model functions and the largest model of hydropower short-term generation. The experimental results show that the proposed algorithm is a more effective way to produce competitive results in the case of limited memory devices.},
DOI = {10.3390/su12030767}
}



@Article{rs12030458,
AUTHOR = {Alganci, Ugur and Soydas, Mehmet and Sertel, Elif},
TITLE = {Comparative Research on Deep Learning Approaches for Airplane Detection from Very High-Resolution Satellite Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {458},
URL = {https://www.mdpi.com/2072-4292/12/3/458},
ISSN = {2072-4292},
ABSTRACT = {Object detection from satellite images has been a challenging problem for many years. With the development of effective deep learning algorithms and advancement in hardware systems, higher accuracies have been achieved in the detection of various objects from very high-resolution (VHR) satellite images. This article provides a comparative evaluation of the state-of-the-art convolutional neural network (CNN)-based object detection models, which are Faster R-CNN, Single Shot Multi-box Detector (SSD), and You Look Only Once-v3 (YOLO-v3), to cope with the limited number of labeled data and to automatically detect airplanes in VHR satellite images. Data augmentation with rotation, rescaling, and cropping was applied on the test images to artificially increase the number of training data from satellite images. Moreover, a non-maximum suppression algorithm (NMS) was introduced at the end of the SSD and YOLO-v3 flows to get rid of the multiple detection occurrences near each detected object in the overlapping areas. The trained networks were applied to five independent VHR test images that cover airports and their surroundings to evaluate their performance objectively. Accuracy assessment results of the test regions proved that Faster R-CNN architecture provided the highest accuracy according to the F1 scores, average precision (AP) metrics, and visual inspection of the results. The YOLO-v3 ranked as second, with a slightly lower performance but providing a balanced trade-off between accuracy and speed. The SSD provided the lowest detection performance, but it was better in object localization. The results were also evaluated in terms of the object size and detection accuracy manner, which proved that large- and medium-sized airplanes were detected with higher accuracy.},
DOI = {10.3390/rs12030458}
}



@Article{agronomy10020215,
AUTHOR = {Xiao, Qinggang and Du, Rui and Yang, Lin and Han, Xiaoqiang and Zhao, Sifeng and Zhang, Guoqiang and Fu, Wei and Wang, Guobin and Lan, Yubin},
TITLE = {Comparison of Droplet Deposition Control Efficacy on Phytophthora capsica and Aphids in the Processing Pepper Field of the Unmanned Aerial Vehicle and Knapsack Sprayer},
JOURNAL = {Agronomy},
VOLUME = {10},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {215},
URL = {https://www.mdpi.com/2073-4395/10/2/215},
ISSN = {2073-4395},
ABSTRACT = {Processing pepper planting and processing have become an important red pillar industry in Xinjiang. With the continuous growth of processing pepper planting areas in Xinjiang, diseases and pests are increasing year by year. The aim of this study was to compare the droplet deposition and control efficiency of unmanned aerial vehicle (UAV) and electric air-pressure knapsack (EAP) sprayers on a processing pepper field. The UAV sprayer had a poor droplet coverage rate, droplet density, and deposition uniformity, but displayed the best deposition (1.01 &mu;g/cm2, which was 98% more than the EAP sprayer). The control efficacy of the UAV sprayer on processing pepper fields with Phytophthora capsici and aphids was slightly lower than that of the EAP sprayer. When the UAV sprayer was used to control processing pepper diseases and pests, it could reduce the pesticide dosage on the premise of ensuring the control effect. Further study of the residue of high concentration pesticides in pepper fruit and environment sprayed by UAVs are needed.},
DOI = {10.3390/agronomy10020215}
}



@Article{rs12030478,
AUTHOR = {Hao, Yuzhu and Chen, Zhenjie and Huang, Qiuhao and Li, Feixue and Wang, Beibei and Ma, Lei},
TITLE = {Bidirectional Segmented Detection of Land Use Change Based on Object-Level Multivariate Time Series},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {478},
URL = {https://www.mdpi.com/2072-4292/12/3/478},
ISSN = {2072-4292},
ABSTRACT = {High-precision information regarding the location, time, and type of land use change is integral to understanding global changes. Time series (TS) analysis of remote sensing images is a powerful method for land use change detection. To address the complexity of sample selection and the salt-and-pepper noise of pixels, we propose a bidirectional segmented detection (BSD) method based on object-level, multivariate TS, that detects the type and time of land use change from Landsat images. In the proposed method, based on the multiresolution segmentation of objects, three dimensions of object-level TS are constructed using the median of the following indices: the normalized difference vegetation index (NDVI), the normalized difference built index (NDBI), and the modified normalized difference water index (MNDWI). Then, BSD with forward and backward detection is performed on the segmented objects to identify the types and times of land use change. Experimental results indicate that the proposed BSD method effectively detects the type and time of land use change with an overall accuracy of 90.49% and a Kappa coefficient of 0.86. It was also observed that the median value of a segmented object is more representative than the commonly used mean value. In addition, compared with traditional methods such as LandTrendr, the proposed method is competitive in terms of time efficiency and accuracy. Thus, the BSD method can promote efficient and accurate land use change detection.},
DOI = {10.3390/rs12030478}
}



@Article{s20030817,
AUTHOR = {Popescu, Dan and Stoican, Florin and Stamatescu, Grigore and Ichim, Loretta and Dragana, Cristian},
TITLE = {Advanced UAV–WSN System for Intelligent Monitoring in Precision Agriculture},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {817},
URL = {https://www.mdpi.com/1424-8220/20/3/817},
ISSN = {1424-8220},
ABSTRACT = {The growing need for food worldwide requires the development of a high-performance, high-productivity, and sustainable agriculture, which implies the introduction of new technologies into monitoring activities related to control and decision-making. In this regard, this paper presents a hierarchical structure based on the collaboration between unmanned aerial vehicles (UAVs) and federated wireless sensor networks (WSNs) for crop monitoring in precision agriculture. The integration of UAVs with intelligent, ground WSNs, and IoT proved to be a robust and efficient solution for data collection, control, analysis, and decisions in such specialized applications. Key advantages lay in online data collection and relaying to a central monitoring point, while effectively managing network load and latency through optimized UAV trajectories and in situ data processing. Two important aspects of the collaboration were considered: designing the UAV trajectories for efficient data collection and implementing effective data processing algorithms (consensus and symbolic aggregate approximation) at the network level for the transmission of the relevant data. The experiments were carried out at a Romanian research institute where different crops and methods are developed. The results demonstrate that the collaborative UAV&ndash;WSN&ndash;IoT approach increases the performances in both precision agriculture and ecological agriculture.},
DOI = {10.3390/s20030817}
}



@Article{rs12030502,
AUTHOR = {Chang, Zhilu and Du, Zhen and Zhang, Fan and Huang, Faming and Chen, Jiawu and Li, Wenbin and Guo, Zizheng},
TITLE = {Landslide Susceptibility Prediction Based on Remote Sensing Images and GIS: Comparisons of Supervised and Unsupervised Machine Learning Models},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {502},
URL = {https://www.mdpi.com/2072-4292/12/3/502},
ISSN = {2072-4292},
ABSTRACT = {Landslide susceptibility prediction (LSP) has been widely and effectively implemented by machine learning (ML) models based on remote sensing (RS) images and Geographic Information System (GIS). However, comparisons of the applications of ML models for LSP from the perspectives of supervised machine learning (SML) and unsupervised machine learning (USML) have not been explored. Hence, this study aims to compare the LSP performance of these SML and USML models, thus further to explore the advantages and disadvantages of these ML models and to realize a more accurate and reliable LSP result. Two representative SML models (support vector machine (SVM) and CHi-squared Automatic Interaction Detection (CHAID)) and two representative USML models (K-means and Kohonen models) are respectively used to scientifically predict the landslide susceptibility indexes, and then these prediction results are discussed. Ningdu County with 446 recorded landslides obtained through field investigations is introduced as case study. A total of 12 conditioning factors are obtained through procession of Landsat TM 8 images and high-resolution aerial images, topographical and hydrological spatial analysis of Digital Elevation Modeling in GIS software, and government reports. The area value under the curve of receiver operating features (AUC) is applied for evaluating the prediction accuracy of SML models, and the frequency ratio (FR) accuracy is then introduced to compare the remarkable prediction performance differences between SML and USML models. Overall, the receiver operation curve (ROC) results show that the AUC of the SVM is 0.892 and is slightly greater than the AUC of the CHAID model (0.872). The FR accuracy results show that the SVM model has the highest accuracy for LSP (77.80%), followed by the CHAID model (74.50%), the Kohonen model (72.8%) and the K-means model (69.7%), which indicates that the SML models can reach considerably better prediction capability than the USML models. It can be concluded that selecting recorded landslides as prior knowledge to train and test the LSP models is the key reason for the higher prediction accuracy of the SML models, while the lack of a priori knowledge and target guidance is an important reason for the low LSP accuracy of the USML models. Nevertheless, the USML models can also be used to implement LSP due to their advantages of efficient modeling processes, dimensionality reduction and strong scalability.},
DOI = {10.3390/rs12030502}
}



@Article{rs12030508,
AUTHOR = {Fu, Zhaopeng and Jiang, Jie and Gao, Yang and Krienke, Brian and Wang, Meng and Zhong, Kaitai and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
TITLE = {Wheat Growth Monitoring and Yield Estimation based on Multi-Rotor Unmanned Aerial Vehicle},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {508},
URL = {https://www.mdpi.com/2072-4292/12/3/508},
ISSN = {2072-4292},
ABSTRACT = {Leaf area index (LAI) and leaf dry matter (LDM) are important indices of crop growth. Real-time, nondestructive monitoring of crop growth is instructive for the diagnosis of crop growth and prediction of grain yield. Unmanned aerial vehicle (UAV)-based remote sensing is widely used in precision agriculture due to its unique advantages in flexibility and resolution. This study was carried out on wheat trials treated with different nitrogen levels and seeding densities in three regions of Jiangsu Province in 2018&ndash;2019. Canopy spectral images were collected by the UAV equipped with a multi-spectral camera during key wheat growth stages. To verify the results of the UAV images, the LAI, LDM, and yield data were obtained by destructive sampling. We extracted the wheat canopy reflectance and selected the best vegetation index for monitoring growth and predicting yield. Simple linear regression (LR), multiple linear regression (MLR), stepwise multiple linear regression (SMLR), partial least squares regression (PLSR), artificial neural network (ANN), and random forest (RF) modeling methods were used to construct a model for wheat yield estimation. The results show that the multi-spectral camera mounted on the multi-rotor UAV has a broad application prospect in crop growth index monitoring and yield estimation. The vegetation index combined with the red edge band and the near-infrared band was significantly correlated with LAI and LDM. Machine learning methods (i.e., PLSR, ANN, and RF) performed better for predicting wheat yield. The RF model constructed by normalized difference vegetation index (NDVI) at the jointing stage, heading stage, flowering stage, and filling stage was the optimal wheat yield estimation model in this study, with an R2 of 0.78 and relative root mean square error (RRMSE) of 0.1030. The results provide a theoretical basis for monitoring crop growth with a multi-rotor UAV platform and explore a technical method for improving the precision of yield estimation.},
DOI = {10.3390/rs12030508}
}



@Article{rs12030547,
AUTHOR = {Maxwell, Aaron E. and Pourmohammadi, Pariya and Poyner, Joey D.},
TITLE = {Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {547},
URL = {https://www.mdpi.com/2072-4292/12/3/547},
ISSN = {2072-4292},
ABSTRACT = {Modern elevation-determining remote sensing technologies such as light-detection and ranging (LiDAR) produce a wealth of topographic information that is increasingly being used in a wide range of disciplines, including archaeology and geomorphology. However, automated methods for mapping topographic features have remained a significant challenge. Deep learning (DL) mask regional-convolutional neural networks (Mask R-CNN), which provides context-based instance mapping, offers the potential to overcome many of the difficulties of previous approaches to topographic mapping. We therefore explore the application of Mask R-CNN to extract valley fill faces (VFFs), which are a product of mountaintop removal (MTR) coal mining in the Appalachian region of the eastern United States. LiDAR-derived slopeshades are provided as the only predictor variable in the model. Model generalization is evaluated by mapping multiple study sites outside the training data region. A range of assessment methods, including precision, recall, and F1 score, all based on VFF counts, as well as area- and a fuzzy area-based user&rsquo;s and producer&rsquo;s accuracy, indicate that the model was successful in mapping VFFs in new geographic regions, using elevation data derived from different LiDAR sensors. Precision, recall, and F1-score values were above 0.85 using VFF counts while user&rsquo;s and producer&rsquo;s accuracy were above 0.75 and 0.85 when using the area- and fuzzy area-based methods, respectively, when averaged across all study areas characterized with LiDAR data. Due to the limited availability of LiDAR data until relatively recently, we also assessed how well the model generalizes to terrain data created using photogrammetric methods that characterize past terrain conditions. Unfortunately, the model was not sufficiently general to allow successful mapping of VFFs using photogrammetrically-derived slopeshades, as all assessment metrics were lower than 0.60; however, this may partially be attributed to the quality of the photogrammetric data. The overall results suggest that the combination of Mask R-CNN and LiDAR has great potential for mapping anthropogenic and natural landscape features. To realize this vision, however, research on the mapping of other topographic features is needed, as well as the development of large topographic training datasets including a variety of features for calibrating and testing new methods.},
DOI = {10.3390/rs12030547}
}



@Article{rs12030577,
AUTHOR = {Perera, Asanka G. and Khanam, Fatema-Tuz-Zohra and Al-Naji, Ali and Chahl, Javaan},
TITLE = {Detection and Localisation of Life Signs from the Air Using Image Registration and Spatio-Temporal Filtering},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {577},
URL = {https://www.mdpi.com/2072-4292/12/3/577},
ISSN = {2072-4292},
ABSTRACT = {In search and rescue operations, it is crucial to rapidly identify those people who are alive from those who are not. If this information is known, emergency teams can prioritize their operations to save more lives. However, in some natural disasters the people may be lying on the ground covered with dust, debris, or ashes making them difficult to detect by video analysis that is tuned to human shapes. We present a novel method to estimate the locations of people from aerial video using image and signal processing designed to detect breathing movements. We have shown that this method can successfully detect clearly visible people and people who are fully occluded by debris. First, the aerial videos were stabilized using the key points of adjacent image frames. Next, the stabilized video was decomposed into tile videos and the temporal frequency bands of interest were motion magnified while the other frequencies were suppressed. Image differencing and temporal filtering were performed on each tile video to detect potential breathing signals. Finally, the detected frequencies were remapped to the image frame creating a life signs map that indicates possible human locations. The proposed method was validated with both aerial and ground recorded videos in a controlled environment. Based on the dataset, the results showed good reliability for aerial videos and no errors for ground recorded videos where the average precision measures for aerial videos and ground recorded videos were 0.913 and 1 respectively.},
DOI = {10.3390/rs12030577}
}



@Article{rs12030579,
AUTHOR = {Agapiou, Athos},
TITLE = {Evaluation of Landsat 8 OLI/TIRS Level-2 and Sentinel 2 Level-1C Fusion Techniques Intended for Image Segmentation of Archaeological Landscapes and Proxies},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {579},
URL = {https://www.mdpi.com/2072-4292/12/3/579},
ISSN = {2072-4292},
ABSTRACT = {The use of medium resolution, open access, and freely distributed satellite images, such as those of Landsat, is still understudied in the domain of archaeological research, mainly due to restrictions of spatial resolution. This investigation aims to showcase how the synergistic use of Landsat and Sentinel optical sensors can efficiently support archaeological research through object-based image analysis (OBIA), a relatively new scientific trend, as highlighted in the relevant literature, in the domain of remote sensing archaeology. Initially, the fusion of a 30 m spatial resolution Landsat 8 OLI/TIRS Level-2 and a 10 m spatial resolution Sentinel 2 Level-1C optical images, over the archaeological site of &ldquo;Nea Paphos&rdquo; in Cyprus, are evaluated in order to improve the spatial resolution of the Landsat image. At this step, various known fusion models are implemented and evaluated, namely Gram&ndash;Schmidt, Brovey, principal component analysis (PCA), and hue-saturation-value (HSV) algorithms. In addition, all four 10 m available spectral bands of the Sentinel 2 sensor, namely the blue, green, red, and near-infrared bands (Bands 2 to 4 and Band 8, respectively) were assessed for each of the different fusion models. On the basis of these findings, the next step of the study, focused on the image segmentation process, through the evaluation of different scale factors. The segmentation process is an important step moving from pixel-based to object-based image analysis. The overall results show that the Gram&ndash;Schmidt fusion method based on the near-infrared band of the Sentinel 2 (Band 8) at a range of scale factor segmentation to 70 are the optimum parameters for the detection of standing visible monuments, monitoring excavated areas, and detecting buried archaeological remains, without any significant spectral distortion of the original Landsat image. The new 10 m fused Landsat 8 image provides further spatial details of the archaeological site and depicts, through the segmentation process, important details within the landscape under examination.},
DOI = {10.3390/rs12030579}
}



@Article{rs12040620,
AUTHOR = {Zhang, Jing and Tian, Haiqing and Wang, Di and Li, Haijun and Mouazen, Abdul Mounem},
TITLE = {A Novel Approach for Estimation of Above-Ground Biomass of Sugar Beet Based on Wavelength Selection and Optimized Support Vector Machine},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {620},
URL = {https://www.mdpi.com/2072-4292/12/4/620},
ISSN = {2072-4292},
ABSTRACT = {Timely diagnosis of sugar beet above-ground biomass (AGB) is critical for the prediction of yield and optimal precision crop management. This study established an optimal quantitative prediction model of AGB of sugar beet by using hyperspectral data. Three experiment campaigns in 2014, 2015 and 2018 were conducted to collect ground-based hyperspectral data at three different growth stages, across different sites, for different cultivars and nitrogen (N) application rates. A competitive adaptive reweighted sampling (CARS) algorithm was applied to select the most sensitive wavelengths to AGB. This was followed by developing a novel modified differential evolution grey wolf optimization algorithm (MDE&ndash;GWO) by introducing differential evolution algorithm (DE) and dynamic non-linear convergence factor to grey wolf optimization algorithm (GWO) to optimize the parameters c and &gamma; of a support vector machine (SVM) model for the prediction of AGB. The prediction performance of SVM models under the three GWO, DE&ndash;GWO and MDE&ndash;GWO optimization methods for CARS selected wavelengths and whole spectral data was examined. Results showed that CARS resulted in a huge wavelength reduction of 97.4% for the rapid growth stage of leaf cluster, 97.2% for the sugar growth stage and 97.4% for the sugar accumulation stage. Models resulted after CARS wavelength selection were found to be more accurate than models developed using the entire spectral data. The best prediction accuracy was achieved after the MDE&ndash;GWO optimization of SVM model parameters for the prediction of AGB in sugar beet, independent of growing stage, years, sites and cultivars. The best coefficient of determination (R2), root mean square error (RMSE) and residual prediction deviation (RPD) ranged, respectively, from 0.74 to 0.80, 46.17 to 65.68 g/m2 and 1.42 to 1.97 for the rapid growth stage of leaf cluster, 0.78 to 0.80, 30.16 to 37.03 g/m2 and 1.69 to 2.03 for the sugar growth stage, and 0.69 to 0.74, 40.17 to 104.08 g/m2 and 1.61 to 1.95 for the sugar accumulation stage. It can be concluded that the methodology proposed can be implemented for the prediction of AGB of sugar beet using proximal hyperspectral sensors under a wide range of environmental conditions.},
DOI = {10.3390/rs12040620}
}



@Article{antiox9020156,
AUTHOR = {Karydas, Christos and Iatrou, Miltiadis and Kouretas, Dimitrios and Patouna, Anastasia and Iatrou, George and Lazos, Nikolaos and Gewehr, Sandra and Tseni, Xanthi and Tekos, Fotis and Zartaloudis, Zois and Mainos, Evangelos and Mourelatos, Spiros},
TITLE = {Prediction of Antioxidant Activity of Cherry Fruits from UAS Multispectral Imagery Using Machine Learning},
JOURNAL = {Antioxidants},
VOLUME = {9},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {156},
URL = {https://www.mdpi.com/2076-3921/9/2/156},
PubMedID = {32075036},
ISSN = {2076-3921},
ABSTRACT = {In this research, a model for the estimation of antioxidant content in cherry fruits from multispectral imagery acquired from drones was developed, based on machine learning methods. For two consecutive cultivation years, the trees were sampled on different dates and then analysed for their fruits&rsquo; radical scavenging activity (DPPH) and Folin&ndash;Ciocalteu (FCR) reducing capacity. Multispectral images from unmanned aerial vehicles were acquired on the same dates with fruit sampling. Soil samples were collected throughout the study fields at the end of the season. Topographic, hydrographic and weather data also were included in modelling. First-year data were used for model-fitting, whereas second-year data for testing. Spatial autocorrelation tests indicated unbiased sampling and, moreover, allowed restriction of modelling input parameters to a smaller group. The optimum model employs 24 input variables resulting in a 6.74 root mean square error. Provided that soil profiles and other ancillary data are known in advance of the cultivation season, capturing drone images in critical growth phases, together with contemporary weather data, can support site- and time-specific harvesting. It could also support site-specific treatments (precision farming) for improving fruit quality in the long-term, with analogous marketing perspectives.},
DOI = {10.3390/antiox9020156}
}



@Article{rs12040638,
AUTHOR = {Hufkens, Koen and de Haulleville, Thalès and Kearsley, Elizabeth and Jacobsen, Kim and Beeckman, Hans and Stoffelen, Piet and Vandelook, Filip and Meeus, Sofie and Amara, Michael and Van Hirtum, Leen and Van den Bulcke, Jan and Verbeeck, Hans and Wingate, Lisa},
TITLE = {Historical Aerial Surveys Map Long-Term Changes of Forest Cover and Structure in the Central Congo Basin},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {638},
URL = {https://www.mdpi.com/2072-4292/12/4/638},
ISSN = {2072-4292},
ABSTRACT = {Given the impact of tropical forest disturbances on atmospheric carbon emissions, biodiversity, and ecosystem productivity, accurate long-term reporting of Land-Use and Land-Cover (LULC) change in the pre-satellite era (&lt;1972) is an imperative. Here, we used a combination of historical (1958) aerial photography and contemporary remote sensing data to map long-term changes in the extent and structure of the tropical forest surrounding Yangambi (DR Congo) in the central Congo Basin. Our study leveraged structure-from-motion and a convolutional neural network-based LULC classifier, using synthetic landscape-based image augmentation to map historical forest cover across a large orthomosaic (~93,431 ha) geo-referenced to ~4.7 ± 4.3 m at submeter resolution. A comparison with contemporary LULC data showed a shift from previously highly regular industrial deforestation of large areas to discrete smallholder farming clearing, increasing landscape fragmentation and providing opportunties for substantial forest regrowth. We estimated aboveground carbon gains through reforestation to range from 811 to 1592 Gg C, partially offsetting historical deforestation (2416 Gg C), in our study area. Efforts to quantify long-term canopy texture changes and their link to aboveground carbon had limited to no success. Our analysis provides methods and insights into key spatial and temporal patterns of deforestation and reforestation at a multi-decadal scale, providing a historical context for past and ongoing forest research in the area.},
DOI = {10.3390/rs12040638}
}



@Article{app10041403,
AUTHOR = {Yu, Zhi and Shi, Xiuzhi and Zhou, Jian and Chen, Xin and Qiu, Xianyang},
TITLE = {Effective Assessment of Blast-Induced Ground Vibration Using an Optimized Random Forest Model Based on a Harris Hawks Optimization Algorithm},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {1403},
URL = {https://www.mdpi.com/2076-3417/10/4/1403},
ISSN = {2076-3417},
ABSTRACT = {Most mines choose the drilling and blasting method which has the characteristics of being a cheap and efficient method to fragment rock mass, but blast-induced ground vibration damages the surrounding rock mass and structure and is a drawback. To predict, analyze and control the blast-induced ground vibration, the random forest (RF) model, Harris hawks optimization (HHO) algorithm and Monte Carlo simulation approach were utilized. A database consisting of 137 datasets was collected at different locations around the Tonglvshan open-cast mine, China. Seven variables were selected and collected as the input variables, and peak particle velocity was chosen as the output variable. At first, an RF model and a hybrid model, namely a HHO-RF model, were developed, and the prediction results checked by 3 performance indices to show that the proposed HHO-RF model can provide higher prediction performance. Then blast-induced ground vibration was simulated by using the Monte Carlo simulation approach and the developed HHO-RF model. After analyzing, the mean peak particle velocity value was 0.98 cm/s, and the peak particle velocity value did not exceed 1.95 cm/s with a probability of 90%. The research results of this study provided a simple, accurate method and basis for predicting, evaluating blast-induced ground vibration and optimizing the blast design before blast operation.},
DOI = {10.3390/app10041403}
}



@Article{iot1010002,
AUTHOR = {Spachos, Petros},
TITLE = {Towards a Low-Cost Precision Viticulture System Using Internet of Things Devices},
JOURNAL = {IoT},
VOLUME = {1},
YEAR = {2020},
NUMBER = {1},
PAGES = {5--20},
URL = {https://www.mdpi.com/2624-831X/1/1/2},
ISSN = {2624-831X},
ABSTRACT = {Precision Agriculture (PA) is an ever-expanding field that takes modern technological advancements and applies it to farming practices to reduce waste and increase output. One advancement that can play a significant role in achieving precision agriculture is wireless technology, and specifically the Internet of Things (IoT) devices. Small, inch scale and low-cost devices can be used to monitor great agricultural areas. In this paper, a system for precision viticulture which uses IoT devices for real-time monitoring is proposed. The different components of the system are programmed properly and the interconnection between them is designed to minimize energy consumption. Wireless sensor nodes measure soil moisture and soil temperature in the field and transmit the information to a base station. If the conditions are optimal for a disease or pest to occur, a drone flies towards the area. When the drone is over the node, pictures are captured and then it returns to the base station for further processing. The feasibility of the system is examined through experimentation in a realistic scenario.},
DOI = {10.3390/iot1010002}
}



@Article{rs12040725,
AUTHOR = {Pastick, Neal J. and Dahal, Devendra and Wylie, Bruce K. and Parajuli, Sujan and Boyte, Stephen P. and Wu, Zhouting},
TITLE = {Characterizing Land Surface Phenology and Exotic Annual Grasses in Dryland Ecosystems Using Landsat and Sentinel-2 Data in Harmony},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {725},
URL = {https://www.mdpi.com/2072-4292/12/4/725},
ISSN = {2072-4292},
ABSTRACT = {Invasive annual grasses, such as cheatgrass (Bromus tectorum L.), have proliferated in dryland ecosystems of the western United States, promoting increased fire activity and reduced biodiversity that can be detrimental to socio-environmental systems. Monitoring exotic annual grass cover and dynamics over large areas requires the use of remote sensing that can support early detection and rapid response initiatives. However, few studies have leveraged remote sensing technologies and computing frameworks capable of providing rangeland managers with maps of exotic annual grass cover at relatively high spatiotemporal resolutions and near real-time latencies. Here, we developed a system for automated mapping of invasive annual grass (%) cover using in situ observations, harmonized Landsat and Sentinel-2 (HLS) data, maps of biophysical variables, and machine learning techniques. A robust and automated cloud, cloud shadow, water, and snow/ice masking procedure (mean overall accuracy &gt;81%) was implemented using time-series outlier detection and data mining techniques prior to spatiotemporal interpolation of HLS data via regression tree models (r = 0.94; mean absolute error (MAE) = 0.02). Weekly, cloud-free normalized difference vegetation index (NDVI) image composites (2016&ndash;2018) were used to construct a suite of spectral and phenological metrics (e.g., start and end of season dates), consistent with information derived from Moderate Resolution Image Spectroradiometer (MODIS) data. These metrics were incorporated into a data mining framework that accurately (r = 0.83; MAE = 11) modeled and mapped exotic annual grass (%) cover throughout dryland ecosystems in the western United States at a native, 30-m spatial resolution. Our results show that inclusion of weekly HLS time-series data and derived indicators improves our ability to map exotic annual grass cover, as compared to distribution models that use MODIS products or monthly, seasonal, or annual HLS composites as primary inputs. This research fills a critical gap in our ability to effectively assess, manage, and monitor drylands by providing a framework that allows for an accurate and timely depiction of land surface phenology and exotic annual grass cover at spatial and temporal resolutions that are meaningful to local resource managers.},
DOI = {10.3390/rs12040725}
}



@Article{rs12040739,
AUTHOR = {Nogueira, Keiller and L. S. Machado, Gabriel and H. T. Gama, Pedro and C. V. da Silva, Caio and Balaniuk, Remis and A. dos Santos, Jefersson},
TITLE = {Facing Erosion Identification in Railway Lines Using Pixel-Wise Deep-Based Approaches},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {739},
URL = {https://www.mdpi.com/2072-4292/12/4/739},
ISSN = {2072-4292},
ABSTRACT = {Soil erosion is considered one of the most expensive natural hazards with a high impact on several infrastructure assets. Among them, railway lines are one of the most likely constructions for the appearance of erosion and, consequently, one of the most troublesome due to the maintenance costs, risks of derailments, and so on. Therefore, it is fundamental to identify and monitor erosion in railway lines to prevent major consequences. Currently, erosion identification is manually performed by humans using huge image sets, a time-consuming and slow task. Hence, automatic machine learning methods appear as an appealing alternative. A crucial step for automatic erosion identification is to create a good feature representation. Towards such objective, deep learning can learn data-driven features and classifiers. In this paper, we propose a novel deep learning-based framework capable of performing erosion identification in railway lines. Six techniques were evaluated and the best one, Dynamic Dilated ConvNet, was integrated into this framework that was then encapsulated into a new ArcGIS plugin to facilitate its use by non-programmer users. To analyze such techniques, we also propose a new dataset, composed of almost 2000 high-resolution images.},
DOI = {10.3390/rs12040739}
}



@Article{aerospace7020019,
AUTHOR = {Mhangara, Paidamwoyo and Mapurisa, Willard and Mudau, Naledzani},
TITLE = {Image Interpretability of nSight-1 Nanosatellite Imagery for Remote Sensing Applications},
JOURNAL = {Aerospace},
VOLUME = {7},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {19},
URL = {https://www.mdpi.com/2226-4310/7/2/19},
ISSN = {2226-4310},
ABSTRACT = {Nanosatellites are increasingly being used in space-related applications to demonstrate and test scientific capability and engineering ingenuity of space-borne instruments and for educational purposes due to their favourable low manufacturing costs, cheaper launch costs, and short development time. The use of CubeSat to demonstrate earth imaging capability has also grown in the last two decades. In 2017, a South African company known as Space Commercial Services launched a low-orbit nanosatellite named nSight-1. The demonstration nanosatellite has three payloads that include a modular designed SCS Gecko imaging payload, FIPEX atmospheric science instrument developed by the University of Dresden and a Radiation mitigation VHDL coding experiment supplied by Nelson Mandela University. The Gecko imager has a swath width of 64 km and captures 30 m spatial resolution images using the red, green, and blue (RGB) spectral bands. The objective of this study was to assess the interpretability of nSight-1 in the spatial dimension using Landsat 8 as a reference and to recommend potential earth observation applications for the mission. A blind image spatial quality evaluator known as Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) was used to compute the image quality for nSight-1 and Landsat 8 imagery in the spatial domain and the National Imagery Interpretability Rating Scale (NIIRS) method to quantify the interpretability of the images. A visual interpretation was used to propose some potential applications for the nSight1 images. The results indicate that Landsat 8 OLI images had significantly higher image quality scores and NIIRS results compared to nSight-1. Landsat 8 has a mean of 19.299 for the image quality score while nSight-1 achieved a mean of 25.873. Landsat 8 had NIIRS mean of 2.345 while nSight-1 had a mean of 1.622. The superior image quality and image interpretability of Landsat could be attributed for the mature optical design on the Landsat 8 satellite that is aimed for operational purposes. Landsat 8 has a GDS of 30-m compared to 32-m on nSight-1. The image degradation resulting from the lossy compression implemented on nSight-1 from 12-bit to 8-bit also has a negative impact on image visual quality and interpretability. Whereas it is evident that Landsat 8 has the better visual quality and NIIRS scores, the results also showed that nSight-1 are still very good if one considers that the categorical ratings consider that images to be of good to excellent quality and a NIIRS mean of 1.6 indicates that the images are interpretable. Our interpretation of the imagery shows that the data has considerable potential for use in geo-visualization and cartographic land use and land cover mapping applications. The image analysis also showed the capability of the nSight-1 sensor to capture features related to structural geology, geomorphology and topography quite prominently.},
DOI = {10.3390/aerospace7020019}
}



@Article{rs12050793,
AUTHOR = {Ding, Hu and Liu, Kai and Chen, Xiaozheng and Xiong, Liyang and Tang, Guoan and Qiu, Fang and Strobl, Josef},
TITLE = {Optimized Segmentation Based on the Weighted Aggregation Method for Loess Bank Gully Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {793},
URL = {https://www.mdpi.com/2072-4292/12/5/793},
ISSN = {2072-4292},
ABSTRACT = {The Chinese Loess Plateau suffers severe gully erosion. Gully mapping is a fundamental task for gully erosion monitoring in this region. Among the different gully types in the Loess Plateau, the bank gully is usually regarded as the most important source for the generation of sediment. However, approaches for bank gully extraction are still limited. This study put forward an integrated framework, including segmentation optimization, evaluation and Extreme Gradient Boosting (XGBoost)-based classification, for the bank gully mapping of Zhifanggou catchment in the Chinese Loess Plateau. The approach was conducted using a 1-m resolution digital elevation model (DEM), based on unmanned aerial vehicle (UAV) photogrammetry and WorldView-3 imagery. The methodology first divided the study area into different watersheds. Then, segmentation by weighted aggregation (SWA) was implemented to generate multi-level segments. For achieving an optimum segmentation, area-weighted variance (WV) and Moran&rsquo;s I (MI) were adopted and calculated within each sub-watershed. After that, a new discrepancy metric, the area-number index (ANI), was developed for evaluating the segmentation results, and the results were compared with the multi-resolution segmentation (MRS) algorithm. Finally, bank gully mappings were obtained based on the XGBoost model after fine-tuning. The experiment results demonstrate that the proposed method can achieve superior segmentation compared to MRS. Moreover, the overall accuracy of the bank gully extraction results was 78.57%. The proposed approach provides a credible tool for mapping bank gullies, which could be useful for the catchment-scale gully erosion process.},
DOI = {10.3390/rs12050793}
}



@Article{rs12050797,
AUTHOR = {Lee, Yong-Suk and Lee, Sunmin and Baek, Won-Kyung and Jung, Hyung-Sup and Park, Sung-Hwan and Lee, Moung-Jin},
TITLE = {Mapping Forest Vertical Structure in Jeju Island from Optical and Radar Satellite Images Using Artificial Neural Network},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {797},
URL = {https://www.mdpi.com/2072-4292/12/5/797},
ISSN = {2072-4292},
ABSTRACT = {Recently, due to the acceleration of global warming, an accurate understanding and management of forest carbon stocks, such as forest aboveground biomass, has become very important. The vertical structure of the forest, which is the internal structure of the forest, was mainly investigated by field surveys that are labor intensive. Recently, remote sensing techniques have been actively used to explore large and inaccessible areas. In addition, machine learning techniques that could classify and analyze large amounts of data are being used in various fields. Thus, this study aims to analyze the forest vertical structure (number of tree layers) to estimate forest aboveground biomass in Jeju Island from optical and radar satellite images using artificial neural networks (ANN). For this purpose, the eight input neurons of the forest related layers, based on remote sensing data, were prepared: normalized difference vegetation index (NDVI), normalized difference water index (NDWI), NDVI texture, NDWI texture, average canopy height, standard deviation canopy height and two types of coherence maps were created using the Kompsat-3 optical image, L-band ALOS PALSAR-1 radar images, digital surface model (DSM), and digital terrain model (DTM). The forest vertical structure data, based on field surveys, was divided into the training/validation and test data and the hyper-parameters of ANN were trained using the training/validation data. The forest vertical classification result from ANN was evaluated by comparison to the test data. It showed about a 65.7% overall accuracy based on the error matrix. This result shows that the forest vertical structure map can be effectively generated from optical and radar satellite images and existing DEM and DTM using the ANN approach, especially for national scale mapping.},
DOI = {10.3390/rs12050797}
}



@Article{rs12050814,
AUTHOR = {Vilar, Pedro and Morais, Tiago G. and Rodrigues, Nuno R. and Gama, Ivo and Monteiro, Marta L. and Domingos, Tiago and Teixeira, Ricardo F. M.},
TITLE = {Object-Based Classification Approaches for Multitemporal Identification and Monitoring of Pastures in Agroforestry Regions using Multispectral Unmanned Aerial Vehicle Products},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {814},
URL = {https://www.mdpi.com/2072-4292/12/5/814},
ISSN = {2072-4292},
ABSTRACT = {Sown Biodiverse Pastures (SBP) are the basis of a high-yield grazing system tailored for Mediterranean ecosystems and widely implemented in Southern Portugal. The application of precision farming methods in SBP requires cost-effective monitoring using remote sensing (RS). The main hurdle for the remote monitoring of SBP is the fact that the bulk of the pastures are installed in open Montado agroforestry systems. Sparsely distributed trees cast shadows that hinder the identification of the underlaying pasture using Unmanned Aerial Vehicles (UAV) imagery. Image acquisition in the Spring is made difficult by the presence of flowers that mislead the classification algorithms. Here, we tested multiple procedures for the geographical, object-based image classification (GEOBIA) of SBP, aiming to reduce the effects of tree shadows and flowers in open Montado systems. We used remotely sensed data acquired between November 2017 and May 2018 in three Portuguese farms. We used three machine learning supervised classification algorithms: Random Forests (RF), Support Vector Machine (SVM) and Artificial Neural Networks (ANN). We classified SBP based on: (1) a single-period image for the maximum Normalized Difference Vegetation Index (NDVI) epoch in each of the three farms, and (2) multi-temporal image stacking. RF, SVM and ANN were trained using some visible (red, green and blue bands) and near-infrared (NIR) reflectance bands, plus NDVI and a Digital Surface Model (DSM). We obtained high overall accuracy and kappa index (higher than 79% and 0.60, respectively). The RF algorithm had the highest overall accuracy (more than 92%) for all farms. Multitemporal image classification increased the accuracy of the algorithms. as it helped to correctly identify as SBP the areas covered by tree shadows and flower patches, which would be misclassified using single image classification. This study thus established the first workflow for SBP monitoring based on remotely sensed data, suggesting an operational approach for SBP identification. The workflow can be applied to other types of pastures in agroforestry regions to reduce the effects of shadows and flowering in classification problems.},
DOI = {10.3390/rs12050814}
}



@Article{app10051759,
AUTHOR = {Guo, Han and Zhou, Jun and Liu, Fei and He, Yong and Huang, He and Wang, Hongyan},
TITLE = {Application of Machine Learning Method to Quantitatively Evaluate the Droplet Size and Deposition Distribution of the UAV Spray Nozzle},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1759},
URL = {https://www.mdpi.com/2076-3417/10/5/1759},
ISSN = {2076-3417},
ABSTRACT = {Unmanned Aerial Vehicle (UAV) spray has been used for efficient and adaptive pesticide applications with its low costs. However, droplet drift is the main problem for UAV spray and will induce pesticide waste and safety concerns. Droplet size and deposition distribution are both highly related to droplet drift and spray effect, which are determined by the nozzle. Therefore, it is necessary to propose an evaluating method for a specific UAV spray nozzles. In this paper, four machine learning methods (REGRESS, least squares support vector machines (LS-SVM), extreme learning machine, and radial basis function neural network (RBFNN)) were applied for quantitatively evaluating one type of UAV spray nozzle (TEEJET XR110015VS), and the case of twin nozzles was investigated. The results showed REGRESS and LS-SVM are good candidates for droplet size evaluation with the coefficient of determination in the calibration set above 0.9 and root means square errors of the prediction set around 2 &micro;m. RBFNN achieved the best performance for the evaluation of deposition distribution and showed its potential for determining the droplet size of overlapping area. Overall, this study proved the accuracy and efficiency of using the machine learning method for UAV spray nozzle evaluation. Additionally, the study demonstrated the feasibility of using machine learning model to predict the droplet size in the overlapping area of twin nozzles.},
DOI = {10.3390/app10051759}
}



@Article{aerospace7030023,
AUTHOR = {Communier, David and Botez, Ruxandra Mihaela and Wong, Tony},
TITLE = {Design and Validation of a New Morphing Camber System by Testing in the Price—Païdoussis Subsonic Wind Tunnel},
JOURNAL = {Aerospace},
VOLUME = {7},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {23},
URL = {https://www.mdpi.com/2226-4310/7/3/23},
ISSN = {2226-4310},
ABSTRACT = {This paper presents the design and wind tunnel testing of a morphing camber system and an estimation of performances on an unmanned aerial vehicle. The morphing camber system is a combination of two subsystems: the morphing trailing edge and the morphing leading edge. Results of the present study show that the aerodynamics effects of the two subsystems are combined, without interfering with each other on the wing. The morphing camber system acts only on the lift coefficient at a 0&deg; angle of attack when morphing the trailing edge, and only on the stall angle when morphing the leading edge. The behavior of the aerodynamics performances from the MTE and the MLE should allow individual control of the morphing camber trailing and leading edges. The estimation of the performances of the morphing camber on an unmanned aerial vehicle indicates that the morphing of the camber allows a drag reduction. This result is due to the smaller angle of attack needed for an unmanned aerial vehicle equipped with the morphing camber system than an unmanned aerial vehicle equipped with classical aileron. In the case study, the morphing camber system was found to allow a reduction of the drag when the lift coefficient was higher than 0.48.},
DOI = {10.3390/aerospace7030023}
}



@Article{su12052099,
AUTHOR = {Zhu, Xiaobo and He, Honglin and Ma, Mingguo and Ren, Xiaoli and Zhang, Li and Zhang, Fawei and Li, Yingnian and Shi, Peili and Chen, Shiping and Wang, Yanfen and Xin, Xiaoping and Ma, Yaoming and Zhang, Yu and Du, Mingyuan and Ge, Rong and Zeng, Na and Li, Pan and Niu, Zhongen and Zhang, Liyun and Lv, Yan and Song, Zengjing and Gu, Qing},
TITLE = {Estimating Ecosystem Respiration in the Grasslands of Northern China Using Machine Learning: Model Evaluation and Comparison},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {2099},
URL = {https://www.mdpi.com/2071-1050/12/5/2099},
ISSN = {2071-1050},
ABSTRACT = {While a number of machine learning (ML) models have been used to estimate RE, systematic evaluation and comparison of these models are still limited. In this study, we developed three traditional ML models and a deep learning (DL) model, stacked autoencoders (SAE), to estimate RE in northern China&rsquo;s grasslands. The four models were trained with two strategies: training for all of northern China&rsquo;s grasslands and separate training for the alpine and temperate grasslands. Our results showed that all four ML models estimated RE in northern China&rsquo;s grasslands fairly well, while the SAE model performed best (R2 = 0.858, RMSE = 0.472 gC m&minus;2 d&minus;1, MAE = 0.304 gC m&minus;2 d&minus;1). Models trained with the two strategies had almost identical performances. The enhanced vegetation index and soil organic carbon density (SOCD) were the two most important environmental variables for estimating RE in the grasslands of northern China. Air temperature (Ta) was more important than the growing season land surface water index (LSWI) in the alpine grasslands, while the LSWI was more important than Ta in the temperate grasslands. These findings may promote the application of DL models and the inclusion of SOCD for RE estimates with increased accuracy.},
DOI = {10.3390/su12052099}
}



@Article{s20061573,
AUTHOR = {Liu, Haojie and Liao, Kang and Lin, Chunyu and Zhao, Yao and Liu, Meiqin},
TITLE = {PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1573},
URL = {https://www.mdpi.com/1424-8220/20/6/1573},
ISSN = {1424-8220},
ABSTRACT = {LiDAR sensors can provide dependable 3D spatial information at a low frequency (around 10 Hz) and have been widely applied in the field of autonomous driving and unmanned aerial vehicle (UAV). However, the camera with a higher frequency (around 20 Hz) has to be decreased so as to match with LiDAR in a multi-sensor system. In this paper, we propose a novel Pseudo-LiDAR interpolation network (PLIN) to increase the frequency of LiDAR sensor data. PLIN can generate temporally and spatially high-quality point cloud sequences to match the high frequency of cameras. To achieve this goal, we design a coarse interpolation stage guided by consecutive sparse depth maps and motion relationship. We also propose a refined interpolation stage guided by the realistic scene. Using this coarse-to-fine cascade structure, our method can progressively perceive multi-modal information and generate accurate intermediate point clouds. To the best of our knowledge, this is the first deep framework for Pseudo-LiDAR point cloud interpolation, which shows appealing applications in navigation systems equipped with LiDAR and cameras. Experimental results demonstrate that PLIN achieves promising performance on the KITTI dataset, significantly outperforming the traditional interpolation method and the state-of-the-art video interpolation technique.},
DOI = {10.3390/s20061573}
}



@Article{rs12060906,
AUTHOR = {Osco, Lucas Prado and Ramos, Ana Paula Marques and Faita Pinheiro, Mayara Maezano and Moriya, Érika Akemi Saito and Imai, Nilton Nobuhiro and Estrabis, Nayara and Ianczyk, Felipe and Araújo, Fábio Fernando de and Liesenberg, Veraldo and Jorge, Lúcio André de Castro and Li, Jonathan and Ma, Lingfei and Gonçalves, Wesley Nunes and Marcato Junior, José and Eduardo Creste, José},
TITLE = {A Machine Learning Framework to Predict Nutrient Content in Valencia-Orange Leaf Hyperspectral Measurements},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {906},
URL = {https://www.mdpi.com/2072-4292/12/6/906},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a framework based on machine learning algorithms to predict nutrient content in leaf hyperspectral measurements. This is the first approach to evaluate macro- and micronutrient content with both machine learning and reflectance/first-derivative data. For this, citrus-leaves collected at a Valencia-orange orchard were used. Their spectral data was measured with a Fieldspec ASD FieldSpec&reg; HandHeld 2 spectroradiometer and the surface reflectance and first-derivative spectra from the spectral range of 380 to 1020 nm (640 spectral bands) was evaluated. A total of 320 spectral signatures were collected, and the leaf-nutrient content (N, P, K, Mg, S, Cu, Fe, Mn, and Zn) was associated with them. For this, 204,800 (320 &times; 640) combinations were used. The following machine learning algorithms were used in this framework: k-Nearest Neighbor (kNN), Lasso Regression, Ridge Regression, Support Vector Machine (SVM), Artificial Neural Network (ANN), Decision Tree (DT), and Random Forest (RF). The training methods were assessed based on Cross-Validation and Leave-One-Out. The Relief-F metric of the algorithms&rsquo; prediction was used to determine the most contributive wavelength or spectral region associated with each nutrient. This approach was able to return, with high predictions (R2), nutrients like N (0.912), Mg (0.832), Cu (0.861), Mn (0.898), and Zn (0.855), and, to a lesser extent, P (0.771), K (0.763), and S (0.727). These accuracies were obtained with different algorithms, but RF was the most suitable to model most of them. The results indicate that, for the Valencia-orange leaves, surface reflectance data is more suitable to predict macronutrients, while first-derivative spectra is better linked to micronutrients. A final contribution of this study is the identification of the wavelengths responsible for contributing to these predictions.},
DOI = {10.3390/rs12060906}
}



@Article{s20061606,
AUTHOR = {Shi, Haiyun and Li, Jie and Li, Zhi},
TITLE = {A Distributed Strategy for Cooperative Autonomous Robots Using Pedestrian Behavior for Multi-Target Search in the Unknown Environment},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1606},
URL = {https://www.mdpi.com/1424-8220/20/6/1606},
ISSN = {1424-8220},
ABSTRACT = {Searching multiple targets with swarm robots is a realistic and significant problem. The goal is to search the targets in the minimum time while avoiding collisions with other robots. In this paper, inspired by pedestrian behavior, swarm robotic pedestrian behavior (SRPB) was proposed. It considered many realistic constraints in the multi-target search problem, including limited communication range, limited working time, unknown sources, unknown extrema, the arbitrary initial location of robots, non-oriented search, and no central coordination. The performance of different cooperative strategies was evaluated in terms of average time to find the first, the half, and the last source, the number of located sources and the collision rate. Several experiments with different target signals, fixed initial location, arbitrary initial location, different population sizes, and the different number of targets were implemented. It was demonstrated by numerous experiments that SRPB had excellent stability, quick source seeking, a high number of located sources, and a low collision rate in various search strategies.},
DOI = {10.3390/s20061606}
}



@Article{rs12060959,
AUTHOR = {Pashaei, Mohammad and Kamangir, Hamid and Starek, Michael J. and Tissot, Philippe},
TITLE = {Review and Evaluation of Deep Learning Architectures for Efficient Land Cover Mapping with UAS Hyper-Spatial Imagery: A Case Study Over a Wetland},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {959},
URL = {https://www.mdpi.com/2072-4292/12/6/959},
ISSN = {2072-4292},
ABSTRACT = {Deep learning has already been proved as a powerful state-of-the-art technique for many image understanding tasks in computer vision and other applications including remote sensing (RS) image analysis. Unmanned aircraft systems (UASs) offer a viable and economical alternative to a conventional sensor and platform for acquiring high spatial and high temporal resolution data with high operational flexibility. Coastal wetlands are among some of the most challenging and complex ecosystems for land cover prediction and mapping tasks because land cover targets often show high intra-class and low inter-class variances. In recent years, several deep convolutional neural network (CNN) architectures have been proposed for pixel-wise image labeling, commonly called semantic image segmentation. In this paper, some of the more recent deep CNN architectures proposed for semantic image segmentation are reviewed, and each model&rsquo;s training efficiency and classification performance are evaluated by training it on a limited labeled image set. Training samples are provided using the hyper-spatial resolution UAS imagery over a wetland area and the required ground truth images are prepared by manual image labeling. Experimental results demonstrate that deep CNNs have a great potential for accurate land cover prediction task using UAS hyper-spatial resolution images. Some simple deep learning architectures perform comparable or even better than complex and very deep architectures with remarkably fewer training epochs. This performance is especially valuable when limited training samples are available, which is a common case in most RS applications.},
DOI = {10.3390/rs12060959}
}



@Article{rs12060962,
AUTHOR = {Liu, Changyu and Huang, Xiaodong and Li, Xubing and Liang, Tiangang},
TITLE = {MODIS Fractional Snow Cover Mapping Using Machine Learning Technology in a Mountainous Area},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {962},
URL = {https://www.mdpi.com/2072-4292/12/6/962},
ISSN = {2072-4292},
ABSTRACT = {To improve the poor accuracy of the MODIS (Moderate Resolution Imaging Spectroradiometer) daily fractional snow cover product over the complex terrain of the Tibetan Plateau (RMSE = 0.30), unmanned aerial vehicle and machine learning technologies are employed to map the fractional snow cover based on MODIS over this terrain. Three machine learning models, including random forest, support vector machine, and back-propagation artificial neural network models, are trained and compared in this study. The results indicate that compared with the MODIS daily fractional snow cover product, the introduction of a highly accurate snow map acquired by unmanned aerial vehicles as a reference into machine learning models can significantly improve the MODIS fractional snow cover mapping accuracy. The random forest model shows the best accuracy among the three machine learning models, with an RMSE (root-mean-square error) of 0.23, especially over forestland and shrubland, with RMSEs of 0.13 and 0.18, respectively. Although the accuracy of the support vector machine and back-propagation artificial neural network models are worse over forestland and shrubland, their average errors are still better than that of MOD10A1. Different fractional snow cover gradients also affect the accuracy of the machine learning algorithms. Nevertheless, the random forest model remains stable in different fractional snow cover gradients and is, therefore, the best machine learning algorithm for MODIS fractional snow cover mapping in Tibetan Plateau areas with complex terrain and severely fragmented snow cover.},
DOI = {10.3390/rs12060962}
}



@Article{plants9030368,
AUTHOR = {Sonobe, Rei and Hirono, Yuhei and Oi, Ayako},
TITLE = {Non-Destructive Detection of Tea Leaf Chlorophyll Content Using Hyperspectral Reflectance and Machine Learning Algorithms},
JOURNAL = {Plants},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {368},
URL = {https://www.mdpi.com/2223-7747/9/3/368},
PubMedID = {32192044},
ISSN = {2223-7747},
ABSTRACT = {Tea trees are kept in shaded locations to increase their chlorophyll content, which influences green tea quality. Therefore, monitoring change in chlorophyll content under low light conditions is important for managing tea trees and producing high-quality green tea. Hyperspectral remote sensing is one of the most frequently used methods for estimating chlorophyll content. Numerous studies based on data collected under relatively low-stress conditions and many hyperspectral indices and radiative transfer models show that shade-grown tea performs poorly. The performance of four machine learning algorithms&mdash;random forest, support vector machine, deep belief nets, and kernel-based extreme learning machine (KELM)&mdash;in evaluating data collected from tea leaves cultivated under different shade treatments was tested. KELM performed best with a root-mean-square error of 8.94 &plusmn; 3.05 &mu;g cm&minus;2 and performance to deviation values from 1.70 to 8.04 for the test data. These results suggest that a combination of hyperspectral reflectance and KELM has the potential to trace changes in the chlorophyll content of shaded tea leaves.},
DOI = {10.3390/plants9030368}
}



@Article{s20061708,
AUTHOR = {Martin-Abadal, Miguel and Ruiz-Frau, Ana and Hinz, Hilmar and Gonzalez-Cid, Yolanda},
TITLE = {Jellytoring: Real-Time Jellyfish Monitoring Based on Deep Learning Object Detection},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1708},
URL = {https://www.mdpi.com/1424-8220/20/6/1708},
ISSN = {1424-8220},
ABSTRACT = {During the past decades, the composition and distribution of marine species have changed due to multiple anthropogenic pressures. Monitoring these changes in a cost-effective manner is of high relevance to assess the environmental status and evaluate the effectiveness of management measures. In particular, recent studies point to a rise of jellyfish populations on a global scale, negatively affecting diverse marine sectors like commercial fishing or the tourism industry. Past monitoring efforts using underwater video observations tended to be time-consuming and costly due to human-based data processing. In this paper, we present Jellytoring, a system to automatically detect and quantify different species of jellyfish based on a deep object detection neural network, allowing us to automatically record jellyfish presence during long periods of time. Jellytoring demonstrates outstanding performance on the jellyfish detection task, reaching an F1 score of 95.2%; and also on the jellyfish quantification task, as it correctly quantifies the number and class of jellyfish on a real-time processed video sequence up to a 93.8% of its duration. The results of this study are encouraging and provide the means towards a efficient way to monitor jellyfish, which can be used for the development of a jellyfish early-warning system, providing highly valuable information for marine biologists and contributing to the reduction of jellyfish impacts on humans.},
DOI = {10.3390/s20061708}
}



@Article{electronics9030513,
AUTHOR = {Shi, Wenlei and Li, Zerui and Lv, Wenjun and Wu, Yuping and Chang, Ji and Li, Xiaochuan},
TITLE = {Laplacian Support Vector Machine for Vibration-Based Robotic Terrain Classification},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {513},
URL = {https://www.mdpi.com/2079-9292/9/3/513},
ISSN = {2079-9292},
ABSTRACT = {The achievement of robot autonomy has environmental perception as a prerequisite. The hazards rendered from uneven, soft and slippery terrains, which are generally named non-geometric hazards, are another potential threat reducing the traversing efficient, and therefore receiving more and more attention from the robotics community. In the paper, the vibration-based terrain classification (VTC) is investigated by taking a very practical issue, i.e., lack of labels, into consideration. According to the intrinsic temporal correlation existing in the sampled terrain sequence, a modified Laplacian SVM is proposed to utilise the unlabelled data to improve the classification performance. To the best of our knowledge, this is the first paper studying semi-supervised learning problem in robotic terrain classification. The experiment demonstrates that: (1) supervised learning (SVM) achieves a relatively low classification accuracy if given insufficient labels; (2) feature-space homogeneity based semi-supervised learning (traditional Laplacian SVM) cannot improve supervised learning&rsquo;s accuracy, and even makes it worse; (3) feature- and temporal-space based semi-supervised learning (modified Laplacian SVM), which is proposed in the paper, could increase the classification accuracy very significantly.},
DOI = {10.3390/electronics9030513}
}



@Article{app10062104,
AUTHOR = {Tomaszewski, Michał and Michalski, Paweł and Osuchowski, Jakub},
TITLE = {Evaluation of Power Insulator Detection Efficiency with the Use of Limited Training Dataset},
JOURNAL = {Applied Sciences},
VOLUME = {10},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {2104},
URL = {https://www.mdpi.com/2076-3417/10/6/2104},
ISSN = {2076-3417},
ABSTRACT = {This article presents an analysis of the effectiveness of object detection in digital images with the application of a limited quantity of input. The possibility of using a limited set of learning data was achieved by developing a detailed scenario of the task, which strictly defined the conditions of detector operation in the considered case of a convolutional neural network. The described solution utilizes known architectures of deep neural networks in the process of learning and object detection. The article presents comparisons of results from detecting the most popular deep neural networks while maintaining a limited training set composed of a specific number of selected images from diagnostic video. The analyzed input material was recorded during an inspection flight conducted along high-voltage lines. The object detector was built for a power insulator. The main contribution of the presented papier is the evidence that a limited training set (in our case, just 60 training frames) could be used for object detection, assuming an outdoor scenario with low variability of environmental conditions. The decision of which network will generate the best result for such a limited training set is not a trivial task. Conducted research suggests that the deep neural networks will achieve different levels of effectiveness depending on the amount of training data. The most beneficial results were obtained for two convolutional neural networks: the faster region-convolutional neural network (faster R-CNN) and the region-based fully convolutional network (R-FCN). Faster R-CNN reached the highest AP (average precision) at a level of 0.8 for 60 frames. The R-FCN model gained a worse AP result; however, it can be noted that the relationship between the number of input samples and the obtained results has a significantly lower influence than in the case of other CNN models, which, in the authors&rsquo; assessment, is a desired feature in the case of a limited training set.},
DOI = {10.3390/app10062104}
}



@Article{rs12061001,
AUTHOR = {Tmušić, Goran and Manfreda, Salvatore and Aasen, Helge and James, Mike R. and Gonçalves, Gil and Ben-Dor, Eyal and Brook, Anna and Polinova, Maria and Arranz, Jose Juan and Mészáros, János and Zhuang, Ruodan and Johansen, Kasper and Malbeteau, Yoann and de Lima, Isabel Pedroso and Davids, Corine and Herban, Sorin and McCabe, Matthew F.},
TITLE = {Current Practices in UAS-based Environmental Monitoring},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1001},
URL = {https://www.mdpi.com/2072-4292/12/6/1001},
ISSN = {2072-4292},
ABSTRACT = {With the increasing role that unmanned aerial systems (UAS) are playing in data collection for environmental studies, two key challenges relate to harmonizing and providing standardized guidance for data collection, and also establishing protocols that are applicable across a broad range of environments and conditions. In this context, a network of scientists are cooperating within the framework of the Harmonious Project to develop and promote harmonized mapping strategies and disseminate operational guidance to ensure best practice for data collection and interpretation. The culmination of these efforts is summarized in the present manuscript. Through this synthesis study, we identify the many interdependencies of each step in the collection and processing chain, and outline approaches to formalize and ensure a successful workflow and product development. Given the number of environmental conditions, constraints, and variables that could possibly be explored from UAS platforms, it is impractical to provide protocols that can be applied universally under all scenarios. However, it is possible to collate and systematically order the fragmented knowledge on UAS collection and analysis to identify the best practices that can best ensure the streamlined and rigorous development of scientific products.},
DOI = {10.3390/rs12061001}
}



@Article{a13030069,
AUTHOR = {Xu, Jin and Wang, Haixia and Cui, Can and Zhao, Baigang and Li, Bo},
TITLE = {Oil Spill Monitoring of Shipborne Radar Image Features Using SVM and Local Adaptive Threshold},
JOURNAL = {Algorithms},
VOLUME = {13},
YEAR = {2020},
NUMBER = {3},
ARTICLE-NUMBER = {69},
URL = {https://www.mdpi.com/1999-4893/13/3/69},
ISSN = {1999-4893},
ABSTRACT = {In the case of marine accidents, monitoring marine oil spills can provide an important basis for identifying liabilities and assessing the damage. Shipborne radar can ensure large-scale, real-time monitoring, in all weather, with high-resolution. It therefore has the potential for broad applications in oil spill monitoring. Considering the original gray-scale image from the shipborne radar acquired in the case of the Dalian 7.16 oil spill accident, a complete oil spill detection method is proposed. Firstly, the co-frequency interferences and speckles in the original image are eliminated by preprocessing. Secondly, the wave information is classified using a support vector machine (SVM), and the effective wave monitoring area is generated according to the gray distribution matrix. Finally, oil spills are detected by a local adaptive threshold and displayed on an electronic chart based on geographic information system (GIS). The results show that the SVM can extract the effective wave information from the original shipborne radar image, and the local adaptive threshold method has strong applicability for oil film segmentation. This method can provide a technical basis for real-time cleaning and liability determination in oil spill accidents.},
DOI = {10.3390/a13030069}
}



@Article{su12062482,
AUTHOR = {Nguyen, Truong Linh and Han, DongYeob},
TITLE = {Detection of Road Surface Changes from Multi-Temporal Unmanned Aerial Vehicle Images Using a Convolutional Siamese Network},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {2482},
URL = {https://www.mdpi.com/2071-1050/12/6/2482},
ISSN = {2071-1050},
ABSTRACT = {Road quality commonly decreases due to aging and deterioration of road surfaces. As the number of roads that need to be surveyed increases, general maintenance&mdash;particularly surveillance&mdash;can be quite costly if carried out using traditional methods. Therefore, using unmanned aerial vehicles (UAVs) and deep learning to detect changes via surveys is a promising strategy. This study proposes a method for detecting changes on road surfaces using pairs of UAV images captured at different times. First, a convolutional Siamese network is introduced to extract the features of an image pair and a Euclidean distance function is applied to calculate the distance between two features. Then, a contrastive loss function is used to enlarge the distance between changed feature pairs and reduce the distance between unchanged feature pairs. Finally, the initial change map is improved based on the preliminary differences between the two input images. Our experimental results confirm the effectiveness of this approach.},
DOI = {10.3390/su12062482}
}



@Article{rs12061034,
AUTHOR = {Elmes, Arthur and Alemohammad, Hamed and Avery, Ryan and Caylor, Kelly and Eastman, J. Ronald and Fishgold, Lewis and Friedl, Mark A. and Jain, Meha and Kohli, Divyani and Laso Bayas, Juan Carlos and Lunga, Dalton and McCarty, Jessica L. and Pontius, Robert Gilmore and Reinmann, Andrew B. and Rogan, John and Song, Lei and Stoynova, Hristiana and Ye, Su and Yi, Zhuang-Fang and Estes, Lyndon},
TITLE = {Accounting for Training Data Error in Machine Learning Applied to Earth Observations},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1034},
URL = {https://www.mdpi.com/2072-4292/12/6/1034},
ISSN = {2072-4292},
ABSTRACT = {Remote sensing, or Earth Observation (EO), is increasingly used to understand Earth system dynamics and create continuous and categorical maps of biophysical properties and land cover, especially based on recent advances in machine learning (ML). ML models typically require large, spatially explicit training datasets to make accurate predictions. Training data (TD) are typically generated by digitizing polygons on high spatial-resolution imagery, by collecting in situ data, or by using pre-existing datasets. TD are often assumed to accurately represent the truth, but in practice almost always have error, stemming from (1) sample design, and (2) sample collection errors. The latter is particularly relevant for image-interpreted TD, an increasingly commonly used method due to its practicality and the increasing training sample size requirements of modern ML algorithms. TD errors can cause substantial errors in the maps created using ML algorithms, which may impact map use and interpretation. Despite these potential errors and their real-world consequences for map-based decisions, TD error is often not accounted for or reported in EO research. Here we review the current practices for collecting and handling TD. We identify the sources of TD error, and illustrate their impacts using several case studies representing different EO applications (infrastructure mapping, global surface flux estimates, and agricultural monitoring), and provide guidelines for minimizing and accounting for TD errors. To harmonize terminology, we distinguish TD from three other classes of data that should be used to create and assess ML models: training reference data, used to assess the quality of TD during data generation; validation data, used to iteratively improve models; and map reference data, used only for final accuracy assessment. We focus primarily on TD, but our advice is generally applicable to all four classes, and we ground our review in established best practices for map accuracy assessment literature. EO researchers should start by determining the tolerable levels of map error and appropriate error metrics. Next, TD error should be minimized during sample design by choosing a representative spatio-temporal collection strategy, by using spatially and temporally relevant imagery and ancillary data sources during TD creation, and by selecting a set of legend definitions supported by the data. Furthermore, TD error can be minimized during the collection of individual samples by using consensus-based collection strategies, by directly comparing interpreted training observations against expert-generated training reference data to derive TD error metrics, and by providing image interpreters with thorough application-specific training. We strongly advise that TD error is incorporated in model outputs, either directly in bias and variance estimates or, at a minimum, by documenting the sources and implications of error. TD should be fully documented and made available via an open TD repository, allowing others to replicate and assess its use. To guide researchers in this process, we propose three tiers of TD error accounting standards. Finally, we advise researchers to clearly communicate the magnitude and impacts of TD error on map outputs, with specific consideration given to the likely map audience.},
DOI = {10.3390/rs12061034}
}



@Article{rs12061039,
AUTHOR = {Godone, Danilo and Allasia, Paolo and Borrelli, Luigi and Gullà, Giovanni},
TITLE = {UAV and Structure from Motion Approach to Monitor the Maierato Landslide Evolution},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {6},
ARTICLE-NUMBER = {1039},
URL = {https://www.mdpi.com/2072-4292/12/6/1039},
ISSN = {2072-4292},
ABSTRACT = {In February 2010 a large landslide affected the Maierato municipality (Calabria, Italy). The landslide, mainly caused by a period of prolonged and intense rainfalls, produced a mass displacement of about 5 million m&sup3; and several damages to farmlands, houses and infrastructures. In the aftermath several conventional monitoring actions were carried out. In the current post emergency phase, the monitoring was resumed by carrying out unmanned aerial vehicles (UAV) flights in order to describe the recent behavior of the landslide and to assess residual risk. Thanks to the potentialities of the structure from motion algorithms and the availability of post emergency reconnaissance photos and a previous 3D dataset, the three-dimensional evolution of the area was computed. Moreover, an experimental multispectral flight was carried out and its results supported the interpretation of local phenomena. The dataset allowed to quantify the elevation losses and raises in several peculiar sectors of the landslide. The obtained results confirm that the UAV monitoring and the structure from motion approach can effectively contribute to manage residual risk in the medium and long term within an integrated geotechnical monitoring network.},
DOI = {10.3390/rs12061039}
}



@Article{rs12071081,
AUTHOR = {Gibril, Mohamed Barakat A. and Kalantar, Bahareh and Al-Ruzouq, Rami and Ueda, Naonori and Saeidi, Vahideh and Shanableh, Abdallah and Mansor, Shattri and Shafri, Helmi Z. M.},
TITLE = {Mapping Heterogeneous Urban Landscapes from the Fusion of Digital Surface Model and Unmanned Aerial Vehicle-Based Images Using Adaptive Multiscale Image Segmentation and Classification},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1081},
URL = {https://www.mdpi.com/2072-4292/12/7/1081},
ISSN = {2072-4292},
ABSTRACT = {Considering the high-level details in an ultrahigh-spatial-resolution (UHSR) unmanned aerial vehicle (UAV) dataset, detailed mapping of heterogeneous urban landscapes is extremely challenging because of the spectral similarity between classes. In this study, adaptive hierarchical image segmentation optimization, multilevel feature selection, and multiscale (MS) supervised machine learning (ML) models were integrated to accurately generate detailed maps for heterogeneous urban areas from the fusion of the UHSR orthomosaic and digital surface model (DSM). The integrated approach commenced through a preliminary MS image segmentation parameter selection, followed by the application of three supervised ML models, namely, random forest (RF), support vector machine (SVM), and decision tree (DT). These models were implemented at the optimal MS levels to identify preliminary information, such as the optimal segmentation level(s) and relevant features, for extracting 12 land use/land cover (LULC) urban classes from the fused datasets. Using the information obtained from the first phase of the analysis, detailed MS classification was iteratively conducted to improve the classification accuracy and derive the final urban LULC maps. Two UAV-based datasets were used to develop and assess the effectiveness of the proposed framework. The hierarchical classification of the pilot study area showed that the RF was superior with an overall accuracy (OA) of 94.40% and a kappa coefficient (K) of 0.938, followed by SVM (OA = 92.50% and K = 0.917) and DT (OA = 91.60% and K = 0.908). The classification results of the second dataset revealed that SVM was superior with an OA of 94.45% and K of 0.938, followed by RF (OA = 92.46% and K = 0.916) and DT (OA = 90.46% and K = 0.893). The proposed framework exhibited an excellent potential for the detailed mapping of heterogeneous urban landscapes from the fusion of UHSR orthophoto and DSM images using various ML models.},
DOI = {10.3390/rs12071081}
}



@Article{rs12071085,
AUTHOR = {Zhang, Weixing and Liljedahl, Anna K. and Kanevskiy, Mikhail and Epstein, Howard E. and Jones, Benjamin M. and Jorgenson, M. Torre and Kent, Kelcy},
TITLE = {Transferability of the Deep Learning Mask R-CNN Model for Automated Mapping of Ice-Wedge Polygons in High-Resolution Satellite and UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1085},
URL = {https://www.mdpi.com/2072-4292/12/7/1085},
ISSN = {2072-4292},
ABSTRACT = {State-of-the-art deep learning technology has been successfully applied to relatively small selected areas of very high spatial resolution (0.15 and 0.25 m) optical aerial imagery acquired by a fixed-wing aircraft to automatically characterize ice-wedge polygons (IWPs) in the Arctic tundra. However, any mapping of IWPs at regional to continental scales requires images acquired on different sensor platforms (particularly satellite) and a refined understanding of the performance stability of the method across sensor platforms through reliable evaluation assessments. In this study, we examined the transferability of a deep learning Mask Region-Based Convolutional Neural Network (R-CNN) model for mapping IWPs in satellite remote sensing imagery (~0.5 m) covering 272 km2 and unmanned aerial vehicle (UAV) (0.02 m) imagery covering 0.32 km2. Multi-spectral images were obtained from the WorldView-2 satellite sensor and pan-sharpened to ~0.5 m, and a 20 mp CMOS sensor camera onboard a UAV, respectively. The training dataset included 25,489 and 6022 manually delineated IWPs from satellite and fixed-wing aircraft aerial imagery near the Arctic Coastal Plain, northern Alaska. Quantitative assessments showed that individual IWPs were correctly detected at up to 72% and 70%, and delineated at up to 73% and 68% F1 score accuracy levels for satellite and UAV images, respectively. Expert-based qualitative assessments showed that IWPs were correctly detected at good (40&ndash;60%) and excellent (80&ndash;100%) accuracy levels for satellite and UAV images, respectively, and delineated at excellent (80&ndash;100%) level for both images. We found that (1) regardless of spatial resolution and spectral bands, the deep learning Mask R-CNN model effectively mapped IWPs in both remote sensing satellite and UAV images; (2) the model achieved a better accuracy in detection with finer image resolution, such as UAV imagery, yet a better accuracy in delineation with coarser image resolution, such as satellite imagery; (3) increasing the number of training data with different resolutions between the training and actual application imagery does not necessarily result in better performance of the Mask R-CNN in IWPs mapping; (4) and overall, the model underestimates the total number of IWPs particularly in terms of disjoint/incomplete IWPs.},
DOI = {10.3390/rs12071085}
}



@Article{rs12071088,
AUTHOR = {Bao, Hanqing and Ming, Dongping and Guo, Ya and Zhang, Kui and Zhou, Keqi and Du, Shigao},
TITLE = {DFCNN-Based Semantic Recognition of Urban Functional Zones by Integrating Remote Sensing Data and POI Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1088},
URL = {https://www.mdpi.com/2072-4292/12/7/1088},
ISSN = {2072-4292},
ABSTRACT = {The urban functional zone, as a special fundamental unit of the city, helps to understand the complex interaction between human space activities and environmental changes. Based on the recognition of physical and social semantics of buildings, combining remote sensing data and social sensing data is an effective way to quickly and accurately comprehend urban functional zone patterns. From the object level, this paper proposes a novel object-wise recognition strategy based on very high spatial resolution images (VHSRI) and social sensing data. First, buildings are extracted according to the physical semantics of objects; second, remote sensing and point of interest (POI) data are combined to comprehend the spatial distribution and functional semantics in the social function context; finally, urban functional zones are recognized and determined by building with physical and social functional semantics. When it comes to building geometrical information extraction, this paper, given the importance of building boundary information, introduces the deeper edge feature map (DEFM) into the segmentation and classification, and improves the result of building boundary recognition. Given the difficulty in understanding deeper semantics and spatial information and the limitation of traditional convolutional neural network (CNN) models in feature extraction, we propose the Deeper-Feature Convolutional Neural Network (DFCNN), which is able to extract more and deeper features for building semantic recognition. Experimental results conducted on a Google Earth image of Shenzhen City show that the proposed method and model are able to effectively, quickly, and accurately recognize urban functional zones by combining building physical semantics and social functional semantics, and are able to ensure the accuracy of urban functional zone recognition.},
DOI = {10.3390/rs12071088}
}



@Article{rs12071099,
AUTHOR = {Song, Ahram and Kim, Yongil},
TITLE = {Transfer Change Rules from Recurrent Fully Convolutional Networks for Hyperspectral Unmanned Aerial Vehicle Images without Ground Truth Data},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1099},
URL = {https://www.mdpi.com/2072-4292/12/7/1099},
ISSN = {2072-4292},
ABSTRACT = {Change detection (CD) networks based on supervised learning have been used in diverse CD tasks. However, such supervised CD networks require a large amount of data and only use information from current images. In addition, it is time consuming to manually acquire the ground truth data for newly obtained images. Here, we proposed a novel method for CD in case of a lack of training data in an area near by another one with the available ground truth data. The proposed method automatically entails generating training data and fine-tuning the CD network. To detect changes in target images without ground truth data, the difference images were generated using spectral similarity measure, and the training data were selected via fuzzy c-means clustering. Recurrent fully convolutional networks with multiscale three-dimensional filters were used to extract objects of various sizes from unmanned aerial vehicle (UAV) images. The CD network was pre-trained on labeled source domain data; then, the network was fine-tuned on target images using generated training data. Two further CD networks were trained with a combined weighted loss function. The training data in the target domain were iteratively updated using he prediction map of the CD network. Experiments on two hyperspectral UAV datasets confirmed that the proposed method is capable of transferring change rules and improving CD results based on training data extracted in an unsupervised way.},
DOI = {10.3390/rs12071099}
}



@Article{rs12071116,
AUTHOR = {Yuzugullu, Onur and Lorenz, Frank and Fröhlich, Peter and Liebisch, Frank},
TITLE = {Understanding Fields by Remote Sensing: Soil Zoning and Property Mapping},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {7},
ARTICLE-NUMBER = {1116},
URL = {https://www.mdpi.com/2072-4292/12/7/1116},
ISSN = {2072-4292},
ABSTRACT = {Precision agriculture aims to optimize field management to increase agronomic yield, reduce environmental impact, and potentially foster soil carbon sequestration. In 2015, the Copernicus mission, with Sentinel-1 and -2, opened a new era by providing freely available high spatial and temporal resolution satellite data. Since then, many studies have been conducted to understand, monitor and improve agricultural systems. This paper presents results from the SolumScire project, focusing on the prediction of the spatial distribution of soil zones and topsoil properties, such as pH, soil organic matter (SOM) and clay content in agricultural fields through random forest algorithms. For this purpose, samples from 120 fields were investigated. The zoning and soil property prediction has an accuracy greater than 90%. This is supported by a high agreement of the derived zones with farmer&rsquo;s observations. The trained models revealed a prediction accuracy of 94%, 89% and 96% for pH, SOM and clay content, respectively. The obtained models for soil properties can support precision field management, the improvement of soil sampling and fertilization strategies, and eventually the management of soil properties such as SOM.},
DOI = {10.3390/rs12071116}
}



