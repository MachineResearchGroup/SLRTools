@article{10.1145/3472288,
author = {Kakaletsis, Efstratios and Symeonidis, Charalampos and Tzelepi, Maria and Mademlis, Ioannis and Tefas, Anastasios and Nikolaidis, Nikos and Pitas, Ioannis},
title = {Computer Vision for Autonomous UAV Flight Safety: An Overview and a Vision-Based Safe Landing Pipeline Example},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3472288},
doi = {10.1145/3472288},
abstract = {Recent years have seen an unprecedented spread of Unmanned Aerial Vehicles (UAVs, or “drones”), which are highly useful for both civilian and military applications. Flight safety is a crucial issue in UAV navigation, having to ensure accurate compliance with recently legislated rules and regulations. The emerging use of autonomous drones and UAV swarms raises additional issues, making it necessary to transfuse safety- and regulations-awareness to relevant algorithms and architectures. Computer vision plays a pivotal role in such autonomous functionalities. Although the main aspects of autonomous UAV technologies (e.g., path planning, navigation control, landing control, mapping and localization, target detection/tracking) are already mature and well-covered, ensuring safe flying in the vicinity of crowds, avoidance of passing over persons, or guaranteed emergency landing capabilities in case of malfunctions, are generally treated as an afterthought when designing autonomous UAV platforms for unstructured environments. This fact is reflected in the fragmentary coverage of the above issues in current literature. This overview attempts to remedy this situation, from the point of view of computer vision. It examines the field from multiple aspects, including regulations across the world and relevant current technologies. Finally, since very few attempts have been made so far towards a complete UAV safety flight and landing pipeline, an example computer vision-based UAV flight safety pipeline is introduced, taking into account all issues present in current autonomous drones. The content is relevant to any kind of autonomous drone flight (e.g., for movie/TV production, news-gathering, search and rescue, surveillance, inspection, mapping, wildlife monitoring, crowd monitoring/management), making this a topic of broad interest.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {181},
numpages = {37},
keywords = {semantic mapping, path planning, landing site detection, UAV flight Safety, obstacle avoidance, crowd/person detection}
}

@article{10.1145/3347713,
author = {Mademlis, Ioannis and Nikolaidis, Nikos and Tefas, Anastasios and Pitas, Ioannis and Wagner, Tilman and Messina, Alberto},
title = {Autonomous UAV Cinematography: A Tutorial and a Formalized Shot-Type Taxonomy},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3347713},
doi = {10.1145/3347713},
abstract = {The emerging field of autonomous UAV cinematography is examined through a tutorial for non-experts, which also presents the required underlying technologies and connections with different UAV application domains. Current industry practices are formalized by presenting a UAV shot-type taxonomy composed of framing shot types, single-UAV camera motion types, and multiple-UAV camera motion types. Visually pleasing combinations of framing shot types and camera motion types are identified, while the presented camera motion types are modeled geometrically and graded into distinct energy consumption classes and required technology complexity levels for autonomous capture. Two specific strategies are prescribed, namely focal length compensation and multidrone compensation, for partially overcoming a number of issues arising in UAV live outdoor event coverage, deemed as the most complex UAV cinematography scenario. Finally, the shot types compatible with each compensation strategy are explicitly identified. Overall, this tutorial both familiarizes readers coming from different backgrounds with the topic in a structured manner and lays necessary groundwork for future advancements.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {105},
numpages = {33},
keywords = {autonomous drones, UAV cinematography, intelligent shooting, UAV shot types}
}

@inproceedings{10.1145/2001858.2002047,
author = {Junges, Robert and Kl\"{u}gl, Franziska},
title = {Evolution for Modeling: A Genetic Programming Framework for Sesam},
year = {2011},
isbn = {9781450306904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2001858.2002047},
doi = {10.1145/2001858.2002047},
abstract = {Developing a valid agent-based simulation model is not always straight forward, but involves a lot of prototyping, testing and analyzing until the right low-level behavior is fully specified and calibrated. Our aim is to replace the try and error search of a modeler by adaptive agents which learn a behavior that then can serve as a source of inspiration for the modeler. In this contribution, we suggest to use genetic programming as the learning mechanism. For this aim we developed a genetic programming framework integrated into the visual agent-based modeling and simulation tool SeSAm, providing similar easy-to-use functionality.},
booktitle = {Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {551–558},
numpages = {8},
keywords = {multiagent simulation},
location = {Dublin, Ireland},
series = {GECCO '11}
}

@inproceedings{10.1109/DS-RT.2015.26,
author = {Kureshi, Ibad and Theodoropoulos, Georgios and Mangina, Eleni and O'Hare, Gregory and Roche, John},
title = {Towards An Info-Symbiotic Decision Support System for Disaster Risk Management},
year = {2015},
isbn = {9781467378222},
publisher = {IEEE Press},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1109/DS-RT.2015.26},
doi = {10.1109/DS-RT.2015.26},
abstract = {This paper outlines a framework for an infosymbiotic modelling system using cyber-physical sensors to assist in decision-making. Using a dynamic data-driven simulation approach, this system can help with the identification of target areas and resource allocation in emergency situations. Using different natural disasters as exemplars, we will show how cyber-physical sensors can enhance ground level intelligence and aid in the creation of dynamic models to capture the state of human casualties. Using a virtual command &amp; control centre communicating with sensors in the field, up-to-date information of the ground realities can be incorporated in a dynamic feedback loop. Using other information (e.g. weather models) a complex and rich model can be created. The framework adaptively manages the heterogeneous collection of data resources and uses agent-based models to create what-if scenarios in order to determine the best course of action.},
booktitle = {Proceedings of the 19th International Symposium on Distributed Simulation and Real Time Applications},
pages = {85–91},
numpages = {7},
keywords = {cyber physical systems, disaster management, dynamic data driven applications, agent based models},
location = {Chengdu, Sichuan, China},
series = {DS-RT 2015}
}

