
@Article{rs61212037,
AUTHOR = {Hung, Calvin and Xu, Zhe and Sukkarieh, Salah},
TITLE = {Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV},
JOURNAL = {Remote Sensing},
VOLUME = {6},
YEAR = {2014},
NUMBER = {12},
PAGES = {12037--12054},
URL = {https://www.mdpi.com/2072-4292/6/12/12037},
ISSN = {2072-4292},
ABSTRACT = {The development of low-cost unmanned aerial vehicles (UAVs) and light weight imaging sensors has resulted in significant interest in their use for remote sensing applications. While significant attention has been paid to the collection, calibration, registration and mosaicking of data collected from small UAVs, the interpretation of these data into semantically meaningful information can still be a laborious task. A standard data collection and classification work-flow requires significant manual effort for segment size tuning, feature selection and rule-based classifier design. In this paper, we propose an alternative learning-based approach using feature learning to minimise the manual effort required. We apply this system to the classification of invasive weed species. Small UAVs are suited to this application, as they can collect data at high spatial resolutions, which is essential for the classification of small or localised weed outbreaks. In this paper, we apply feature learning to generate a bank of image filters that allows for the extraction of features that discriminate between the weeds of interest and background objects. These features are pooled to summarise the image statistics and form the input to a texton-based linear classifier that classifies an image patch as weed or background. We evaluated our approach to weed classification on three weeds of significance in Australia: water hyacinth, tropical soda apple and serrated tussock. Our results showed that collecting images at 5–10 m resulted in the highest classifier accuracy, indicated by F1 scores of up to 94%.},
DOI = {10.3390/rs61212037}
}



@Article{s150923805,
AUTHOR = {Gökçe, Fatih and Üçoluk, Göktürk and Şahin, Erol and Kalkan, Sinan},
TITLE = {Vision-Based Detection and Distance Estimation of Micro Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {15},
YEAR = {2015},
NUMBER = {9},
PAGES = {23805--23846},
URL = {https://www.mdpi.com/1424-8220/15/9/23805},
PubMedID = {26393599},
ISSN = {1424-8220},
ABSTRACT = {Detection and distance estimation of micro unmanned aerial vehicles (mUAVs) is crucial for (i) the detection of intruder mUAVs in protected environments; (ii) sense and avoid purposes on mUAVs or on other aerial vehicles and (iii) multi-mUAV control scenarios, such as environmental monitoring, surveillance and exploration. In this article, we evaluate vision algorithms as alternatives for detection and distance estimation of mUAVs, since other sensing modalities entail certain limitations on the environment or on the distance. For this purpose, we test Haar-like features, histogram of gradients (HOG) and local binary patterns (LBP) using cascades of boosted classifiers. Cascaded boosted classifiers allow fast processing by performing detection tests at multiple stages, where only candidates passing earlier simple stages are processed at the preceding more complex stages. We also integrate a distance estimation method with our system utilizing geometric cues with support vector regressors. We evaluated each method on indoor and outdoor videos that are collected in a systematic way and also on videos having motion blur. Our experiments show that, using boosted cascaded classifiers with LBP, near real-time detection and distance estimation of mUAVs are possible in about 60 ms indoors (1032 × 778 resolution) and 150 ms outdoors (1280 × 720 resolution) per frame, with a detection rate of 0.96 F-score. However, the cascaded classifiers using Haar-like features lead to better distance estimation since they can position the bounding boxes on mUAVs more accurately. On the other hand, our time analysis yields that the cascaded classifiers using HOG train and run faster than the other algorithms.},
DOI = {10.3390/s150923805}
}



@Article{rs71215841,
AUTHOR = {Ali, Iftikhar and Greifeneder, Felix and Stamenkovic, Jelena and Neumann, Maxim and Notarnicola, Claudia},
TITLE = {Review of Machine Learning Approaches for Biomass and Soil Moisture Retrievals from Remote Sensing Data},
JOURNAL = {Remote Sensing},
VOLUME = {7},
YEAR = {2015},
NUMBER = {12},
PAGES = {16398--16421},
URL = {https://www.mdpi.com/2072-4292/7/12/15841},
ISSN = {2072-4292},
ABSTRACT = {The enormous increase of remote sensing data from airborne and space-borne platforms, as well as ground measurements has directed the attention of scientists towards new and efficient retrieval methodologies. Of particular importance is the consideration of the large extent and the high dimensionality (spectral, temporal and spatial) of remote sensing data. Moreover, the launch of the Sentinel satellite family will increase the availability of data, especially in the temporal domain, at no cost to the users. To analyze these data and to extract relevant features, such as essential climate variables (ECV), specific methodologies need to be exploited. Among these, greater attention is devoted to machine learning methods due to their flexibility and the capability to process large number of inputs and to handle non-linear problems. The main objective of this paper is to provide a review of research that is being carried out to retrieve two critically important terrestrial biophysical quantities (vegetation biomass and soil moisture) from remote sensing data using machine learning methods.},
DOI = {10.3390/rs71215841}
}



@Article{s16010128,
AUTHOR = {Fremont, Vincent and Bui, Manh Tuan and Boukerroui, Djamal and Letort, Pierrick},
TITLE = {Vision-Based People Detection System for Heavy Machine Applications},
JOURNAL = {Sensors},
VOLUME = {16},
YEAR = {2016},
NUMBER = {1},
ARTICLE-NUMBER = {128},
URL = {https://www.mdpi.com/1424-8220/16/1/128},
PubMedID = {26805838},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a vision-based people detection system for improving safety in heavy machines. We propose a perception system composed of a monocular fisheye camera and a LiDAR. Fisheye cameras have the advantage of a wide field-of-view, but the strong distortions that they create must be handled at the detection stage. Since people detection in fisheye images has not been well studied, we focus on investigating and quantifying the impact that strong radial distortions have on the appearance of people, and we propose approaches for handling this specificity, adapted from state-of-the-art people detection approaches. These adaptive approaches nevertheless have the drawback of high computational cost and complexity. Consequently, we also present a framework for harnessing the LiDAR modality in order to enhance the detection algorithm for different camera positions. A sequential LiDAR-based fusion architecture is used, which addresses directly the problem of reducing false detections and computational cost in an exclusively vision-based system. A heavy machine dataset was built, and different experiments were carried out to evaluate the performance of the system. The results are promising, in terms of both processing speed and performance.},
DOI = {10.3390/s16010128}
}



@Article{s16030288,
AUTHOR = {Yue, Bo and Wang, Shuang and Liang, Xuefeng and Jiao, Licheng and Xu, Caijin},
TITLE = {Joint Prior Learning for Visual Sensor Network Noisy Image Super-Resolution},
JOURNAL = {Sensors},
VOLUME = {16},
YEAR = {2016},
NUMBER = {3},
ARTICLE-NUMBER = {288},
URL = {https://www.mdpi.com/1424-8220/16/3/288},
ISSN = {1424-8220},
ABSTRACT = {The visual sensor network (VSN), a new type of wireless sensor network composed of low-cost wireless camera nodes, is being applied for numerous complex visual analyses in wild environments, such as visual surveillance, object recognition, etc. However, the captured images/videos are often low resolution with noise. Such visual data cannot be directly delivered to the advanced visual analysis. In this paper, we propose a joint-prior image super-resolution (JPISR) method using expectation maximization (EM) algorithm to improve VSN image quality. Unlike conventional methods that only focus on upscaling images, JPISR alternatively solves upscaling mapping and denoising in the E-step and M-step. To meet the requirement of the M-step, we introduce a novel non-local group-sparsity image filtering method to learn the explicit prior and induce the geometric duality between images to learn the implicit prior. The EM algorithm inherently combines the explicit prior and implicit prior by joint learning. Moreover, JPISR does not rely on large external datasets for training, which is much more practical in a VSN. Extensive experiments show that JPISR outperforms five state-of-the-art methods in terms of both PSNR, SSIM and visual perception.},
DOI = {10.3390/s16030288}
}



@Article{s16071117,
AUTHOR = {Kim, Sungho and Song, Woo-Jin and Kim, So-Hyun},
TITLE = {Robust Ground Target Detection by SAR and IR Sensor Fusion Using Adaboost-Based Feature Selection},
JOURNAL = {Sensors},
VOLUME = {16},
YEAR = {2016},
NUMBER = {7},
ARTICLE-NUMBER = {1117},
URL = {https://www.mdpi.com/1424-8220/16/7/1117},
ISSN = {1424-8220},
ABSTRACT = {Long-range ground targets are difficult to detect in a noisy cluttered environment using either synthetic aperture radar (SAR) images or infrared (IR) images. SAR-based detectors can provide a high detection rate with a high false alarm rate to background scatter noise. IR-based approaches can detect hot targets but are affected strongly by the weather conditions. This paper proposes a novel target detection method by decision-level SAR and IR fusion using an Adaboost-based machine learning scheme to achieve a high detection rate and low false alarm rate. The proposed method consists of individual detection, registration, and fusion architecture. This paper presents a single framework of a SAR and IR target detection method using modified Boolean map visual theory (modBMVT) and feature-selection based fusion. Previous methods applied different algorithms to detect SAR and IR targets because of the different physical image characteristics. One method that is optimized for IR target detection produces unsuccessful results in SAR target detection. This study examined the image characteristics and proposed a unified SAR and IR target detection method by inserting a median local average filter (MLAF, pre-filter) and an asymmetric morphological closing filter (AMCF, post-filter) into the BMVT. The original BMVT was optimized to detect small infrared targets. The proposed modBMVT can remove the thermal and scatter noise by the MLAF and detect extended targets by attaching the AMCF after the BMVT. Heterogeneous SAR and IR images were registered automatically using the proposed RANdom SAmple Region Consensus (RANSARC)-based homography optimization after a brute-force correspondence search using the detected target centers and regions. The final targets were detected by feature-selection based sensor fusion using Adaboost. The proposed method showed good SAR and IR target detection performance through feature selection-based decision fusion on a synthetic database generated by OKTAL-SE.},
DOI = {10.3390/s16071117}
}



@Article{s16121966,
AUTHOR = {Gong, Wenjuan and Zhang, Xuena and Gonzàlez, Jordi and Sobral, Andrews and Bouwmans, Thierry and Tu, Changhe and Zahzah, El-hadi},
TITLE = {Human Pose Estimation from Monocular Images: A Comprehensive Survey},
JOURNAL = {Sensors},
VOLUME = {16},
YEAR = {2016},
NUMBER = {12},
ARTICLE-NUMBER = {1966},
URL = {https://www.mdpi.com/1424-8220/16/12/1966},
ISSN = {1424-8220},
ABSTRACT = {Human pose estimation refers to the estimation of the location of body parts and how they are connected in an image. Human pose estimation from monocular images has wide applications (e.g., image indexing). Several surveys on human pose estimation can be found in the literature, but they focus on a certain category; for example, model-based approaches or human motion analysis, etc. As far as we know, an overall review of this problem domain has yet to be provided. Furthermore, recent advancements based on deep learning have brought novel algorithms for this problem. In this paper, a comprehensive survey of human pose estimation from monocular images is carried out including milestone works and recent advancements. Based on one standard pipeline for the solution of computer vision problems, this survey splits the problem into several modules: feature extraction and description, human body models, and modeling methods. Problem modeling methods are approached based on two means of categorization in this survey. One way to categorize includes top-down and bottom-up methods, and another way includes generative and discriminative methods. Considering the fact that one direct application of human pose estimation is to provide initialization for automatic video surveillance, there are additional sections for motion-related methods in all modules: motion features, motion models, and motion-based methods. Finally, the paper also collects 26 publicly available data sets for validation and provides error measurement methods that are frequently used.},
DOI = {10.3390/s16121966}
}



@Article{rs9010022,
AUTHOR = {Li, Weijia and Fu, Haohuan and Yu, Le and Cracknell, Arthur},
TITLE = {Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {1},
ARTICLE-NUMBER = {22},
URL = {https://www.mdpi.com/2072-4292/9/1/22},
ISSN = {2072-4292},
ABSTRACT = {Oil palm trees are important economic crops in Malaysia and other tropical areas. The number of oil palm trees in a plantation area is important information for predicting the yield of palm oil, monitoring the growing situation of palm trees and maximizing their productivity, etc. In this paper, we propose a deep learning based framework for oil palm tree detection and counting using high-resolution remote sensing images for Malaysia. Unlike previous palm tree detection studies, the trees in our study area are more crowded and their crowns often overlap. We use a number of manually interpreted samples to train and optimize the convolutional neural network (CNN), and predict labels for all the samples in an image dataset collected through the sliding window technique. Then, we merge the predicted palm coordinates corresponding to the same palm tree into one palm coordinate and obtain the final palm tree detection results. Based on our proposed method, more than 96% of the oil palm trees in our study area can be detected correctly when compared with the manually interpreted ground truth, and this is higher than the accuracies of the other three tree detection methods used in this study.},
DOI = {10.3390/rs9010022}
}



@Article{s17010103,
AUTHOR = {Ramon Soria, Pablo and Arrue, Begoña C. and Ollero, Anibal},
TITLE = {Detection, Location and Grasping Objects Using a Stereo Sensor on UAV in Outdoor Environments},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {1},
ARTICLE-NUMBER = {103},
URL = {https://www.mdpi.com/1424-8220/17/1/103},
ISSN = {1424-8220},
ABSTRACT = {The article presents a vision system for the autonomous grasping of objects with Unmanned Aerial Vehicles (UAVs) in real time. Giving UAVs the capability to manipulate objects vastly extends their applications, as they are capable of accessing places that are difficult to reach or even unreachable for human beings. This work is focused on the grasping of known objects based on feature models. The system runs in an on-board computer on a UAV equipped with a stereo camera and a robotic arm. The algorithm learns a feature-based model in an offline stage, then it is used online for detection of the targeted object and estimation of its position. This feature-based model was proved to be robust to both occlusions and the presence of outliers. The use of stereo cameras improves the learning stage, providing 3D information and helping to filter features in the online stage. An experimental system was derived using a rotary-wing UAV and a small manipulator for final proof of concept. The robotic arm is designed with three degrees of freedom and is lightweight due to payload limitations of the UAV. The system has been validated with different objects, both indoors and outdoors.},
DOI = {10.3390/s17010103}
}



@Article{s17020336,
AUTHOR = {Tang, Tianyu and Zhou, Shilin and Deng, Zhipeng and Zou, Huanxin and Lei, Lin},
TITLE = {Vehicle Detection in Aerial Images Based on Region Convolutional Neural Networks and Hard Negative Example Mining},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {2},
ARTICLE-NUMBER = {336},
URL = {https://www.mdpi.com/1424-8220/17/2/336},
ISSN = {1424-8220},
ABSTRACT = {Detecting vehicles in aerial imagery plays an important role in a wide range of applications. The current vehicle detection methods are mostly based on sliding-window search and handcrafted or shallow-learning-based features, having limited description capability and heavy computational costs. Recently, due to the powerful feature representations, region convolutional neural networks (CNN) based detection methods have achieved state-of-the-art performance in computer vision, especially Faster R-CNN. However, directly using it for vehicle detection in aerial images has many limitations: (1) region proposal network (RPN) in Faster R-CNN has poor performance for accurately locating small-sized vehicles, due to the relatively coarse feature maps; and (2) the classifier after RPN cannot distinguish vehicles and complex backgrounds well. In this study, an improved detection method based on Faster R-CNN is proposed in order to accomplish the two challenges mentioned above. Firstly, to improve the recall, we employ a hyper region proposal network (HRPN) to extract vehicle-like targets with a combination of hierarchical feature maps. Then, we replace the classifier after RPN by a cascade of boosted classifiers to verify the candidate regions, aiming at reducing false detection by negative example mining. We evaluate our method on the Munich vehicle dataset and the collected vehicle dataset, with improvements in accuracy and robustness compared to existing methods.},
DOI = {10.3390/s17020336}
}



@Article{rs9040312,
AUTHOR = {Ammour, Nassim and Alhichri, Haikel and Bazi, Yakoub and Benjdira, Bilel and Alajlan, Naif and Zuair, Mansour},
TITLE = {Deep Learning Approach for Car Detection in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {4},
ARTICLE-NUMBER = {312},
URL = {https://www.mdpi.com/2072-4292/9/4/312},
ISSN = {2072-4292},
ABSTRACT = {This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.},
DOI = {10.3390/rs9040312}
}



@Article{info8020043,
AUTHOR = {Song, Zhiguo and Sun, Jifeng and Yu, Jialin},
TITLE = {Object Tracking by a Combination of Discriminative Global and Generative Multi-Scale Local Models},
JOURNAL = {Information},
VOLUME = {8},
YEAR = {2017},
NUMBER = {2},
ARTICLE-NUMBER = {43},
URL = {https://www.mdpi.com/2078-2489/8/2/43},
ISSN = {2078-2489},
ABSTRACT = {Object tracking is a challenging task in many computer vision applications due to occlusion, scale variation and background clutter, etc. In this paper, we propose a tracking algorithm by combining discriminative global and generative multi-scale local models. In the global model, we teach a classifier with sparse discriminative features to separate the target object from the background based on holistic templates. In the multi-scale local model, the object is represented by multi-scale local sparse representation histograms, which exploit the complementary partial and spatial information of an object across different scales. Finally, a collaborative similarity score of one candidate target is input into a Bayesian inference framework to estimate the target state sequentially during tracking. Experimental results on the various challenging video sequences show that the proposed method performs favorably compared to several state-of-the-art trackers.},
DOI = {10.3390/info8020043}
}



@Article{rs9050494,
AUTHOR = {Li, Feimo and Li, Shuxiao and Zhu, Chengfei and Lan, Xiaosong and Chang, Hongxing},
TITLE = {Cost-Effective Class-Imbalance Aware CNN for Vehicle Localization and Categorization in High Resolution Aerial Images},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {5},
ARTICLE-NUMBER = {494},
URL = {https://www.mdpi.com/2072-4292/9/5/494},
ISSN = {2072-4292},
ABSTRACT = {Joint vehicle localization and categorization in high resolution aerial images can provide useful information for applications such as traffic flow structure analysis. To maintain sufficient features to recognize small-scaled vehicles, a regions with convolutional neural network features (R-CNN) -like detection structure is employed. In this setting, cascaded localization error can be averted by equally treating the negatives and differently typed positives as a multi-class classification task, but the problem of class-imbalance remains. To address this issue, a cost-effective network extension scheme is proposed. In it, the correlated convolution and connection costs during extension are reduced by feature map selection and bi-partite main-side network construction, which are realized with the assistance of a novel feature map class-importance measurement and a new class-imbalance sensitive main-side loss function. By using an image classification dataset established from a set of traditional real-colored aerial images with 0.13 m ground sampling distance which are taken from the height of 1000 m by an imaging system composed of non-metric cameras, the effectiveness of the proposed network extension is verified by comparing with its similarly shaped strong counter-parts. Experiments show an equivalent or better performance, while requiring the least parameter and memory overheads are required.},
DOI = {10.3390/rs9050494}
}



@Article{su9061010,
AUTHOR = {Ampatzidis, Yiannis and De Bellis, Luigi and Luvisi, Andrea},
TITLE = {iPathology: Robotic Applications and Management of Plants and Plant Diseases},
JOURNAL = {Sustainability},
VOLUME = {9},
YEAR = {2017},
NUMBER = {6},
ARTICLE-NUMBER = {1010},
URL = {https://www.mdpi.com/2071-1050/9/6/1010},
ISSN = {2071-1050},
ABSTRACT = {The rapid development of new technologies and the changing landscape of the online world (e.g., Internet of Things (IoT), Internet of All, cloud-based solutions) provide a unique opportunity for developing automated and robotic systems for urban farming, agriculture, and forestry. Technological advances in machine vision, global positioning systems, laser technologies, actuators, and mechatronics have enabled the development and implementation of robotic systems and intelligent technologies for precision agriculture. Herein, we present and review robotic applications on plant pathology and management, and emerging agricultural technologies for intra-urban agriculture. Greenhouse advanced management systems and technologies have been greatly developed in the last years, integrating IoT and WSN (Wireless Sensor Network). Machine learning, machine vision, and AI (Artificial Intelligence) have been utilized and applied in agriculture for automated and robotic farming. Intelligence technologies, using machine vision/learning, have been developed not only for planting, irrigation, weeding (to some extent), pruning, and harvesting, but also for plant disease detection and identification. However, plant disease detection still represents an intriguing challenge, for both abiotic and biotic stress. Many recognition methods and technologies for identifying plant disease symptoms have been successfully developed; still, the majority of them require a controlled environment for data acquisition to avoid false positives. Machine learning methods (e.g., deep and transfer learning) present promising results for improving image processing and plant symptom identification. Nevertheless, diagnostic specificity is a challenge for microorganism control and should drive the development of mechatronics and robotic solutions for disease management.},
DOI = {10.3390/su9061010}
}



@Article{jimaging3020021,
AUTHOR = {Radovic, Matija and Adarkwa, Offei and Wang, Qiaosong},
TITLE = {Object Recognition in Aerial Images Using Convolutional Neural Networks},
JOURNAL = {Journal of Imaging},
VOLUME = {3},
YEAR = {2017},
NUMBER = {2},
ARTICLE-NUMBER = {21},
URL = {https://www.mdpi.com/2313-433X/3/2/21},
ISSN = {2313-433X},
ABSTRACT = {There are numerous applications of unmanned aerial vehicles (UAVs) in the management of civil infrastructure assets. A few examples include routine bridge inspections, disaster management, power line surveillance and traffic surveying. As UAV applications become widespread, increased levels of autonomy and independent decision-making are necessary to improve the safety, efficiency, and accuracy of the devices. This paper details the procedure and parameters used for the training of convolutional neural networks (CNNs) on a set of aerial images for efficient and automated object recognition. Potential application areas in the transportation field are also highlighted. The accuracy and reliability of CNNs depend on the network’s training and the selection of operational parameters. This paper details the CNN training procedure and parameter selection. The object recognition results show that by selecting a proper set of parameters, a CNN can detect and classify objects with a high level of accuracy (97.5%) and computational efficiency. Furthermore, using a convolutional neural network implemented in the “YOLO” (“You Only Look Once”) platform, objects can be tracked, detected (“seen”), and classified (“comprehended”) from video feeds supplied by UAVs in real-time.},
DOI = {10.3390/jimaging3020021}
}



@Article{aerospace4020032,
AUTHOR = {Yang, Yurong and Gong, Huajun and Wang, Xinhua and Sun, Peng},
TITLE = {Aerial Target Tracking Algorithm Based on Faster R-CNN Combined with Frame Differencing},
JOURNAL = {Aerospace},
VOLUME = {4},
YEAR = {2017},
NUMBER = {2},
ARTICLE-NUMBER = {32},
URL = {https://www.mdpi.com/2226-4310/4/2/32},
ISSN = {2226-4310},
ABSTRACT = {We propose a robust approach to detecting and tracking moving objects for a naval unmanned aircraft system (UAS) landing on an aircraft carrier. The frame difference algorithm follows a simple principle to achieve real-time tracking, whereas Faster Region-Convolutional Neural Network (R-CNN) performs highly precise detection and tracking characteristics. We thus combine Faster R-CNN with the frame difference method, which is demonstrated to exhibit robust and real-time detection and tracking performance. In our UAS landing experiments, two cameras placed on both sides of the runway are used to capture the moving UAS. When the UAS is captured, the joint algorithm uses frame difference to detect the moving target (UAS). As soon as the Faster R-CNN algorithm accurately detects the UAS, the detection priority is given to Faster R-CNN. In this manner, we also perform motion segmentation and object detection in the presence of changes in the environment, such as illumination variation or “walking persons”. By combining the 2 algorithms we can accurately detect and track objects with a tracking accuracy rate of up to 99% and a frame per second of up to 40 Hz. Thus, a solid foundation is laid for subsequent landing guidance.},
DOI = {10.3390/aerospace4020032}
}



@Article{rs9070721,
AUTHOR = {Jiang, Hao and Chen, Shuisen and Li, Dan and Wang, Chongyang and Yang, Ji},
TITLE = {Papaya Tree Detection with UAV Images Using a GPU-Accelerated Scale-Space Filtering Method},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {7},
ARTICLE-NUMBER = {721},
URL = {https://www.mdpi.com/2072-4292/9/7/721},
ISSN = {2072-4292},
ABSTRACT = {The use of unmanned aerial vehicles (UAV) can allow individual tree detection for forest inventories in a cost-effective way. The scale-space filtering (SSF) algorithm is commonly used and has the capability of detecting trees of different crown sizes. In this study, we made two improvements with regard to the existing method and implementations. First, we incorporated SSF with a Lab color transformation to reduce over-detection problems associated with the original luminance image. Second, we ported four of the most time-consuming processes to the graphics processing unit (GPU) to improve computational efficiency. The proposed method was implemented using PyCUDA, which enabled access to NVIDIA’s compute unified device architecture (CUDA) through high-level scripting of the Python language. Our experiments were conducted using two images captured by the DJI Phantom 3 Professional and a most recent NVIDIA GPU GTX1080. The resulting accuracy was high, with an F-measure larger than 0.94. The speedup achieved by our parallel implementation was 44.77 and 28.54 for the first and second test image, respectively. For each 4000 × 3000 image, the total runtime was less than 1 s, which was sufficient for real-time performance and interactive application.},
DOI = {10.3390/rs9070721}
}



@Article{rs9080775,
AUTHOR = {Ahmed, Asmau M. and Duran, Olga and Zweiri, Yahya and Smith, Mike},
TITLE = {Hybrid Spectral Unmixing: Using Artificial Neural Networks for Linear/Non-Linear Switching},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {8},
ARTICLE-NUMBER = {775},
URL = {https://www.mdpi.com/2072-4292/9/8/775},
ISSN = {2072-4292},
ABSTRACT = {Spectral unmixing is a key process in identifying spectral signature of materials and quantifying their spatial distribution over an image. The linear model is expected to provide acceptable results when two assumptions are satisfied: (1) The mixing process should occur at macroscopic level and (2) Photons must interact with single material before reaching the sensor. However, these assumptions do not always hold and more complex nonlinear models are required. This study proposes a new hybrid method for switching between linear and nonlinear spectral unmixing of hyperspectral data based on artificial neural networks. The neural networks was trained with parameters within a window of the pixel under consideration. These parameters are computed to represent the diversity of the neighboring pixels and are based on the Spectral Angular Distance, Covariance and a non linearity parameter. The endmembers were extracted using Vertex Component Analysis while the abundances were estimated using the method identified by the neural networks (Vertex Component Analysis, Fully Constraint Least Square Method, Polynomial Post Nonlinear Mixing Model or Generalized Bilinear Model). Results show that the hybrid method performs better than each of the individual techniques with high overall accuracy, while the abundance estimation error is significantly lower than that obtained using the individual methods. Experiments on both synthetic dataset and real hyperspectral images demonstrated that the proposed hybrid switch method is efficient for solving spectral unmixing of hyperspectral images as compared to individual algorithms.},
DOI = {10.3390/rs9080775}
}



@Article{s17091957,
AUTHOR = {Yan, Yiming and Tan, Zhichao and Su, Nan and Zhao, Chunhui},
TITLE = {Building Extraction Based on an Optimized Stacked Sparse Autoencoder of Structure and Training Samples Using LIDAR DSM and Optical Images},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {9},
ARTICLE-NUMBER = {1957},
URL = {https://www.mdpi.com/1424-8220/17/9/1957},
ISSN = {1424-8220},
ABSTRACT = {In this paper, a building extraction method is proposed based on a stacked sparse autoencoder with an optimized structure and training samples. Building extraction plays an important role in urban construction and planning. However, some negative effects will reduce the accuracy of extraction, such as exceeding resolution, bad correction and terrain influence. Data collected by multiple sensors, as light detection and ranging (LIDAR), optical sensor etc., are used to improve the extraction. Using digital surface model (DSM) obtained from LIDAR data and optical images, traditional method can improve the extraction effect to a certain extent, but there are some defects in feature extraction. Since stacked sparse autoencoder (SSAE) neural network can learn the essential characteristics of the data in depth, SSAE was employed to extract buildings from the combined DSM data and optical image. A better setting strategy of SSAE network structure is given, and an idea of setting the number and proportion of training samples for better training of SSAE was presented. The optical data and DSM were combined as input of the optimized SSAE, and after training by an optimized samples, the appropriate network structure can extract buildings with great accuracy and has good robustness.},
DOI = {10.3390/s17091957}
}



@Article{s17091987,
AUTHOR = {Nguyen, Phong Ha and Kim, Ki Wan and Lee, Young Won and Park, Kang Ryoung},
TITLE = {Remote Marker-Based Tracking for UAV Landing Using Visible-Light Camera Sensor},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {9},
ARTICLE-NUMBER = {1987},
URL = {https://www.mdpi.com/1424-8220/17/9/1987},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs), which are commonly known as drones, have proved to be useful not only on the battlefields where manned flight is considered too risky or difficult, but also in everyday life purposes such as surveillance, monitoring, rescue, unmanned cargo, aerial video, and photography. More advanced drones make use of global positioning system (GPS) receivers during the navigation and control loop which allows for smart GPS features of drone navigation. However, there are problems if the drones operate in heterogeneous areas with no GPS signal, so it is important to perform research into the development of UAVs with autonomous navigation and landing guidance using computer vision. In this research, we determined how to safely land a drone in the absence of GPS signals using our remote maker-based tracking algorithm based on the visible light camera sensor. The proposed method uses a unique marker designed as a tracking target during landing procedures. Experimental results show that our method significantly outperforms state-of-the-art object trackers in terms of both accuracy and processing time, and we perform test on an embedded system in various environments.},
DOI = {10.3390/s17091987}
}



@Article{s17092052,
AUTHOR = {Kim, Hyunjun and Lee, Junhwa and Ahn, Eunjong and Cho, Soojin and Shin, Myoungsu and Sim, Sung-Han},
TITLE = {Concrete Crack Identification Using a UAV Incorporating Hybrid Image Processing},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {9},
ARTICLE-NUMBER = {2052},
URL = {https://www.mdpi.com/1424-8220/17/9/2052},
ISSN = {1424-8220},
ABSTRACT = {Crack assessment is an essential process in the maintenance of concrete structures. In general, concrete cracks are inspected by manual visual observation of the surface, which is intrinsically subjective as it depends on the experience of inspectors. Further, it is time-consuming, expensive, and often unsafe when inaccessible structural members are to be assessed. Unmanned aerial vehicle (UAV) technologies combined with digital image processing have recently been applied to crack assessment to overcome the drawbacks of manual visual inspection. However, identification of crack information in terms of width and length has not been fully explored in the UAV-based applications, because of the absence of distance measurement and tailored image processing. This paper presents a crack identification strategy that combines hybrid image processing with UAV technology. Equipped with a camera, an ultrasonic displacement sensor, and a WiFi module, the system provides the image of cracks and the associated working distance from a target structure on demand. The obtained information is subsequently processed by hybrid image binarization to estimate the crack width accurately while minimizing the loss of the crack length information. The proposed system has shown to successfully measure cracks thicker than 0.1 mm with the maximum length estimation error of 7.3%.},
DOI = {10.3390/s17092052}
}



@Article{s17092140,
AUTHOR = {Meng, Xiaoli and Wang, Heng and Liu, Bingbing},
TITLE = {A Robust Vehicle Localization Approach Based on GNSS/IMU/DMI/LiDAR Sensor Fusion for Autonomous Vehicles},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {9},
ARTICLE-NUMBER = {2140},
URL = {https://www.mdpi.com/1424-8220/17/9/2140},
ISSN = {1424-8220},
ABSTRACT = {Precise and robust localization in a large-scale outdoor environment is essential for an autonomous vehicle. In order to improve the performance of the fusion of GNSS (Global Navigation Satellite System)/IMU (Inertial Measurement Unit)/DMI (Distance-Measuring Instruments), a multi-constraint fault detection approach is proposed to smooth the vehicle locations in spite of GNSS jumps. Furthermore, the lateral localization error is compensated by the point cloud-based lateral localization method proposed in this paper. Experiment results have verified the algorithms proposed in this paper, which shows that the algorithms proposed in this paper are capable of providing precise and robust vehicle localization.},
DOI = {10.3390/s17092140}
}



@Article{jimaging3040047,
AUTHOR = {Ejofodomi, O’tega and Ofualagba, Godswill},
TITLE = {Detection and Classification of Land Crude Oil Spills Using Color Segmentation and Texture Analysis},
JOURNAL = {Journal of Imaging},
VOLUME = {3},
YEAR = {2017},
NUMBER = {4},
ARTICLE-NUMBER = {47},
URL = {https://www.mdpi.com/2313-433X/3/4/47},
ISSN = {2313-433X},
ABSTRACT = {Crude oil spills have negative consequences on the economy, environment, health and society in which they occur, and the severity of the consequences depends on how quickly these spills are detected once they begin. Several methods have been employed for spill detection, including real time remote surveillance by flying aircrafts with surveillance teams. Other methods employ various sensors, including visible sensors. This paper presents an algorithm to automatically detect the presence of crude oil spills in images acquired using visible light sensors. Images of crude oil spills used in the development of the algorithm were obtained from the Shell Petroleum Development Company (SPDC) Nigeria website The major steps of the detection algorithm are image preprocessing, crude oil color segmentation, sky elimination segmentation, Region of Interest (ROI) extraction, ROI texture feature extraction, and ROI texture feature analysis and classification. The algorithm was developed using 25 sample images containing crude oil spills and demonstrated a sensitivity of 92% and an FPI of 1.43. The algorithm was further tested on a set of 56 case images and demonstrated a sensitivity of 82% and an FPI of 0.66. This algorithm can be incorporated into spill detection systems that utilize visible sensors for early detection of crude oil spills.},
DOI = {10.3390/jimaging3040047}
}



@Article{ijgi6110323,
AUTHOR = {Liu, Yuan and Wang, Jun and Song, Jingwei and Song, Zihui},
TITLE = {Globally Consistent Indoor Mapping via a Decoupling Rotation and Translation Algorithm Applied to RGB-D Camera Output},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {6},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {323},
URL = {https://www.mdpi.com/2220-9964/6/11/323},
ISSN = {2220-9964},
ABSTRACT = {This paper presents a novel RGB-D 3D reconstruction algorithm for the indoor environment. The method can produce globally-consistent 3D maps for potential GIS applications. As the consumer RGB-D camera provides a noisy depth image, the proposed algorithm decouples the rotation and translation for a more robust camera pose estimation, which makes full use of the information, but also prevents inaccuracies caused by noisy depth measurements. The uncertainty in the image depth is not only related to the camera device, but also the environment; hence, a novel uncertainty model for depth measurements was developed using Gaussian mixture applied to multi-windows. The plane features in the indoor environment contain valuable information about the global structure, which can guide the convergence of camera pose solutions, and plane and feature point constraints are incorporated in the proposed optimization framework. The proposed method was validated using publicly-available RGB-D benchmarks and obtained good quality trajectory and 3D models, which are difficult for traditional 3D reconstruction algorithms.},
DOI = {10.3390/ijgi6110323}
}



@Article{s17112535,
AUTHOR = {Hoshiba, Kotaro and Washizaki, Kai and Wakabayashi, Mizuho and Ishiki, Takahiro and Kumon, Makoto and Bando, Yoshiaki and Gabriel, Daniel and Nakadai, Kazuhiro and Okuno, Hiroshi G.},
TITLE = {Design of UAV-Embedded Microphone Array System for Sound Source Localization in Outdoor Environments},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {2535},
URL = {https://www.mdpi.com/1424-8220/17/11/2535},
ISSN = {1424-8220},
ABSTRACT = {In search and rescue activities, unmanned aerial vehicles (UAV) should exploit sound information to compensate for poor visual information. This paper describes the design and implementation of a UAV-embedded microphone array system for sound source localization in outdoor environments. Four critical development problems included water-resistance of the microphone array, efficiency in assembling, reliability of wireless communication, and sufficiency of visualization tools for operators. To solve these problems, we developed a spherical microphone array system (SMAS) consisting of a microphone array, a stable wireless network communication system, and intuitive visualization tools. The performance of SMAS was evaluated with simulated data and a demonstration in the field. Results confirmed that the SMAS provides highly accurate localization, water resistance, prompt assembly, stable wireless communication, and intuitive information for observers and operators.},
DOI = {10.3390/s17112535}
}



@Article{s17112557,
AUTHOR = {Yamamoto, Kyosuke and Togami, Takashi and Yamaguchi, Norio},
TITLE = {Super-Resolution of Plant Disease Images for the Acceleration of Image-based Phenotyping and Vigor Diagnosis in Agriculture},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {2557},
URL = {https://www.mdpi.com/1424-8220/17/11/2557},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs or drones) are a very promising branch of technology, and they have been utilized in agriculture—in cooperation with image processing technologies—for phenotyping and vigor diagnosis. One of the problems in the utilization of UAVs for agricultural purposes is the limitation in flight time. It is necessary to fly at a high altitude to capture the maximum number of plants in the limited time available, but this reduces the spatial resolution of the captured images. In this study, we applied a super-resolution method to the low-resolution images of tomato diseases to recover detailed appearances, such as lesions on plant organs. We also conducted disease classification using high-resolution, low-resolution, and super-resolution images to evaluate the effectiveness of super-resolution methods in disease classification. Our results indicated that the super-resolution method outperformed conventional image scaling methods in spatial resolution enhancement of tomato disease images. The results of disease classification showed that the accuracy attained was also better by a large margin with super-resolution images than with low-resolution images. These results indicated that our approach not only recovered the information lost in low-resolution images, but also exerted a beneficial influence on further image analysis. The proposed approach will accelerate image-based phenotyping and vigor diagnosis in the field, because it not only saves time to capture images of a crop in a cultivation field but also secures the accuracy of these images for further analysis.},
DOI = {10.3390/s17112557}
}



@Article{rs9111166,
AUTHOR = {Li, Chang and Ma, Yong and Mei, Xiaoguang and Fan, Fan and Huang, Jun and Ma, Jiayi},
TITLE = {Sparse Unmixing of Hyperspectral Data with Noise Level Estimation},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {1166},
URL = {https://www.mdpi.com/2072-4292/9/11/1166},
ISSN = {2072-4292},
ABSTRACT = {Recently, sparse unmixing has received particular attention in the analysis of hyperspectral images (HSIs). However, traditional sparse unmixing ignores the different noise levels in different bands of HSIs, making such methods sensitive to different noise levels. To overcome this problem, the noise levels at different bands are assumed to be different in this paper, and a general sparse unmixing method based on noise level estimation (SU-NLE) under the sparse regression framework is proposed. First, the noise in each band is estimated on the basis of the multiple regression theory in hyperspectral applications, given that neighboring spectral bands are usually highly correlated. Second, the noise weighting matrix can be obtained from the estimated noise. Third, the noise weighting matrix is integrated into the sparse regression unmixing framework, which can alleviate the impact of different noise levels at different bands. Finally, the proposed SU-NLE is solved by the alternative direction method of multipliers. Experiments on synthetic datasets show that the signal-to-reconstruction error of the proposed SU-NLE is considerably higher than those of the corresponding traditional sparse regression unmixing methods without noise level estimation, which demonstrates the efficiency of integrating noise level estimation into the sparse regression unmixing framework. The proposed SU-NLE also shows promising results in real HSIs.},
DOI = {10.3390/rs9111166}
}



@Article{rs9111170,
AUTHOR = {Tang, Tianyu and Zhou, Shilin and Deng, Zhipeng and Lei, Lin and Zou, Huanxin},
TITLE = {Arbitrary-Oriented Vehicle Detection in Aerial Imagery with Single Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {1170},
URL = {https://www.mdpi.com/2072-4292/9/11/1170},
ISSN = {2072-4292},
ABSTRACT = {Vehicle detection with orientation estimation in aerial images has received widespread interest as it is important for intelligent traffic management. This is a challenging task, not only because of the complex background and relatively small size of the target, but also the various orientations of vehicles in aerial images captured from the top view. The existing methods for oriented vehicle detection need several post-processing steps to generate final detection results with orientation, which are not efficient enough. Moreover, they can only get discrete orientation information for each target. In this paper, we present an end-to-end single convolutional neural network to generate arbitrarily-oriented detection results directly. Our approach, named Oriented_SSD (Single Shot MultiBox Detector, SSD), uses a set of default boxes with various scales on each feature map location to produce detection bounding boxes. Meanwhile, offsets are predicted for each default box to better match the object shape, which contain the angle parameter for oriented bounding boxes’ generation. Evaluation results on the public DLR Vehicle Aerial dataset and Vehicle Detection in Aerial Imagery (VEDAI) dataset demonstrate that our method can detect both the location and orientation of the vehicle with high accuracy and fast speed. For test images in the DLR Vehicle Aerial dataset with a size of     5616 × 3744    , our method achieves 76.1% average precision (AP) and 78.7% correct direction classification at 5.17 s on an NVIDIA GTX-1060.},
DOI = {10.3390/rs9111170}
}



@Article{s17112641,
AUTHOR = {Malek, Salim and Melgani, Farid and Mekhalfi, Mohamed Lamine and Bazi, Yakoub},
TITLE = {Real-Time Indoor Scene Description for the Visually Impaired Using Autoencoder Fusion Strategies with Visible Cameras},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {2641},
URL = {https://www.mdpi.com/1424-8220/17/11/2641},
ISSN = {1424-8220},
ABSTRACT = {This paper describes three coarse image description strategies, which are meant to promote a rough perception of surrounding objects for visually impaired individuals, with application to indoor spaces. The described algorithms operate on images (grabbed by the user, by means of a chest-mounted camera), and provide in output a list of objects that likely exist in his context across the indoor scene. In this regard, first, different colour, texture, and shape-based feature extractors are generated, followed by a feature learning step by means of AutoEncoder (AE) models. Second, the produced features are fused and fed into a multilabel classifier in order to list the potential objects. The conducted experiments point out that fusing a set of AE-learned features scores higher classification rates with respect to using the features individually. Furthermore, with respect to reference works, our method: (i) yields higher classification accuracies, and (ii) runs (at least four times) faster, which enables a potential full real-time application.},
DOI = {10.3390/s17112641}
}



@Article{rs9111198,
AUTHOR = {Cai, Bowen and Jiang, Zhiguo and Zhang, Haopeng and Zhao, Danpei and Yao, Yuan},
TITLE = {Airport Detection Using End-to-End Convolutional Neural Network with Hard Example Mining},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {11},
ARTICLE-NUMBER = {1198},
URL = {https://www.mdpi.com/2072-4292/9/11/1198},
ISSN = {2072-4292},
ABSTRACT = {Deep convolutional neural network (CNN) achieves outstanding performance in the field of target detection. As one of the most typical targets in remote sensing images (RSIs), airport has attracted increasing attention in recent years. However, the essential challenge for using deep CNN to detect airport is the great imbalance between the number of airports and background examples in large-scale RSIs, which may lead to over-fitting. In this paper, we develop a hard example mining and weight-balanced strategy to construct a novel end-to-end convolutional neural network for airport detection. The initial motivation of the proposed method is that backgrounds contain an overwhelming number of easy examples and a few hard examples. Therefore, we design a hard example mining layer to automatically select hard examples by their losses, and implement a new weight-balanced loss function to optimize CNN. Meanwhile, the cascade design of proposal extraction and object detection in our network releases the constraint on input image size and reduces spurious false positives. Compared with geometric characteristics and low-level manually designed features, the hard example mining based network could extract high-level features, which is more robust for airport detection in complex environment. The proposed method is validated on a multi-scale dataset with complex background collected from Google Earth. The experimental results demonstrate that our proposed method is robust, and superior to the state-of-the-art airport detection models.},
DOI = {10.3390/rs9111198}
}



@Article{s17122720,
AUTHOR = {Zhong, Jiandan and Lei, Tao and Yao, Guangle},
TITLE = {Robust Vehicle Detection in Aerial Images Based on Cascaded Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {12},
ARTICLE-NUMBER = {2720},
URL = {https://www.mdpi.com/1424-8220/17/12/2720},
ISSN = {1424-8220},
ABSTRACT = {Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.},
DOI = {10.3390/s17122720}
}



@Article{s17122726,
AUTHOR = {Su, Jinya and Yi, Dewei and Liu, Cunjia and Guo, Lei and Chen, Wen-Hua},
TITLE = {Dimension Reduction Aided Hyperspectral Image Classification with a Small-sized Training Dataset: Experimental Comparisons},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {12},
ARTICLE-NUMBER = {2726},
URL = {https://www.mdpi.com/1424-8220/17/12/2726},
ISSN = {1424-8220},
ABSTRACT = {Hyperspectral images (HSI) provide rich information which may not be captured by other sensing technologies and therefore gradually find a wide range of applications. However, they also generate a large amount of irrelevant or redundant data for a specific task. This causes a number of issues including significantly increased computation time, complexity and scale of prediction models mapping the data to semantics (e.g., classification), and the need of a large amount of labelled data for training. Particularly, it is generally difficult and expensive for experts to acquire sufficient training samples in many applications. This paper addresses these issues by exploring a number of classical dimension reduction algorithms in machine learning communities for HSI classification. To reduce the size of training dataset, feature selection (e.g., mutual information, minimal redundancy maximal relevance) and feature extraction (e.g., Principal Component Analysis (PCA), Kernel PCA) are adopted to augment a baseline classification method, Support Vector Machine (SVM). The proposed algorithms are evaluated using a real HSI dataset. It is shown that PCA yields the most promising performance in reducing the number of features or spectral bands. It is observed that while significantly reducing the computational complexity, the proposed method can achieve better classification results over the classic SVM on a small training dataset, which makes it suitable for real-time applications or when only limited training data are available. Furthermore, it can also achieve performances similar to the classic SVM on large datasets but with much less computing time.},
DOI = {10.3390/s17122726}
}



@Article{rs9121220,
AUTHOR = {Guirado, Emilio and Tabik, Siham and Alcaraz-Segura, Domingo and Cabello, Javier and Herrera, Francisco},
TITLE = {Deep-learning Versus OBIA for Scattered Shrub Detection with Google Earth Imagery: Ziziphus lotus as Case Study},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {12},
ARTICLE-NUMBER = {1220},
URL = {https://www.mdpi.com/2072-4292/9/12/1220},
ISSN = {2072-4292},
ABSTRACT = {There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been traditionally performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image often cannot be extrapolated to a different image. Recently, deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in computer vision and are offering promising results in land cover mapping. This paper analyzes the potential of CNN-based methods for detection of plant species of conservation concern using free high-resolution Google Earth     TM     images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. Compared to the best performing OBIA-method, the best CNN-detector achieved up to 12% better precision, up to 30% better recall and up to 20% better balance between precision and recall. Besides, the knowledge that CNNs acquired in the first image can be re-utilized in other regions, which makes the detection process very fast. A natural conclusion of this work is that including CNN-models as classifiers, e.g., ResNet-classifier, could further improve OBIA methods. The provided methodology can be systematically reproduced for other species detection using our codes available through (https://github.com/EGuirado/CNN-remotesensing).},
DOI = {10.3390/rs9121220}
}



@Article{s17122742,
AUTHOR = {Zhang, Wei and Wei, Shilin and Teng, Yanbin and Zhang, Jianku and Wang, Xiufang and Yan, Zheping},
TITLE = {Dynamic Obstacle Avoidance for Unmanned Underwater Vehicles Based on an Improved Velocity Obstacle Method},
JOURNAL = {Sensors},
VOLUME = {17},
YEAR = {2017},
NUMBER = {12},
ARTICLE-NUMBER = {2742},
URL = {https://www.mdpi.com/1424-8220/17/12/2742},
ISSN = {1424-8220},
ABSTRACT = {In view of a dynamic obstacle environment with motion uncertainty, we present a dynamic collision avoidance method based on the collision risk assessment and improved velocity obstacle method. First, through the fusion optimization of forward-looking sonar data, the redundancy of the data is reduced and the position, size and velocity information of the obstacles are obtained, which can provide an accurate decision-making basis for next-step collision avoidance. Second, according to minimum meeting time and the minimum distance between the obstacle and unmanned underwater vehicle (UUV), this paper establishes the collision risk assessment model, and screens key obstacles to avoid collision. Finally, the optimization objective function is established based on the improved velocity obstacle method, and a UUV motion characteristic is used to calculate the reachable velocity sets. The optimal collision speed of UUV is searched in velocity space. The corresponding heading and speed commands are calculated, and outputted to the motion control module. The above is the complete dynamic obstacle avoidance process. The simulation results show that the proposed method can obtain a better collision avoidance effect in the dynamic environment, and has good adaptability to the unknown dynamic environment.},
DOI = {10.3390/s17122742}
}



@Article{rs9121233,
AUTHOR = {Yu, Huai and Yang, Wen and Hua, Guang and Ru, Hui and Huang, Pingping},
TITLE = {Change Detection Using High Resolution Remote Sensing Images Based on Active Learning and Markov Random Fields},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {12},
ARTICLE-NUMBER = {1233},
URL = {https://www.mdpi.com/2072-4292/9/12/1233},
ISSN = {2072-4292},
ABSTRACT = {Change detection has been widely used in remote sensing, such as for disaster assessment and urban expansion detection. Although it is convenient to use unsupervised methods to detect changes from multi-temporal images, the results could be further improved. In supervised methods, heavy data labelling tasks are needed, and the sample annotation process with real categories is tedious and costly. To relieve the burden of labelling and to obtain satisfactory results, we propose an interactive change detection framework based on active learning and Markov random field (MRF). More specifically, a limited number of representative objects are found in an unsupervised way at the beginning. Then, the very limited samples are labelled as “change” or “no change” to train a simple binary classification model, i.e., a Gaussian process model. By using this model, we then select and label the most informative samples by “the easiest” sample selection strategy to update the former weak classification model until the detection results do not change notably. Finally, the maximum a posteriori (MAP) change detection is efficiently computed via the min-cut-based integer optimization algorithm. The time consuming and laborious manual labelling process can be reduced substantially, and a desirable detection result can be obtained. The experiments on several WorldView-2 images demonstrate the effectiveness of the proposed method.},
DOI = {10.3390/rs9121233}
}



@Article{rs9121244,
AUTHOR = {Chen, Suting and Li, Xin and Zhang, Yanyan and Feng, Rui and Zhang, Chuang},
TITLE = {Local Deep Hashing Matching of Aerial Images Based on Relative Distance and Absolute Distance Constraints},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {12},
ARTICLE-NUMBER = {1244},
URL = {https://www.mdpi.com/2072-4292/9/12/1244},
ISSN = {2072-4292},
ABSTRACT = {Aerial images have features of high resolution, complex background, and usually require large amounts of calculation, however, most algorithms used in matching of aerial images adopt the shallow hand-crafted features expressed as floating-point descriptors (e.g., SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features)), which may suffer from poor matching speed and are not well represented in the literature. Here, we propose a novel Local Deep Hashing Matching (LDHM) method for matching of aerial images with large size and with lower complexity or fast matching speed. The basic idea of the proposed algorithm is to utilize the deep network model in the local area of the aerial images, and study the local features, as well as the hash function of the images. Firstly, according to the course overlap rate of aerial images, the algorithm extracts the local areas for matching to avoid the processing of redundant information. Secondly, a triplet network structure is proposed to mine the deep features of the patches of the local image, and the learned features are imported to the hash layer, thus obtaining the representation of a binary hash code. Thirdly, the constraints of the positive samples to the absolute distance are added on the basis of the triplet loss, a new objective function is constructed to optimize the parameters of the network and enhance the discriminating capabilities of image patch features. Finally, the obtained deep hash code of each image patch is used for the similarity comparison of the image patches in the Hamming space to complete the matching of aerial images. The proposed LDHM algorithm evaluates the UltraCam-D dataset and a set of actual aerial images, simulation result demonstrates that it may significantly outperform the state-of-the-art algorithm in terms of the efficiency and performance.},
DOI = {10.3390/rs9121244}
}



@Article{fi9040093,
AUTHOR = {Latif, Siddique and Qadir, Junaid and Farooq, Shahzad and Imran, Muhammad Ali},
TITLE = {How 5G Wireless (and Concomitant Technologies) Will Revolutionize Healthcare?},
JOURNAL = {Future Internet},
VOLUME = {9},
YEAR = {2017},
NUMBER = {4},
ARTICLE-NUMBER = {93},
URL = {https://www.mdpi.com/1999-5903/9/4/93},
ISSN = {1999-5903},
ABSTRACT = {The need to have equitable access to quality healthcare is enshrined in the United Nations (UN) Sustainable Development Goals (SDGs), which defines the developmental agenda of the UN for the next 15 years. In particular, the third SDG focuses on the need to “ensure healthy lives and promote well-being for all at all ages”. In this paper, we build the case that 5G wireless technology, along with concomitant emerging technologies (such as IoT, big data, artificial intelligence and machine learning), will transform global healthcare systems in the near future. Our optimism around 5G-enabled healthcare stems from a confluence of significant technical pushes that are already at play: apart from the availability of high-throughput low-latency wireless connectivity, other significant factors include the democratization of computing through cloud computing; the democratization of Artificial Intelligence (AI) and cognitive computing (e.g., IBM Watson); and the commoditization of data through crowdsourcing and digital exhaust. These technologies together can finally crack a dysfunctional healthcare system that has largely been impervious to technological innovations. We highlight the persistent deficiencies of the current healthcare system and then demonstrate how the 5G-enabled healthcare revolution can fix these deficiencies. We also highlight open technical research challenges, and potential pitfalls, that may hinder the development of such a 5G-enabled health revolution.},
DOI = {10.3390/fi9040093}
}



@Article{rs10010002,
AUTHOR = {Weinmann, Martin and Weinmann, Michael},
TITLE = {Geospatial Computer Vision Based on Multi-Modal Data—How Valuable Is Shape Information for the Extraction of Semantic Information?},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {2},
URL = {https://www.mdpi.com/2072-4292/10/1/2},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we investigate the value of different modalities and their combination for the analysis of geospatial data of low spatial resolution. For this purpose, we present a framework that allows for the enrichment of geospatial data with additional semantics based on given color information, hyperspectral information, and shape information. While the different types of information are used to define a variety of features, classification based on these features is performed using a random forest classifier. To draw conclusions about the relevance of different modalities and their combination for scene analysis, we present and discuss results which have been achieved with our framework on the MUUFL Gulfport Hyperspectral and LiDAR Airborne Data Set.},
DOI = {10.3390/rs10010002}
}



@Article{rs10010007,
AUTHOR = {Fan, Junqing and Yan, Jining and Ma, Yan and Wang, Lizhe},
TITLE = {Big Data Integration in Remote Sensing across a Distributed Metadata-Based Spatial Infrastructure},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {7},
URL = {https://www.mdpi.com/2072-4292/10/1/7},
ISSN = {2072-4292},
ABSTRACT = {Since Landsat-1 first started to deliver volumes of pixels in 1972, the volumes of archived data in remote sensing data centers have increased continuously. Due to various satellite orbit parameters and the specifications of different sensors, the storage formats, projections, spatial resolutions, and revisit periods of these archived data are vastly different. In addition, the remote sensing data received continuously by each data center arrives at a faster code rate; it is best to ingest and archive the newly received data to ensure users have access to the latest data retrieval and distribution services. Hence, an excellent data integration, organization, and management program is urgently needed. However, the multi-source, massive, heterogeneous, and distributed storage features of remote sensing data have not only caused difficulties for integration across distributed data center spatial infrastructures, but have also resulted in the current modes of data organization and management being unable meet the rapid retrieval and access requirements of users. Hence, this paper proposes an object-oriented data technology (OODT) and SolrCloud-based remote sensing data integration and management framework across a distributed data center spatial infrastructure. In this framework, all of the remote sensing metadata in the distributed sub-centers are transformed into the International Standardization Organization (ISO) 19115-based unified format, and then ingested and transferred to the main center by OODT components, continuously or at regular intervals. In the main data center, in order to improve the efficiency of massive data retrieval, we proposed a logical segmentation indexing (LSI) model-based data organization approach, and took SolrCloud to realize the distributed index and retrieval of massive metadata. Finally, a series of distributed data integration, retrieval, and comparative experiments showed that our proposed distributed data integration and management program is effective and promises superior results. Specifically, the LSI model-based data organization and the SolrCloud-based distributed indexing schema was able to effectively improve the efficiency of massive data retrieval.},
DOI = {10.3390/rs10010007}
}



@Article{rs10010015,
AUTHOR = {Chen, Weitao and Li, Xianju and He, Haixia and Wang, Lizhe},
TITLE = {A Review of Fine-Scale Land Use and Land Cover Classification in Open-Pit Mining Areas by Remote Sensing Techniques},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {15},
URL = {https://www.mdpi.com/2072-4292/10/1/15},
ISSN = {2072-4292},
ABSTRACT = {Over recent decades, fine-scale land use and land cover classification in open-pit mine areas (LCCMA) has become very important for understanding the influence of mining activities on the regional geo-environment, and for environmental impact assessment procedure. This research reviews advances in fine-scale LCCMA from the following aspects. Firstly, it analyzes and proposes classification thematic resolution for LCCMA. Secondly, remote sensing data sources, features, feature selection methods, and classification algorithms for LCCMA are summarized. Thirdly, three major factors that affect LCCMA are discussed: significant three-dimensional terrain features, strong LCCMA feature variability, and homogeneity of spectral-spatial features. Correspondingly, three key scientific issues that limit the accuracy of LCCMA are presented. Finally, several future research directions are discussed: (1) unitization of new sensors, particularly those with stereo survey ability; (2) procurement of sensitive features by new sensors and combinations of sensitive features using novel feature selection methods; (3) development of robust and self-adjusted classification algorithms, such as ensemble learning and deep learning for LCCMA; and (4) application of fine-scale mining information for regularity and management of mines.},
DOI = {10.3390/rs10010015}
}



@Article{rs10010075,
AUTHOR = {Ji, Shunping and Zhang, Chi and Xu, Anjian and Shi, Yun and Duan, Yulin},
TITLE = {3D Convolutional Neural Networks for Crop Classification with Multi-Temporal Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {75},
URL = {https://www.mdpi.com/2072-4292/10/1/75},
ISSN = {2072-4292},
ABSTRACT = {This study describes a novel three-dimensional (3D) convolutional neural networks (CNN) based method that automatically classifies crops from spatio-temporal remote sensing images. First, 3D kernel is designed according to the structure of multi-spectral multi-temporal remote sensing data. Secondly, the 3D CNN framework with fine-tuned parameters is designed for training 3D crop samples and learning spatio-temporal discriminative representations, with the full crop growth cycles being preserved. In addition, we introduce an active learning strategy to the CNN model to improve labelling accuracy up to a required threshold with the most efficiency. Finally, experiments are carried out to test the advantage of the 3D CNN, in comparison to the two-dimensional (2D) CNN and other conventional methods. Our experiments show that the 3D CNN is especially suitable in characterizing the dynamics of crop growth and outperformed the other mainstream methods.},
DOI = {10.3390/rs10010075}
}



@Article{s18010156,
AUTHOR = {Li, Hongguang and Shi, Yang and Zhang, Baochang and Wang, Yufeng},
TITLE = {Superpixel-Based Feature for Aerial Image Scene Recognition},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {156},
URL = {https://www.mdpi.com/1424-8220/18/1/156},
ISSN = {1424-8220},
ABSTRACT = {Image scene recognition is a core technology for many aerial remote sensing applications. Different landforms are inputted as different scenes in aerial imaging, and all landform information is regarded as valuable for aerial image scene recognition. However, the conventional features of the Bag-of-Words model are designed using local points or other related information and thus are unable to fully describe landform areas. This limitation cannot be ignored when the aim is to ensure accurate aerial scene recognition. A novel superpixel-based feature is proposed in this study to characterize aerial image scenes. Then, based on the proposed feature, a scene recognition method of the Bag-of-Words model for aerial imaging is designed. The proposed superpixel-based feature that utilizes landform information establishes top-task superpixel extraction of landforms to bottom-task expression of feature vectors. This characterization technique comprises the following steps: simple linear iterative clustering based superpixel segmentation, adaptive filter bank construction, Lie group-based feature quantification, and visual saliency model-based feature weighting. Experiments of image scene recognition are carried out using real image data captured by an unmanned aerial vehicle (UAV). The recognition accuracy of the proposed superpixel-based feature is 95.1%, which is higher than those of scene recognition algorithms based on other local features.},
DOI = {10.3390/s18010156}
}



@Article{rs10010072,
AUTHOR = {Kim, Sungho and Song, Woo-Jin and Kim, So-Hyun},
TITLE = {Double Weight-Based SAR and Infrared Sensor Fusion for Automatic Ground Target Recognition with Deep Learning},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {72},
URL = {https://www.mdpi.com/2072-4292/10/1/72},
ISSN = {2072-4292},
ABSTRACT = {This paper presents a novel double weight-based synthetic aperture radar (SAR) and infrared (IR) sensor fusion method (DW-SIF) for automatic ground target recognition (ATR). IR-based ATR can provide accurate recognition because of its high image resolution but it is affected by the weather conditions. On the other hand, SAR-based ATR shows a low recognition rate due to the noisy low resolution but can provide consistent performance regardless of the weather conditions. The fusion of an active sensor (SAR) and a passive sensor (IR) can lead to upgraded performance. This paper proposes a doubly weighted neural network fusion scheme at the decision level. The first weight (   α   ) can measure the offline sensor confidence per target category based on the classification rate for an evaluation set. The second weight (   β   ) can measure the online sensor reliability based on the score distribution for a test target image. The LeNet architecture-based deep convolution network (14 layers) is used as an individual classifier. Doubly weighted sensor scores are fused by two types of fusion schemes, such as the sum-based linear fusion scheme (    α β    -sum) and neural network-based nonlinear fusion scheme (    α β    -NN). The experimental results confirmed the proposed linear fusion method (    α β    -sum) to have the best performance among the linear fusion schemes available (SAR-CNN, IR-CNN,    α   -sum,    β   -sum,     α β    -sum, and Bayesian fusion). In addition, the proposed nonlinear fusion method (    α β    -NN) showed superior target recognition performance to linear fusion on the OKTAL-SE-based synthetic database.},
DOI = {10.3390/rs10010072}
}



@Article{s18010225,
AUTHOR = {Qu, Yufu and Huang, Jianyu and Zhang, Xuan},
TITLE = {Rapid 3D Reconstruction for Image Sequence Acquired from UAV Camera},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {225},
URL = {https://www.mdpi.com/1424-8220/18/1/225},
ISSN = {1424-8220},
ABSTRACT = {In order to reconstruct three-dimensional (3D) structures from an image sequence captured by unmanned aerial vehicles’ camera (UAVs) and improve the processing speed, we propose a rapid 3D reconstruction method that is based on an image queue, considering the continuity and relevance of UAV camera images. The proposed approach first compresses the feature points of each image into three principal component points by using the principal component analysis method. In order to select the key images suitable for 3D reconstruction, the principal component points are used to estimate the interrelationships between images. Second, these key images are inserted into a fixed-length image queue. The positions and orientations of the images are calculated, and the 3D coordinates of the feature points are estimated using weighted bundle adjustment. With this structural information, the depth maps of these images can be calculated. Next, we update the image queue by deleting some of the old images and inserting some new images into the queue, and a structural calculation of all the images can be performed by repeating the previous steps. Finally, a dense 3D point cloud can be obtained using the depth–map fusion method. The experimental results indicate that when the texture of the images is complex and the number of images exceeds 100, the proposed method can improve the calculation speed by more than a factor of four with almost no loss of precision. Furthermore, as the number of images increases, the improvement in the calculation speed will become more noticeable.},
DOI = {10.3390/s18010225}
}



@Article{urbansci2010008,
AUTHOR = {Mahabir, Ron and Croitoru, Arie and Crooks, Andrew T. and Agouris, Peggy and Stefanidis, Anthony},
TITLE = {A Critical Review of High and Very High-Resolution Remote Sensing Approaches for Detecting and Mapping Slums: Trends, Challenges and Emerging Opportunities},
JOURNAL = {Urban Science},
VOLUME = {2},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {8},
URL = {https://www.mdpi.com/2413-8851/2/1/8},
ISSN = {2413-8851},
ABSTRACT = {Slums are a global urban challenge, with less developed countries being particularly impacted. To adequately detect and map them, data is needed on their location, spatial extent and evolution. High- and very high-resolution remote sensing imagery has emerged as an important source of data in this regard. The purpose of this paper is to critically review studies that have used such data to detect and map slums. Our analysis shows that while such studies have been increasing over time, they tend to be concentrated to a few geographical areas and often focus on the use of a single approach (e.g., image texture and object-based image analysis), thus limiting generalizability to understand slums, their population, and evolution within the global context. We argue that to develop a more comprehensive framework that can be used to detect and map slums, other emerging sourcing of geospatial data should be considered (e.g., volunteer geographic information) in conjunction with growing trends and advancements in technology (e.g., geosensor networks). Through such data integration and analysis we can then create a benchmark for determining the most suitable methods for mapping slums in a given locality, thus fostering the creation of new approaches to address this challenge.},
DOI = {10.3390/urbansci2010008}
}



@Article{ijgi7020039,
AUTHOR = {Feng, Yu and Sester, Monika},
TITLE = {Extraction of Pluvial Flood Relevant Volunteered Geographic Information (VGI) by Deep Learning from User Generated Texts and Photos},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {39},
URL = {https://www.mdpi.com/2220-9964/7/2/39},
ISSN = {2220-9964},
ABSTRACT = {In recent years, pluvial floods caused by extreme rainfall events have occurred frequently. Especially in urban areas, they lead to serious damages and endanger the citizens’ safety. Therefore, real-time information about such events is desirable. With the increasing popularity of social media platforms, such as Twitter or Instagram, information provided by voluntary users becomes a valuable source for emergency response. Many applications have been built for disaster detection and flood mapping using crowdsourcing. Most of the applications so far have merely used keyword filtering or classical language processing methods to identify disaster relevant documents based on user generated texts. As the reliability of social media information is often under criticism, the precision of information retrieval plays a significant role for further analyses. Thus, in this paper, high quality eyewitnesses of rainfall and flooding events are retrieved from social media by applying deep learning approaches on user generated texts and photos. Subsequently, events are detected through spatiotemporal clustering and visualized together with these high quality eyewitnesses in a web map application. Analyses and case studies are conducted during flooding events in Paris, London and Berlin.},
DOI = {10.3390/ijgi7020039}
}



@Article{drones2010007,
AUTHOR = {Mueller, Markus S. and Jutzi, Boris},
TITLE = {UAS Navigation with SqueezePoseNet—Accuracy Boosting for Pose Regression by Data Augmentation},
JOURNAL = {Drones},
VOLUME = {2},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {7},
URL = {https://www.mdpi.com/2504-446X/2/1/7},
ISSN = {2504-446X},
ABSTRACT = {The navigation of Unmanned Aerial Vehicles (UAVs) nowadays is mostly based on Global Navigation Satellite Systems (GNSSs). Drawbacks of satellite-based navigation are failures caused by occlusions or multi-path interferences. Therefore, alternative methods have been developed in recent years. Visual navigation methods such as Visual Odometry (VO) or visual Simultaneous Localization and Mapping (SLAM) aid global navigation solutions by closing trajectory gaps or performing loop closures. However, if the trajectory estimation is interrupted or not available, a re-localization is mandatory. Furthermore, the latest research has shown promising results on pose regression in 6 Degrees of Freedom (DoF) based on Convolutional Neural Networks (CNNs). Additionally, existing navigation methods can benefit from these networks. In this article, a method for GNSS-free and fast image-based pose regression by utilizing a small Convolutional Neural Network is presented. Therefore, a small CNN (SqueezePoseNet) is utilized, transfer learning is applied and the network is tuned for pose regression. Furthermore, recent drawbacks are overcome by applying data augmentation on a training dataset utilizing simulated images. Experiments with small CNNs show promising results for GNSS-free and fast localization compared to larger networks. By training a CNN with an extended data set including simulated images, the accuracy on pose regression is improved up to 61.7% for position and up to 76.0% for rotation compared to training on a standard not-augmented data set.},
DOI = {10.3390/drones2010007}
}



@Article{ijgi7020065,
AUTHOR = {Yang, Liping and MacEachren, Alan M. and Mitra, Prasenjit and Onorati, Teresa},
TITLE = {Visually-Enabled Active Deep Learning for (Geo) Text and Image Classification: A Review},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {65},
URL = {https://www.mdpi.com/2220-9964/7/2/65},
ISSN = {2220-9964},
ABSTRACT = {This paper investigates recent research on active learning for (geo) text and image classification, with an emphasis on methods that combine visual analytics and/or deep learning. Deep learning has attracted substantial attention across many domains of science and practice, because it can find intricate patterns in big data; but successful application of the methods requires a big set of labeled data. Active learning, which has the potential to address the data labeling challenge, has already had success in geospatial applications such as trajectory classification from movement data and (geo) text and image classification. This review is intended to be particularly relevant for extension of these methods to GISience, to support work in domains such as geographic information retrieval from text and image repositories, interpretation of spatial language, and related geo-semantics challenges. Specifically, to provide a structure for leveraging recent advances, we group the relevant work into five categories: active learning, visual analytics, active learning with visual analytics, active deep learning, plus GIScience and Remote Sensing (RS) using active learning and active deep learning. Each category is exemplified by recent influential work. Based on this framing and our systematic review of key research, we then discuss some of the main challenges of integrating active learning with visual analytics and deep learning, and point out research opportunities from technical and application perspectives—for application-based opportunities, with emphasis on those that address big data with geospatial components.},
DOI = {10.3390/ijgi7020065}
}



@Article{rs10020351,
AUTHOR = {Bashmal, Laila and Bazi, Yakoub and AlHichri, Haikel and AlRahhal, Mohamad M. and Ammour, Nassim and Alajlan, Naif},
TITLE = {Siamese-GAN: Learning Invariant Representations for Aerial Vehicle Image Categorization},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {2},
ARTICLE-NUMBER = {351},
URL = {https://www.mdpi.com/2072-4292/10/2/351},
ISSN = {2072-4292},
ABSTRACT = {In this paper, we present a new algorithm for cross-domain classification in aerial vehicle images based on generative adversarial networks (GANs). The proposed method, called Siamese-GAN, learns invariant feature representations for both labeled and unlabeled images coming from two different domains. To this end, we train in an adversarial manner a Siamese encoder–decoder architecture coupled with a discriminator network. The encoder–decoder network has the task of matching the distributions of both domains in a shared space regularized by the reconstruction ability, while the discriminator seeks to distinguish between them. After this phase, we feed the resulting encoded labeled and unlabeled features to another network composed of two fully-connected layers for training and classification, respectively. Experiments on several cross-domain datasets composed of extremely high resolution (EHR) images acquired by manned/unmanned aerial vehicles (MAV/UAV) over the cities of Vaihingen, Toronto, Potsdam, and Trento are reported and discussed.},
DOI = {10.3390/rs10020351}
}



@Article{s18030698,
AUTHOR = {Guo, Qiangliang and Xiao, Jin and Hu, Xiaoguang},
TITLE = {New Keypoint Matching Method Using Local Convolutional Features for Power Transmission Line Icing Monitoring},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {698},
URL = {https://www.mdpi.com/1424-8220/18/3/698},
ISSN = {1424-8220},
ABSTRACT = {Power transmission line icing (PTLI) problems, which cause tremendous damage to the power grids, has drawn much attention. Existing three-dimensional measurement methods based on binocular stereo vision was recently introduced to measure the ice thickness in PTLI, but failed to meet requirements of practical applications due to inefficient keypoint matching in the complex PTLI scene. In this paper, a new keypoint matching method is proposed based on the local multi-layer convolutional neural network (CNN) features, termed Local Convolutional Features (LCFs). LCFs are deployed to extract more discriminative features than the conventional CNNs. Particularly in LCFs, a multi-layer features fusion scheme is exploited to boost the matching performance. Together with a location constraint method, the correspondence of neighboring keypoints is further refined. Our approach achieves 1.5%, 5.3%, 13.1%, 27.3% improvement in the average matching precision compared with SIFT, SURF, ORB and MatchNet on the public Middlebury dataset, and the measurement accuracy of ice thickness can reach 90.9% compared with manual measurement on the collected PTLI dataset.},
DOI = {10.3390/s18030698}
}



@Article{s18030712,
AUTHOR = {Zhao, Yi and Ma, Jiale and Li, Xiaohui and Zhang, Jie},
TITLE = {Saliency Detection and Deep Learning-Based Wildfire Identification in UAV Imagery},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {712},
URL = {https://www.mdpi.com/1424-8220/18/3/712},
ISSN = {1424-8220},
ABSTRACT = {An unmanned aerial vehicle (UAV) equipped with global positioning systems (GPS) can provide direct georeferenced imagery, mapping an area with high resolution. So far, the major difficulty in wildfire image classification is the lack of unified identification marks, the fire features of color, shape, texture (smoke, flame, or both) and background can vary significantly from one scene to another. Deep learning (e.g., DCNN for Deep Convolutional Neural Network) is very effective in high-level feature learning, however, a substantial amount of training images dataset is obligatory in optimizing its weights value and coefficients. In this work, we proposed a new saliency detection algorithm for fast location and segmentation of core fire area in aerial images. As the proposed method can effectively avoid feature loss caused by direct resizing; it is used in data augmentation and formation of a standard fire image dataset ‘UAV_Fire’. A 15-layered self-learning DCNN architecture named ‘Fire_Net’ is then presented as a self-learning fire feature exactor and classifier. We evaluated different architectures and several key parameters (drop out ratio, batch size, etc.) of the DCNN model regarding its validation accuracy. The proposed architecture outperformed previous methods by achieving an overall accuracy of 98%. Furthermore, ‘Fire_Net’ guarantied an average processing speed of 41.5 ms per image for real-time wildfire inspection. To demonstrate its practical utility, Fire_Net is tested on 40 sampled images in wildfire news reports and all of them have been accurately identified.},
DOI = {10.3390/s18030712}
}



@Article{s18030737,
AUTHOR = {Cao, Xiaoguang and Wang, Peng and Meng, Cai and Bai, Xiangzhi and Gong, Guoping and Liu, Miaoming and Qi, Jun},
TITLE = {Region Based CNN for Foreign Object Debris Detection on Airfield Pavement},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {737},
URL = {https://www.mdpi.com/1424-8220/18/3/737},
ISSN = {1424-8220},
ABSTRACT = {In this paper, a novel algorithm based on convolutional neural network (CNN) is proposed to detect foreign object debris (FOD) based on optical imaging sensors. It contains two modules, the improved region proposal network (RPN) and spatial transformer network (STN) based CNN classifier. In the improved RPN, some extra select rules are designed and deployed to generate high quality candidates with fewer numbers. Moreover, the efficiency of CNN detector is significantly improved by introducing STN layer. Compared to faster R-CNN and single shot multiBox detector (SSD), the proposed algorithm achieves better result for FOD detection on airfield pavement in the experiment.},
DOI = {10.3390/s18030737}
}



@Article{rs10030396,
AUTHOR = {Li, Jiaojiao and Xi, Bobo and Li, Yunsong and Du, Qian and Wang, Keyan},
TITLE = {Hyperspectral Classification Based on Texture Feature Enhancement and Deep Belief Networks},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {396},
URL = {https://www.mdpi.com/2072-4292/10/3/396},
ISSN = {2072-4292},
ABSTRACT = {With success of Deep Belief Networks (DBNs) in computer vision, DBN has attracted great attention in hyperspectral classification. Many deep learning based algorithms have been focused on deep feature extraction for classification improvement. Multi-features, such as texture feature, are widely utilized in classification process to enhance classification accuracy greatly. In this paper, a novel hyperspectral classification framework based on an optimal DBN and a novel texture feature enhancement (TFE) is proposed. Through band grouping, sample band selection and guided filtering, the texture features of hyperspectral data are improved. After TFE, the optimal DBN is employed on the hyperspectral reconstructed data for feature extraction and classification. Experimental results demonstrate that the proposed classification framework outperforms some state-of-the-art classification algorithms, and it can achieve outstanding hyperspectral classification performance. Furthermore, our proposed TFE method can play a significant role in improving classification accuracy.},
DOI = {10.3390/rs10030396}
}



@Article{app8030379,
AUTHOR = {Jin, Xue-Bo and Su, Ting-Li and Kong, Jian-Lei and Bai, Yu-Ting and Miao, Bei-Bei and Dou, Chao},
TITLE = {State-of-the-Art Mobile Intelligence: Enabling Robots to Move Like Humans by Estimating Mobility with Artificial Intelligence},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {379},
URL = {https://www.mdpi.com/2076-3417/8/3/379},
ISSN = {2076-3417},
ABSTRACT = {Mobility is a significant robotic task. It is the most important function when robotics is applied to domains such as autonomous cars, home service robots, and autonomous underwater vehicles. Despite extensive research on this topic, robots still suffer from difficulties when moving in complex environments, especially in practical applications. Therefore, the ability to have enough intelligence while moving is a key issue for the success of robots. Researchers have proposed a variety of methods and algorithms, including navigation and tracking. To help readers swiftly understand the recent advances in methodology and algorithms for robot movement, we present this survey, which provides a detailed review of the existing methods of navigation and tracking. In particular, this survey features a relation-based architecture that enables readers to easily grasp the key points of mobile intelligence. We first outline the key problems in robot systems and point out the relationship among robotics, navigation, and tracking. We then illustrate navigation using different sensors and the fusion methods and detail the state estimation and tracking models for target maneuvering. Finally, we address several issues of deep learning as well as the mobile intelligence of robots as suggested future research topics. The contributions of this survey are threefold. First, we review the literature of navigation according to the applied sensors and fusion method. Second, we detail the models for target maneuvering and the existing tracking based on estimation, such as the Kalman filter and its series developed form, according to their model-construction mechanisms: linear, nonlinear, and non-Gaussian white noise. Third, we illustrate the artificial intelligence approach—especially deep learning methods—and discuss its combination with the estimation method.},
DOI = {10.3390/app8030379}
}



@Article{drones2010009,
AUTHOR = {Palazzolo, Emanuele and Stachniss, Cyrill},
TITLE = {Effective Exploration for MAVs Based on the Expected Information Gain},
JOURNAL = {Drones},
VOLUME = {2},
YEAR = {2018},
NUMBER = {1},
ARTICLE-NUMBER = {9},
URL = {https://www.mdpi.com/2504-446X/2/1/9},
ISSN = {2504-446X},
ABSTRACT = {Micro aerial vehicles (MAVs) are an excellent platform for autonomous exploration. Most MAVs rely mainly on cameras for buliding a map of the 3D environment. Therefore, vision-based MAVs require an efficient exploration algorithm to select viewpoints that provide informative measurements. In this paper, we propose an exploration approach that selects in real time the next-best-view that maximizes the expected information gain of new measurements. In addition, we take into account the cost of reaching a new viewpoint in terms of distance and predictability of the flight path for a human observer. Finally, our approach selects a path that reduces the risk of crashes when the expected battery life comes to an end, while still maximizing the information gain in the process. We implemented and thoroughly tested our approach and the experiments show that it offers an improved performance compared to other state-of-the-art algorithms in terms of precision of the reconstruction, execution time, and smoothness of the path.},
DOI = {10.3390/drones2010009}
}



@Article{rs10030457,
AUTHOR = {Liu, Tao and Abd-Elrahman, Amr},
TITLE = {An Object-Based Image Analysis Method for Enhancing Classification of Land Covers Using Fully Convolutional Networks and Multi-View Images of Small Unmanned Aerial System},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {457},
URL = {https://www.mdpi.com/2072-4292/10/3/457},
ISSN = {2072-4292},
ABSTRACT = {Fully Convolutional Networks (FCN) has shown better performance than other classifiers like Random Forest (RF), Support Vector Machine (SVM) and patch-based Deep Convolutional Neural Network (DCNN), for object-based classification using orthoimage only in previous studies; however, for further improving deep learning algorithm performance, multi-view data should be considered for training data enrichment, which has not been investigated for FCN. The present study developed a novel OBIA classification using FCN and multi-view data extracted from small Unmanned Aerial System (UAS) for mapping landcovers. Specifically, this study proposed three methods to automatically generate multi-view training samples from orthoimage training datasets to conduct multi-view object-based classification using FCN, and compared their performances with each other and also with RF, SVM, and DCNN classifiers. The first method does not consider the object surrounding information, while the other two utilized object context information. We demonstrated that all the three versions of FCN multi-view object-based classification outperformed their counterparts utilizing orthoimage data only. Furthermore, the results also showed that when multi-view training samples were prepared with consideration of object surroundings, FCN trained with these samples gave much better accuracy than FCN classification trained without context information. Similar accuracies were achieved from the two methods utilizing object surrounding information, although sample preparation was conducted using two different ways. When comparing FCN with RF, SVM, DCNN implies that FCN generally produced better accuracy than the other classifiers, regardless of using orthoimage or multi-view data.},
DOI = {10.3390/rs10030457}
}



@Article{su10030816,
AUTHOR = {Sung, Yunsick and Jin, Yong and Kwak, Jeonghoon and Lee, Sang-Geol and Cho, Kyungeun},
TITLE = {Advanced Camera Image Cropping Approach for CNN-Based End-to-End Controls on Sustainable Computing},
JOURNAL = {Sustainability},
VOLUME = {10},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {816},
URL = {https://www.mdpi.com/2071-1050/10/3/816},
ISSN = {2071-1050},
ABSTRACT = {Recent research on deep learning has been applied to a diversity of fields. In particular, numerous studies have been conducted on self-driving vehicles using end-to-end approaches based on images captured by a single camera. End-to-end controls learn the output vectors of output devices directly from the input vectors of available input devices. In other words, an end-to-end approach learns not by analyzing the meaning of input vectors, but by extracting optimal output vectors based on input vectors. Generally, when end-to-end control is applied to self-driving vehicles, the steering wheel and pedals are controlled autonomously by learning from the images captured by a camera. However, high-resolution images captured from a car cannot be directly used as inputs to Convolutional Neural Networks (CNNs) owing to memory limitations; the image size needs to be efficiently reduced. Therefore, it is necessary to extract features from captured images automatically and to generate input images by merging the parts of the images that contain the extracted features. This paper proposes a learning method for end-to-end control that generates input images for CNNs by extracting road parts from input images, identifying the edges of the extracted road parts, and merging the parts of the images that contain the detected edges. In addition, a CNN model for end-to-end control is introduced. Experiments involving the Open Racing Car Simulator (TORCS), a sustainable computing environment for cars, confirmed the effectiveness of the proposed method for self-driving by comparing the accumulated difference in the angle of the steering wheel in the images generated by it with those of resized images containing the entire captured area and cropped images containing only a part of the captured area. The results showed that the proposed method reduced the accumulated difference by 0.839% and 0.850% compared to those yielded by the resized images and cropped images, respectively.},
DOI = {10.3390/su10030816}
}



@Article{e20030198,
AUTHOR = {Zhang, Zhen and Li, Yibing and Jin, Shanshan and Zhang, Zhaoyue and Wang, Hui and Qi, Lin and Zhou, Ruolin},
TITLE = {Modulation Signal Recognition Based on Information Entropy and Ensemble Learning},
JOURNAL = {Entropy},
VOLUME = {20},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {198},
URL = {https://www.mdpi.com/1099-4300/20/3/198},
ISSN = {1099-4300},
ABSTRACT = {In this paper, information entropy and ensemble learning based signal recognition theory and algorithms have been proposed. We have extracted 16 kinds of entropy features out of 9 types of modulated signals. The types of information entropy used are numerous, including Rényi entropy and energy entropy based on S Transform and Generalized S Transform. We have used three feature selection algorithms, including sequence forward selection (SFS), sequence forward floating selection (SFFS) and RELIEF-F to select the optimal feature subset from 16 entropy features. We use five classifiers, including k-nearest neighbor (KNN), support vector machine (SVM), Adaboost, Gradient Boosting Decision Tree (GBDT) and eXtreme Gradient Boosting (XGBoost) to classify the original feature set and the feature subsets selected by different feature selection algorithms. The simulation results show that the feature subsets selected by SFS and SFFS algorithms are the best, with a 48% increase in recognition rate over the original feature set when using KNN classifier and a 34% increase when using SVM classifier. For the other three classifiers, the original feature set can achieve the best recognition performance. The XGBoost classifier has the best recognition performance, the overall recognition rate is 97.74% and the recognition rate can reach 82% when the signal to noise ratio (SNR) is −10 dB.},
DOI = {10.3390/e20030198}
}



@Article{s18030924,
AUTHOR = {Zhang, Duona and Ding, Wenrui and Zhang, Baochang and Xie, Chunyu and Li, Hongguang and Liu, Chunhui and Han, Jungong},
TITLE = {Automatic Modulation Classification Based on Deep Learning for Unmanned Aerial Vehicles},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {924},
URL = {https://www.mdpi.com/1424-8220/18/3/924},
ISSN = {1424-8220},
ABSTRACT = {Deep learning has recently attracted much attention due to its excellent performance in processing audio, image, and video data. However, few studies are devoted to the field of automatic modulation classification (AMC). It is one of the most well-known research topics in communication signal recognition and remains challenging for traditional methods due to complex disturbance from other sources. This paper proposes a heterogeneous deep model fusion (HDMF) method to solve the problem in a unified framework. The contributions include the following: (1) a convolutional neural network (CNN) and long short-term memory (LSTM) are combined by two different ways without prior knowledge involved; (2) a large database, including eleven types of single-carrier modulation signals with various noises as well as a fading channel, is collected with various signal-to-noise ratios (SNRs) based on a real geographical environment; and (3) experimental results demonstrate that HDMF is very capable of coping with the AMC problem, and achieves much better performance when compared with the independent network.},
DOI = {10.3390/s18030924}
}



@Article{s18040944,
AUTHOR = {Sandino, Juan and Pegg, Geoff and Gonzalez, Felipe and Smith, Grant},
TITLE = {Aerial Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral Sensors, and Artificial Intelligence},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {944},
URL = {https://www.mdpi.com/1424-8220/18/4/944},
ISSN = {1424-8220},
ABSTRACT = {The environmental and economic impacts of exotic fungal species on natural and plantation forests have been historically catastrophic. Recorded surveillance and control actions are challenging because they are costly, time-consuming, and hazardous in remote areas. Prolonged periods of testing and observation of site-based tests have limitations in verifying the rapid proliferation of exotic pathogens and deterioration rates in hosts. Recent remote sensing approaches have offered fast, broad-scale, and affordable surveys as well as additional indicators that can complement on-ground tests. This paper proposes a framework that consolidates site-based insights and remote sensing capabilities to detect and segment deteriorations by fungal pathogens in natural and plantation forests. This approach is illustrated with an experimentation case of myrtle rust (Austropuccinia psidii) on paperbark tea trees (Melaleuca quinquenervia) in New South Wales (NSW), Australia. The method integrates unmanned aerial vehicles (UAVs), hyperspectral image sensors, and data processing algorithms using machine learning. Imagery is acquired using a Headwall Nano-Hyperspec     ®     camera, orthorectified in Headwall SpectralView     ®    , and processed in Python programming language using eXtreme Gradient Boosting (XGBoost), Geospatial Data Abstraction Library (GDAL), and Scikit-learn third-party libraries. In total, 11,385 samples were extracted and labelled into five classes: two classes for deterioration status and three classes for background objects. Insights reveal individual detection rates of 95% for healthy trees, 97% for deteriorated trees, and a global multiclass detection rate of 97%. The methodology is versatile to be applied to additional datasets taken with different image sensors, and the processing of large datasets with freeware tools.},
DOI = {10.3390/s18040944}
}



@Article{rs10040511,
AUTHOR = {Gallego, Antonio-Javier and Pertusa, Antonio and Gil, Pablo},
TITLE = {Automatic Ship Classification from Optical Aerial Images with Convolutional Neural Networks},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {511},
URL = {https://www.mdpi.com/2072-4292/10/4/511},
ISSN = {2072-4292},
ABSTRACT = {The automatic classification of ships from aerial images is a considerable challenge. Previous works have usually applied image processing and computer vision techniques to extract meaningful features from visible spectrum images in order to use them as the input for traditional supervised classifiers. We present a method for determining if an aerial image of visible spectrum contains a ship or not. The proposed architecture is based on Convolutional Neural Networks (CNN), and it combines neural codes extracted from a CNN with a k-Nearest Neighbor method so as to improve performance. The kNN results are compared to those obtained with the CNN Softmax output. Several CNN models have been configured and evaluated in order to seek the best hyperparameters, and the most suitable setting for this task was found by using transfer learning at different levels. A new dataset (named MASATI) composed of aerial imagery with more than 6000 samples has also been created to train and evaluate our architecture. The experimentation shows a success rate of over 99% for our approach, in contrast with the 79% obtained with traditional methods in classification of ship images, also outperforming other methods based on CNNs. A dataset of images (MWPU VHR-10) used in previous works was additionally used to evaluate the proposed approach. Our best setup achieves a success ratio of 86% with these data, significantly outperforming previous state-of-the-art ship classification methods.},
DOI = {10.3390/rs10040511}
}



@Article{s18041000,
AUTHOR = {Kong, Xiangxiong and Li, Jian},
TITLE = {Image Registration-Based Bolt Loosening Detection of Steel Joints},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {1000},
URL = {https://www.mdpi.com/1424-8220/18/4/1000},
ISSN = {1424-8220},
ABSTRACT = {Self-loosening of bolts caused by repetitive loads and vibrations is one of the common defects that can weaken the structural integrity of bolted steel joints in civil structures. Many existing approaches for detecting loosening bolts are based on physical sensors and, hence, require extensive sensor deployment, which limit their abilities to cost-effectively detect loosened bolts in a large number of steel joints. Recently, computer vision-based structural health monitoring (SHM) technologies have demonstrated great potential for damage detection due to the benefits of being low cost, easy to deploy, and contactless. In this study, we propose a vision-based non-contact bolt loosening detection method that uses a consumer-grade digital camera. Two images of the monitored steel joint are first collected during different inspection periods and then aligned through two image registration processes. If the bolt experiences rotation between inspections, it will introduce differential features in the registration errors, serving as a good indicator for bolt loosening detection. The performance and robustness of this approach have been validated through a series of experimental investigations using three laboratory setups including a gusset plate on a cross frame, a column flange, and a girder web. The bolt loosening detection results are presented for easy interpretation such that informed decisions can be made about the detected loosened bolts.},
DOI = {10.3390/s18041000}
}



@Article{rs10040527,
AUTHOR = {Zhu, Xiaolin and Cai, Fangyi and Tian, Jiaqi and Williams, Trecia Kay-Ann},
TITLE = {Spatiotemporal Fusion of Multisource Remote Sensing Data: Literature Survey, Taxonomy, Principles, Applications, and Future Directions},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {527},
URL = {https://www.mdpi.com/2072-4292/10/4/527},
ISSN = {2072-4292},
ABSTRACT = {Satellite time series with high spatial resolution is critical for monitoring land surface dynamics in heterogeneous landscapes. Although remote sensing technologies have experienced rapid development in recent years, data acquired from a single satellite sensor are often unable to satisfy our demand. As a result, integrated use of data from different sensors has become increasingly popular in the past decade. Many spatiotemporal data fusion methods have been developed to produce synthesized images with both high spatial and temporal resolutions from two types of satellite images, frequent coarse-resolution images, and sparse fine-resolution images. These methods were designed based on different principles and strategies, and therefore show different strengths and limitations. This diversity brings difficulties for users to choose an appropriate method for their specific applications and data sets. To this end, this review paper investigates literature on current spatiotemporal data fusion methods, categorizes existing methods, discusses the principal laws underlying these methods, summarizes their potential applications, and proposes possible directions for future studies in this field.},
DOI = {10.3390/rs10040527}
}



@Article{rs10040620,
AUTHOR = {Cardim, Guilherme Pina and Silva, Erivaldo Antônio da and Dias, Mauricio Araújo and Bravo, Ignácio and Gardel, Alfredo},
TITLE = {Statistical Evaluation and Analysis of Road Extraction Methodologies Using a Unique Dataset from Remote Sensing},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {620},
URL = {https://www.mdpi.com/2072-4292/10/4/620},
ISSN = {2072-4292},
ABSTRACT = {In the scientific literature, multiple studies address the application of road extraction methodologies to a particular cartographic dataset. However, it is difficult for any study to perform a more reliable comparison among road extraction methodologies when their results come from different cartographic datasets. Therefore, aiming to enable a more reliable comparison among different road extraction methodologies from the scientific literature, this study proposed a statistical evaluation and analysis of road extraction methodologies using a common image dataset. To achieve this goal, we setup a dataset containing remote sensing images of three different road types, highways, cities network and rural paths, and a group of images from the ISPRS (International Society for Photogrammetry and Remote Sensing) dataset. Furthermore, three road extraction methodologies were selected from the literature, in accordance with their availability, to be processed and evaluated using well-known statistical metrics. The achieved results are encouraging and indicate that the proposed statistical evaluation and analysis can allow researchers to evaluate and compare road extraction methodologies using this common dataset extracting similar characteristics to obtain a more reliable comparison among them.},
DOI = {10.3390/rs10040620}
}



@Article{rs10040624,
AUTHOR = {Zhuo, Xiangyu and Fraundorfer, Friedrich and Kurz, Franz and Reinartz, Peter},
TITLE = {Optimization of OpenStreetMap Building Footprints Based on Semantic Information of Oblique UAV Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {624},
URL = {https://www.mdpi.com/2072-4292/10/4/624},
ISSN = {2072-4292},
ABSTRACT = {Building footprint information is vital for 3D building modeling. Traditionally, in remote sensing, building footprints are extracted and delineated from aerial imagery and/or LiDAR point cloud. Taking a different approach, this paper is dedicated to the optimization of OpenStreetMap (OSM) building footprints exploiting the contour information, which is derived from deep learning-based semantic segmentation of oblique images acquired by the Unmanned Aerial Vehicle (UAV). First, a simplified 3D building model of Level of Detail 1 (LoD 1) is initialized using the footprint information from OSM and the elevation information from Digital Surface Model (DSM). In parallel, a deep neural network for pixel-wise semantic image segmentation is trained in order to extract the building boundaries as contour evidence. Subsequently, an optimization integrating the contour evidence from multi-view images as a constraint results in a refined 3D building model with optimized footprints and height. Our method is leveraged to optimize OSM building footprints for four datasets with different building types, demonstrating robust performance for both individual buildings and multiple buildings regardless of image resolution. Finally, we compare our result with reference data from German Authority Topographic-Cartographic Information System (ATKIS). Quantitative and qualitative evaluations reveal that the original OSM building footprints have large offset, but can be significantly improved from meter level to decimeter level after optimization.},
DOI = {10.3390/rs10040624}
}



@Article{agronomy8040057,
AUTHOR = {Tripodi, Pasquale and Massa, Daniele and Venezia, Accursio and Cardi, Teodoro},
TITLE = {Sensing Technologies for Precision Phenotyping in Vegetable Crops: Current Status and Future Challenges},
JOURNAL = {Agronomy},
VOLUME = {8},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {57},
URL = {https://www.mdpi.com/2073-4395/8/4/57},
ISSN = {2073-4395},
ABSTRACT = {Increasing the ability to investigate plant functions and structure through non-invasive methods with high accuracy has become a major target in plant breeding and precision agriculture. Emerging approaches in plant phenotyping play a key role in unraveling quantitative traits responsible for growth, production, quality, and resistance to various stresses. Beyond fully automatic phenotyping systems, several promising technologies can help accurately characterize a wide range of plant traits at affordable costs and with high-throughput. In this review, we revisit the principles of proximal and remote sensing, describing the application of non-invasive devices for precision phenotyping applied to the protected horticulture. Potentiality and constraints of big data management and integration with &ldquo;omics&rdquo; disciplines will also be discussed.},
DOI = {10.3390/agronomy8040057}
}



@Article{rs10040649,
AUTHOR = {Ayrey, Elias and Hayes, Daniel J.},
TITLE = {The Use of Three-Dimensional Convolutional Neural Networks to Interpret LiDAR for Forest Inventory},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {649},
URL = {https://www.mdpi.com/2072-4292/10/4/649},
ISSN = {2072-4292},
ABSTRACT = {As light detection and ranging (LiDAR) technology becomes more available, it has become common to use these datasets to generate remotely sensed forest inventories across landscapes. Traditional methods for generating these inventories employ the use of height and proportion metrics to measure LiDAR returns and relate these back to field data using predictive models. Here, we employ a three-dimensional convolutional neural network (CNN), a deep learning technique that scans the LiDAR data and automatically generates useful features for predicting forest attributes. We test the accuracy in estimating forest attributes using the three-dimensional implementations of different CNN models commonly used in the field of image recognition. Using the best performing model architecture, we compared CNN performance to models developed using traditional height metrics. The results of this comparison show that CNNs produced 12% less prediction error when estimating biomass, 6% less in estimating tree count, and 2% less when estimating the percentage of needleleaf trees. We conclude that using CNNs can be a more accurate means of interpreting LiDAR data for forest inventories compared to standard approaches.},
DOI = {10.3390/rs10040649}
}



@Article{s18041292,
AUTHOR = {Guo, Siqiu and Zhang, Tao and Song, Yulong and Qian, Feng},
TITLE = {Color Feature-Based Object Tracking through Particle Swarm Optimization with Improved Inertia Weight},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {1292},
URL = {https://www.mdpi.com/1424-8220/18/4/1292},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a particle swarm tracking algorithm with improved inertia weight based on color features. The weighted color histogram is used as the target feature to reduce the contribution of target edge pixels in the target feature, which makes the algorithm insensitive to the target non-rigid deformation, scale variation, and rotation. Meanwhile, the influence of partial obstruction on the description of target features is reduced. The particle swarm optimization algorithm can complete the multi-peak search, which can cope well with the object occlusion tracking problem. This means that the target is located precisely where the similarity function appears multi-peak. When the particle swarm optimization algorithm is applied to the object tracking, the inertia weight adjustment mechanism has some limitations. This paper presents an improved method. The concept of particle maturity is introduced to improve the inertia weight adjustment mechanism, which could adjust the inertia weight in time according to the different states of each particle in each generation. Experimental results show that our algorithm achieves state-of-the-art performance in a wide range of scenarios.},
DOI = {10.3390/s18041292}
}



@Article{rs10040652,
AUTHOR = {Zhang, Yongjun and Wang, Xiang and Xie, Xunwei and Li, Yansheng},
TITLE = {Salient Object Detection via Recursive Sparse Representation},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {652},
URL = {https://www.mdpi.com/2072-4292/10/4/652},
ISSN = {2072-4292},
ABSTRACT = {Object-level saliency detection is an attractive research field which is useful for many content-based computer vision and remote-sensing tasks. This paper introduces an efficient unsupervised approach to salient object detection from the perspective of recursive sparse representation. The reconstruction error determined by foreground and background dictionaries other than common local and global contrasts is used as the saliency indication, by which the shortcomings of the object integrity can be effectively improved. The proposed method consists of the following four steps: (1) regional feature extraction; (2) background and foreground dictionaries extraction according to the initial saliency map and image boundary constraints; (3) sparse representation and saliency measurement; and (4) recursive processing with a current saliency map updating the initial saliency map in step 2 and repeating step 3. This paper also presents the experimental results of the proposed method compared with seven state-of-the-art saliency detection methods using three benchmark datasets, as well as some satellite and unmanned aerial vehicle remote-sensing images, which confirmed that the proposed method was more effective than current methods and could achieve more favorable performance in the detection of multiple objects as well as maintaining the integrity of the object area.},
DOI = {10.3390/rs10040652}
}



@Article{rs10050661,
AUTHOR = {Krylov, Vladimir A. and Kenny, Eamonn and Dahyot, Rozenn},
TITLE = {Automatic Discovery and Geotagging of Objects from Street View Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {661},
URL = {https://www.mdpi.com/2072-4292/10/5/661},
ISSN = {2072-4292},
ABSTRACT = {Many applications, such as autonomous navigation, urban planning, and asset monitoring, rely on the availability of accurate information about objects and their geolocations. In this paper, we propose the automatic detection and computation of the coordinates of recurring stationary objects of interest using street view imagery. Our processing pipeline relies on two fully convolutional neural networks: the first segments objects in the images, while the second estimates their distance from the camera. To geolocate all the detected objects coherently we propose a novel custom Markov random field model to estimate the objects&rsquo; geolocation. The novelty of the resulting pipeline is the combined use of monocular depth estimation and triangulation to enable automatic mapping of complex scenes with the simultaneous presence of multiple, visually similar objects of interest. We validate experimentally the effectiveness of our approach on two object classes: traffic lights and telegraph poles. The experiments report high object recall rates and position precision of approximately 2 m, which is approaching the precision of single-frequency GPS receivers.},
DOI = {10.3390/rs10050661}
}



@Article{agriculture8050064,
AUTHOR = {Al Shidi, Rashid H. and Kumar, Lalit and Al-Khatri, Salim A. H. and Albahri, Malik M. and Alaufi, Mohammed S.},
TITLE = {Relationship of Date Palm Tree Density to Dubas Bug Ommatissus lybicus Infestation in Omani Orchards},
JOURNAL = {Agriculture},
VOLUME = {8},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {64},
URL = {https://www.mdpi.com/2077-0472/8/5/64},
ISSN = {2077-0472},
ABSTRACT = {Date palm trees, Phoenix dactylifera, are the primary crop in Oman. Most date palm cultivation is under the traditional agricultural system. The plants are usually under dense planting, which makes them prone to pest infestation. The main pest attacking date palm crops in Oman is the Dubas bug Ommatissus lybicus. This study integrated modern technology, remote sensing and geographic information systems to determine the number of date palm trees in traditional agriculture locations to find the relationship between date palm tree density and O. lybicus infestation. A local maxima method for tree identification was used to determine the number of date palm trees from high spatial resolution satellite imagery captured by WorldView-3 satellite. Window scale sizes of 3, 5 and 7 m were tested and the results showed that the best window size for date palm trees number detection was 7 m, with an overall estimation accuracy 88.2%. Global regression ordinary least square (OLS) and local geographic weighted regression (GWR) were used to test the relationship between infestation intensity and tree density. The GWR model showed a good positive significant relationship between infestation and tree density in the spring season with R2 = 0.59 and medium positive significant relationship in the autumn season with R2 = 0.30. In contrast, the OLS model results showed a weak positive significant relationship in the spring season with R2 = 0.02, p &lt; 0.05 and insignificant relationship in the autumn season with R2 = 0.01, p &gt; 0.05. The results indicated that there was a geographic effect on the infestation of O. lybicus, which had a greater impact on infestation severity, and that the impact of tree density was higher in the spring season than in autumn season.},
DOI = {10.3390/agriculture8050064}
}



@Article{rs10050706,
AUTHOR = {Moy de Vitry, Matthew and Schindler, Konrad and Rieckermann, Jörg and Leitão, João P.},
TITLE = {Sewer Inlet Localization in UAV Image Clouds: Improving Performance with Multiview Detection},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {706},
URL = {https://www.mdpi.com/2072-4292/10/5/706},
ISSN = {2072-4292},
ABSTRACT = {Sewer and drainage infrastructure are often not as well catalogued as they should be, considering the immense investment they represent. In this work, we present a fully automatic framework for localizing sewer inlets from image clouds captured from an unmanned aerial vehicle (UAV). The framework exploits the high image overlap of UAV imaging surveys with a multiview approach to improve detection performance. The framework uses a Viola–Jones classifier trained to detect sewer inlets in aerial images with a ground sampling distance of 3–3.5 cm/pixel. The detections are then projected into three-dimensional space where they are clustered and reclassified to discard false positives. The method is evaluated by cross-validating results from an image cloud of 252 UAV images captured over a 0.57-km2 study area with 228 sewer inlets. Compared to an equivalent single-view detector, the multiview approach improves both recall and precision, increasing average precision from 0.65 to 0.73. The source code and case study data are publicly available for reuse.},
DOI = {10.3390/rs10050706}
}



@Article{s18051427,
AUTHOR = {Shamwell, E. Jared and Nothwang, William D. and Perlis, Donald},
TITLE = {An Embodied Multi-Sensor Fusion Approach to Visual Motion Estimation Using Unsupervised Deep Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1427},
URL = {https://www.mdpi.com/1424-8220/18/5/1427},
ISSN = {1424-8220},
ABSTRACT = {Aimed at improving size, weight, and power (SWaP)-constrained robotic vision-aided state estimation, we describe our unsupervised, deep convolutional-deconvolutional sensor fusion network, Multi-Hypothesis DeepEfference (MHDE). MHDE learns to intelligently combine noisy heterogeneous sensor data to predict several probable hypotheses for the dense, pixel-level correspondence between a source image and an unseen target image. We show how our multi-hypothesis formulation provides increased robustness against dynamic, heteroscedastic sensor and motion noise by computing hypothesis image mappings and predictions at 76&ndash;357 Hz depending on the number of hypotheses being generated. MHDE fuses noisy, heterogeneous sensory inputs using two parallel, inter-connected architectural pathways and n (1&ndash;20 in this work) multi-hypothesis generating sub-pathways to produce n global correspondence estimates between a source and a target image. We evaluated MHDE on the KITTI Odometry dataset and benchmarked it against the vision-only DeepMatching and Deformable Spatial Pyramids algorithms and were able to demonstrate a significant runtime decrease and a performance increase compared to the next-best performing method.},
DOI = {10.3390/s18051427}
}



@Article{geosciences8050165,
AUTHOR = {Yu, Manzhu and Yang, Chaowei and Li, Yun},
TITLE = {Big Data in Natural Disaster Management: A Review},
JOURNAL = {Geosciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {165},
URL = {https://www.mdpi.com/2076-3263/8/5/165},
ISSN = {2076-3263},
ABSTRACT = {Undoubtedly, the age of big data has opened new options for natural disaster management, primarily because of the varied possibilities it provides in visualizing, analyzing, and predicting natural disasters. From this perspective, big data has radically changed the ways through which human societies adopt natural disaster management strategies to reduce human suffering and economic losses. In a world that is now heavily dependent on information technology, the prime objective of computer experts and policy makers is to make the best of big data by sourcing information from varied formats and storing it in ways that it can be effectively used during different stages of natural disaster management. This paper aimed at making a systematic review of the literature in analyzing the role of big data in natural disaster management and highlighting the present status of the technology in providing meaningful and effective solutions in natural disaster management. The paper has presented the findings of several researchers on varied scientific and technological perspectives that have a bearing on the efficacy of big data in facilitating natural disaster management. In this context, this paper reviews the major big data sources, the associated achievements in different disaster management phases, and emerging technological topics associated with leveraging this new ecosystem of Big Data to monitor and detect natural hazards, mitigate their effects, assist in relief efforts, and contribute to the recovery and reconstruction processes.},
DOI = {10.3390/geosciences8050165}
}



@Article{rs10050719,
AUTHOR = {Chen, Guanzhou and Zhang, Xiaodong and Tan, Xiaoliang and Cheng, Yufeng and Dai, Fan and Zhu, Kun and Gong, Yuanfu and Wang, Qing},
TITLE = {Training Small Networks for Scene Classification of Remote Sensing Images via Knowledge Distillation},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {719},
URL = {https://www.mdpi.com/2072-4292/10/5/719},
ISSN = {2072-4292},
ABSTRACT = {Scene classification, aiming to identify the land-cover categories of remotely sensed image patches, is now a fundamental task in the remote sensing image analysis field. Deep-learning-model-based algorithms are widely applied in scene classification and achieve remarkable performance, but these high-level methods are computationally expensive and time-consuming. Consequently in this paper, we introduce a knowledge distillation framework, currently a mainstream model compression method, into remote sensing scene classification to improve the performance of smaller and shallower network models. Our knowledge distillation training method makes the high-temperature softmax output of a small and shallow student model match the large and deep teacher model. In our experiments, we evaluate knowledge distillation training method for remote sensing scene classification on four public datasets: AID dataset, UCMerced dataset, NWPU-RESISC dataset, and EuroSAT dataset. Results show that our proposed training method was effective and increased overall accuracy (3% in AID experiments, 5% in UCMerced experiments, 1% in NWPU-RESISC and EuroSAT experiments) for small and shallow models. We further explored the performance of the student model on small and unbalanced datasets. Our findings indicate that knowledge distillation can improve the performance of small network models on datasets with lower spatial resolution images, numerous categories, as well as fewer training samples.},
DOI = {10.3390/rs10050719}
}



@Article{ijgi7050182,
AUTHOR = {Deng, Zhipeng and Sun, Hao and Zhou, Shilin},
TITLE = {Semi-Supervised Ground-to-Aerial Adaptation with Heterogeneous Features Learning for Scene Classification},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {182},
URL = {https://www.mdpi.com/2220-9964/7/5/182},
ISSN = {2220-9964},
ABSTRACT = {Currently, huge quantities of remote sensing images (RSIs) are becoming available. Nevertheless, the scarcity of labeled samples hinders the semantic understanding of RSIs. Fortunately, many ground-level image datasets with detailed semantic annotations have been collected in the vision community. In this paper, we attempt to exploit the abundant labeled ground-level images to build discriminative models for overhead-view RSI classification. However, images from the ground-level and overhead view are represented by heterogeneous features with different distributions; how to effectively combine multiple features and reduce the mismatch of distributions are two key problems in this scene-model transfer task. Specifically, a semi-supervised manifold-regularized multiple-kernel-learning (SMRMKL) algorithm is proposed for solving these problems. We employ multiple kernels over several features to learn an optimal combined model automatically. Multi-kernel Maximum Mean Discrepancy (MK-MMD) is utilized to measure the data mismatch. To make use of unlabeled target samples, a manifold regularized semi-supervised learning process is incorporated into our framework. Extensive experimental results on both cross-view and aerial-to-satellite scene datasets demonstrate that: (1) SMRMKL has an appealing extension ability to effectively fuse different types of visual features; and (2) manifold regularization can improve the adaptation performance by utilizing unlabeled target samples.},
DOI = {10.3390/ijgi7050182}
}



@Article{rs10050745,
AUTHOR = {Ma, Dandan and Yuan, Yuan and Wang, Qi},
TITLE = {Hyperspectral Anomaly Detection via Discriminative Feature Learning with Multiple-Dictionary Sparse Representation},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {745},
URL = {https://www.mdpi.com/2072-4292/10/5/745},
ISSN = {2072-4292},
ABSTRACT = {Most hyperspectral anomaly detection methods directly utilize all the original spectra to recognize anomalies. However, the inherent characteristics of high spectral dimension and complex spectral correlation commonly make their detection performance unsatisfactory. Therefore, an effective feature extraction technique is necessary. To this end, this paper proposes a novel anomaly detection method via discriminative feature learning with multiple-dictionary sparse representation. Firstly, a new spectral feature selection framework based on sparse presentation is designed, which is closely guided by the anomaly detection task. Then, the representative spectra which can significantly enlarge anomaly’s deviation from background are picked out by minimizing residues between background spectrum reconstruction error and anomaly spectrum recovery error. Finally, through comprehensively considering the virtues of different groups of representative features selected from multiple dictionaries, a global multiple-view detection strategy is presented to improve the detection accuracy. The proposed method is compared with ten state-of-the-art methods including LRX, SRD, CRD, LSMAD, RSAD, BACON, BACON-target, GRX, GKRX, and PCA-GRX on three real-world hyperspectral images. Corresponding to each competitor, it has the average detection performance improvement of about     9.9 %    ,     7.4 %    ,     24.2 %    ,     10.1 %    ,     26.2 %    ,     20.1 %    ,     5.1 %    ,     19.3 %    ,     10.7 %    , and     2.0 %     respectively. Extensive experiments demonstrate its superior performance in effectiveness and efficiency.},
DOI = {10.3390/rs10050745}
}



@Article{rs10050779,
AUTHOR = {Tao, Yiting and Xu, Miaozhong and Lu, Zhongyuan and Zhong, Yanfei},
TITLE = {DenseNet-Based Depth-Width Double Reinforced Deep Learning Neural Network for High-Resolution Remote Sensing Image Per-Pixel Classification},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {779},
URL = {https://www.mdpi.com/2072-4292/10/5/779},
ISSN = {2072-4292},
ABSTRACT = {Deep neural networks (DNNs) face many problems in the very high resolution remote sensing (VHRRS) per-pixel classification field. Among the problems is the fact that as the depth of the network increases, gradient disappearance influences classification accuracy and the corresponding increasing number of parameters to be learned increases the possibility of overfitting, especially when only a small amount of VHRRS labeled samples are acquired for training. Further, the hidden layers in DNNs are not transparent enough, which results in extracted features not being sufficiently discriminative and significant amounts of redundancy. This paper proposes a novel depth-width-reinforced DNN that solves these problems to produce better per-pixel classification results in VHRRS. In the proposed method, densely connected neural networks and internal classifiers are combined to build a deeper network and balance the network depth and performance. This strengthens the gradients, decreases negative effects from gradient disappearance as the network depth increases and enhances the transparency of hidden layers, making extracted features more discriminative and reducing the risk of overfitting. In addition, the proposed method uses multi-scale filters to create a wider neural network. The depth of the filters from each scale is controlled to decrease redundancy and the multi-scale filters enable utilization of joint spatio-spectral information and diverse local spatial structure simultaneously. Furthermore, the concept of network in network is applied to better fuse the deeper and wider designs, making the network operate more smoothly. The results of experiments conducted on BJ02, GF02, geoeye and quickbird satellite images verify the efficacy of the proposed method. The proposed method not only achieves competitive classification results but also proves that the network can continue to be robust and perform well even while the amount of labeled training samples is decreasing, which fits the small training samples situation faced by VHRRS per-pixel classification.},
DOI = {10.3390/rs10050779}
}



@Article{s18051611,
AUTHOR = {Karimi, Hadi and Skovsen, Søren and Dyrmann, Mads and Nyholm Jørgensen, Rasmus},
TITLE = {A Novel Locating System for Cereal Plant Stem Emerging Points’ Detection Using a Convolutional Neural Network},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1611},
URL = {https://www.mdpi.com/1424-8220/18/5/1611},
ISSN = {1424-8220},
ABSTRACT = {Determining the individual location of a plant, besides evaluating sowing performance, would make subsequent treatment for each plant across a field possible. In this study, a system for locating cereal plant stem emerging points (PSEPs) has been developed. In total, 5719 images were gathered from several cereal fields. In 212 of these images, the PSEPs of the cereal plants were marked manually and used to train a fully-convolutional neural network. In the training process, a cost function was made, which incorporates predefined penalty regions and PSEPs. The penalty regions were defined based on fault prediction of the trained model without penalty region assignment. By adding penalty regions to the training, the network&rsquo;s ability to precisely locate emergence points of the cereal plants was enhanced significantly. A coefficient of determination of about 87 percent between the predicted PSEP number of each image and the manually marked one implies the ability of the system to count PSEPs. With regard to the obtained results, it was concluded that the developed model can give a reliable clue about the quality of PSEPs&rsquo; distribution and the performance of seed drills in fields.},
DOI = {10.3390/s18051611}
}



@Article{s18051643,
AUTHOR = {Ahmad Yousef, Khalil M. and AlMajali, Anas and Ghalyon, Salah Abu and Dweik, Waleed and Mohd, Bassam J.},
TITLE = {Analyzing Cyber-Physical Threats on Robotic Platforms},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1643},
URL = {https://www.mdpi.com/1424-8220/18/5/1643},
ISSN = {1424-8220},
ABSTRACT = {Robots are increasingly involved in our daily lives. Fundamental to robots are the communication link (or stream) and the applications that connect the robots to their clients or users. Such communication link and applications are usually supported through client/server network connection. This networking system is amenable of being attacked and vulnerable to the security threats. Ensuring security and privacy for robotic platforms is thus critical, as failures and attacks could have devastating consequences. In this paper, we examine several cyber-physical security threats that are unique to the robotic platforms; specifically the communication link and the applications. Threats target integrity, availability and confidential security requirements of the robotic platforms, which use MobileEyes/arnlServer client/server applications. A robot attack tool (RAT) was developed to perform specific security attacks. An impact-oriented approach was adopted to analyze the assessment results of the attacks. Tests and experiments of attacks were conducted in simulation environment and physically on the robot. The simulation environment was based on MobileSim; a software tool for simulating, debugging and experimenting on MobileRobots/ActivMedia platforms and their environments. The robot platform PeopleBotTM was used for physical experiments. The analysis and testing results show that certain attacks were successful at breaching the robot security. Integrity attacks modified commands and manipulated the robot behavior. Availability attacks were able to cause Denial-of-Service (DoS) and the robot was not responsive to MobileEyes commands. Integrity and availability attacks caused sensitive information on the robot to be hijacked. To mitigate security threats, we provide possible mitigation techniques and suggestions to raise awareness of threats on the robotic platforms, especially when the robots are involved in critical missions or applications.},
DOI = {10.3390/s18051643}
}



@Article{su10051682,
AUTHOR = {Lyu, Hai-Min and Xu, Ye-Shuang and Cheng, Wen-Chieh and Arulrajah, Arul},
TITLE = {Flooding Hazards across Southern China and Prospective Sustainability Measures},
JOURNAL = {Sustainability},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {1682},
URL = {https://www.mdpi.com/2071-1050/10/5/1682},
ISSN = {2071-1050},
ABSTRACT = {The Yangtze River Basin and Huaihe River Basin in Southern China experienced severe floods 1998 and 2016. The reasons for the flooding hazards include the following two factors: hazardous weather conditions and degradation of the hydrological environment due to anthropogenic activities. This review work investigated the weather conditions based on recorded data, which showed that both 1998 and 2016 were in El Nino periods. Human activities include the degradations of rivers and lakes and the effects caused by the building of the Three Gorges Dam. In addition, the flooding in 2016 had a lower hazard scale than that in 1998 but resulted in larger economic losses than that of 1998. To mitigate urban waterlogging caused by flooding hazards, China proposed a new strategy named Spongy City (SPC) in 2014. SPC promotes sustainable city development so that a city has the resilience to adapt to climate change, to mitigate the impacts of waterlogging caused by extreme rainfall events. The countermeasures used to tackle the SPC construction-related problems, such as local inundation, water resource shortage, storm water usage, and water pollution control, are proposed for city management to improve the environment.},
DOI = {10.3390/su10051682}
}



@Article{electronics7060078,
AUTHOR = {Liu, Xiaofei and Yang, Tao and Li, Jing},
TITLE = {Real-Time Ground Vehicle Detection in Aerial Infrared Imagery Based on Convolutional Neural Network},
JOURNAL = {Electronics},
VOLUME = {7},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {78},
URL = {https://www.mdpi.com/2079-9292/7/6/78},
ISSN = {2079-9292},
ABSTRACT = {An infrared sensor is a commonly used imaging device. Unmanned aerial vehicles, the most promising moving platform, each play a vital role in their own field, respectively. However, the two devices are seldom combined in automatic ground vehicle detection tasks. Therefore, how to make full use of them&mdash;especially in ground vehicle detection based on aerial imagery&ndash;has aroused wide academic concern. However, due to the aerial imagery&rsquo;s low-resolution and the vehicle detection&rsquo;s complexity, how to extract remarkable features and handle pose variations, view changes as well as surrounding radiation remains a challenge. In fact, these typical abstract features extracted by convolutional neural networks are more recognizable than the engineering features, and those complex conditions involved can be learned and memorized before. In this paper, a novel approach towards ground vehicle detection in aerial infrared images based on a convolutional neural network is proposed. The UAV and the infrared sensor used in this application are firstly introduced. Then, a novel aerial moving platform is built and an aerial infrared vehicle dataset is unprecedentedly constructed. We publicly release this dataset (NPU_CS_UAV_IR_DATA), which can be used for the following research in this field. Next, an end-to-end convolutional neural network is built. With large amounts of recognized features being iteratively learned, a real-time ground vehicle model is constructed. It has the unique ability to detect both the stationary vehicles and moving vehicles in real urban environments. We evaluate the proposed algorithm on some low&ndash;resolution aerial infrared images. Experiments on the NPU_CS_UAV_IR_DATA dataset demonstrate that the proposed method is effective and efficient to recognize the ground vehicles. Moreover it can accomplish the task in real-time while achieving superior performances in leak and false alarm ratio.},
DOI = {10.3390/electronics7060078}
}



@Article{s18061703,
AUTHOR = {Nguyen, Phong Ha and Arsalan, Muhammad and Koo, Ja Hyung and Naqvi, Rizwan Ali and Truong, Noi Quang and Park, Kang Ryoung},
TITLE = {LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1703},
URL = {https://www.mdpi.com/1424-8220/18/6/1703},
ISSN = {1424-8220},
ABSTRACT = {Autonomous landing of an unmanned aerial vehicle or a drone is a challenging problem for the robotics research community. Previous researchers have attempted to solve this problem by combining multiple sensors such as global positioning system (GPS) receivers, inertial measurement unit, and multiple camera systems. Although these approaches successfully estimate an unmanned aerial vehicle location during landing, many calibration processes are required to achieve good detection accuracy. In addition, cases where drones operate in heterogeneous areas with no GPS signal should be considered. To overcome these problems, we determined how to safely land a drone in a GPS-denied environment using our remote-marker-based tracking algorithm based on a single visible-light-camera sensor. Instead of using hand-crafted features, our algorithm includes a convolutional neural network named lightDenseYOLO to extract trained features from an input image to predict a marker&rsquo;s location by visible light camera sensor on drone. Experimental results show that our method significantly outperforms state-of-the-art object trackers both using and not using convolutional neural network in terms of both accuracy and processing time.},
DOI = {10.3390/s18061703}
}



@Article{s18061796,
AUTHOR = {Wang, Baoxian and Zhao, Weigang and Gao, Po and Zhang, Yufeng and Wang, Zhe},
TITLE = {Crack Damage Detection Method via Multiple Visual Features and Efficient Multi-Task Learning Model},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1796},
URL = {https://www.mdpi.com/1424-8220/18/6/1796},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes an effective and efficient model for concrete crack detection. The presented work consists of two modules: multi-view image feature extraction and multi-task crack region detection. Specifically, multiple visual features (such as texture, edge, etc.) of image regions are calculated, which can suppress various background noises (such as illumination, pockmark, stripe, blurring, etc.). With the computed multiple visual features, a novel crack region detector is advocated using a multi-task learning framework, which involves restraining the variability for different crack region features and emphasizing the separability between crack region features and complex background ones. Furthermore, the extreme learning machine is utilized to construct this multi-task learning model, thereby leading to high computing efficiency and good generalization. Experimental results of the practical concrete images demonstrate that the developed algorithm can achieve favorable crack detection performance compared with traditional crack detectors.},
DOI = {10.3390/s18061796}
}



@Article{rs10060887,
AUTHOR = {Zhu, Jiasong and Sun, Ke and Jia, Sen and Lin, Weidong and Hou, Xianxu and Liu, Bozhi and Qiu, Guoping},
TITLE = {Bidirectional Long Short-Term Memory Network for Vehicle Behavior Recognition},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {887},
URL = {https://www.mdpi.com/2072-4292/10/6/887},
ISSN = {2072-4292},
ABSTRACT = {Vehicle behavior recognition is an attractive research field which is useful for many computer vision and intelligent traffic analysis tasks. This paper presents an all-in-one behavior recognition framework for moving vehicles based on the latest deep learning techniques. Unlike traditional traffic analysis methods which rely on low-resolution videos captured by road cameras, we capture 4K (    3840 × 2178    ) traffic videos at a busy road intersection of a modern megacity by flying a unmanned aerial vehicle (UAV) during the rush hours. We then manually annotate locations and types of road vehicles. The proposed method consists of the following three steps: (1) vehicle detection and type recognition based on deep neural networks; (2) vehicle tracking by data association and vehicle trajectory modeling; (3) vehicle behavior recognition by nearest neighbor search and by bidirectional long short-term memory network, respectively. This paper also presents experimental results of the proposed framework in comparison with state-of-the-art approaches on the 4K testing traffic video, which demonstrated the effectiveness and superiority of the proposed method.},
DOI = {10.3390/rs10060887}
}



@Article{s18061871,
AUTHOR = {Mao , Keming and Lu , Duo and E , Dazhi and Tan , Zhenhua},
TITLE = {A Case Study on Attribute Recognition of Heated Metal Mark Image Using Deep Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1871},
URL = {https://www.mdpi.com/1424-8220/18/6/1871},
ISSN = {1424-8220},
ABSTRACT = {Heated metal mark is an important trace to identify the cause of fire. However, traditional methods mainly focus on the knowledge of physics and chemistry for qualitative analysis and make it still a challenging problem. This paper presents a case study on attribute recognition of the heated metal mark image using computer vision and machine learning technologies. The proposed work is composed of three parts. Material is first generated. According to national standards, actual needs and feasibility, seven attributes are selected for research. Data generation and organization are conducted, and a small size benchmark dataset is constructed. A recognition model is then implemented. Feature representation and classifier construction methods are introduced based on deep convolutional neural networks. Finally, the experimental evaluation is carried out. Multi-aspect testings are performed with various model structures, data augments, training modes, optimization methods and batch sizes. The influence of parameters, recognitio efficiency and execution time are also analyzed. The results show that with a fine-tuned model, the recognition rate of attributes metal type, heating mode, heating temperature, heating duration, cooling mode, placing duration and relative humidity are 0.925, 0.908, 0.835, 0.917, 0.928, 0.805 and 0.92, respectively. The proposed method recognizes the attribute of heated metal mark with preferable effect, and it can be used in practical application.},
DOI = {10.3390/s18061871}
}



@Article{s18061881,
AUTHOR = {Kim, In-Ho and Jeon, Haemin and Baek, Seung-Chan and Hong, Won-Hwa and Jung, Hyung-Jo},
TITLE = {Application of Crack Identification Techniques for an Aging Concrete Bridge Inspection Using an Unmanned Aerial Vehicle},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1881},
URL = {https://www.mdpi.com/1424-8220/18/6/1881},
ISSN = {1424-8220},
ABSTRACT = {Bridge inspection using unmanned aerial vehicles (UAV) with high performance vision sensors has received considerable attention due to its safety and reliability. As bridges become obsolete, the number of bridges that need to be inspected increases, and they require much maintenance cost. Therefore, a bridge inspection method based on UAV with vision sensors is proposed as one of the promising strategies to maintain bridges. In this paper, a crack identification method by using a commercial UAV with a high resolution vision sensor is investigated in an aging concrete bridge. First, a point cloud-based background model is generated in the preliminary flight. Then, cracks on the structural surface are detected with the deep learning algorithm, and their thickness and length are calculated. In the deep learning method, region with convolutional neural networks (R-CNN)-based transfer learning is applied. As a result, a new network for the 384 collected crack images of 256 &times; 256 pixel resolution is generated from the pre-trained network. A field test is conducted to verify the proposed approach, and the experimental results proved that the UAV-based bridge inspection is effective at identifying and quantifying the cracks on the structures.},
DOI = {10.3390/s18061881}
}



@Article{s18061888,
AUTHOR = {You, Ilsun and Kwon, Soonhyun and Choudhary, Gaurav and Sharma, Vishal and Seo, Jung Taek},
TITLE = {An Enhanced LoRaWAN Security Protocol for Privacy Preservation in IoT with a Case Study on a Smart Factory-Enabled Parking System},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {6},
ARTICLE-NUMBER = {1888},
URL = {https://www.mdpi.com/1424-8220/18/6/1888},
ISSN = {1424-8220},
ABSTRACT = {The Internet of Things (IoT) utilizes algorithms to facilitate intelligent applications across cities in the form of smart-urban projects. As the majority of devices in IoT are battery operated, their applications should be facilitated with a low-power communication setup. Such facility is possible through the Low-Power Wide-Area Network (LPWAN), but at a constrained bit rate. For long-range communication over LPWAN, several approaches and protocols are adopted. One such protocol is the Long-Range Wide Area Network (LoRaWAN), which is a media access layer protocol for long-range communication between the devices and the application servers via LPWAN gateways. However, LoRaWAN comes with fewer security features as a much-secured protocol consumes more battery because of the exorbitant computational overheads. The standard protocol fails to support end-to-end security and perfect forward secrecy while being vulnerable to the replay attack that makes LoRaWAN limited in supporting applications where security (especially end-to-end security) is important. Motivated by this, an enhanced LoRaWAN security protocol is proposed, which not only provides the basic functions of connectivity between the application server and the end device, but additionally averts these listed security issues. The proposed protocol is developed with two options, the Default Option (DO) and the Security-Enhanced Option (SEO). The protocol is validated through Burrows&ndash;Abadi&ndash;Needham (BAN) logic and the Automated Validation of Internet Security Protocols and Applications (AVISPA) tool. The proposed protocol is also analyzed for overheads through system-based and low-power device-based evaluations. Further, a case study on a smart factory-enabled parking system is considered for its practical application. The results, in terms of network latency with reliability fitting and signaling overheads, show paramount improvements and better performance for the proposed protocol compared with the two handshake options, Pre-Shared Key (PSK) and Elliptic Curve Cryptography (ECC), of Datagram Transport Layer Security (DTLS).},
DOI = {10.3390/s18061888}
}



@Article{s18071985,
AUTHOR = {Wang, Ruihua and Xiao, Xiongwu and Guo, Bingxuan and Qin, Qianqing and Chen, Ruizhi},
TITLE = {An Effective Image Denoising Method for UAV Images via Improved Generative Adversarial Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1985},
URL = {https://www.mdpi.com/1424-8220/18/7/1985},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) are an inexpensive platform for collecting remote sensing images, but UAV images suffer from a content loss problem caused by noise. In order to solve the noise problem of UAV images, we propose a new methods to denoise UAV images. This paper introduces a novel deep neural network method based on generative adversarial learning to trace the mapping relationship between noisy and clean images. In our approach, perceptual reconstruction loss is used to establish a loss equation that continuously optimizes a min-max game theoretic model to obtain better UAV image denoising results. The generated denoised images by the proposed method enjoy clearer ground objects edges and more detailed textures of ground objects. In addition to the traditional comparison method, denoised UAV images and corresponding original clean UAV images were employed to perform image matching based on local features. At the same time, the classification experiment on the denoised images was also conducted to compare the denoising results of UAV images with others. The proposed method had achieved better results in these comparison experiments.},
DOI = {10.3390/s18071985}
}



@Article{s18072048,
AUTHOR = {Rivas, Alberto and Chamoso, Pablo and González-Briones, Alfonso and Corchado, Juan Manuel},
TITLE = {Detection of Cattle Using Drones and Convolutional Neural Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2048},
URL = {https://www.mdpi.com/1424-8220/18/7/2048},
ISSN = {1424-8220},
ABSTRACT = {Multirotor drones have been one of the most important technological advances of the last decade. Their mechanics are simple compared to other types of drones and their possibilities in flight are greater. For example, they can take-off vertically. Their capabilities have therefore brought progress to many professional activities. Moreover, advances in computing and telecommunications have also broadened the range of activities in which drones may be used. Currently, artificial intelligence and information analysis are the main areas of research in the field of computing. The case study presented in this article employed artificial intelligence techniques in the analysis of information captured by drones. More specifically, the camera installed in the drone took images which were later analyzed using Convolutional Neural Networks (CNNs) to identify the objects captured in the images. In this research, a CNN was trained to detect cattle, however the same training process could be followed to develop a CNN for the detection of any other object. This article describes the design of the platform for real-time analysis of information and its performance in the detection of cattle.},
DOI = {10.3390/s18072048}
}



@Article{s18072071,
AUTHOR = {Guerra, Edmundo and Munguía, Rodrigo and Grau, Antoni},
TITLE = {UAV Visual and Laser Sensors Fusion for Detection and Positioning in Industrial Applications},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2071},
URL = {https://www.mdpi.com/1424-8220/18/7/2071},
ISSN = {1424-8220},
ABSTRACT = {This work presents a solution to localize Unmanned Autonomous Vehicles with respect to pipes and other cylindrical elements found in inspection and maintenance tasks both in industrial and civilian infrastructures. The proposed system exploits the different features of vision and laser based sensors, combining them to obtain accurate positioning of the robot with respect to the cylindrical structures. A probabilistic (RANSAC-based) procedure is used to segment possible cylinders found in the laser scans, and this is used as a seed to accurately determine the robot position through a computer vision system. The priors obtained from the laser scan registration help to solve the problem of determining the apparent contour of the cylinders. In turn this apparent contour is used in a degenerate quadratic conic estimation, enabling to visually estimate the pose of the cylinder.},
DOI = {10.3390/s18072071}
}



@Article{s18072113,
AUTHOR = {Huang, Huasheng and Lan, Yubin and Deng, Jizhong and Yang, Aqing and Deng, Xiaoling and Zhang, Lei and Wen, Sheng},
TITLE = {A Semantic Labeling Approach for Accurate Weed Mapping of High Resolution UAV Imagery},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2113},
URL = {https://www.mdpi.com/1424-8220/18/7/2113},
ISSN = {1424-8220},
ABSTRACT = {Weed control is necessary in rice cultivation, but the excessive use of herbicide treatments has led to serious agronomic and environmental problems. Suitable site-specific weed management (SSWM) is a solution to address this problem while maintaining the rice production quality and quantity. In the context of SSWM, an accurate weed distribution map is needed to provide decision support information for herbicide treatment. UAV remote sensing offers an efficient and effective platform to monitor weeds thanks to its high spatial resolution. In this work, UAV imagery was captured in a rice field located in South China. A semantic labeling approach was adopted to generate the weed distribution maps of the UAV imagery. An ImageNet pre-trained CNN with residual framework was adapted in a fully convolutional form, and transferred to our dataset by fine-tuning. Atrous convolution was applied to extend the field of view of convolutional filters; the performance of multi-scale processing was evaluated; and a fully connected conditional random field (CRF) was applied after the CNN to further refine the spatial details. Finally, our approach was compared with the pixel-based-SVM and the classical FCN-8s. Experimental results demonstrated that our approach achieved the best performance in terms of accuracy. Especially for the detection of small weed patches in the imagery, our approach significantly outperformed other methods. The mean intersection over union (mean IU), overall accuracy, and Kappa coefficient of our method were 0.7751, 0.9445, and 0.9128, respectively. The experiments showed that our approach has high potential in accurate weed mapping of UAV imagery.},
DOI = {10.3390/s18072113}
}



@Article{sym10070250,
AUTHOR = {Le, Tuong and Hoang Son, Le and Vo, Minh Thanh and Lee, Mi Young and Baik, Sung Wook},
TITLE = {A Cluster-Based Boosting Algorithm for Bankruptcy Prediction in a Highly Imbalanced Dataset},
JOURNAL = {Symmetry},
VOLUME = {10},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {250},
URL = {https://www.mdpi.com/2073-8994/10/7/250},
ISSN = {2073-8994},
ABSTRACT = {Bankruptcy prediction has been a popular and challenging research topic in both computer science and economics due to its importance to financial institutions, fund managers, lenders, governments, as well as economic stakeholders in recent years. In a bankruptcy dataset, the problem of class imbalance, in which the number of bankruptcy companies is smaller than the number of normal companies, leads to a standard classification algorithm that does not work well. Therefore, this study proposes a cluster-based boosting algorithm as well as a robust framework using the CBoost algorithm and Instance Hardness Threshold (RFCI) for effective bankruptcy prediction of a financial dataset. This framework first resamples the imbalance dataset by the undersampling method using Instance Hardness Threshold (IHT), which is used to remove the noise instances having large IHT value in the majority class. Then, this study proposes a Cluster-based Boosting algorithm, namely CBoost, for dealing with the class imbalance. In this algorithm, the majority class will be clustered into a number of clusters. The distance from each sample to its closest centroid will be used to initialize its weight. This algorithm will perform several iterations for finding weak classifiers and combining them to create a strong classifier. The resample set resulting from the previous module, will be used to train CBoost, which will be used to predict bankruptcy for the validation set. The proposed framework is verified by the Korean bankruptcy dataset (KBD), which has a very small balancing ratio in both the training and the testing phases. The experimental results of this research show that the proposed framework achieves 86.8% in AUC (area under the ROC curve) and outperforms several methods for dealing with the imbalanced data problem for bankruptcy prediction such as GMBoost algorithm, the oversampling-based method using SMOTEENN, and the clustering-based undersampling method for bankruptcy prediction in the experimental dataset.},
DOI = {10.3390/sym10070250}
}



@Article{rs10071041,
AUTHOR = {Guo, Xingjian and Shao, Quanqin and Li, Yuzhe and Wang, Yangchun and Wang, Dongliang and Liu, Jiyuan and Fan, Jiangwen and Yang, Fan},
TITLE = {Application of UAV Remote Sensing for a Population Census of Large Wild Herbivores—Taking the Headwater Region of the Yellow River as an Example},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1041},
URL = {https://www.mdpi.com/2072-4292/10/7/1041},
ISSN = {2072-4292},
ABSTRACT = {We used unmanned aerial vehicles (UAVs) to carry out a relatively complete population census of large wild herbivores in Maduo County on the Tibetan Plateau in the spring of 2017. The effective area covered by aerial surveys was 326.6 km2, and 23,784 images were acquired. Interpretation tag libraries for UAV images were created for wild animals, including Kiang (Equus kiang), Tibetan gazelle (Procapra picticaudata), and blue sheep (Pseudois nayaur), as well as livestock, including yaks and Tibetan sheep. Large wild herbivores in the survey transect were identified through manual imagery interpretation. Densities ranged from 1.15/km2 for Kiang, 0.61/km2 for Tibetan gazelle, 0.62/km2 for blue sheep, 4.12/km2 for domestic yak, and 7.34/km2 for domestic sheep. A method based on meadows in the cold and warm seasons was used for estimating the densities and numbers of large wild herbivores and livestock, and was verified against records of livestock numbers. Population estimates for Kiang, Tibetan gazelle, blue sheep, domestic yak, and domestic sheep were 17,109, 15,961, 9324, 70,846, and 102,194, respectively. Based on published consumption estimates, the results suggest that domestic stock consume 4.5 times the amount of vegetation of large wild herbivores. Compared with traditional ground survey methods, performance of UAV remote sensing surveys of large wild herbivore populations was fast, economical and reliable, providing an effective future method for surveying wild animals.},
DOI = {10.3390/rs10071041}
}



@Article{geosciences8070244,
AUTHOR = {Buscombe, Daniel and Ritchie, Andrew C.},
TITLE = {Landscape Classification with Deep Neural Networks},
JOURNAL = {Geosciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {244},
URL = {https://www.mdpi.com/2076-3263/8/7/244},
ISSN = {2076-3263},
ABSTRACT = {The application of deep learning, specifically deep convolutional neural networks (DCNNs), to the classification of remotely-sensed imagery of natural landscapes has the potential to greatly assist in the analysis and interpretation of geomorphic processes. However, the general usefulness of deep learning applied to conventional photographic imagery at a landscape scale is, at yet, largely unproven. If DCNN-based image classification is to gain wider application and acceptance within the geoscience community, demonstrable successes need to be coupled with accessible tools to retrain deep neural networks to discriminate landforms and land uses in landscape imagery. Here, we present an efficient approach to train/apply DCNNs with/on sets of photographic images, using a powerful graphical method called a conditional random field (CRF), to generate DCNN training and testing data using minimal manual supervision. We apply the method to several sets of images of natural landscapes, acquired from satellites, aircraft, unmanned aerial vehicles, and fixed camera installations. We synthesize our findings to examine the general effectiveness of transfer learning to landscape-scale image classification. Finally, we show how DCNN predictions on small regions of images might be used in conjunction with a CRF for highly accurate pixel-level classification of images.},
DOI = {10.3390/geosciences8070244}
}



@Article{rs10071050,
AUTHOR = {Oishi, Yu and Oguma, Hiroyuki and Tamura, Ayako and Nakamura, Ryosuke and Matsunaga, Tsuneo},
TITLE = {Animal Detection Using Thermal Images and Its Required Observation Conditions},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1050},
URL = {https://www.mdpi.com/2072-4292/10/7/1050},
ISSN = {2072-4292},
ABSTRACT = {Information about changes in the population sizes of wild animals is extremely important for conservation and management. Wild animal populations have been estimated using statistical methods, but it is difficult to apply such methods to large areas. To address this problem, we have developed several support systems for the automated detection of wild animals in remote sensing images. In this study, we applied one of the developed algorithms, the computer-aided detection of moving wild animals (DWA) algorithm, to thermal remote sensing images. We also performed several analyses to confirm that the DWA algorithm is useful for thermal images and to clarify the optimal conditions for obtaining thermal images (during predawn hours and on overcast days). We developed a method based on the algorithm to extract moving wild animals from thermal remote sensing images. Then, accuracy was evaluated by applying the method to airborne thermal images in a wide area. We found that the producer&rsquo;s accuracy of the method was approximately 77.3% and the user&rsquo;s accuracy of the method was approximately 29.3%. This means that the proposed method can reduce the person-hours required to survey moving wild animals from large numbers of thermal remote sensing images. Furthermore, we confirmed the extracted sika deer candidates in a pair of images and found 24 moving objects that were not identified by visual inspection by an expert. Therefore, the proposed method can also reduce oversight when identifying moving wild animals. The detection accuracy is expected to increase by setting suitable observation conditions for surveying moving wild animals. Accordingly, we also discuss the required observation conditions. The discussions about the required observation conditions would be extremely useful for people monitoring animal population changes using thermal remote sensing images.},
DOI = {10.3390/rs10071050}
}



@Article{ICEM18-05387,
AUTHOR = {Silva, Wilson Ricardo Leal da and Lucena, Diogo Schwerz de},
TITLE = {Concrete Cracks Detection Based on Deep Learning Image Classification},
JOURNAL = {Proceedings},
VOLUME = {2},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {489},
URL = {https://www.mdpi.com/2504-3900/2/8/489},
ISSN = {2504-3900},
ABSTRACT = {This work aims at developing a machine learning-based model to detect cracks on concrete surfaces. Such model is intended to increase the level of automation on concrete infrastructure inspection when combined to unmanned aerial vehicles (UAV). The developed crack detection model relies on a deep learning convolutional neural network (CNN) image classification algorithm. Provided a relatively heterogeneous dataset, the use of deep learning enables the development of a concrete cracks detection system that can account for several conditions, e.g., different light, surface finish and humidity that a concrete surface might exhibit. These conditions are a limiting factor when working with computer vision systems based on conventional digital image processing methods. For this work, a dataset with 3500 images of concrete surfaces balanced between images with and without cracks was used. This dataset was divided into training and testing data at an 80/20 ratio. Since our dataset is rather small to enable a robust training of a complete deep learning model, a transfer-learning methodology was applied; in particular, the open-source model VGG16 was used as basis for the development of the model. The influence of the model’s parameters such as learning rate, number of nodes in the last fully connected layer and training dataset size were investigated. In each experiment, the model’s accuracy was recorded to identify the best result. For the dataset used in this work, the best experiment yielded a model with accuracy of 92.27%, showcasing the potential of using deep learning for concrete crack detection.},
DOI = {10.3390/ICEM18-05387}
}



@Article{rs10071082,
AUTHOR = {Näsi, Roope and Viljanen, Niko and Kaivosoja, Jere and Alhonoja, Katja and Hakala, Teemu and Markelin, Lauri and Honkavaara, Eija},
TITLE = {Estimating Biomass and Nitrogen Amount of Barley and Grass Using UAV and Aircraft Based Spectral and Photogrammetric 3D Features},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {1082},
URL = {https://www.mdpi.com/2072-4292/10/7/1082},
ISSN = {2072-4292},
ABSTRACT = {The timely estimation of crop biomass and nitrogen content is a crucial step in various tasks in precision agriculture, for example in fertilization optimization. Remote sensing using drones and aircrafts offers a feasible tool to carry out this task. Our objective was to develop and assess a methodology for crop biomass and nitrogen estimation, integrating spectral and 3D features that can be extracted using airborne miniaturized multispectral, hyperspectral and colour (RGB) cameras. We used the Random Forest (RF) as the estimator, and in addition Simple Linear Regression (SLR) was used to validate the consistency of the RF results. The method was assessed with empirical datasets captured of a barley field and a grass silage trial site using a hyperspectral camera based on the Fabry-P&eacute;rot interferometer (FPI) and a regular RGB camera onboard a drone and an aircraft. Agricultural reference measurements included fresh yield (FY), dry matter yield (DMY) and amount of nitrogen. In DMY estimation of barley, the Pearson Correlation Coefficient (PCC) and the normalized Root Mean Square Error (RMSE%) were at best 0.95% and 33.2%, respectively; and in the grass DMY estimation, the best results were 0.79% and 1.9%, respectively. In the nitrogen amount estimations of barley, the PCC and RMSE% were at best 0.97% and 21.6%, respectively. In the biomass estimation, the best results were obtained when integrating hyperspectral and 3D features, but the integration of RGB images and 3D features also provided results that were almost as good. In nitrogen content estimation, the hyperspectral camera gave the best results. We concluded that the integration of spectral and high spatial resolution 3D features and radiometric calibration was necessary to optimize the accuracy.},
DOI = {10.3390/rs10071082}
}



@Article{s18072194,
AUTHOR = {Bachmann, Daniel and Weichert, Frank and Rinkenauer, Gerhard},
TITLE = {Review of Three-Dimensional Human-Computer Interaction with Focus on the Leap Motion Controller},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2194},
URL = {https://www.mdpi.com/1424-8220/18/7/2194},
ISSN = {1424-8220},
ABSTRACT = {Modern hardware and software development has led to an evolution of user interfaces from command-line to natural user interfaces for virtual immersive environments. Gestures imitating real-world interaction tasks increasingly replace classical two-dimensional interfaces based on Windows/Icons/Menus/Pointers (WIMP) or touch metaphors. Thus, the purpose of this paper is to survey the state-of-the-art Human-Computer Interaction (HCI) techniques with a focus on the special field of three-dimensional interaction. This includes an overview of currently available interaction devices, their applications of usage and underlying methods for gesture design and recognition. Focus is on interfaces based on the Leap Motion Controller (LMC) and corresponding methods of gesture design and recognition. Further, a review of evaluation methods for the proposed natural user interfaces is given.},
DOI = {10.3390/s18072194}
}



@Article{s18072244,
AUTHOR = {De Oliveira, Diulhio Candido and Wehrmeister, Marco Aurelio},
TITLE = {Using Deep Learning and Low-Cost RGB and Thermal Cameras to Detect Pedestrians in Aerial Images Captured by Multirotor UAV},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {2244},
URL = {https://www.mdpi.com/1424-8220/18/7/2244},
ISSN = {1424-8220},
ABSTRACT = {The use of Unmanned Aerial Vehicles (UAV) has been increasing over the last few years in many sorts of applications due mainly to the decreasing cost of this technology. One can see the use of the UAV in several civilian applications such as surveillance and search and rescue. Automatic detection of pedestrians in aerial images is a challenging task. The computing vision system must deal with many sources of variability in the aerial images captured with the UAV, e.g., low-resolution images of pedestrians, images captured at distinct angles due to the degrees of freedom that a UAV can move, the camera platform possibly experiencing some instability while the UAV flies, among others. In this work, we created and evaluated different implementations of Pattern Recognition Systems (PRS) aiming at the automatic detection of pedestrians in aerial images captured with multirotor UAV. The main goal is to assess the feasibility and suitability of distinct PRS implementations running on top of low-cost computing platforms, e.g., single-board computers such as the Raspberry Pi or regular laptops without a GPU. For that, we used four machine learning techniques in the feature extraction and classification steps, namely Haar cascade, LBP cascade, HOG + SVM and Convolutional Neural Networks (CNN). In order to improve the system performance (especially the processing time) and also to decrease the rate of false alarms, we applied the Saliency Map (SM) and Thermal Image Processing (TIP) within the segmentation and detection steps of the PRS. The classification results show the CNN to be the best technique with 99.7% accuracy, followed by HOG + SVM with 92.3%. In situations of partial occlusion, the CNN showed 71.1% sensitivity, which can be considered a good result in comparison with the current state-of-the-art, since part of the original image data is missing. As demonstrated in the experiments, by combining TIP with CNN, the PRS can process more than two frames per second (fps), whereas the PRS that combines TIP with HOG + SVM was able to process 100 fps. It is important to mention that our experiments show that a trade-off analysis must be performed during the design of a pedestrian detection PRS. The faster implementations lead to a decrease in the PRS accuracy. For instance, by using HOG + SVM with TIP, the PRS presented the best performance results, but the obtained accuracy was 35 percentage points lower than the CNN. The obtained results indicate that the best detection technique (i.e., the CNN) requires more computational resources to decrease the PRS computation time. Therefore, this work shows and discusses the pros/cons of each technique and trade-off situations, and hence, one can use such an analysis to improve and tailor the design of a PRS to detect pedestrians in aerial images.},
DOI = {10.3390/s18072244}
}



@Article{f9070432,
AUTHOR = {Feduck, Corey and McDermid, Gregory J. and Castilla, Guillermo},
TITLE = {Detection of Coniferous Seedlings in UAV Imagery},
JOURNAL = {Forests},
VOLUME = {9},
YEAR = {2018},
NUMBER = {7},
ARTICLE-NUMBER = {432},
URL = {https://www.mdpi.com/1999-4907/9/7/432},
ISSN = {1999-4907},
ABSTRACT = {Rapid assessment of forest regeneration using unmanned aerial vehicles (UAVs) is likely to decrease the cost of establishment surveys in a variety of resource industries. This research tests the feasibility of using UAVs to rapidly identify coniferous seedlings in replanted forest-harvest areas in Alberta, Canada. In developing our protocols, we gave special consideration to creating a workflow that could perform in an operational context, avoiding comprehensive wall-to-wall surveys and complex photogrammetric processing in favor of an efficient sampling-based approach, consumer-grade cameras, and straightforward image handling. Using simple spectral decision rules from a red, green, and blue (RGB) camera, we documented a seedling detection rate of 75.8 % (n = 149), on the basis of independent test data. While moderate imbalances between the omission and commission errors suggest that our workflow has a tendency to underestimate the seedling density in a harvest block, the plot-level associations with ground surveys were very high (Pearson&rsquo;s r = 0.98; n = 14). Our results were promising enough to suggest that UAVs can be used to detect coniferous seedlings in an operational capacity with standard RGB cameras alone, although our workflow relies on seasonal leaf-off windows where seedlings are visible and spectrally distinct from their surroundings. In addition, the differential errors between the pine seedlings and spruce seedlings suggest that operational workflows could benefit from multiple decision rules designed to handle diversity in species and other sources of spectral variability.},
DOI = {10.3390/f9070432}
}



@Article{data3030028,
AUTHOR = {Gopalakrishnan, Kasthurirangan},
TITLE = {Deep Learning in Data-Driven Pavement Image Analysis and Automated Distress Detection: A Review},
JOURNAL = {Data},
VOLUME = {3},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {28},
URL = {https://www.mdpi.com/2306-5729/3/3/28},
ISSN = {2306-5729},
ABSTRACT = {Deep learning, more specifically deep convolutional neural networks, is fast becoming a popular choice for computer vision-based automated pavement distress detection. While pavement image analysis has been extensively researched over the past three decades or so, recent ground-breaking achievements of deep learning algorithms in the areas of machine translation, speech recognition, and computer vision has sparked interest in the application of deep learning to automated detection of distresses in pavement images. This paper provides a narrative review of recently published studies in this field, highlighting the current achievements and challenges. A comparison of the deep learning software frameworks, network architecture, hyper-parameters employed by each study, and crack detection performance is provided, which is expected to provide a good foundation for driving further research on this important topic in the context of smart pavement or asset management systems. The review concludes with potential avenues for future research; especially in the application of deep learning to not only detect, but also characterize the type, extent, and severity of distresses from 2D and 3D pavement images.},
DOI = {10.3390/data3030028}
}



@Article{ijgi7080294,
AUTHOR = {Chabot, Dominique and Dillon, Christopher and Shemrock, Adam and Weissflog, Nicholas and Sager, Eric P. S.},
TITLE = {An Object-Based Image Analysis Workflow for Monitoring Shallow-Water Aquatic Vegetation in Multispectral Drone Imagery},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {294},
URL = {https://www.mdpi.com/2220-9964/7/8/294},
ISSN = {2220-9964},
ABSTRACT = {High-resolution drone aerial surveys combined with object-based image analysis are transforming our capacity to monitor and manage aquatic vegetation in an era of invasive species. To better exploit the potential of these technologies, there is a need to develop more efficient and accessible analysis workflows and focus more efforts on the distinct challenge of mapping submerged vegetation. We present a straightforward workflow developed to monitor emergent and submerged invasive water soldier (Stratiotes aloides) in shallow waters of the Trent-Severn Waterway in Ontario, Canada. The main elements of the workflow are: (1) collection of radiometrically calibrated multispectral imagery including a near-infrared band; (2) multistage segmentation of the imagery involving an initial separation of above-water from submerged features; and (3) automated classification of features with a supervised machine-learning classifier. The approach yielded excellent classification accuracy for emergent features (overall accuracy = 92%; kappa = 88%; water soldier producer&rsquo;s accuracy = 92%; user&rsquo;s accuracy = 91%) and good accuracy for submerged features (overall accuracy = 84%; kappa = 75%; water soldier producer&rsquo;s accuracy = 71%; user&rsquo;s accuracy = 84%). The workflow employs off-the-shelf graphical software tools requiring no programming or coding, and could therefore be used by anyone with basic GIS and image analysis skills for a potentially wide variety of aquatic vegetation monitoring operations.},
DOI = {10.3390/ijgi7080294}
}



@Article{app8081269,
AUTHOR = {Seo, Dae Kyo and Kim, Yong Hyun and Eo, Yang Dam and Park, Wan Yong},
TITLE = {Learning-Based Colorization of Grayscale Aerial Images Using Random Forest Regression},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1269},
URL = {https://www.mdpi.com/2076-3417/8/8/1269},
ISSN = {2076-3417},
ABSTRACT = {Image colorization assigns colors to a grayscale image, which is an important yet difficult image-processing task encountered in various applications. In particular, grayscale aerial image colorization is a poorly posed problem that is affected by the sun elevation angle, seasons, sensor parameters, etc. Furthermore, since different colors may have the same intensity, it is difficult to solve this problem using traditional methods. This study proposes a novel method for the colorization of grayscale aerial images using random forest (RF) regression. The algorithm uses one grayscale image for input and one-color image for reference, both of which have similar seasonal features at the same location. The reference color image is then converted from the Red-Green-Blue (RGB) color space to the CIE L*a*b (Lab) color space in which the luminance is used to extract training pixels; this is done by performing change detection with the input grayscale image, and color information is used to establish color relationships. The proposed method directly establishes color relationships between features of the input grayscale image and color information of the reference color image based on the corresponding training pixels. The experimental results show that the proposed method outperforms several state-of-the-art algorithms in terms of both visual inspection and quantitative evaluation.},
DOI = {10.3390/app8081269}
}



@Article{s18082484,
AUTHOR = {Zhang, Weixing and Witharana, Chandi and Li, Weidong and Zhang, Chuanrong and Li, Xiaojiang and Parent, Jason},
TITLE = {Using Deep Learning to Identify Utility Poles with Crossarms and Estimate Their Locations from Google Street View Images},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2484},
URL = {https://www.mdpi.com/1424-8220/18/8/2484},
ISSN = {1424-8220},
ABSTRACT = {Traditional methods of detecting and mapping utility poles are inefficient and costly because of the demand for visual interpretation with quality data sources or intense field inspection. The advent of deep learning for object detection provides an opportunity for detecting utility poles from side-view optical images. In this study, we proposed using a deep learning-based method for automatically mapping roadside utility poles with crossarms (UPCs) from Google Street View (GSV) images. The method combines the state-of-the-art DL object detection algorithm (i.e., the RetinaNet object detection algorithm) and a modified brute-force-based line-of-bearing (LOB, a LOB stands for the ray towards the location of the target [UPC at here] from the original location of the sensor [GSV mobile platform]) measurement method to estimate the locations of detected roadside UPCs from GSV. Experimental results indicate that: (1) both the average precision (AP) and the overall accuracy (OA) are around 0.78 when the intersection-over-union (IoU) threshold is greater than 0.3, based on the testing of 500 GSV images with a total number of 937 objects; and (2) around 2.6%, 47%, and 79% of estimated locations of utility poles are within 1 m, 5 m, and 10 m buffer zones, respectively, around the referenced locations of utility poles. In general, this study indicates that even in a complex background, most utility poles can be detected with the use of DL, and the LOB measurement method can estimate the locations of most UPCs.},
DOI = {10.3390/s18082484}
}



@Article{computers7030041,
AUTHOR = {Da Silva, Bruno and Braeken, An and Touhafi, Abdellah},
TITLE = {FPGA-Based Architectures for Acoustic Beamforming with Microphone Arrays: Trends, Challenges and Research Opportunities},
JOURNAL = {Computers},
VOLUME = {7},
YEAR = {2018},
NUMBER = {3},
ARTICLE-NUMBER = {41},
URL = {https://www.mdpi.com/2073-431X/7/3/41},
ISSN = {2073-431X},
ABSTRACT = {Over the past decades, many systems composed of arrays of microphones have been developed to satisfy the quality demanded by acoustic applications. Such microphone arrays are sound acquisition systems composed of multiple microphones used to sample the sound field with spatial diversity. The relatively recent adoption of Field-Programmable Gate Arrays (FPGAs) to manage the audio data samples and to perform the signal processing operations such as filtering or beamforming has lead to customizable architectures able to satisfy the most demanding computational, power or performance acoustic applications. The presented work provides an overview of the current FPGA-based architectures and how FPGAs are exploited for different acoustic applications. Current trends on the use of this technology, pending challenges and open research opportunities on the use of FPGAs for acoustic applications using microphone arrays are presented and discussed.},
DOI = {10.3390/computers7030041}
}



@Article{rs10081222,
AUTHOR = {Wang, Yanjun and Chen, Qi and Liu, Lin and Li, Xiong and Sangaiah, Arun Kumar and Li, Kai},
TITLE = {Systematic Comparison of Power Line Classification Methods from ALS and MLS Point Cloud Data},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1222},
URL = {https://www.mdpi.com/2072-4292/10/8/1222},
ISSN = {2072-4292},
ABSTRACT = {Power lines classification is important for electric power management and geographical objects extraction using LiDAR (light detection and ranging) point cloud data. Many supervised classification approaches have been introduced for the extraction of features such as ground, trees, and buildings, and several studies have been conducted to evaluate the framework and performance of such supervised classification methods in power lines applications. However, these studies did not systematically investigate all of the relevant factors affecting the classification results, including the segmentation scale, feature selection, classifier variety, and scene complexity. In this study, we examined these factors systematically using airborne laser scanning and mobile laser scanning point cloud data. Our results indicated that random forest and neural network were highly suitable for power lines classification in forest, suburban, and urban areas in terms of the precision, recall, and quality rates of the classification results. In contrast to some previous studies, random forest yielded the best results, while Na&iuml;ve Bayes was the worst classifier in most cases. Random forest was the more robust classifier with or without feature selection for various LiDAR point cloud data. Furthermore, the classification accuracies were directly related to the selection of the local neighborhood, classifier, and feature set. Finally, it was suggested that random forest should be considered in most cases for power line classification.},
DOI = {10.3390/rs10081222}
}



@Article{rs10081229,
AUTHOR = {Zhao, Qi and Zhang, Boxue and Lyu, Shuchang and Zhang, Hong and Sun, Daniel and Li, Guoqiang and Feng, Wenquan},
TITLE = {A CNN-SIFT Hybrid Pedestrian Navigation Method Based on First-Person Vision},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1229},
URL = {https://www.mdpi.com/2072-4292/10/8/1229},
ISSN = {2072-4292},
ABSTRACT = {The emergence of new wearable technologies, such as action cameras and smart glasses, has driven the use of the first-person perspective in computer applications. This field is now attracting the attention and investment of researchers aiming to develop methods to process first-person vision (FPV) video. The current approaches present particular combinations of different image features and quantitative methods to accomplish specific objectives, such as object detection, activity recognition, user&ndash;machine interaction, etc. FPV-based navigation is necessary in some special areas, where Global Position System (GPS) or other radio-wave strength methods are blocked, and is especially helpful for visually impaired people. In this paper, we propose a hybrid structure with a convolutional neural network (CNN) and local image features to achieve FPV pedestrian navigation. A novel end-to-end trainable global pooling operator, called AlphaMEX, has been designed to improve the scene classification accuracy of CNNs. A scale-invariant feature transform (SIFT)-based tracking algorithm is employed for movement estimation and trajectory tracking of the person through each frame of FPV images. Experimental results demonstrate the effectiveness of the proposed method. The top-1 error rate of the proposed AlphaMEX-ResNet outperforms the original ResNet (k = 12) by 1.7% on the ImageNet dataset. The CNN-SIFT hybrid pedestrian navigation system reaches 0.57 m average absolute error, which is an adequate accuracy for pedestrian navigation. Both positions and movements can be well estimated by the proposed pedestrian navigation algorithm with a single wearable camera.},
DOI = {10.3390/rs10081229}
}



@Article{rs10081257,
AUTHOR = {Gray, Patrick C. and Ridge, Justin T. and Poulin, Sarah K. and Seymour, Alexander C. and Schwantes, Amanda M. and Swenson, Jennifer J. and Johnston, David W.},
TITLE = {Integrating Drone Imagery into High Resolution Satellite Remote Sensing Assessments of Estuarine Environments},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1257},
URL = {https://www.mdpi.com/2072-4292/10/8/1257},
ISSN = {2072-4292},
ABSTRACT = {Very high-resolution satellite imagery (&le;5 m resolution) has become available on a spatial and temporal scale appropriate for dynamic wetland management and conservation across large areas. Estuarine wetlands have the potential to be mapped at a detailed habitat scale with a frequency that allows immediate monitoring after storms, in response to human disturbances, and in the face of sea-level rise. Yet mapping requires significant fieldwork to run modern classification algorithms and estuarine environments can be difficult to access and are environmentally sensitive. Recent advances in unoccupied aircraft systems (UAS, or drones), coupled with their increased availability, present a solution. UAS can cover a study site with ultra-high resolution (&lt;5 cm) imagery allowing visual validation. In this study we used UAS imagery to assist training a Support Vector Machine to classify WorldView-3 and RapidEye satellite imagery of the Rachel Carson Reserve in North Carolina, USA. UAS and field-based accuracy assessments were employed for comparison across validation methods. We created and examined an array of indices and layers including texture, NDVI, and a LiDAR DEM. Our results demonstrate classification accuracy on par with previous extensive fieldwork campaigns (93% UAS and 93% field for WorldView-3; 92% UAS and 87% field for RapidEye). Examining change between 2004 and 2017, we found drastic shoreline change but general stability of emergent wetlands. Both WorldView-3 and RapidEye were found to be valuable sources of imagery for habitat classification with the main tradeoff being WorldView&rsquo;s fine spatial resolution versus RapidEye&rsquo;s temporal frequency. We conclude that UAS can be highly effective in training and validating satellite imagery.},
DOI = {10.3390/rs10081257}
}



@Article{s18082640,
AUTHOR = {Gallo, Mariano and De Luca, Giuseppina},
TITLE = {Spatial Extension of Road Traffic Sensor Data with Artificial Neural Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2640},
URL = {https://www.mdpi.com/1424-8220/18/8/2640},
ISSN = {1424-8220},
ABSTRACT = {This paper proposes a method for estimating traffic flows on some links of a road network knowing the data on other links that are monitored with sensors. In this way, it is possible to obtain more information on traffic conditions without increasing the number of monitored links. The proposed method is based on artificial neural networks (ANNs), wherein the input data are the traffic flows on some monitored road links and the output data are the traffic flows on some unmonitored links. We have implemented and tested several single-layer feed-forward ANNs that differ in the number of neurons and the method of generating datasets for training. The proposed ANNs were trained with a supervised learning approach where input and output example datasets were generated through traffic simulation techniques. The proposed method was tested on a real-scale network and gave very good results if the travel demand patterns were known and used for generating example datasets, and promising results if the demand patterns were not considered in the procedure. Numerical results have underlined that the ANNs with few neurons were more effective than the ones with many neurons in this specific problem.},
DOI = {10.3390/s18082640}
}



@Article{s18082674,
AUTHOR = {Liakos, Konstantinos G. and Busato, Patrizia and Moshou, Dimitrios and Pearson, Simon and Bochtis, Dionysis},
TITLE = {Machine Learning in Agriculture: A Review},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2674},
URL = {https://www.mdpi.com/1424-8220/18/8/2674},
ISSN = {1424-8220},
ABSTRACT = {Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.},
DOI = {10.3390/s18082674}
}



@Article{s18082693,
AUTHOR = {Huh, Jun-Ho},
TITLE = {PLC-Integrated Sensing Technology in Mountain Regions for Drone Landing Sites: Focusing on Software Technology},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {2693},
URL = {https://www.mdpi.com/1424-8220/18/8/2693},
ISSN = {1424-8220},
ABSTRACT = {In the Republic of Korea, one of the most widely discussed subjects related to future logistics technology is the drone-based delivery (transportation) system. Much (around 75%) of Korea&rsquo;s territory consists of mountainous areas; however, the costs of installing internet facilities for drone landing sites are very high compared to other countries. Therefore, this paper proposes the power-line communication (PLC) system introduced in the author&rsquo;s previous study as an alternative solution. For the system design, a number of lightning rods are used together with a monitoring system. The system algorithm performs substantial data analysis. Also, as the author found that instantaneous high-voltage currents were a major cause of fire incidents, a three-phase three-wire connection was used for the installation of the lightning rods (Bipolar Conventional Air Terminal). Thus, based on the PLC technology, an artificial intelligence (AI) which avoids lightning strikes at the drone landing site by interworking with a closed-circuit television (CCTV) monitoring system when a drone flies over the mountain regions is proposed in this paper. The algorithm was implemented with C++ and Unity/C#, whereas the application for the part concerning the integrated sensing was developed with Java Android.},
DOI = {10.3390/s18082693}
}



@Article{rs10081320,
AUTHOR = {Malihi, Shirin and Valadan Zoej, Mohammad Javad and Hahn, Michael and Mokhtarzade, Mehdi},
TITLE = {Window Detection from UAS-Derived Photogrammetric Point Cloud Employing Density-Based Filtering and Perceptual Organization},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {8},
ARTICLE-NUMBER = {1320},
URL = {https://www.mdpi.com/2072-4292/10/8/1320},
ISSN = {2072-4292},
ABSTRACT = {Point clouds with ever-increasing volume are regular data in 3D city modelling, in which building reconstruction is a significant part. The photogrammetric point cloud, generated from UAS (Unmanned Aerial System) imagery, is a novel type of data in building reconstruction. Its positive characteristics, alongside its challenging qualities, provoke discussions on this theme of research. In this paper, patch-wise detection of the points of window frames on facades and roofs are undertaken using this kind of data. A density-based multi-scale filter is devised in the feature space of normal vectors to globally handle the matter of high volume of data and to detect edges. Color information is employed for the downsized data to remove the inner clutter of the building. Perceptual organization directs the approach via grouping and the Gestalt principles, to segment the filtered point cloud and to later detect window patches. The evaluation of the approach displays a completeness of 95% and 92%, respectively, as well as a correctness of 95% and 96%, respectively, for the detection of rectangular and partially curved window frames in two big heterogeneous cluttered datasets. Moreover, most intrusions and protrusions cannot mislead the window detection approach. Several doors with glass parts and a number of parallel parts of the scaffolding are mistaken as windows when using the large-scale object detection approach due to their similar patterns with window frames. Sensitivity analysis of the input parameters demonstrates that the filter functionality depends on the radius of density calculation in the feature space. Furthermore, successfully employing the Gestalt principles in the detection of window frames is influenced by the width determination of window partitioning.},
DOI = {10.3390/rs10081320}
}



@Article{rs10091339,
AUTHOR = {Liu, Shuo and Ding, Wenrui and Liu, Chunhui and Liu, Yu and Wang, Yufeng and Li, Hongguang},
TITLE = {ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1339},
URL = {https://www.mdpi.com/2072-4292/10/9/1339},
ISSN = {2072-4292},
ABSTRACT = {The semantic segmentation of remote sensing images faces two major challenges: high inter-class similarity and interference from ubiquitous shadows. In order to address these issues, we develop a novel edge loss reinforced semantic segmentation network (ERN) that leverages the spatial boundary context to reduce the semantic ambiguity. The main contributions of this paper are as follows: (1) we propose a novel end-to-end semantic segmentation network for remote sensing, which involves multiple weighted edge supervisions to retain spatial boundary information; (2) the main representations of the network are shared between the edge loss reinforced structures and semantic segmentation, which means that the ERN simultaneously achieves semantic segmentation and edge detection without significantly increasing the model complexity; and (3) we explore and discuss different ERN schemes to guide the design of future networks. Extensive experimental results on two remote sensing datasets demonstrate the effectiveness of our approach both in quantitative and qualitative evaluation. Specifically, the semantic segmentation performance in shadow-affected regions is significantly improved.},
DOI = {10.3390/rs10091339}
}



@Article{rs10091347,
AUTHOR = {Chen, Ting and Pennisi, Andrea and Li, Zhi and Zhang, Yanning and Sahli, Hichem},
TITLE = {A Hierarchical Association Framework for Multi-Object Tracking in Airborne Videos},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1347},
URL = {https://www.mdpi.com/2072-4292/10/9/1347},
ISSN = {2072-4292},
ABSTRACT = {Multi-Object Tracking (MOT) in airborne videos is a challenging problem due to the uncertain airborne vehicle motion, vibrations of the mounted camera, unreliable detections, changes of size, appearance and motion of the moving objects and occlusions caused by the interaction between moving and static objects in the scene. To deal with these problems, this work proposes a four-stage hierarchical association framework for multiple object tracking in airborne video. The proposed framework combines Data Association-based Tracking (DAT) methods and target tracking using a compressive tracking approach, to robustly track objects in complex airborne surveillance scenes. In each association stage, different sets of tracklets and detections are associated to efficiently handle local tracklet generation, local trajectory construction, global drifting tracklet correction and global fragmented tracklet linking. Experiments with challenging airborne videos show significant tracking improvement compared to existing state-of-the-art methods.},
DOI = {10.3390/rs10091347}
}



@Article{electronics7090160,
AUTHOR = {Kung, Chien-Chun},
TITLE = {Study on Consulting Air Combat Simulation of Cluster UAV Based on Mixed Parallel Computing Framework of Graphics Processing Unit},
JOURNAL = {Electronics},
VOLUME = {7},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {160},
URL = {https://www.mdpi.com/2079-9292/7/9/160},
ISSN = {2079-9292},
ABSTRACT = {This paper combines matrix game theory with negotiating theory and uses U-solution to study the framework of the consulting air combat of UAV cluster. The processes to determine the optimal strategy in this paper follow three points: first, the UAV cluster are grouped into fleets; second, the best paring for the joint operations of the fleet member with the enemy fleet members are calculated; thirdly, consultations within the fleet are conducted to discuss the problems of optimal tactic, roles of main/assistance, and situational assessment within the fleet. In order to improve the computing efficiency of the framework, this article explores the use of the NVIDIA graphics processor programmed through MATLAB mixed C++/CUDA toolkit to accelerate the calculations of equations of motion of unmanned aerial vehicles, the prediction of superiority values and U values, computations of consultation, the evaluation of situational assessment and the optimal strategies. The effectiveness evaluation of GPGPU and CPU can be observed by the simulation results. When the number of team air combat is small, the CPU alone has better efficiency; however, when the number of air combat clusters exceeds 6 to 6, the architecture presented in this article can provide higher performance improvements and run faster than optimized CPU-only code.},
DOI = {10.3390/electronics7090160}
}



@Article{s18092869,
AUTHOR = {Wang, Yanzhao and Xiu, Chundi and Zhang, Xuanli and Yang, Dongkai},
TITLE = {WiFi Indoor Localization with CSI Fingerprinting-Based Random Forest},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2869},
URL = {https://www.mdpi.com/1424-8220/18/9/2869},
ISSN = {1424-8220},
ABSTRACT = {WiFi fingerprinting indoor positioning systems have extensive applied prospects. However, a vast amount of data in a particular environment has to be gathered to establish a fingerprinting database. Deficiencies of these systems are the lack of universality of multipath effects and a burden of heavy workload on fingerprint storage. Thus, this paper presents a novel Random Forest fingerprinting localization (RFFP) method using channel state information (CSI), which utilizes the Random Forest model trained in the offline stage as fingerprints in order to economize memory space and possess a good anti-multipath characteristic. Furthermore, a series of specific experiments are conducted in a microwave anechoic chamber and an office to detail the localization performance of RFFP with different wireless channel circumstances, system parameters, algorithms, and input datasets. In addition, compared with other algorithms including K-Nearest-Neighbor (KNN), Weighted K-Nearest-Neighbor (WKNN), REPTree, CART, and J48, the RFFP method provides far greater classification accuracy as well as lower mean location error. The proposed method offers outstanding comprehensive performance including accuracy, robustness, low workload, and better anti-multipath-fading.},
DOI = {10.3390/s18092869}
}



@Article{s18092874,
AUTHOR = {Wang, Ruihua and Ma, Guorui and Qin, Qianqing and Shi, Qiang and Huang, Juntao},
TITLE = {Blind UAV Images Deblurring Based on Discriminative Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2874},
URL = {https://www.mdpi.com/1424-8220/18/9/2874},
ISSN = {1424-8220},
ABSTRACT = {Unmanned aerial vehicles (UAVs) have become an important technology for acquiring high-resolution remote sensing images. Because most space optical imaging systems of UAVs work in environments affected by vibrations, the optical axis motion and image plane jitter caused by these vibrations easily result in blurring of UAV images. In the paper; we propose an advanced UAV image deblurring method based on a discriminative model comprising a classifier for blurred and sharp UAV images which is embedded into the maximum a posteriori framework as a regularization term that constantly optimizes ill-posed problem of blind image deblurring to obtain sharper UAV images. Compared with other methods, the results show that in image deblurring experiments using both simulated and real UAV images the proposed method delivers sharper images of various ground objects.},
DOI = {10.3390/s18092874}
}



@Article{s18092886,
AUTHOR = {Lee, Jungshin and Bang, Hyochoong},
TITLE = {A Robust Terrain Aided Navigation Using the Rao-Blackwellized Particle Filter Trained by Long Short-Term Memory Networks},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2886},
URL = {https://www.mdpi.com/1424-8220/18/9/2886},
ISSN = {1424-8220},
ABSTRACT = {Terrain-aided navigation (TAN) is a technology that estimates the position of the vehicle by comparing the altitude measured by an altimeter and height from the digital elevation model (DEM). The particle filter (PF)-based TAN has been commonly used to obtain stable real-time navigation solutions in cases where the unmanned aerial vehicle (UAV) operates at a high altitude. Even though TAN performs well on rough and unique terrains, its performance degrades in flat and repetitive terrains. In particular, in the case of PF-based TAN, there has been no verified technique for deciding its terrain validity. Therefore, this study designed a Rao-Blackwellized PF (RBPF)-based TAN, used long short-term memory (LSTM) networks to endure flat and repetitive terrains, and trained the noise covariances and measurement model of RBPF. LSTM is a modified recurrent neural network (RNN), which is an artificial neural network that recognizes patterns from time series data. Using this, this study tuned the noise covariances and measurement model of RBPF to minimize the navigation errors in various flight trajectories. This paper designed a TAN algorithm based on combining RBPF and LSTM and confirmed that it can enable a more precise navigation performance than conventional RBPF based TAN through simulations.},
DOI = {10.3390/s18092886}
}



@Article{s18092905,
AUTHOR = {Yu, Lingli and Shao, Xuanya and Wei, Yadong and Zhou, Kaijun},
TITLE = {Intelligent Land-Vehicle Model Transfer Trajectory Planning Method Based on Deep Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {2905},
URL = {https://www.mdpi.com/1424-8220/18/9/2905},
ISSN = {1424-8220},
ABSTRACT = {To address the problem of model error and tracking dependence in the process of intelligent vehicle motion planning, an intelligent vehicle model transfer trajectory planning method based on deep reinforcement learning is proposed, which is able to obtain an effective control action sequence directly. Firstly, an abstract model of the real environment is extracted. On this basis, a deep deterministic policy gradient (DDPG) and a vehicle dynamic model are adopted to jointly train a reinforcement learning model, and to decide the optimal intelligent driving maneuver. Secondly, the actual scene is transferred to an equivalent virtual abstract scene using a transfer model. Furthermore, the control action and trajectory sequences are calculated according to the trained deep reinforcement learning model. Thirdly, the optimal trajectory sequence is selected according to an evaluation function in the real environment. Finally, the results demonstrate that the proposed method can deal with the problem of intelligent vehicle trajectory planning for continuous input and continuous output. The model transfer method improves the model&rsquo;s generalization performance. Compared with traditional trajectory planning, the proposed method outputs continuous rotation-angle control sequences. Moreover, the lateral control errors are also reduced.},
DOI = {10.3390/s18092905}
}



@Article{rs10091403,
AUTHOR = {Wu, Jianwei and Yao, Wei and Polewski, Przemyslaw},
TITLE = {Mapping Individual Tree Species and Vitality along Urban Road Corridors with LiDAR and Imaging Sensors: Point Density versus View Perspective},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1403},
URL = {https://www.mdpi.com/2072-4292/10/9/1403},
ISSN = {2072-4292},
ABSTRACT = {To meet a growing demand for accurate high-fidelity vegetation cover mapping in urban areas toward biodiversity conservation and assessing the impact of climate change, this paper proposes a complete approach to species and vitality classification at single tree level by synergistic use of multimodality 3D remote sensing data. So far, airborne laser scanning system(ALS or airborne LiDAR) has shown promising results in tree cover mapping for urban areas. This paper analyzes the potential of mobile laser scanning system/mobile mapping system (MLS/MMS)-based methods for recognition of urban plant species and characterization of growth conditions using ultra-dense LiDAR point clouds and provides an objective comparison with the ALS-based methods. Firstly, to solve the extremely intensive computational burden caused by the classification of ultra-dense MLS data, a new method for the semantic labeling of LiDAR data in the urban road environment is developed based on combining a conditional random field (CRF) for the context-based classification of 3D point clouds with shape priors. These priors encode geometric primitives found in the scene through sample consensus segmentation. Then, single trees are segmented from the labelled tree points using the 3D graph cuts algorithm. Multinomial logistic regression classifiers are used to determine the fine deciduous urban tree species of conversation concern and their growth vitality. Finally, the weight-of-evidence (WofE) based decision fusion method is applied to combine the probability outputs of classification results from the MLS and ALS data. The experiment results obtained in city road corridors demonstrated that point cloud data acquired from the airborne platform achieved even slightly better results in terms of tree detection rate, tree species and vitality classification accuracy, although the tree vitality distribution in the test site is less balanced compared to the species distribution. When combined with MLS data, overall accuracies of 78% and 74% for tree species and vitality classification can be achieved, which has improved by 5.7% and 4.64% respectively compared to the usage of airborne data only.},
DOI = {10.3390/rs10091403}
}



@Article{ijgi7090367,
AUTHOR = {Tianyang, Dong and Jian, Zhang and Sibin, Gao and Ying, Shen and Jing, Fan},
TITLE = {Single-Tree Detection in High-Resolution Remote-Sensing Images Based on a Cascade Neural Network},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {7},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {367},
URL = {https://www.mdpi.com/2220-9964/7/9/367},
ISSN = {2220-9964},
ABSTRACT = {Traditional single-tree detection methods usually need to set different thresholds and parameters manually according to different forest conditions. As a solution to the complicated detection process for non-professionals, this paper presents a single-tree detection method for high-resolution remote-sensing images based on a cascade neural network. In this method, we firstly calibrated the tree and non-tree samples in high-resolution remote-sensing images to train a classifier with the backpropagation (BP) neural network. Then, we analyzed the differences in the first-order statistic features, such as energy, entropy, mean, skewness, and kurtosis of the tree and non-tree samples. Finally, we used these features to correct the BP neural network model and build a cascade neural network classifier to detect a single tree. To verify the validity and practicability of the proposed method, six forestlands including two areas of oil palm in Thailand, and four areas of small seedlings, red maples, or longan trees in China were selected as test areas. The results from different methods, such as the region-growing method, template-matching method, BP neural network, and proposed cascade-neural-network method were compared considering these test areas. The experimental results show that the single-tree detection method based on the cascade neural network exhibited the highest root mean square of the matching rate (RMS_Rmat = 90%) and matching score (RMS_M = 68) in all the considered test areas.},
DOI = {10.3390/ijgi7090367}
}



@Article{app8091575,
AUTHOR = {Tao, Xian and Zhang, Dapeng and Ma, Wenzhi and Liu, Xilong and Xu, De},
TITLE = {Automatic Metallic Surface Defect Detection and Recognition with Convolutional Neural Networks},
JOURNAL = {Applied Sciences},
VOLUME = {8},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1575},
URL = {https://www.mdpi.com/2076-3417/8/9/1575},
ISSN = {2076-3417},
ABSTRACT = {Automatic metallic surface defect inspection has received increased attention in relation to the quality control of industrial products. Metallic defect detection is usually performed against complex industrial scenarios, presenting an interesting but challenging problem. Traditional methods are based on image processing or shallow machine learning techniques, but these can only detect defects under specific detection conditions, such as obvious defect contours with strong contrast and low noise, at certain scales, or under specific illumination conditions. This paper discusses the automatic detection of metallic defects with a twofold procedure that accurately localizes and classifies defects appearing in input images captured from real industrial environments. A novel cascaded autoencoder (CASAE) architecture is designed for segmenting and localizing defects. The cascading network transforms the input defect image into a pixel-wise prediction mask based on semantic segmentation. The defect regions of segmented results are classified into their specific classes via a compact convolutional neural network (CNN). Metallic defects under various conditions can be successfully detected using an industrial dataset. The experimental results demonstrate that this method meets the robustness and accuracy requirements for metallic defect detection. Meanwhile, it can also be extended to other detection applications.},
DOI = {10.3390/app8091575}
}



@Article{rs10091423,
AUTHOR = {Sa, Inkyu and Popović, Marija and Khanna, Raghav and Chen, Zetao and Lottes, Philipp and Liebisch, Frank and Nieto, Juan and Stachniss, Cyrill and Walter, Achim and Siegwart, Roland},
TITLE = {WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1423},
URL = {https://www.mdpi.com/2072-4292/10/9/1423},
ISSN = {2072-4292},
ABSTRACT = {The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB (red, green, and blue) inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.},
DOI = {10.3390/rs10091423}
}



@Article{rs10091425,
AUTHOR = {Liu, Xuefeng and Sun, Qiaoqiao and Meng, Yue and Fu, Min and Bourennane, Salah},
TITLE = {Hyperspectral Image Classification Based on Parameter-Optimized 3D-CNNs Combined with Transfer Learning and Virtual Samples},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1425},
URL = {https://www.mdpi.com/2072-4292/10/9/1425},
ISSN = {2072-4292},
ABSTRACT = {Recent research has shown that spatial-spectral information can help to improve the classification of hyperspectral images (HSIs). Therefore, three-dimensional convolutional neural networks (3D-CNNs) have been applied to HSI classification. However, a lack of HSI training samples restricts the performance of 3D-CNNs. To solve this problem and improve the classification, an improved method based on 3D-CNNs combined with parameter optimization, transfer learning, and virtual samples is proposed in this paper. Firstly, to optimize the network performance, the parameters of the 3D-CNN of the HSI to be classified (target data) are adjusted according to the single variable principle. Secondly, in order to relieve the problem caused by insufficient samples, the weights in the bottom layers of the parameter-optimized 3D-CNN of the target data can be transferred from another well trained 3D-CNN by a HSI (source data) with enough samples and the same feature space as the target data. Then, some virtual samples can be generated from the original samples of the target data to further alleviate the lack of HSI training samples. Finally, the parameter-optimized 3D-CNN with transfer learning can be trained by the training samples consisting of the virtual and the original samples. Experimental results on real-world hyperspectral satellite images have shown that the proposed method has great potential prospects in HSI classification.},
DOI = {10.3390/rs10091425}
}



@Article{rs10091435,
AUTHOR = {Lotte, Rodolfo Georjute and Haala, Norbert and Karpina, Mateusz and Aragão, Luiz Eduardo Oliveira e Cruz de and Shimabukuro, Yosio Edemir},
TITLE = {3D Façade Labeling over Complex Scenarios: A Case Study Using Convolutional Neural Network and Structure-From-Motion},
JOURNAL = {Remote Sensing},
VOLUME = {10},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {1435},
URL = {https://www.mdpi.com/2072-4292/10/9/1435},
ISSN = {2072-4292},
ABSTRACT = {Urban environments are regions in which spectral variability and spatial variability are extremely high, with a huge range of shapes and sizes, and they also demand high resolution images for applications involving their study. Due to the fact that these environments can grow even more over time, applications related to their monitoring tend to turn to autonomous intelligent systems, which together with remote sensing data could help or even predict daily life situations. The task of mapping cities by autonomous operators was usually carried out by aerial optical images due to its scale and resolution; however new scientific questions have arisen, and this has led research into a new era of highly-detailed data extraction. For many years, using artificial neural models to solve complex problems such as automatic image classification was commonplace, owing much of their popularity to their ability to adapt to complex situations without needing human intervention. In spite of that, their popularity declined in the mid-2000s, mostly due to the complex and time-consuming nature of their methods and workflows. However, newer neural network architectures have brought back the interest in their application for autonomous classifiers, especially for image classification purposes. Convolutional Neural Networks (CNN) have been a trend for pixel-wise image segmentation, showing flexibility when detecting and classifying any kind of object, even in situations where humans failed to perceive differences, such as in city scenarios. In this paper, we aim to explore and experiment with state-of-the-art technologies to semantically label 3D urban models over complex scenarios. To achieve these goals, we split the problem into two main processing lines: first, how to correctly label the fa&ccedil;ade features in the 2D domain, where a supervised CNN is used to segment ground-based fa&ccedil;ade images into six feature classes, roof, window, wall, door, balcony and shop; second, a Structure-from-Motion (SfM) and Multi-View-Stereo (MVS) workflow is used to extract the geometry of the fa&ccedil;ade, wherein the segmented images in the previous stage are then used to label the generated mesh by a &ldquo;reverse&rdquo; ray-tracing technique. This paper demonstrates that the proposed methodology is robust in complex scenarios. The fa&ccedil;ade feature inferences have reached up to 93% accuracy over most of the datasets used. Although it still presents some deficiencies in unknown architectural styles and needs some improvements to be made regarding 3D-labeling, we present a consistent and simple methodology to handle the problem.},
DOI = {10.3390/rs10091435}
}



@Article{s18093017,
AUTHOR = {Choi, Jongseong and Yeum, Chul Min and Dyke, Shirley J. and Jahanshahi, Mohammad R.},
TITLE = {Computer-Aided Approach for Rapid Post-Event Visual Evaluation of a Building Façade},
JOURNAL = {Sensors},
VOLUME = {18},
YEAR = {2018},
NUMBER = {9},
ARTICLE-NUMBER = {3017},
URL = {https://www.mdpi.com/1424-8220/18/9/3017},
ISSN = {1424-8220},
ABSTRACT = {After a disaster strikes an urban area, damage to the fa&ccedil;ades of a building may produce dangerous falling hazards that jeopardize pedestrians and vehicles. Thus, building fa&ccedil;ades must be rapidly inspected to prevent potential loss of life and property damage. Harnessing the capacity to use new vision sensors and associated sensing platforms, such as unmanned aerial vehicles (UAVs) would expedite this process and alleviate spatial and temporal limitations typically associated with human-based inspection in high-rise buildings. In this paper, we have developed an approach to perform rapid and accurate visual inspection of building fa&ccedil;ades using images collected from UAVs. An orthophoto corresponding to any reasonably flat region on the building (e.g., a fa&ccedil;ade or building side) is automatically constructed using a structure-from-motion (SfM) technique, followed by image stitching and blending. Based on the geometric relationship between the collected images and the constructed orthophoto, high-resolution region-of-interest are automatically extracted from the collected images, enabling efficient visual inspection. We successfully demonstrate the capabilities of the technique using an abandoned building of which a fa&ccedil;ade has damaged building components (e.g., window panes or external drainage pipes).},
DOI = {10.3390/s18093017}
}



