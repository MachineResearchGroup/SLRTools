@inproceedings{10.1145/3454127.3456576,
author = {Arqane, Aouatif and Boutkhoum, Omar and Boukhriss, Hicham and El Moutaouakkil, Abdelmajid},
title = {A Review of Intrusion Detection Systems: Datasets and Machine Learning Methods},
year = {2021},
isbn = {9781450388719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3454127.3456576},
doi = {10.1145/3454127.3456576},
abstract = {At the present time, Security is a crucial issue for all organizations and companies, because intruders are constantly developing new techniques to infiltrate their infrastructure to steal or manipulate sensitive data. Thus, Intrusion Detection System (IDS) has emerged as new technology to protect networks and systems against suspicious activities. Numerous cybersecurity experts highlight the importance of IDS to strength the defensive capacities of systems by alerting for suspicious activities and malicious attacks. Over the years, many techniques like Machine learning (ML) and Deep Learning (DL) have been used to increase the detection accuracy and reduce the false alerts of IDSs. This survey paper presents an overview of some ML and DL algorithms among the most used for IDS. Additionally, because these algorithms depend on the characteristics of malicious events stored in datasets to identify anomalies, we list some publicly available cybersecurity datasets. Furthermore, we highlight the challenges that experts must overcome to enhance the performance of their methods.},
booktitle = {Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security},
articleno = {7},
numpages = {6},
location = {KENITRA, AA, Morocco},
series = {NISS2021}
}

@article{10.1145/3425867,
author = {Djenouri, Youcef and Djenouri, Djamel and Lin, Jerry Chun-Wei},
title = {Trajectory Outlier Detection: New Problems and Solutions for Smart Cities},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4681},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3425867},
doi = {10.1145/3425867},
abstract = {This article introduces two new problems related to trajectory outlier detection: (1) group trajectory outlier (GTO) detection and (2) deviation point detection for both individual and group of trajectory outliers. Five algorithms are proposed for the first problem by adapting DBSCAN, k nearest neighbors (kNN), and feature selection (FS). DBSCAN-GTO first applies DBSCAN to derive the micro clusters, which are considered as potential candidates. A pruning strategy based on density computation measure is then suggested to find the group of trajectory outliers. kNN-GTO recursively derives the trajectory candidates from the individual trajectory outliers and prunes them based on their density. The overall process is repeated for all individual trajectory outliers. FS-GTO considers the set of individual trajectory outliers as the set of all features, while the FS process is used to retrieve the group of trajectory outliers. The proposed algorithms are improved by incorporating ensemble learning and high-performance computing during the detection process. Moreover, we propose a general two-phase-based algorithm for detecting the deviation points, as well as a version for graphic processing units implementation using sliding windows. Experiments on a real trajectory dataset have been carried out to demonstrate the performance of the proposed approaches. The results show that they can efficiently identify useful patterns represented by group of trajectory outliers, deviation points, and that they outperform the baseline group detection algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {20},
numpages = {28},
keywords = {data mining, Trajectory analysis, road traffic management, smart city application, outlier detection}
}

@inproceedings{10.1145/3410566.3410597,
author = {Kang, Dylan Myungchul and Lee, Charles Cheolgi and Lee, Suan and Lee, Wookey},
title = {Patent Prior Art Search Using Deep Learning Language Model},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3410566.3410597},
doi = {10.1145/3410566.3410597},
abstract = {A patent is one of the essential indicators of new technologies and business processes, which becomes the main driving force of the companies and even the national competitiveness as well, that has recently been submitted and exploited in a large scale of quantities of information sources. Since the number of patent processing personnel, however, can hardly keep up with the increasing number of patents, and thus may have been worried about from deteriorating the quality of examinations. In this regard, the advancement of deep learning for the language processing capabilities has been developed significantly so that the prior art search by the deep learning models also can be accomplished for the labor-intensive and expensive patent document search tasks. The prior art search requires differentiation tasks, usually with the sheer volume of relevant documents; thus, the recall is much more important than the precision, which is the primary difference from the conventional search engines. This paper addressed a method to effectively handle the patent documents using BERT, one of the major deep learning-based language models. We proved through experiments that our model had outperformed the conventional approaches and the combinations of the key components with the recall value of up to '94.29%' from the real patent dataset.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering &amp; Applications},
articleno = {1},
numpages = {5},
keywords = {prior art search, patent document classification, language model},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@inbook{10.1145/3459637.3482000,
author = {Jiang, Renhe and Yin, Du and Wang, Zhaonan and Wang, Yizhuo and Deng, Jiewen and Liu, Hangchen and Cai, Zekun and Deng, Jinliang and Song, Xuan and Shibasaki, Ryosuke},
title = {DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3459637.3482000},
abstract = {Nowadays, with the rapid development of IoT (Internet of Things) and CPS (Cyber-Physical Systems) technologies, big spatiotemporal data are being generated from mobile phones, car navigation systems, and traffic sensors. By leveraging state-of-the-art deep learning technologies on such data, urban traffic prediction has drawn a lot of attention in AI and Intelligent Transportation System community. The problem can be uniformly modeled with a 3D tensor (T, N, C), where T denotes the total time steps, N denotes the size of the spatial domain (i.e., mesh-grids or graph-nodes), and C denotes the channels of information. According to the specific modeling strategy, the state-of-the-art deep learning models can be divided into three categories: grid-based, graph-based, and multivariate time-series models. In this study, we first synthetically review the deep traffic models as well as the widely used datasets, then build a standard benchmark to comprehensively evaluate their performances with the same settings and metrics. Our study named DL-Traff is implemented with two most popular deep learning frameworks, i.e., TensorFlow and PyTorch, which is already publicly available as two GitHub repositories https://github.com/deepkashiwa20/DL-Traff-Grid and https://github.com/deepkashiwa20/DL-Traff-Graph. With DL-Traff, we hope to deliver a useful resource to researchers who are interested in spatiotemporal data analysis.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4515–4525},
numpages = {11}
}

@inproceedings{10.1145/3485730.3493685,
author = {Goyal, Adit and Elhence, Anubhav and Chamola, Vinay and Sikdar, Biplab},
title = {A Blockchain and Machine Learning Based Framework for Efficient Health Insurance Management},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3485730.3493685},
doi = {10.1145/3485730.3493685},
abstract = {Having a health insurance is important for everybody, bearing in mind the increasing medical costs. Medical emergencies can have a severe financial and emotional impact. However, the current insurance system is very expensive and the claim settlement process is excessively lengthy, making it tedious. This results in policyholders not being able to successfully make a claim with their insurance company. In this paper, we focus on developing a fast and cost-effective framework based on blockchain technology and machine learning for the health insurance industry. Blockchain is capable of removing all third-party organisations by forming a smart contract, making the entire process more smooth, secure, and efficient. The contract settles the claim on documents submitted by the claimant. A ridge regression model is used for computing the premiums optimally, based on the total amount claimed under the current policy tenure, along with several other factors. A random forest classifier is applied for predicting the risk that helps in the computation of risk-rated premium rebate.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {511–515},
numpages = {5},
keywords = {Smart contract, Machine Learning, Insurance, Blockchain, Random Forest},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@article{10.1145/3469890,
author = {Lv, Zhihan and Lou, Ranran and Feng, Hailin and Chen, Dongliang and Lv, Haibin},
title = {Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2158-656X},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3469890},
doi = {10.1145/3469890},
abstract = {Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {oct},
articleno = {7},
numpages = {21},
keywords = {lightGBM, accuracy rate, big data analysis, intelligent support information system, Machine learning}
}

@article{10.1145/3308897.3308963,
author = {Manzo, Gaetano and Otalora, Juan Sebastian and Marsan, Marco Ajmone and Rizzo, Gianluca},
title = {A Deep Learning Strategy for Vehicular Floating Content Management},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0163-5999},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3308897.3308963},
doi = {10.1145/3308897.3308963},
abstract = {Floating Content (FC) is a communication paradigm for the local dissemination of contextualized information through D2D connectivity, in a way which minimizes the use of resources while achieving some specified performance target. Existing approaches to FC dimensioning are based on unrealistic system assumptions that make them, highly inaccurate and overly conservative when applied in realistic settings. In this paper, we present a first step towards the development of a cognitive approach to efficient dynamic management of FC. We propose a deep learning strategy for FC dimensioning, which exploits a Convolutional Neural Network (CNN) to efficiently modulate over time the resources employed by FC in a QoS-aware manner. Numerical evaluations show that our approach achieves a maximum rejection rate of 3%, and resource savings of 37.5% with respect to the benchmark strategy.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {159–162},
numpages = {4}
}

@article{10.1145/3418206,
author = {Ullah, Farhan and Naeem, Muhammad Rashid and Bajahzar, Abdullah S. and Al-Turjman, Fadi},
title = {IoT-Based Cloud Service for Secured Android Markets Using PDG-Based Deep Learning Classification},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1533-5399},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3418206},
doi = {10.1145/3418206},
abstract = {Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones.Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48% among clones from different application stores.},
journal = {ACM Trans. Internet Technol.},
month = {oct},
articleno = {40},
numpages = {17},
keywords = {deep learning, Internet of Things, program dependency graph, cloud services, Clone detection}
}

@article{10.1145/3417987,
author = {Waheed, Nazar and He, Xiangjian and Ikram, Muhammad and Usman, Muhammad and Hashmi, Saad Sajid and Usman, Muhammad},
title = {Security and Privacy in IoT Using Machine Learning and Blockchain: Threats and Countermeasures},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3417987},
doi = {10.1145/3417987},
abstract = {Security and privacy of users have become significant concerns due to the involvement of the Internet of Things (IoT) devices in numerous applications. Cyber threats are growing at an explosive pace making the existing security and privacy measures inadequate. Hence, everyone on the Internet is a product for hackers. Consequently, Machine Learning (ML) algorithms are used to produce accurate outputs from large complex databases, where the generated outputs can be used to predict and detect vulnerabilities in IoT-based systems. Furthermore, Blockchain (BC) techniques are becoming popular in modern IoT applications to solve security and privacy issues. Several studies have been conducted on either ML algorithms or BC techniques. However, these studies target either security or privacy issues using ML algorithms or BC techniques, thus posing a need for a combined survey on efforts made in recent years addressing both security and privacy issues using ML algorithms and BC techniques. In this article, we provide a summary of research efforts made in the past few years, from 2008 to 2019, addressing security and privacy issues using ML algorithms and BC techniques in the IoT domain. First, we discuss and categorize various security and privacy threats reported in the past 12 years in the IoT domain. We then classify the literature on security and privacy efforts based on ML algorithms and BC techniques in the IoT domain. Finally, we identify and illuminate several challenges and future research directions using ML algorithms and BC techniques to address security and privacy issues in the IoT domain.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {122},
numpages = {37},
keywords = {machine learning, cybersecurity, Internet of Things, Blockchain}
}

@inproceedings{10.1145/3148055.3148081,
author = {Yaseen, Muhammad Usman and Anjum, Ashiq and Antonopoulos, Nick},
title = {Modeling and Analysis of a Deep Learning Pipeline for Cloud Based Video Analytics},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3148055.3148081},
doi = {10.1145/3148055.3148081},
abstract = {Video analytics systems based on deep learning approaches are becoming the basis of many widespread applications including smart cities to aid people and traffic monitoring. These systems necessitate massive amounts of labeled data and training time to perform fine tuning of hyper-parameters for object classification. We propose a cloud based video analytics system built upon an optimally tuned deep learning model to classify objects from video streams. The tuning of the hyper-parameters including learning rate, momentum, activation function and optimization algorithm is optimized through a mathematical model for efficient analysis of video streams. The system is capable of enhancing its own training data by performing transformations including rotation, flip and skew on the input dataset making it more robust and self-adaptive. The use of in-memory distributed training mechanism rapidly incorporates large number of distinguishing features from the training dataset - enabling the system to perform object classification with least human assistance and external support. The validation of the system is performed by means of an object classification case-study using a dataset of 100GB in size comprising of 88,432 video frames on an 8 node cloud. The extensive experimentation reveals an accuracy and precision of 0.97 and 0.96 respectively after a training of 6.8 hours. The system is scalable, robust to classification errors and can be customized for any real-life situation.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {121–130},
numpages = {10},
keywords = {cloud computing, convolutional neural network, video analytics},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@article{10.1145/3466721,
author = {Chowdhury, Morshed and Ray, Biplob and Chowdhury, Sujan and Rajasegarar, Sutharshan},
title = {A Novel Insider Attack and Machine Learning Based Detection for the Internet of Things},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1914},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3466721},
doi = {10.1145/3466721},
abstract = {Due to the widespread functional benefits, such as supporting internet connectivity, having high visibility and enabling easy connectivity between sensors, the Internet of Things (IoT) has become popular and used in many applications, such as for smart city, smart health, smart home, and smart vehicle realizations. These IoT-based systems contribute to both daily life and business, including sensitive and emergency situations. In general, the devices or sensors used in the IoT have very limited computational power, storage capacity, and communication capabilities, but they help to collect a large amount of data as well as maintain communication with the other devices in the network. Since most of the IoT devices have no physical security, and often are open to everyone via radio communication and via the internet, they are highly vulnerable to existing and emerging novel security attacks. Further, the IoT devices are usually integrated with the corporate networks; in this case, the impact of attacks will be much more significant than operating in isolation. Due to the constraints of the IoT devices, and the nature of their operation, existing security mechanisms are less effective for countering the attacks that are specific to the IoT-based systems. This article presents a new insider attack, named loophole attack, that exploits the vulnerabilities present in a widely used IPv6 routing protocol in IoT-based systems, called RPL (Routing over Low Power and Lossy Networks). To protect the IoT system from this insider attack, a machine learning based security mechanism is presented. The proposed attack has been implemented using a Contiki IoT operating system that runs on the Cooja simulator, and the impacts of the attack are analyzed. Evaluation on the collected network traffic data demonstrates that the machine learning based approaches, along with the proposed features, help to accurately detect the insider attack from the network traffic data.},
journal = {ACM Trans. Internet Things},
month = {jul},
articleno = {26},
numpages = {23},
keywords = {RPL, insider attack, RPL security, machine learning, IoT, Contiki}
}

@inproceedings{10.1145/3324921.3328783,
author = {Pajola, Luca and Pasa, Luca and Conti, Mauro},
title = {Threat is in the Air: Machine Learning for Wireless Network Applications},
year = {2019},
isbn = {9781450367691},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3324921.3328783},
doi = {10.1145/3324921.3328783},
abstract = {With the spread of wireless application, huge amount of data is generated every day. Thanks to its elasticity, machine learning is becoming a fundamental brick in this field, and many of applications are developed with the use of it and the several techniques that it offers. However, machine learning suffers on different problems and people that use it often are not aware of the possible threats. Often, an adversary tries to exploit these vulnerabilities in order to obtain benefits; because of this, adversarial machine learning is becoming wide studied in the scientific community. In this paper, we show state-of-the-art adversarial techniques and possible countermeasures, with the aim of warning people regarding sensible argument related to the machine learning.},
booktitle = {Proceedings of the ACM Workshop on Wireless Security and Machine Learning},
pages = {16–21},
numpages = {6},
keywords = {machine learning, security, Wireless network applications, adversarial machine learning},
location = {Miami, FL, USA},
series = {WiseML 2019}
}

@inproceedings{10.1145/3368756.3369024,
author = {Abakouy, Redouan and En-naimi, El Mokhtar and Haddadi, Anass El and Lotfi, Elaachak},
title = {Data-Driven Marketing: How Machine Learning Will Improve Decision-Making for Marketers},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3368756.3369024},
doi = {10.1145/3368756.3369024},
abstract = {Email marketing is an effective channel in marketing strategies not only as a tool to increase brand visibility and brand awareness, but also as an excellent tool to help promote and sell. It will continue to be a critical channel for content marketers. Even with the advent of social media and networking platforms, email marketing still remains the most preferred channel for generating leads, informing and influencing customers.In this paper, we present our experiences using a learning model on predicting the "click" and "conversion" of email-marketing. We present a comparative study on the most popular machine learning methods applied to the challenging problem of email marketing personalization. Subject and sender lines have a strong influence on click rates of the emails, as the customers often open and click emails based on the subject and the sender. We propose a method to aid the marketers by predicting subject-line click rates by learning from history of subject lines. In the first step of our experiences, all models were applied and evaluated by cross-validation. In the second step, the improvement of the performance offered by the boosting has been studied. In order to determine the most efficient parameter combinations we performed a series of simulations for each method and for a wide range of parameters. Our results demonstrate that it is possible to predict the rate for a targeted marketing email to be clicked or not.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {46},
numpages = {5},
keywords = {marketing, machine learning, prediction},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3427477.3429781,
author = {Mandal, Atanu and Sinaeepourfard, Amir and Naskar, Sudip Kumar},
title = {VDA: Deep Learning Based Visual Data Analysis in Integrated Edge to Cloud Computing Environment},
year = {2021},
isbn = {9781450381840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3427477.3429781},
doi = {10.1145/3427477.3429781},
abstract = { In recent years, video surveillance technology has become pervasive in every sphere. The manual generation of videos’ descriptions requires enormous time and labor, and sometimes essential aspects of videos are overlooked in human summaries. The present work is an attempt towards the automated description generation of Surveillance Video. The proposed method consists of the extraction of key-frames from a surveillance video, objects detection in the key-frames, natural language (English) description generation of the key-frames, and summarizing the descriptions. The key-frames are identified based on a structural similarity index measure. Object detection in a key-frame is performed using the architecture of Single Shot Detection. We used Long Short Term Memory (LSTM) to generate captions from frames. Translation Error Rate (TER) is used to identify and remove duplicate event descriptions. Term frequency-inverse document frequency (TF-IDF) is used to rank the event descriptions generated from a video, and the top-ranked the description is returned as the system generated a summary of the video. We evaluated the Microsoft Video Description Corpus (MSVD) data set to validate our proposed approach, and the system produces a Bilingual Evaluation Understudy (BLEU) score of 46.83.},
booktitle = {Adjunct Proceedings of the 2021 International Conference on Distributed Computing and Networking},
pages = {7–12},
numpages = {6},
keywords = {Internet of Things, Smart Surveillance, Smart City, Content-based Video Retrieval, Video Summarization},
location = {Nara, Japan},
series = {ICDCN '21}
}

@inproceedings{10.1145/3269961.3269972,
author = {Rahbari, Dadmehr and Nickray, Mohsen and Heydari, Gholamreza},
title = {A Two-Stage Technique for Quick and Low Power Offloading in IoT},
year = {2018},
isbn = {9781450365321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3269961.3269972},
doi = {10.1145/3269961.3269972},
abstract = {Fog computing (FC) as an extension of cloud computing provides a lot of smart devices at the network edge, which can store and process data near end users. Since FC reduces latency and power consumption, it is suitable for the Internet of Things (IoT) applications in smart cities. In FC, the Mobile Devices (MDs) can offload its heavy tasks to Fog Devices (FDs). The selection of best FD for offloading has serious challenges in the time and energy. In this paper, we present a Module Placement method by Classification and regression tree Algorithm (MPCA). We select the best FDs for modules by MPCA. Initially, if the total power consumption of CPU and memory in MDs is greater than Wi-Fi's power consumption, the offloading will be done. The MPCA's decision parameters for selecting of best FD include authentication, confidentiality, integrity, availability, capacity, speed, and cost. To evaluate our proposed approach, we simulate MPCA and compare it with First-Fit (FF) and local mobile processing methods in Cloud, fog, and MDs. The results show that the proposed method is superior to other compared methods in the average power consumption by 38.41%, response time by 37%, and 13.76% in performance.},
booktitle = {Proceedings of the International Conference on Smart Cities and Internet of Things},
articleno = {4},
numpages = {8},
keywords = {Module placement, Internet of Things, Smart City, Mobile fog computing},
location = {Mashhad, Iran},
series = {SCIOT '18}
}

@inbook{10.1145/3459637.3481999,
author = {Draschner, Carsten Felix and Stadler, Claus and Bakhshandegan Moghaddam, Farshad and Lehmann, Jens and Jabeen, Hajira},
title = {DistRDF2ML - Scalable Distributed In-Memory Machine Learning Pipelines for RDF Knowledge Graphs},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3459637.3481999},
abstract = {This paper presents DistRDF2ML, the generic, scalable, and distributed framework for creating in-memory data preprocessing pipelines for Spark-based machine learning on RDF knowledge graphs. This framework introduces software modules that transform large-scale RDF data into ML-ready fixed-length numeric feature vectors. The developed modules are optimized to the multi-modal nature of knowledge graphs. DistRDF2ML provides aligned software design and usage principles as common data science stacks that offer an easy-to-use package for creating machine learning pipelines. The modules used in the pipeline, the hyper-parameters and the results are exported as a semantic structure that can be used to enrich the original knowledge graph. The semantic representation of metadata and machine learning results offers the advantage of increasing the machine learning pipelines' reusability, explainability, and reproducibility. The entire framework of DistRDF2ML is open source, integrated into the holistic SANSA stack, documented in scala-docs, and covered by unit tests. DistRDF2ML demonstrates its scalable design across different processing power configurations and (hyper-)parameter setups within various experiments. The framework brings the three worlds of knowledge graph engineers, distributed computation developers, and data scientists closer together and offers all of them the creation of explainable ML pipelines using a few lines of code.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4465–4474},
numpages = {10}
}

@inproceedings{10.1145/3460426.3466932,
author = {Ni, Yu-Shu and Tsai, Chia-Chi and Guo, Jiun-In and Hwang, Jenq-Neng and Wu, Bo-Xun and Hu, Po-Chi and Kuo, Ted T. and Chen, Po-Yu and Kuo, Hsien-Kai},
title = {Summary of the 2021 Embedded Deep Learning Object Detection Model Compression Competition for Traffic in Asian Countries},
year = {2021},
isbn = {9781450384636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3460426.3466932},
doi = {10.1145/3460426.3466932},
abstract = {The 2021 embedded deep learning object detection model compression competition for traffic in Asian countries held in IEEE ICMR2021 Grand Challenges focuses on the object detection technologies in autonomous driving scenarios. The competition aims to detect objects in traffic with low complexity and small model size in the Asia countries (e.g., Taiwan), which contains several harsh driving environments. The target detected objects include vehicles, pedestrians, bicycles and crowded scooters. There are 89,002 annotated images provided for model training and 1,000 images for validation. Additional 5,400 testing images are used in the contest evaluation process, in which 2,700 of them are used in the qualification stage competition, and the rest are used in the final stage competition. There are in total 308 registered teams joining this competition this year, and the top 15 teams with the highest detection accuracy entering the final stage competition, from which 9 teams submitted the final results. The overall best model belongs to team "as798792", followed by team "Deep Learner" and team "UCBH." Two special awards of best accuracy award best and bicycle detections go to the same team "as798792," and the other special award of scooter detection goes to team "abcda."},
booktitle = {Proceedings of the 2021 International Conference on Multimedia Retrieval},
pages = {244–249},
numpages = {6},
keywords = {object detection, embedded deep learning, autonomous driving},
location = {Taipei, Taiwan},
series = {ICMR '21}
}

@inproceedings{10.1145/3370748.3406588,
author = {Cerutti, Gianmarco and Andri, Renzo and Cavigelli, Lukas and Farella, Elisabetta and Magno, Michele and Benini, Luca},
title = {Sound Event Detection with Binary Neural Networks on Tightly Power-Constrained IoT Devices},
year = {2020},
isbn = {9781450370530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3370748.3406588},
doi = {10.1145/3370748.3406588},
abstract = {Sound event detection (SED) is a hot topic in consumer and smart city applications. Existing approaches based on deep neural networks (DNNs) are very effective, but highly demanding in terms of memory, power, and throughput when targeting ultra-low power always-on devices.Latency, availability, cost, and privacy requirements are pushing recent IoT systems to process the data on the node, close to the sensor, with a very limited energy supply, and tight constraints on the memory size and processing capabilities precluding to run state-of-the-art DNNs.In this paper, we explore the combination of extreme quantization to a small-footprint binary neural network (BNN) with the highly energy-efficient, RISC-V-based (8+1)-core GAP8 microcontroller. Starting from an existing CNN for SED whose footprint (815 kB) exceeds the 512 kB of memory available on our platform, we retrain the network using binary filters and activations to match these memory constraints. (Fully) binary neural networks come with a natural drop in accuracy of 12-18% on the challenging ImageNet object recognition challenge compared to their equivalent full-precision baselines. This BNN reaches a 77.9% accuracy, just 7% lower than the full-precision version, with 58 kB (7.2\texttimes{} less) for the weights and 262 kB (2.4\texttimes{} less) memory in total. With our BNN implementation, we reach a peak throughput of 4.6 GMAC/s and 1.5 GMAC/s over the full network, including preprocessing with Mel bins, which corresponds to an efficiency of 67.1 GMAC/s/W and 31.3 GMAC/s/W, respectively. Compared to the performance of an ARM Cortex-M4 implementation, our system has a 10.3\texttimes{} faster execution time and a 51.1\texttimes{} higher energy-efficiency.},
booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
pages = {19–24},
numpages = {6},
keywords = {ultra low power, binary neural networks, sound event detection},
location = {Boston, Massachusetts},
series = {ISLPED '20}
}

@article{10.1145/3190579,
author = {Bertino, Elisa and Jahanshahi, Mohammad R.},
title = {Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3190579},
doi = {10.1145/3190579},
journal = {J. Data and Information Quality},
month = {may},
articleno = {1},
numpages = {6},
keywords = {Civil engineering, device swarms, edge computing}
}

@inproceedings{10.1145/3312714.3312726,
author = {P\'{e}rez, Paula Carballo and Ortega, Felipe and Garc\'{\i}a, Jorge Navarro and Diego, Isaac Mart\'{\i}n de},
title = {Combining Machine Learning and Symbolic Representation of Time Series for Classification of Behavioural Patterns},
year = {2019},
isbn = {9781450362351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3312714.3312726},
doi = {10.1145/3312714.3312726},
abstract = {The emergence of affordable wireless sensors has enabled the development of information systems combining sophisticated data processing and machine learning algorithms for pattern recognition. In many cases, these systems deal with time-series data, continuously gathered by sensors that compile detailed activity records. However, these datasets are frequently affected by numerous problems, including noisy data acquisition, missing data and utilization of inefficient techniques for information representation, which lead to deficient performance in machine learning applications. In this paper, we introduce a novel method to combine the efficient symbolic representation of time-series data with machine learning to improve the performance of classification systems tailored to detection of behavioural patterns of interest.},
booktitle = {Proceedings of the 2019 the 5th International Conference on E-Society, e-Learning and e-Technologies},
pages = {93–97},
numpages = {5},
keywords = {symbolic aggregate approximation, time series, Machine learning, behavioural patterns, support vector machines},
location = {Vienna, Austria},
series = {ICSLT 2019}
}

@inproceedings{10.1145/3321408.3323089,
author = {Fan, Xiaochen and Xiang, Chaocan and Gong, Liangyi and He, Xiangjian and Chen, Chao and Huang, Xiang},
title = {UrbanEdge: Deep Learning Empowered Edge Computing for Urban IoT Time Series Prediction},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3321408.3323089},
doi = {10.1145/3321408.3323089},
abstract = {The revolution of smart city has led to rapid development and proliferation of Internet of Things (IoT) technologies, with the focus on transmitting raw sensory data into valuable knowledge. Meanwhile, the ubiquitous deployments of IoT are raising the importance of processing data in real-time at the edge of networks rather than in remote cloud data centers. Based on above, edge computing has been proposed to exploit the capabilities of edge devices in providing in-proximity computing services for various IoT applications. In this paper, we present UrbanEdge, a conceptual edge computing architecture empowered by deep learning for urban IoT time series prediction. We design a hierarchical architecture to process correlated IoT time series and illustrate the work-flow of UrbanEdge in data collection, data transmission and data processing. As a core component of UrbanEdge, a deep learning model is developed with attention-based recurrent neural networks. Composed with multiple processing layers, the deep learning model can extract feature representations from raw IoT data for monitoring and prediction. We evaluate the designed deep learning model of UrbanEdge on real-world datasets, evaluation results show that the UrbanEdge outperforms other baseline methods in time series prediction.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {12},
numpages = {6},
keywords = {internet of things, deep learning, edge computing, time series prediction},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inbook{10.1145/3459637.3482482,
author = {Wang, Zhaonan and Jiang, Renhe and Cai, Zekun and Fan, Zipei and Liu, Xin and Kim, Kyoung-Sook and Song, Xuan and Shibasaki, Ryosuke},
title = {Spatio-Temporal-Categorical Graph Neural Networks for Fine-Grained Multi-Incident Co-Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3459637.3482482},
abstract = {Forecasting incident occurrences (e.g. crime, EMS, traffic accident) is a crucial task for emergency service providers and transportation agencies in performing response time optimization and dynamic fleet management. However, such events are by nature rare and sparse, which causes the label imbalance problem and inferior performance of models relying on data sufficiency. The existing studies circumvent, instead of truly solving, this issue by defining the incident prediction problem in a coarse-grained temporal (e.g. daily) setting, which leaves the proposed models unrobust to fine-grained dynamics and trivial for the real-world decision making. In this paper, we tackle the temporally fine-grained incident prediction problem in a sparse setting by explicitly exploiting the behind-the-scene chainlike triggering mechanism. Moreover, this chain effect roots in multiple domains (i.e. spatial, categorical), which further entangles with the temporal dimension and happens to be time-variant. To be specific, we propose a novel deep learning framework, namely Spatio-Temporal-Categorical Graph Neural Networks (STC-GNN), to handle the multidimensional and dynamic chain effect for performing fine-grained multi-incident co-prediction. Extensive experiments on three real-world city-level incident datasets verify the insightfulness of our perspective and effectiveness of the proposed model.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2060–2069},
numpages = {10}
}

@inproceedings{10.1145/3360774.3360803,
author = {Kulkarni, Adita and Seetharam, Anand and Ramesh, Arti},
title = {DeepFit: Deep Learning Based Fitness Center Equipment Use Modeling and Prediction},
year = {2019},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3360774.3360803},
doi = {10.1145/3360774.3360803},
abstract = {In today's busy modern life, modeling and accurately predicting fitness center equipment usage and availability is essential for improving human fitness and well-being as it provides people the flexibility to plan their schedule and exercise at their convenience. In addition to its crucial role in ensuring a healthy and sustainable future, adopting a data-driven approach for modeling and predicting fitness center equipment usage is necessary for planning the optimal square footage for developing a fitness center, and determining the kinds of equipment to purchase and install. In this paper, we develop DeepFit, a deep learning based system that predicts future fitness center equipment usage based on historical data. To this end, we design a Long Short Term Memory (LSTM) based sequence-to-sequence model that captures the dependencies in the data. The sequence-to-sequence model comprises of an encoder and a decoder, each of which separately is a deep Recurrent Neural Network (RNN). The basic cell structure in the RNN architecture is an LSTM cell.We evaluate DeepFit on equipment usage data collected from a university campus fitness center over a period of 1.5 years and demonstrate that it is able to accurately predict future fitness center equipment usage. We show that DeepFit outperforms the linear regression and ARIMA baselines in terms of Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) providing 17% performance improvement on average. We then present a discussion on hyper-parameter tuning and selection in our model. Finally, we investigate the benefits of augmenting the deep learning model in DeepFit with features such as whether the school is in session and month of the year and observe that the enhanced DeepFit system obtains performance improvements of 35% and 32% over linear regression and ARIMA, respectively. Our experiments show that the trained DeepFit model requires limited computational resources at test time, thus making it an attractive system for practical deployment.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {394–403},
numpages = {10},
keywords = {time series modeling, deep learning, RNN},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@inproceedings{10.1145/3410670.3410862,
author = {Ceccarini, Chiara and Delnevo, Giovanni and Prandi, Catia},
title = {FruGar: Exploiting Deep Learning and Crowdsourcing for Frugal Gardening},
year = {2020},
isbn = {9781450380782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3410670.3410862},
doi = {10.1145/3410670.3410862},
abstract = {To make smart cities more sustainable, and technologies and smart services accessible to a broader range of people, the frugal innovation paradigm comes in handy. In such a scenario, an interesting issue to investigate is gardening (both home and community gardening), considered a possibility toward sustainable development and environmental resilience. Following this line of thought, in this paper, we present our system, called FruGar (frugal gardening), designed and developed to facilitate casual citizens while gardening. In particular, our approach takes advantage of machine learning and crowdsourcing to provide casual citizens with a frugal tool for plant disease detection.},
booktitle = {Proceedings of the 1st Workshop on Experiences with the Design and Implementation of Frugal Smart Objects},
pages = {7–11},
numpages = {5},
keywords = {convolutional neural network, plant disease identification, web application},
location = {London, United Kingdom},
series = {FRUGALTHINGS'20}
}

@inproceedings{10.1145/3139958.3140053,
author = {Zhang, Sheng and Zhao, Shenglin and Yuan, Mingxuan and Zeng, Jia and Yao, Jianguo and Lyu, Michael R. and King, Irwin},
title = {Traffic Prediction Based Power Saving in Cellular Networks: A Machine Learning Method},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3139958.3140053},
doi = {10.1145/3139958.3140053},
abstract = {In smart cities, green cellular networks play a crucial role to support wireless access for numerous devices anywhere and anytime with efficiency and sustainability. Because base stations (BSes) consume more than 70% of overall cellular network infrastructure energy, saving the power consumption of BSes is the key task to build a green cellular network. Except for low power design of the BS hardware and software, the traffic-driven BS sleeping operation is an economical way to improve existing cellular networks, which can reduce the BS power consumption at low traffic load. However, prior BS sleeping strategies establish on the static temporal characteristics of traffic load, which ignore the fact that network traffic is influenced by many factors such as time, human mobility, holiday, weather, etc. Hence, prior traffic estimation is coarse, and the BS sleeping strategies cannot apply to the changing network traffic. In this paper, we exploit a machine learning method to estimate the BS traffic and propose a BS sleeping strategy based on predicted traffic for power saving in the cellular network. We analyze network traffic in multi-views: temporal influence, spatial influence, and event influence. Then, we propose a multi-view ensemble learning model to predict network traffic load, which learns the traffic in multi-views and combine the results with ensemble. Furthermore, we formulate a BS sleeping strategy based on the predicted traffic load. Finally, we evaluate our traffic prediction algorithm on real cellular network data. The evaluation shows that our traffic prediction algorithm improves about 40% than state-of-the-art machine learning methods. Also, we evaluate the proposed BS sleeping strategy, which yields about 10% more energy savings and less device damage than the competitors in the simulated environment.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {29},
numpages = {10},
keywords = {Multi-view Learning, Spatio-Temporal Data Analysis, Green Cellular Network, Network Traffic Prediction, Smart City},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3411764.3445736,
author = {Anik, Ariful Islam and Bunt, Andrea},
title = {Data-Centric Explanations: Explaining Training Data of Machine Learning Systems to Promote Transparency},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3411764.3445736},
doi = {10.1145/3411764.3445736},
abstract = {Training datasets fundamentally impact the performance of machine learning (ML) systems. Any biases introduced during training (implicit or explicit) are often reflected in the system's behaviors leading to questions about fairness and loss of trust in the system. Yet, information on training data is rarely communicated to stakeholders. In this work, we explore the concept of data-centric explanations for ML systems that describe the training data to end-users. Through a formative study, we investigate the potential utility of such an approach, including the information about training data that participants find most compelling. In a second study, we investigate reactions to our explanations across four different system scenarios. Our results suggest that data-centric explanations have the potential to impact how users judge the trustworthiness of a system and to assist users in assessing fairness. We discuss the implications of our findings for designing explanations to support users’ perceptions of ML systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {75},
numpages = {13},
keywords = {User Expertise, Explanations, Trust, Machine Learning Systems, Transparency, Fairness, Training Data},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3418204,
author = {Shahid, Huniya and Shah, Munam Ali and Almogren, Ahmad and Khattak, Hasan Ali and Din, Ikram Ud and Kumar, Neeraj and Maple, Carsten},
title = {Machine Learning-Based Mist Computing Enabled Internet of Battlefield Things},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1533-5399},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3418204},
doi = {10.1145/3418204},
abstract = {The rapid advancement in information and communication technology has revolutionized military departments and their operations. This advancement also gave birth to the idea of the Internet of Battlefield Things (IoBT). The IoBT refers to the fusion of the Internet of Things (IoT) with military operations on the battlefield. Various IoBT-based frameworks have been developed for the military. Nonetheless, many of these frameworks fail to maintain a high Quality of Service (QoS) due to the demanding and critical nature of IoBT. This study makes the use of mist computing while leveraging machine learning. Mist computing places computational capabilities on the edge itself (mist nodes), e.g., on end devices, wearables, sensors, and micro-controllers. This way, mist computing not only decreases latency but also saves power consumption and bandwidth as well by eliminating the need to communicate all data acquired, produced, or sensed. A mist-based version of the IoTNetWar framework is also proposed in this study. The mist-based IoTNetWar framework is a four-layer structure that aims at decreasing latency while maintaining QoS. Additionally, to further minimize delays, mist nodes utilize machine learning. Specifically, they use the delay-based K nearest neighbour algorithm for device-to-device communication purposes. The primary research objective of this work is to develop a system that is not only energy, time, and bandwidth-efficient, but it also helps military organizations with time-critical and resources-critical scenarios to monitor troops. By doing so, the system improves the overall decision-making process in a military campaign or battle. The proposed work is evaluated with the help of simulations in the EdgeCloudSim. The obtained results indicate that the proposed framework can achieve decreased network latency of 0.01 s and failure rate of 0.25% on average while maintaining high QoS in comparison to existing solutions.},
journal = {ACM Trans. Internet Technol.},
month = {aug},
articleno = {101},
numpages = {26},
keywords = {fog computing, battlefield monitoring system, device-to-device communication, real-time system, IoBT, K nearest neighbours, Mist computing}
}

@inproceedings{10.1145/3474717.3483921,
author = {Li, He and Jin, Duo and Li, Xuejiao and Huang, Jianbin and Yoo, Jaesoo},
title = {Multi-Task Synchronous Graph Neural Networks for Traffic Spatial-Temporal Prediction},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3474717.3483921},
doi = {10.1145/3474717.3483921},
abstract = {Traffic spatial-temporal prediction is of great significance to traffic management and urban construction. In this paper, we propose a multi-task graph Synchronous neural network (MTSGNN) to synchronously predict the spatial-temporal data at the regions and transitions between regions. The method of constructing "multitask graph representation" is proposed to retain the information of regions and transitions that existing works can not reflect. Then our model synchronously captures multiple types of dynamic spatial correlations, models dynamic temporal dependencies and re-weights different time steps to solve the problem of long-term time modeling. In three real data sets, we verify the validity of the proposed model.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {137–140},
numpages = {4},
keywords = {Graph Neural Networks, Spatial-Temporal Prediction, Spatial-Temporal Correlations},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1109/TASLP.2017.2785283,
author = {Tan, Chuanqi and Wei, Furu and Zhou, Qingyu and Yang, Nan and Du, Bowen and Lv, Weifeng and Zhou, Ming},
title = {Context-Aware Answer Sentence Selection With Hierarchical Gated Recurrent Neural Networks},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1109/TASLP.2017.2785283},
doi = {10.1109/TASLP.2017.2785283},
abstract = {In this paper, we study the task of reading comprehension style answer sentence selection that aims to select the best sentence from a given passage to answer a question. Unlike most previous works that match the question and each candidate sentence separately, we observe that the context information among sentences in the same passage plays a vital role in this task. We propose modeling context information with hierarchical gated recurrent neural networks. Specifically, we first apply a word level recurrent neural network to model the context independent matching between the question and each candidate sentence. We then employ a sentence level recurrent neural network to incorporate the context information among all candidate sentences. Moreover, we introduce the gate mechanism to select matching information before feeding into recurrent neural networks at both word and sentence level. Experiments on the WikiQA and SQuAD datasets show that our model outperforms state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {540–549},
numpages = {10}
}

@article{10.1145/3406962,
author = {Arabghalizi, Tahereh and Labrinidis, Alexandros},
title = {Data-Driven Bus Crowding Prediction Models Using Context-Specific Features},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-1922},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3406962},
doi = {10.1145/3406962},
abstract = {Public transit is one of the first things that come to mind when someone talks about “smart cities.” As a result, many technologies, applications, and infrastructure have already been deployed to bring the promise of the smart city to public transportation. Most of these have focused on answering the question, “When will my bus arrive?”; little has been done to answer the question, “How full will my next bus be?” which also dramatically affects commuters’ quality of life. In this article, we consider the bus fullness problem. In particular, we propose two different formulations of the problem, develop multiple predictive models, and evaluate their accuracy using data from the Pittsburgh region. Our predictive models consistently outperform the baselines (by up to 8 times).},
journal = {ACM/IMS Trans. Data Sci.},
month = {sep},
articleno = {23},
numpages = {33},
keywords = {intelligent transportation, urban computing, Smart city, crowdedness prediction}
}

@article{10.1145/3510829,
author = {Li, He and Li, Xuejiao and Su, Liangcai and Jin, Duo and Huang, Jianbin and Huang, Deshuang},
title = {Deep Spatio-Temporal Adaptive 3D Convolutional Neural Networks for Traffic Flow Prediction},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2157-6904},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3510829},
doi = {10.1145/3510829},
abstract = {Traffic flow prediction is the upstream problem of path planning, intelligent transportation system, and other tasks. Many studies have been carried out on the traffic flow prediction of the spatio-temporal network, but the effects of spatio-temporal flexibility (historical data of the same type of time intervals in the same location will change flexibly) and spatio-temporal correlation (different road conditions have different effects at different times) have not been considered at the same time. We propose the Deep Spatio-temporal Adaptive 3D Convolution Neural Network (ST-A3DNet), which is a new scheme to solve both spatio-temporal correlation and flexibility, and consider spatio-temporal complexity (complex external factors, such as weather and holidays). Different from other traffic forecasting models, ST-A3DNet captures the spatio-temporal relationship at the same time through the Adaptive 3D convolution module, assigns different weights flexibly according to the influence of historical data, and obtains the impact of external factors on the flow through the ex-mask module. Considering the holidays and weather conditions, we train our model for experiments in Xi’an and Chengdu. We evaluate the ST-A3DNet and the results show that we have better results than the other 11 baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {19},
numpages = {21},
keywords = {spatial-temporal information, Convolutional neural networks, traffic prediction}
}

@inbook{10.1145/3442442.3451140,
author = {Zhang, Lei and Liu, Jie and Zhang, Fuquan and Mao, Yu},
title = {Distributed Fog Computing Based on Improved LT Codes for Deep Learning in Web of Things},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3442442.3451140},
abstract = {With the rapid development of the Web of Things, there have been a lot of sensors deployed. Advanced knowledge can be achieved by deep learning method and easier integration with open Web standards. A large number of the data generated by sensors required extra processing resources due to the limited resources of the sensors. Due to the limitation of bandwidth or requirement of low latency, it is impossible to transfer such large amounts of data to cloud servers for processing. Thus, the concept of distributed fog computing has been proposed to process such big data into knowledge in real-time. Large scale fog computing system is built using cheap devices, denotes as fog nodes. Therefore, the resiliency to fog node failures should be considered in design of distributed fog computing. LT codes (LTC) have important applications in the design of modern distributed computing, which can reduce the latency of the computing tasks, such as matrix multiplication in deep learning methods. In this paper, we consider that fog nodes may be failure, and an improved LT codes are applied to matrix multiplication of distributed fog computing process to reduce latency. Numerical results show that the improved LTC based scheme can reduce average overhead and degree simultaneously, which reduce the latency and computation complexity of distributed fog computing.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {57–62},
numpages = {6}
}

@inproceedings{10.1145/3236461.3241972,
author = {Xu, Weijia and Ruiz-Juri, Natalia and Huang, Ruizhu and Duthie, Jennifer and Clary, John},
title = {Automated Pedestrian Safety Analysis Using Data from Traffic Monitoring Cameras},
year = {2018},
isbn = {9781450357869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3236461.3241972},
doi = {10.1145/3236461.3241972},
abstract = {Transportation agencies often own extensive networks of monocular traffic cameras, which are typically used for traffic monitoring. However, the information captured by such cameras can also be of great value for transportation planning and operations applications, particularly when large data sets may be systematically analyzed. In this paper, we propose an approach to use data collected by existing monitoring cameras to automatically identify locations where pedestrian safety may be a concern. Our methodology utilizes a convolutional-neural-network-based method to recognize pedestrians in traffic camera feeds. Results are stored and aggregated, and may be queried for further analyses. The proposed computational approach may leverage hardware such as GPUs and distributed computing clusters to enable the analysis of large volumes of data. The post recognition analysis utilizes unsupervised learning methods to identify the spatial and temporal patterns of pedestrian positions, which are then correlated to specific scenarios such as usage of crosswalk, compliance with traffic signals, and pedestrian-vehicle interactions. Applications include the identification of potential safety concerns, measuring the effectiveness of proposed safety strategies, and identifying the need for improvements. This work provides preliminary results based on data from cameras owned by the City of Austin. We also discuss outputs such as pedestrian volume estimation and crossing hot-zones identification in the context of Smart Cities, and identify potential challenges and limitations.},
booktitle = {Proceedings of the 1st ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {3},
numpages = {8},
keywords = {Video Analysis, Transportation, Smart City, Deep Learning},
location = {Portland, OR, USA},
series = {SCC '18}
}

@inproceedings{10.1145/3041021.3054716,
author = {Gyrard, Amelie and Serrano, Martin and Jares, Joao Bosco and Datta, Soumya Kanti and Ali, Muhammad Intizar},
title = {Sensor-Based Linked Open Rules (S-LOR): An Automated Rule Discovery Approach for IoT Applications and Its Use in Smart Cities},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3041021.3054716},
doi = {10.1145/3041021.3054716},
abstract = {This paper introduces an automated rule discovery approach for IoT device data (S-LOR: Sensor-based Linked Open Rules) and its use in smart cities. S-LOR is built following Linked Open Data (LOD) standards and provides support for semantics-based mechanisms to share, reuse and execute logical rules for interpreting data produced by IoT systems. S-LOR follows LOD principles for data re-usability, semantics-based reasoning and interoperability. In this paper, S-LOR main capability is demonstrated in the context of enabling semantics-based reasoning mechanisms and tools according to application-demand and user requirements. S-LOR (i) supports an automated interpretation of IoT data by executing rules, and (ii) allows an automated rule discovery interface. The implemented S-LOR mechanism can automatically process and interpret data from IoT devices by using rule-based discovery paradigm. Its extension called Linked Open Reasoning (LOR) enables and encourages re-usability of reasoning mechanisms and tools for different IoT smart city applications. The use cases described in this paper fits in the domain of smart city applications within Internet of Things deployed systems.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1153–1159},
numpages = {7},
keywords = {semantic web technologies, knowledge, internet of things, reasoning, semantic web of things},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1145/3374749,
author = {Tian, Zhihong and Luo, Chaochao and Lu, Hui and Su, Shen and Sun, Yanbin and Zhang, Man},
title = {User and Entity Behavior Analysis under Urban Big Data},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-1922},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3374749},
doi = {10.1145/3374749},
abstract = {Recently, the urban network infrastructure has undergone a rapid expansion that is increasingly generating a large quantity of data and transforming our cities into smart cities. However, serious security problems arise with this development with more and more smart devices collecting private information under smart city scenario. In this article, we investigate the task of detecting insiders’ anomalous behaviors to prevent urban big data leakage. Specifically, we characterize a user's daily activities from four perspectives and use several deep learning algorithms (long short-term memory (LSTM) and convolutional LSTM (convLSTM)) to calculate deviations between realistic actions and normalcy of daily behaviors and use multilayer perceptron (MLP) to identify abnormal behaviors according to those deviations. To evaluate the proposed multimodel-based system (MBS), we conducted experiments on the CERT (United States Computer Emergency Readiness Team) dataset. The experimental results show that our proposed MBS has a remarkable ability to learn the normal pattern of users’ daily activities and detect anomalous behaviors.},
journal = {ACM/IMS Trans. Data Sci.},
month = {sep},
articleno = {16},
numpages = {19},
keywords = {security, UEBA, anomaly detection, deep learning}
}

@inbook{10.1145/3378393.3402284,
author = {Tiwari, Saurabh and Bhandari, Ravi and Raman, Bhaskaran},
title = {RoadCare: A Deep-Learning Based Approach to Quantifying Road Surface Quality},
year = {2020},
isbn = {9781450371292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3378393.3402284},
abstract = {Roads form a critical part of any region's infrastructure. Their constant monitoring and maintenance is thus essential. Traditional monitoring mechanisms are heavy-weight, and hence have insufficient coverage. In this paper, we explore the use of crowd-sourced intelligent measurements from commuters' smart-phone sensors. Specifically, we propose a deep-learning based approach to road surface quality monitoring, using accelerometer and GPS sensor readings. Through extensive data collection of over 36 hours on different kinds of roads, and subsequent evaluation based on this, we show that the approach can achieve high accuracy (98.5%) in a three-way classification of road surface quality. We also show how the classification can be extended to a finer grained 11-point scale of road quality. The model is also efficient: it can be implemented on today's smart-phones, thus making it practical. Our approach, called RoadCare, enables several useful smart-city applications such as spatio-temporal monitoring of the city's roads, early warning of bad road conditions, as well as choosing the "smoothest" road route to a destination.},
booktitle = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {231–242},
numpages = {12}
}

@inproceedings{10.1145/2797143.2797150,
author = {Vlachostergiou, Aggeliki and Stratogiannis, Georgios and Caridakis, George and Siolas, Georgios and Mylonas, Phivos},
title = {Smart Home Context Awareness Based on Smart and Innovative Cities},
year = {2015},
isbn = {9781450335805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2797143.2797150},
doi = {10.1145/2797143.2797150},
abstract = {In the emerging Smart Cities - Smart Homes computing paradigms developing a formalization for context information is increasingly important. In the present paper, basedon the EU FIRE research project "Social and Smart" we aim to formalize and build a complete formal definition of context in both home and city scale. Using sensors as a Smart City service and local sensors installed locally in Smart Homes, it is possible to collect continuously context data, such as temperature, humidity, noise and pollution levels. This context information can be used to adapt to user-specific needs in the Smart Home environment via the incorporation of user defined home rules. Semantic web technologies are used to support the knowledge representation of this ecosystem. The overall architecture has been experimentally verified using input from the SmartSantander Smart City project and applying it to the SandS Smart Home within the FIRE and FIWARE framework. Finally, two examples are presented in order to stress how the smart home appliances adapt their function to home rules and context information.},
booktitle = {Proceedings of the 16th International Conference on Engineering Applications of Neural Networks (INNS)},
articleno = {32},
numpages = {10},
keywords = {Pervasive Human Computer Interaction, Semantic User and Pervasive Representation, Smart Cities, Context Awareness, Smart Homes, Innovative Cities, User Modeling, Personalization},
location = {Rhodes, Island, Greece},
series = {EANN '15}
}

@inproceedings{10.1145/3240508.3240654,
author = {Chen, Ziqian and Wang, Shiqi and Wu, Dapeng Oliver and Huang, Tiejun and Duan, Ling-Yu},
title = {From Data to Knowledge: Deep Learning Model Compression, Transmission and Communication},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3240508.3240654},
doi = {10.1145/3240508.3240654},
abstract = {With the advances of artificial intelligence, recent years have witnessed a gradual transition from the big data to the big knowledge. Based on the knowledge-powered deep learning models, the big data such as the vast text, images and videos can be efficiently analyzed. As such, in addition to data, the communication of knowledge implied in the deep learning models is also strongly desired. As a specific example regarding the concept of knowledge creation and communication in the context of Knowledge Centric Networking (KCN), we investigate the deep learning model compression and demonstrate its promise use through a set of experiments. In particular, towards future KCN, we introduce efficient transmission of deep learning models in terms of both single model compression and multiple model prediction. The necessity, importance and open problems regarding the standardization of deep learning models, which enables the interoperability with the standardized compact model representation bitstream syntax, are also discussed.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1625–1633},
numpages = {9},
keywords = {deep learning model compression, standardization, knowledge communication},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1145/3339308,
author = {Zhou, Junhao and Dai, Hong-Ning and Wang, Hao},
title = {Lightweight Convolution Neural Networks for Mobile Edge Computing in Transportation Cyber Physical Systems},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3339308},
doi = {10.1145/3339308},
abstract = {Cloud computing extends Transportation Cyber-Physical Systems (T-CPS) with provision of enhanced computing and storage capability via offloading computing tasks to remote cloud servers. However, cloud computing cannot fulfill the requirements such as low latency and context awareness in T-CPS. The appearance of Mobile Edge Computing (MEC) can overcome the limitations of cloud computing via offloading the computing tasks at edge servers in approximation to users, consequently reducing the latency and improving the context awareness. Although MEC has the potential in improving T-CPS, it is incapable of processing computational-intensive tasks such as deep learning algorithms due to the intrinsic storage and computing-capability constraints. Therefore, we design and develop a lightweight deep learning model to support MEC applications in T-CPS. In particular, we put forth a stacked convolutional neural network (CNN) consisting of factorization convolutional layers alternating with compression layers (namely, lightweight CNN-FC). Extensive experimental results show that our proposed lightweight CNN-FC can greatly decrease the number of unnecessary parameters, thereby reducing the model size while maintaining the high accuracy in contrast to conventional CNN models. In addition, we also evaluate the performance of our proposed model via conducting experiments at a realistic MEC platform. Specifically, experimental results at this MEC platform show that our model can maintain the high accuracy while preserving the portable model size.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {oct},
articleno = {67},
numpages = {20},
keywords = {cyber physical systems, model compression, Jetson TX2 module, mobile edge computing, Convolutional neural network, factorization}
}

@inproceedings{10.1145/3301551.3301574,
author = {Fabregas, Aleta C. and Cruz, Debrelie and Marmeto, Mark Daniel},
title = {SUGPO: A White Spot Disease Detection in Shrimps Using Hybrid Neural Networks with Fuzzy Logic Algorithm},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3301551.3301574},
doi = {10.1145/3301551.3301574},
abstract = {Shrimp is a high-value commodity and one of the major aquaculture species not only in the Philippines but in the world. White Spot Syndrome Virus (WSSV) is a virus that has been infecting shrimp farms and affecting the shrimp productions across the globe. Manual disease inspection among the shrimps is a time consuming process in the management of shrimp farms. Thus, the researchers conducted this study in able to provide a fast disease inspection tool in order to early detect the infection before the disease results to an outbreak. The researchers developed a system that detects the white spot disease among the shrimps. Image processing techniques was utilized in the system as well as Artificial Intelligence algorithms such as Artificial Neural Network (ANN) and Fuzzy Logic. ANN has the ability to learn and classify an input base on its learnings, whereas Fuzzy Logic is known for dealing with uncertainties.The main objective of this study is to determine the accuracy and reliability rate of the tool in detecting the WSSV using the hybrid algorithm of ANN and Fuzzy Logic considering the color of the spot, location of the white spots and the discoloration in the body of the shrimp. For the testing of the system, 50 samples of shrimps were used. The system assessed each samples by analyzing the images of the shrimps. In evaluating a set of Shrimp samples, the system's diagnosis and the expert's diagnosis were compared and analyzed. The researchers used the confusion matrix and the accuracy rate formula for the computation of the accuracy rate and the Test-Retest Reliability equation for the computation of the reliability rate. The result of the tool produced a system performance with 90% accuracy rate and a reliability rate of 0.8 which is equivalent to Good Reliability. Thus, the result of this study shows that ANN and Fuzzy Logic are effective algorithms to utilize in automated disease diagnostics like the white spot disease detection.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {199–203},
numpages = {5},
keywords = {white spot syndrome, Hybrid neural networks, image processing, white spot syndrome virus, fuzzy logic algorithm},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/3394885.3439194,
author = {Wang, Zhepeng and Wu, Yawen and Jia, Zhenge and Shi, Yiyu and Hu, Jingtong},
title = {Lightweight Run-Time Working Memory Compression for Deployment of Deep Neural Networks on Resource-Constrained MCUs},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3394885.3439194},
doi = {10.1145/3394885.3439194},
abstract = {This work aims to achieve intelligence on embedded devices by deploying deep neural networks (DNNs) onto resource-constrained microcontroller units (MCUs). Apart from the low frequency (e.g., 1-16 MHz) and limited storage (e.g., 16KB to 256KB ROM), one of the largest challenges is the limited RAM (e.g., 2KB to 64KB), which is needed to save the intermediate feature maps of a DNN. Most existing neural network compression algorithms aim to reduce the model size of DNNs so that they can fit into limited storage. However, they do not reduce the size of intermediate feature maps significantly, which is referred to as working memory and might exceed the capacity of RAM. Therefore, it is possible that DNNs cannot run in MCUs even after compression. To address this problem, this work proposes a technique to dynamically prune the activation values of the intermediate output feature maps in the runtime to ensure that they can fit into limited RAM. The results of our experiments show that this method could significantly reduce the working memory of DNNs to satisfy the hard constraint of RAM size, while maintaining satisfactory accuracy with relatively low overhead on memory and run-time latency.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {607–614},
numpages = {8},
keywords = {Neural Network Deployment, Neural Network Compression, Edge Computing, Artificial Intelligence of Things (AIoT)},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@inproceedings{10.1145/3175628.3175633,
author = {Mourtaji, Youness and Bouhorma, Mohammed and Alghazzawi},
title = {Perception of a New Framework for Detecting Phishing Web Pages},
year = {2017},
isbn = {9781450352116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3175628.3175633},
doi = {10.1145/3175628.3175633},
abstract = {To be able to access a webpage or website we need its URL (Uniform Resource Locator) address, the problem is that not all of those websites are safe. We are interested on phishing web pages, those who try to imitate original ones-generally login or sale webpages -, the dangerousness of this kind of activities is that, it looks as trusting and the principal goal is to steal any user's (normal users, communities, societies, laboratories, etc.) personal information like name, date of birth, e-mail, credentials, login and passwords from e-banking services for example or any other web services. When victim fill his personal informations then it will be to the original one.Our aim in this paper is to explore four techniques for detecting phishing web pages: Black-list based, Lexical based, Content based and Security and Identity based methods. After that, we construct a model combined with machine learning classifiers to classify if a test URL represents a safe or phishing web page, and we will show our framework that is based on a combination of those methods and the learning way is made by machine learning classifier algorithms, also it is based on rules. The data used to build our model is a gathered from PhishTank[1].},
booktitle = {Proceedings of the Mediterranean Symposium on Smart City Application},
articleno = {11},
numpages = {6},
keywords = {phishing, smart systems and communication, malicious URL detection, network security intelligence, machine learning, spark framework},
location = {Tangier, Morocco},
series = {SCAMS '17}
}

@article{10.1145/3447866,
author = {Boukerche, Azzedine and Ma, Xiren},
title = {Vision-Based Autonomous Vehicle Recognition: A New Challenge for Deep Learning-Based Systems},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3447866},
doi = {10.1145/3447866},
abstract = {Vision-based Automated Vehicle Recognition (VAVR) has attracted considerable attention recently. Particularly given the reliance on emerging deep learning methods, which have powerful feature extraction and pattern learning abilities, vehicle recognition has made significant progress. VAVR is an essential part of Intelligent Transportation Systems. The VAVR system can fast and accurately locate a target vehicle, which significantly helps improve regional security. A comprehensive VAVR system contains three components: Vehicle Detection (VD), Vehicle Make and Model Recognition (VMMR), and Vehicle Re-identification (VRe-ID). These components perform coarse-to-fine recognition tasks in three steps. In this article, we conduct a thorough review and comparison of the state-of-the-art deep learning--based models proposed for VAVR. We present a detailed introduction to different vehicle recognition datasets used for a comprehensive evaluation of the proposed models. We also critically discuss the major challenges and future research trends involved in each task. Finally, we summarize the characteristics of the methods for each task. Our comprehensive model analysis will help researchers that are interested in VD, VMMR, and VRe-ID and provide them with possible directions to solve current challenges and further improve the performance and robustness of models.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {84},
numpages = {37},
keywords = {fine-grained recognition, vehicle detection, Deep learning, vehicle re-identification, video surveillance, intelligent transportation system, convolutional neural network, vehicle make and model recognition}
}

@inproceedings{10.1145/3377170.3377259,
author = {Kim, Donghyeon and Lee, Younglo and Ko, Hanseok},
title = {Multi-Task Learning for Animal Species and Group Category Classification},
year = {2019},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3377170.3377259},
doi = {10.1145/3377170.3377259},
abstract = {Accurate animal sound classification is an important task in automated animal monitoring system. Such monitoring system is essential for preventing epidemics caused by animal disease. Based on such needs, there has been a variety of efforts to develop an accurate system performing animal sound classification in deep learning framework. Although many research issues and methods to address the issues were introduced, no one has yet to address overcoming the machine learning barriers induced by a single objective function. As learnable parameters only consider a single penalty at the output prediction for training, they cannot capture other characteristics contained in the dataset to extract more generalized prediction. This paper proposes a deep learning based multi-task learning framework for animal sound classification. Both animal species and group classification are performed in an end-to-end learning process. Experimental results show that the proposed multi-task method outperforms single-task method in our recorded animal sound dataset.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {435–438},
numpages = {4},
keywords = {multi-task learning, animal group, animal species, deep learning, classification},
location = {Shanghai, China},
series = {ICIT 2019}
}

@inproceedings{10.1145/2836127.2836130,
author = {De Coninck, Elias and Verbelen, Tim and Vankeirsbilck, Bert and Bohez, Steven and Leroux, Sam and Simoens, Pieter},
title = {DIANNE: Distributed Artificial Neural Networks for the Internet of Things},
year = {2015},
isbn = {9781450337311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/2836127.2836130},
doi = {10.1145/2836127.2836130},
abstract = {Nowadays artificial neural networks are widely used to accurately classify and recognize patterns. An interesting application area is the Internet of Things (IoT), where physical things are connected to the Internet, and generate a huge amount of sensor data that can be used for a myriad of new, pervasive applications. Neural networks' ability to comprehend unstructured data make them a useful building block for such IoT applications. As neural networks require a lot of processing power, especially during the training phase, these are most often deployed in a cloud environment, or on specialized servers with dedicated GPU hardware. However, for IoT applications, sending all raw data to a remote back-end might not be feasible, taking into account the high and variable latency to the cloud, or could lead to issues concerning privacy. In this paper the DIANNE middleware framework is presented that is optimized for single sample feed-forward execution and facilitates distributing artificial neural networks across multiple IoT devices. The modular approach enables executing neural network components on a large number of heterogeneous devices, allowing us to exploit the local compute power at hand, and mitigating the need for a large server-side infrastructure at runtime.},
booktitle = {Proceedings of the 2nd Workshop on Middleware for Context-Aware Applications in the IoT},
pages = {19–24},
numpages = {6},
keywords = {Internet of Things, Distributed Artificial Neural Networks, Middleware},
location = {Vancouver, BC, Canada},
series = {M4IoT 2015}
}

@inproceedings{10.1145/3401895.3401919,
author = {Sousa, Paulo Miranda e Silva and de Freitas Costa, Jos\'{e} Robertty and Coutinho, Emanuel F. and Bezerra, Carla I. M.},
title = {An IoT Solution for Monitoring and Prediction of Bus Stops on University Transportation Using Machine Learning Algorithms},
year = {2020},
isbn = {9781450377119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3401895.3401919},
doi = {10.1145/3401895.3401919},
abstract = {Due to the growth of urbanization, cities have faced social, economic and environmental transformations. In addition, many vehicles currently have several sensors and actuators, capable of performing not only the sensing of the condition of vehicles, but also the environment around them, and this data can be used for various services. The environment of a large university may resemble urban environments, considering that these institutions compare to cities in various aspects, especially in relation to infrastructure problems. The objective of this work is to develop a solution for the monitoring and prediction of bus stops in university transportation. Tests were performed with six online and offline machine learning algorithms in order to analyze which algorithm is most efficient based on the fixed metrics. The best algorithm presented an absolute prediction error of 20 seconds, which shows the quality of the generated final model.},
booktitle = {Proceedings of the 10th Euro-American Conference on Telematics and Information Systems},
articleno = {10},
numpages = {7},
keywords = {prediction, bus stops, infrastructure, monitoring},
location = {Aveiro, Portugal},
series = {EATIS '20}
}

@inproceedings{10.1145/3377170.3377261,
author = {Volkov, Serge},
title = {City Services Management Methodology Based on Socio-Cyber-Physical Approach},
year = {2019},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3377170.3377261},
doi = {10.1145/3377170.3377261},
abstract = {This article presents the management methodology of city services based on socio-cyber-physical systems. The methodology development is caused by the need to form the stable basis for the constantly developing city services. Based on modern socio-cyber-physical systems, urban consumers can receive more and more services, while the number of potential opportunities provided by these services multiplies every day. Such an active development of urban services poses a challenge for city administrations to choose which services to develop, how to transform existing and what services need to be created for urban consumers. The developed algorithm allows to evaluate the current state of urban services, to determine options for development taking into consideration the city's development vector and compare the costs of creating services with the ability of residents to pay for them.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {373–376},
numpages = {4},
keywords = {Socio-Cyber-Physical system, digital twin, service development, City services, algorithm, information modelling, Smart city management},
location = {Shanghai, China},
series = {ICIT 2019}
}

@inproceedings{10.1145/3293614.3293625,
author = {Neto, Gerson V. A. and Viana, Johnattan D. F. and Braga, Reinaldo B. and Oliveira, Carina T.},
title = {Surfaces Categorization Based on Data Collected by Bike Sensors},
year = {2018},
isbn = {9781450365727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3293614.3293625},
doi = {10.1145/3293614.3293625},
abstract = {Today computing is being applied in several areas of knowledge and, when it is used with dynamic technologies as Internet of Things and Artificial Intelligence, can take users experience to a higher level. This work, for example, proposes an application in the context of smart cities to analyze data intelligently, knowing that, the concept of Smart Cities involves a wide range of innovations created for the comfort of the citizens. This paper proposes, through data collection, the recognition of vibratory patterns for the classification of surfaces using machine learning techniques. This is an important issue, as it offers a proposal for greater security of bicycle circulation points with the identification of possible irregularities. The analysis of roads surface quality is possible with the use of an accelerometer to collect data important for the audition of tracks. This data is then classified generating information, classified as patterns (asphalt and pavement surfaces). We have performed field data gathering and applied algorithms calculations to classify data to identify the surface the bicycle ridden, with results in percentages of accuracy up to more than 96%.},
booktitle = {Proceedings of the Euro American Conference on Telematics and Information Systems},
articleno = {32},
numpages = {7},
keywords = {Smart Cities, Machine Learning, Surface Classification},
location = {Fortaleza, Brazil},
series = {EATIS '18}
}

@inproceedings{10.1145/3408308.3427604,
author = {Vazquez-Canteli, Jose R. and Henze, Gregor and Nagy, Zoltan},
title = {MARLISA: Multi-Agent Reinforcement Learning with Iterative Sequential Action Selection for Load Shaping of Grid-Interactive Connected Buildings},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3408308.3427604},
doi = {10.1145/3408308.3427604},
abstract = {We demonstrate that multi-agent reinforcement learning (RL) controllers can cooperate to provide more effective load shaping in a model-free, decentralized, and scalable way with very limited sharing of anonymous information. Rapid urbanization, increasing electrification, the integration of renewable energy resources, and the potential shift towards electric vehicles create new challenges for the planning and control of energy systems in smart cities. Energy storage resources can help better align peaks of renewable energy generation with peaks of electricity consumption and flatten the curve of electricity demand. Model-based controllers, such as MPC, require developing models of the systems controlled, which is often not cost-effective or scalable. Model-free controllers, such as RL, have the potential to provide good control policies cost-effectively and leverage the use of historical data for training. However, it is unclear how RL algorithms can control a multitude of energy systems in a scalable coordinated way. In this paper, we introduce MARLISA, a controller that combines multi-agent RL with our proposed iterative sequential action selection algorithm for load shaping in urban energy systems. This approach uses a reward function with individual and collective goals, and the agents predict their own future electricity consumption and share this information with each other following a leader-follower schema. The RL agents are tested in four groups of nine simulated buildings, with each group located in a different climate. The buildings have diverse load and domestic hot water profiles, PV panels, thermal storage devices, heat pumps, and electric heaters. The agents are evaluated on the average of five normalized metrics: annual net electric consumption, 1 -- load factor, average daily peak demand, annual peak demand, and ramping. MARLISA achieves superior results over multiple independent/uncooperative RL agents using the same reward function. Our results outperformed a manually optimized rule-based controller (RBC) benchmark by reducing the average daily peak load by 15%, ramping by 35%, and increasing the load factor by 10%. A multi-year case study on real weather data shows that MARLISA significantly outperforms the RBC in within a year and converges in less than 2 years. Combining MARLISA and the RBC for the first year improves overall initial performance by learning from the RBC rather than random exploration.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {170–179},
numpages = {10},
keywords = {Reinforcement learning, demand response, microgrid, multi-agent coordination},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@inbook{10.1145/3450267.3451998,
author = {Jahangir, Hamidreza and Konstantinou, Charalambos},
title = {Plug-in Electric Vehicles Demand Modeling in Smart Grids: A Deep Learning-Based Approach: Wip Abstract},
year = {2021},
isbn = {9781450383530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ez294.periodicos.capes.gov.br/10.1145/3450267.3451998},
abstract = {In smart grids, Plug-in Electric Vehicles (PEVs) are considered components of the power demand. PEVs have highly stochastic behavior, and to manage this stochastic load efficiently, intermediary bodies, widely known as aggregators, have been developed in the literature. In order to handle the PEVs charging demand from both technical and financial points of view, aggregators include tools based on Internet-of-Things (IoT) technology, which can observe the users' historical behavior and estimate their travel behavior and the requested charging demand. In the near future, the increase in the share of PEV adoption will transform the PEVs demand modeling framework into a "big-data" Cyber-Physical System (CPS). We present a novel artificial intelligence approach based on the deep learning concept to tackle this large dimension problem. To investigate users' different behavior, the PEVs are classified into different groups based on their driving patterns. Then, each class is assigned to its respective deep convolutional neural networks. The proposed method's performance will be investigated in the day-ahead energy market.},
booktitle = {Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems},
pages = {221–222},
numpages = {2}
}

