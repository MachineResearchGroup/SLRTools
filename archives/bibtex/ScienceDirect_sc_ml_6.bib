@article{ZHU2021110,
title = {PFLU and FPFLU: Two novel non-monotonic activation functions in convolutional neural networks},
journal = {Neurocomputing},
volume = {429},
pages = {110-117},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.11.068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220318749},
author = {Meng Zhu and Weidong Min and Qi Wang and Song Zou and Xinhao Chen},
keywords = {Convolutional Neural Network (CNN), Activation function, ReLU, Power Function Linear Unit (PFLU), Faster PFLU (FPFLU)},
abstract = {The choice of activation functions in Convolutional Neural Networks (CNNs) is very important. Rectified Linear Unit (ReLU) has been widely-used in most CNNs. Recently, a series of non-monotonic activation functions gradually become the new standard to enhance performance of CNNs. Inspired by them, this paper firstly proposes a novel non-monotonic activation function called Power Function Linear Unit (PFLU). The negative part of PFLU is non-monotonic and closer to zero with the negative input decreasing, which can maintain sparsity of the negative part while introducing negative activation values and non-zero derivative values for the negative part. The positive part of PFLU does not use identity mapping but is closer to identity mapping with the positive input increasing, which can bring non-linearity property for the positive part. Next, this paper proposes faster PFLU (FPFLU). A wide range of classification experiments show that PFLU tends to work better than current state-of-the-art non-monotonic activation functions, and FPFLU can run faster than most non-monotonic activation functions.}
}
@article{KURNIAWAN2018291,
title = {Traffic Congestion Detection: Learning from CCTV Monitoring Images using Convolutional Neural Network},
journal = {Procedia Computer Science},
volume = {144},
pages = {291-297},
year = {2018},
note = {INNS Conference on Big Data and Deep Learning},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.530},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322397},
author = {Jason Kurniawan and Sensa G.S. Syahra and Chandra K. Dewa and  Afiahayati},
keywords = {convolutional neural network, deep learning, image classification, traffic congestion detection},
abstract = {In this paper, we present an intelligent traffic congestion detection method using image classification approach on CCTV camera image feeds. We use a deep learning architecture, convolutional neural network (CNN) which is currently the state-of-the art for image processing method. We only do minimal image preprocessing steps on the small size image, where the conventional methods require a high quality, handcrafted features need to do manual calculation. The CNN model is trained to do binary classification about road traffic condition using 1000 CCTV monitoring image feeds with balance distribution. The result shows that a CNN with simple, basic architecture that trained on small grayscale images has an average classification accuracy of 89.50%.}
}
@article{JAISWAL20211,
title = {Entity-aware capsule network for multi-class classification of big data: A deep learning approach},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {1-11},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330363},
author = {Amit Kumar Jaiswal and Prayag Tiwari and Sahil Garg and M. Shamim Hossain},
keywords = {Natural language processing, Named entity recognition, Capsule network},
abstract = {Named entity recognition (NER) is one of the most challenging natural language processing (NLP) tasks, as its performance is related to constantly evolving languages and dependency on expert (human) annotation. The diverse and dynamic content on the web significantly raises the need for a more generalized approach—one that is capable of correctly classifying terms in a corpus and feeding subsequent NLP tasks, such as machine translation, query expansion, and many other applications. Although extensively researched in recent times, the variety of public corpora available nowadays provides room for new and more accurate methods to tackle the NER problem. This paper presents a novel method that uses deep learning techniques based on the capsule network architecture for predicting entities in a corpus. This type of network groups neurons into so-called capsules to detect specific features of an object without reducing the original input unlike convolutional neural networks and their ‘max-pooling’ strategy. Our extensive evaluation on several benchmarked datasets demonstrates how competitive our method is in comparison with state-of-the-art techniques and how the usage of the proposed architecture may represent a significant benefit to further NLP tasks, especially in cases where experts are needed. Also, we explore NER using a theoretical framework that leverages big data for security. For the sake of reproducibility, we make the codebase open-source2 2https://github.com/amitkumarj441/CapsRoute_NER..}
}
@article{POTOCNIK2021110673,
title = {Machine-learning-based multi-step heat demand forecasting in a district heating system},
journal = {Energy and Buildings},
volume = {233},
pages = {110673},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110673},
url = {https://www.sciencedirect.com/science/article/pii/S0378778820334599},
author = {Primož Potočnik and Primož Škerl and Edvard Govekar},
keywords = {District heating, Heat demand, Short-term forecasting, Machine learning, Gaussian process regression},
abstract = {Short-term heat demand forecasting in district heating (DH) systems is essential for a sufficient heat supply and optimal operation of the DH. In this study, a machine learning based multi-step short-term heat demand forecasting approach using the data of the largest Slovenian DH system is considered. The proposed approach involved feature extraction and comparative analysis of different representative machine learning based forecasting models. Nonlinear models performed better than linear models, and the best forecasting results were obtained by Gaussian process regression (GPR), where the mean absolute normalized error was 2.94% of the maximum heating power of the DH system. The analysis confirmed the importance of accurate temperature forecasts but did not confirm the relevance of using future solar irradiation forecasts. The optimal length of training data is shown to be 3 years, and past data of up to 4 days can be used as input to increase the forecasting accuracy. The forecasting model (GPR) proposed in this study can be fitted to different DH systems. In the presented case study, it was selected to implement the online forecasting solution for the DH of Ljubljana and has been generating forecasts with a mean absolute normalized error of 2.70% since November 2019.}
}
@article{YANG20221852,
title = {Artificial intelligence image recognition based on 5G deep learning edge algorithm of Digestive endoscopy on medical construction},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {3},
pages = {1852-1863},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821004774},
author = {Lili Yang and Zhichao Li and Shilan Ma and Xinghua Yang},
keywords = {Smart medical construction, Artificial intelligence image recognition, 5G deep learning edge algorithm, Digestive endoscopy},
abstract = {In this paper, we use artificial intelligence image recognition to obtain Digestive endoscopy image, and process the image based on 5G Deep learning edge algorithm to judge the disease type of the patient, and then consider the treatment plan. The combination of body area network and edge computing technology can meet the demand of low delay in body area network. In this case, the resource constrained body area network gateway node can process the physiological data collected by it into an offloadable task, and then unload the task and data to the edge computing node according to a certain strategy. The edge computing node completes the corresponding task processing and data storage, and finally provides the results to the relevant medical institutions and body area network users for reading auxiliary diagnosis and treatment of diseases. Studies have shown that 25% of patients with colon polyps have CD4 cells in peripheral blood based on 5G deep learning edge algorithm under artificial intelligence image recognition of Digestive endoscopy. The number of lymphocyte in group of differentiation was less than 200/μL, and the blood RNA in 92.3% patients was lower than 100 IU/ml, while fam CTP (A-cyclic peptide) was lower than 100 IU/ml. Opportunistic infections of the intestine and viruses can directly cause enteropathy because the fluorescence intensity of the probe is essentially unchanged and cannot form a triple helix structure. In terms of feature recognition accuracy, the 5G deep learning edge algorithm in this paper improves accuracy by 68% compared to the simple Yolo algorithm, and is similar in speed. Compared with RCNN algorithm, the accuracy and speed are improved by 21% and 85% respectively. Therefore, the 5G deep learning edge algorithm based on artificial intelligence image recognition has the advantages of accuracy and speed in digestive endoscopy of intelligent medical.}
}
@article{PRAVEENKUMAR20191,
title = {Machine learning algorithms for wireless sensor networks: A survey},
journal = {Information Fusion},
volume = {49},
pages = {1-25},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S156625351830277X},
author = {D. {Praveen Kumar} and Tarachand Amgoth and Chandra Sekhara Rao Annavarapu},
keywords = {Wireless sensor networks, Machine learning, Energy efficiency, Network lifetime, Data aggregation},
abstract = {Wireless sensor network (WSN) is one of the most promising technologies for some real-time applications because of its size, cost-effective and easily deployable nature. Due to some external or internal factors, WSN may change dynamically and therefore it requires depreciating dispensable redesign of the network. The traditional WSN approaches have been explicitly programmed which make the networks hard to respond dynamically. To overcome such scenarios, machine learning (ML) techniques can be applied to react accordingly. ML is the process of self-learning from the experiences and acts without human intervention or re-program. The survey of the ML techniques for WSNs is presented in [1], covering period of 2002–2013. In this survey, we present various ML-based algorithms for WSNs with their advantages, drawbacks, and parameters effecting the network lifetime, covering the period from 2014–March 2018. In addition, we also discuss ML algorithms for synchronization, congestion control, mobile sink scheduling and energy harvesting. Finally, we present a statistical analysis of the survey, the reasons for selection of a particular ML techniques to address an issue in WSNs followed by some discussion on the open issues.}
}
@article{AMATO2017327,
title = {Deep learning for decentralized parking lot occupancy detection},
journal = {Expert Systems with Applications},
volume = {72},
pages = {327-334},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.10.055},
url = {https://www.sciencedirect.com/science/article/pii/S095741741630598X},
author = {Giuseppe Amato and Fabio Carrara and Fabrizio Falchi and Claudio Gennaro and Carlo Meghini and Claudio Vairo},
keywords = {Machine learning, Classification, Deep learning, Convolutional neural networks, Parking space dataset},
abstract = {A smart camera is a vision system capable of extracting application-specific information from the captured images. The paper proposes a decentralized and efficient solution for visual parking lot occupancy detection based on a deep Convolutional Neural Network (CNN) specifically designed for smart cameras. This solution is compared with state-of-the-art approaches using two visual datasets: PKLot, already existing in literature, and CNRPark-EXT. The former is an existing dataset, that allowed us to exhaustively compare with previous works. The latter dataset has been created in the context of this research, accumulating data across various seasons of the year, to test our approach in particularly challenging situations, exhibiting occlusions, and diverse and difficult viewpoints. This dataset is public available to the scientific community and is another contribution of our research. Our experiments show that our solution outperforms and generalizes the best performing approaches on both datasets. The performance of our proposed CNN architecture on the parking lot occupancy detection task, is comparable to the well-known AlexNet, which is three orders of magnitude larger.}
}
@article{GOMEZCARMONA2020670,
title = {Exploring the computational cost of machine learning at the edge for human-centric Internet of Things},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {670-683},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20304106},
author = {Oihane Gómez-Carmona and Diego Casado-Mansilla and Frank Alexander Kraemer and Diego López-de-Ipiña and Javier García-Zubia},
keywords = {Internet of Things, Edge computing, Machine learning, Cost-accuracy, Edge intelligence, Embedded systems},
abstract = {In response to users’ demand for privacy, trust and control over their data, executing machine learning tasks at the edge of the system has the potential to make the Internet of Things (IoT) applications and services more human-centric. This implies moving complex computation to a local stage, where edge devices must balance the computational cost of the machine learning techniques to meet the available resources. Thus, in this paper, we analyze all the factors affecting the classification process and empirically evaluate their impact in terms of performance and cost. We put the focus on Human Activity Recognition (HAR) systems, which represent a standard type of classification problems in human-centered IoT applications. We present a holistic optimization approach through input data reduction and feature engineering that aims to enhance all the stages of the classification pipeline and integrate both inference and training at the edge. The results of the conducted evaluation show that there is a highly non-linear trade-off to make between the computational cost, in terms of processing time, and the achieved classification accuracy. In the presented case of study, the computational effort can be reduced by 80% assuming a decline of the classification accuracy of only 3%. The potential impact of the optimization strategy highlights the importance of understanding the initial data and studying the most relevant characteristics of the signal to meet the cost–accuracy requirements. This would contribute to bringing embedded machine learning to the edge and, hence, creating spaces where human and machine intelligence could collaborate.}
}
@article{KOURTIT2021102665,
title = {Safe cities in the new urban world: A comparative cluster dynamics analysis through machine learning},
journal = {Sustainable Cities and Society},
volume = {66},
pages = {102665},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102665},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720308817},
author = {Karima Kourtit and Miruna Mazurencu {Marinescu Pele} and Peter Nijkamp and Daniel {Traian Pele}},
keywords = {, Global cities, Safety, Security, Magnetism, Cluster analysis, Machine learning},
abstract = {Cities in the ‘New Urban World’ display an enormous diversity in appearance, growth and performance. The awareness is growing that the urban development potential (‘magnetism’) of cities is closely related to safety and security conditions in these cities. This paper develops a new analytical framework based on a wealth of empirical data on both safety/security and socio-economic magnetism achievements of many world cities, by combining two comprehensive relevant global urban data bases. The aim of the study is to offer a comparative analysis of the combined safety/security data and socio-economic performance data of 30 global cities, through the use of an advanced sequential cluster dynamics analysis that is (partly) inspired by a novel machine learning approach (using Python software). In this way, cities can be categorized according to their quantitative characteristic features represented by the relevant clusters. It appears that city safety/security features are an important predictor of the variability in overall urban performance regarding magnetism. This study allows also for drawing relevant policy lessons.}
}
@article{ZHANG202148,
title = {LearningADD: Machine learning based acoustic defect detection in factory automation},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {48-58},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000856},
author = {Tao Zhang and Biyun Ding and Xin Zhao and Ganjun Liu and Zhibo Pang},
keywords = {Acoustic defect detection, Edge computing, Factory automation, Feature extraction algorithm},
abstract = {Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.}
}
@article{MHAISEN202039,
title = {To chain or not to chain: A reinforcement learning approach for blockchain-enabled IoT monitoring applications},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {39-51},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19334399},
author = {Naram Mhaisen and Noora Fetais and Aiman Erbad and Amr Mohamed and Mohsen Guizani},
keywords = {Blockchain, Smart contracts, Monitoring applications, Internet of things, Reinforcement learning, Cost optimization},
abstract = {Traceability and autonomous business logic execution are highly desirable features in IoT monitoring applications. Traceability enables verifying signals’ history for security or analytical purposes. On the other hand, the autonomous execution of pre-defined rules establishes trust between parties involved in such applications and improves the efficiency of their workflow. Smart Contracts (SCs) firmly guarantee these two requirements due to the blockchain’s immutable distributed ledger and secure cryptographic consensus rules. Thus, SCs emerged as an appealing technology for monitoring applications. However, the cost of using public blockchains to harvest these guarantees can be prohibitive, especially with the considerable fluctuation of coin prices and different use case requirements. In this paper, we introduce a general SC-based IoT monitoring framework that can leverage the security features of public blockchains while minimizing the corresponding monetary cost. The framework contains a reinforcement learning agent that adapts to users’ needs and acts in real-time to smartly set the data submission rate of IoT sensors. Results based on the Ethereum protocol show significant potential cost saving depending on users’ preferences.}
}
@article{DORESWAMY20202057,
title = {Forecasting Air Pollution Particulate Matter (PM2.5) Using Machine Learning Regression Models},
journal = {Procedia Computer Science},
volume = {171},
pages = {2057-2066},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.221},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920312060},
author = { Doreswamy and Harishkumar {K S} and Yogesh KM and Ibrahim Gad},
keywords = {TAQMN, Air Pollution, Prediction, Forecasting, Particulate matter (PM), Gradient Boosting regression},
abstract = {From the past few decades, it has been observed that the urbanization and industrialization are expanding in the developed nations and are confronting the overwhelming air contamination issue. The citizens and governments have experienced and expressed the increasingly concerned regarding the impact of air pollution affecting human health and proposed sustainable development for overriding air pollution issues across the worldwide. The outcome of modern industrialization contains the liquid droplets, solid particles and gas molecules and is spreading in the atmospheric air. The heavy concentration of particulate matter of size PM10 and PM2.5 is seriously caused adverse health effect. Through the determination of particulate matter concentration in atmospheric air for the betterment of human being well in primary importance. In this paper machine learning predictive models for forecasting particulate matter concentration in atmospheric air are investigated on Taiwan Air Quality Monitoring data sets, which were obtained from 2012 to 2017. These models were compared with the existing traditional models and perform better in predictive performance. The performance of these models was evaluated with statistical measures: Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Square Error (MSE), and Coefficient of Determination (R2).}
}
@article{JELEN2021127684,
title = {Contextual prediction of parking spot availability: A step towards sustainable parking},
journal = {Journal of Cleaner Production},
volume = {312},
pages = {127684},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127684},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621019028},
author = {Goran Jelen and Vedran Podobnik and Jurica Babic},
keywords = {Sustainable parking, Greenhouse gas pollution reduction, Parking spot availability, Smart city, Data science, Contextually enriched data},
abstract = {One of the challenges of living in today's cities is parking availability. Searching for available parking spots can be a time-consuming task that simultaneously increases traffic congestion and greenhouse gas pollution by a significant 40%. A solution that would increase drivers' ability to locate an empty parking spot would represent an important step towards more sustainable parking as it would have a direct impact on reducing greenhouse gas pollution in urban areas. This paper proposes how data science can help by evaluating the prediction performance of four machine learning models. Analysed machine learning models are based on different machine learning methods (i.e., CatBoost and Random Forest) and use different real-world data sets (i.e., parking sensor data only or contextually enriched parking sensor data). The dummy (baseline) model is considered as well, but with a R2 score of 61.29% is outperformed by more advanced data science approaches. Prediction performance in the case of using parking sensor data only gives R2 score of 84.31% and 88.16% for Random Forest and CatBoost, respectively. The best prediction performance is achieved using CatBoost and contextually enriched data, resulting in the high-performing machine learning model with the R2 value of 88.83%, thus outperforming the Random Forest model by 1.7%. In fact, for both machine learning methods, the contextually enriched data approach gives better results for predicting parking spot availability. This suggests that parking data should be enriched with contextual data when designing and building sustainable parking solutions for smart cities of the future.}
}
@article{LI2021205,
title = {A deep learning framework for autonomous flame detection},
journal = {Neurocomputing},
volume = {448},
pages = {205-216},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221003830},
author = {Zhenglin Li and Lyudmila Mihaylova and Le Yang},
keywords = {Flame detection, Flame R-CNN, Dirichlet process Gaussian mixture model, Variational inference},
abstract = {This paper proposes a novel framework of flame region-based convolutional neural network for autonomous flame detection. The task of flame detection is especially challenging since flames have greater diversity in colour, texture, and shape than regular rigid objects. To cope with these difficulties due to the various appearances and unclear edges of flames, a proposal generation approach is developed to effectively select candidate flame regions based on two crucial properties of flames, i.e., their dynamics and colours. The candidate flame regions together with a convolutional feature map are further processed by additional layers to output detected flames. The diversity in flame colours is well represented by approximating the distribution using a Dirichlet Process Gaussian mixture model with variational inference. The proposed framework is evaluated on publicly available videos and achieves an average frame-wise accuracy higher than 88%, which outperforms the state-of-the-art methods.}
}
@article{SHORFUZZAMAN2021107700,
title = {MetaCOVID: A Siamese neural network framework with contrastive loss for n-shot diagnosis of COVID-19 patients},
journal = {Pattern Recognition},
volume = {113},
pages = {107700},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107700},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320305033},
author = {Mohammad Shorfuzzaman and M. Shamim Hossain},
keywords = {COVID-19 diagnosis, Multi-shot learning, Contrastive loss, CXR images, Siamese network},
abstract = {Various AI functionalities such as pattern recognition and prediction can effectively be used to diagnose (recognize) and predict coronavirus disease 2019 (COVID-19) infections and propose timely response (remedial action) to minimize the spread and impact of the virus. Motivated by this, an AI system based on deep meta learning has been proposed in this research to accelerate analysis of chest X-ray (CXR) images in automatic detection of COVID-19 cases. We present a synergistic approach to integrate contrastive learning with a fine-tuned pre-trained ConvNet encoder to capture unbiased feature representations and leverage a Siamese network for final classification of COVID-19 cases. We validate the effectiveness of our proposed model using two publicly available datasets comprising images from normal, COVID-19 and other pneumonia infected categories. Our model achieves 95.6% accuracy and AUC of 0.97 in diagnosing COVID-19 from CXR images even with a limited number of training samples.}
}
@article{AYYILDIZ2021102683,
title = {Physical layer authentication for extending battery life},
journal = {Ad Hoc Networks},
volume = {123},
pages = {102683},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102683},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521001918},
author = {Cem Ayyildiz and Ramazan Cetin and Zulfidin Khodzhaev and Taskin Kocak and Ece Gelal Soyak and V. Cagri Gungor and Gunes Karabulut Kurt},
keywords = {Internet of Things (IoT), Smart city, Physical layer security, RF fingerprinting, Battery life, Convolutional Neural Networks (CNN), Energy efficient},
abstract = {Increasing population density in cities, and the increasing demand for efficiency in resource usage call for architectures enabling smart cities, such as the Internet of Things (IoT). In most such scenarios, the data generated by IoT sensors is not confidential, but its integrity is critical. Data integrity can be achieved by establishing certification mechanisms that provide cryptographic message authentication protocols; however, this requires relatively expensive components for storing and processing the encryption key on the sensor and consumes more power while processing and transmitting data, which leads to the renunciation of security issues in cost sensitive deployments. In this paper, we propose a security solution that provides data integrity without draining the batteries of IoT sensors. Our solution consists of, (i) differentiating legitimate sensors by taking advantage of their impurities formed during the manufacturing process of the transceiver components, and (ii) eliminating the complex components that carry out cryptography as well as the redundant packet header fields, thereby yielding power savings. The testbed implementation of the proposed solution yields power measurement results providing an estimate of 2.52 times improvement in battery life without compromising the integrity of communications in the system, in addition to offering an increase in spectral efficiency and a decrease in the overall IoT device cost.}
}
@article{ROSERO2021117770,
title = {Cloud and machine learning experiments applied to the energy management in a microgrid cluster},
journal = {Applied Energy},
volume = {304},
pages = {117770},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117770},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921011090},
author = {D.G. Rosero and N.L. Díaz and C.L. Trujillo},
keywords = {Cloud computing, Machine learning, Energy management system, Prosumer, Microgrid clustering, Renewable energy},
abstract = {The way to organize the generation, storage, and management of renewable energy and energy consumption features has taken relevance in recent years due to demands that define the social welfare of this century. Like demand increases, other factors require grid infrastructure improvement, updates, and opening to other technologies that assuage the final customer needs. Precisely, the interest in renewable energy sources, the constant evolution of energy storage technologies, the continuous research involving microgrid management systems, and the evolution of cloud computing technologies and machine learning strategies motivate the development of this article. Tasks associated with a microgrid cluster like the integration of a considerable number of heterogeneous devices, real-time support, information processing, massive storage capabilities, security considerations, and advanced optimization techniques usage could take place in an autonomous and scalable energy management system architecture under a machine learning perspective running in real-time and using Cloud resources. This paper focuses on identifying the elements considered by different authors to define a cloud-based architecture and ensure the appropriately supervised learning functionality under a microgrids cluster environment. Namely, it was necessary to revise and run microgrid simulations, real-time simulation platforms usage, connection to a virtual server for microgrid control and set the energy management system using cloud computing and machine learning. Based on the review and considering the scenarios mentioned, this article presents a scalable and autonomous cloud-based architecture that allows power generation forecast, energy consumption prediction, a real-time energy management system using machine learning techniques.}
}
@article{MOHANTY2020101097,
title = {Deep learning with LSTM based distributed data mining model for energy efficient wireless sensor networks},
journal = {Physical Communication},
volume = {40},
pages = {101097},
year = {2020},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2020.101097},
url = {https://www.sciencedirect.com/science/article/pii/S1874490720301737},
author = {Sachi Nandan Mohanty and E. Laxmi Lydia and Mohamed Elhoseny and Majid M. Gethami {Al Otaibi} and K. Shankar},
keywords = {Deep learning, Energy efficiency, Recurrent neural network, WSN},
abstract = {Wireless sensor network (WSN) comprises a collection of sensor nodes employed to monitor and record the status of the physical environment and organize the gathered data at a central location. This paper presents a deep learning based distributed data mining (DDM) model to achieve energy efficiency and optimal load balancing at the fusion center of WSN. The presented DMM model includes a recurrent neural network (RNN) based long short-term memory (LSTM) called RNN-LSTM, which divides the network into various layers and place them into the sensor nodes. The proposed model reduces the overhead at the fusion center along with a reduction in the number of data transmission. The presented RNN-LSTM model is tested under a wide set of experimentation with varying number of hidden layer nodes and signaling intervals. At the same time, the amount of energy needed to transmit data by RNN-LSTM model is considerably lower than energy needed to transmit actual data. The simulation results indicated that the RNN-LSTM reduces the signaling overhead, average delay and maximizes the overall throughput compared to other methods. It is noted that under the signaling interval of 240 ms, it can be shown that the RNN-LSTM achieves a minimum average delay of 190 ms whereas the OSPF and DNN models shows average delay of 230 ms and 230 ms respectively.}
}
@article{SALAM202197,
title = {Energy consumption prediction model with deep inception residual network inspiration and LSTM},
journal = {Mathematics and Computers in Simulation},
volume = {190},
pages = {97-109},
year = {2021},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2021.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378475421001774},
author = {Abdulwahed Salam and Abdelaaziz {El Hibaoui}},
keywords = {Prediction, Machine learning, Deep learning, Power production and consumption},
abstract = {Predicting electricity consumption is not an easy task depending on many factors that affect energy consumption. Therefore, electricity utilities and governments are always searching for intelligent models to improve the accuracy of prediction and recently, deep learning becomes the most used field in prediction. In this paper, we introduce a deep learning model based on deep feedforward neural networks and Long Short-Term Memory. The deep feedforward neural networks architecture was inspired by the Inception Residual Network v2, which achieved the highest accuracy in image classification. We compared our proposed model to other recent deep learning models in two different datasets: dataset from the Distribution Network Station of Tetouan city in Morocco and dataset from the North American Utility. The proposed model achieved the smallest error of Root Mean Square Error comparing to its counterparts.}
}
@article{VIMAL2020355,
title = {Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT},
journal = {Computer Communications},
volume = {151},
pages = {355-364},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419319255},
author = {S. Vimal and Manju Khari and Nilanjan Dey and Rubén González Crespo and Y. {Harold Robinson}},
keywords = {Mobile edge computing, Industrial IOT, Reinforcement learning, Multi objective ant colony optimization, Resource allocation, Cognitive agent},
abstract = {The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.}
}
@article{SARA2020700,
title = {Predict France trains delays using visualization and machine learning techniques},
journal = {Procedia Computer Science},
volume = {175},
pages = {700-705},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.103},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318032},
author = {Lbazri Sara and Ounacer soumaya and Jihal Houda and Azouazi Mohamed},
keywords = {machine learning, visualization, SNCF},
abstract = {The main problem plaguing rail transport is the punctuality of trains. By complying with the French Transport Service Quality Authority (AQST) in France, 2018 is the railway group’s "worst" year in terms of punctuality [1]. And according to SNCF over the period from March to November 2017, 18% of TGVs and 17% of Intercities did not arrive on schedule. The Lyon Part Dieu, Paris-Lyon and Marseille Saint Charles stations are particularly prone to these punctuality problems. For example, 32.3% of TGVs that leave Marseille to reach Lille are delayed by at least 15 minutes [2]. In France, a train is considered "late" after five minutes for a journey of less than 1.5 hours, 10 minutes for a journey from 1:30 to 3 hours and 15 minutes for a journey of more than 3 hours. The aim of our research is to use machine learning methods and advanced visualization techniques in order to predict train delays in advance and help rail users to plan their journey and know their train arrival time in advance.}
}
@article{STAHL2021374,
title = {Artificial intelligence for human flourishing – Beyond principles for machine learning},
journal = {Journal of Business Research},
volume = {124},
pages = {374-388},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320307839},
author = {B.C. Stahl and A. Andreou and P. Brey and T. Hatzakis and A. Kirichenko and K. Macnish and S. {Laulhé Shaelou} and A. Patel and M. Ryan and D. Wright},
keywords = {Ethics, Artificial intelligence, Big data, Human rights, Governance},
abstract = {The technical and economic benefits of artificial intelligence (AI) are counterbalanced by legal, social and ethical issues. It is challenging to conceptually capture and empirically measure both benefits and downsides. We therefore provide an account of the findings and implications of a multi-dimensional study of AI, comprising 10 case studies, five scenarios, an ethical impact analysis of AI, a human rights analysis of AI and a technical analysis of known and potential threats and vulnerabilities. Based on our findings, we separate AI ethics discourse into three streams: (1) specific issues related to the application of machine learning, (2) social and political questions arising in a digitally enabled society and (3) metaphysical questions about the nature of reality and humanity. Human rights principles and legislation have a key role to play in addressing the ethics of AI. This work helps to steer AI to contribute to human flourishing.}
}
@article{CHERMPRAYONG2021100543,
title = {Convolutional Neural Network for Thailand's Eastern Economic Corridor (EEC) land cover classification using overlapping process on satellite images},
journal = {Remote Sensing Applications: Society and Environment},
volume = {23},
pages = {100543},
year = {2021},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2021.100543},
url = {https://www.sciencedirect.com/science/article/pii/S2352938521000793},
author = {P. Chermprayong and N. Hongkarnjanakul and D. Rouquette and C. Schwob and L. Mezeix},
keywords = {Convolutional neural network, Artificial intelligence, Land cover, Image processing, Overlapped images, Satellite image},
abstract = {Land cover is a powerful tool in urban management as a source of information to support authorities’ decision making. In this paper, land cover of Eastern Economic Corridor cities in Thailand is performed using the aggregation of results from two Convolutional Neural Network models, with both using the same architecture. The first model is for the detection of water and the second is for the classification of land type consisting of 3 classes: city, forest and crop. In Firstly, the size of the 4 class existing dataset is increased resulting in an accuracy of 98% and 93% for the binary and three class CNN model respectively. To improve the land cover on satellite images an overlapping process is introduced in order to reduce the classification area from 0.25 km2 to 0.004 km2, using the same image resolution of 8 m per pixel. The use of the overlapping allows to propose a largely better land cover where the contour of the detected class is well produced. Moreover, this methods shows its better ability to detect smaller surface size and especially for the water, crops and forest class. However, it is showed that the overlapping method does not improved the accuracy of the prediction that is mainly related to the dataset size. Finally, the robustness of the proposed method to quickly perform a global land cover with limited computer power is demonstrated.}
}
@article{IDOWU2016478,
title = {Applied machine learning: Forecasting heat load in district heating system},
journal = {Energy and Buildings},
volume = {133},
pages = {478-488},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816310155},
author = {Samuel Idowu and Saguna Saguna and Christer Åhlund and Olov Schelén},
keywords = {Data driven modeling, District heating, Energy efficiency, Machine learning, Smart cities},
abstract = {Forecasting energy consumption in buildings is a key step towards the realization of optimized energy production, distribution and consumption. This paper presents a data driven approach for analysis and forecast of aggregate space and water thermal load in buildings. The analysis and the forecast models are built using district heating data unobtrusively collected from 10 residential and commercial buildings located in Skellefteå, Sweden. The load forecast models are generated using supervised machine learning techniques, namely, support vector machine, regression tree, feed forward neural network, and multiple linear regression. The model takes the outdoor temperature, historical values of heat load, time factor variables and physical parameters of district heating substations as its input. A performance comparison among the machine learning methods and identification of the importance of models input variables is carried out. The models are evaluated with varying forecast horizons of every hour from 1 up to 48h. Our results show that support vector machine, feed forward neural network and multiple linear regression are more suitable machine learning methods with lower performance errors than the regression tree. Support vector machine has the least normalized root mean square error of 0.07 for a forecast horizon of 24h.}
}
@article{ROLDAN2020113251,
title = {Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacks},
journal = {Expert Systems with Applications},
volume = {149},
pages = {113251},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113251},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300762},
author = {José Roldán and Juan Boubeta-Puig and José {Luis Martínez} and Guadalupe Ortiz},
keywords = {Complex event processing, Machine learning, Software architecture, Intelligent decision making, Internet of Things, Security attack},
abstract = {The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives.}
}
@article{LEE2021120653,
title = {From technological development to social advance: A review of Industry 4.0 through machine learning},
journal = {Technological Forecasting and Social Change},
volume = {167},
pages = {120653},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120653},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521000858},
author = {Changhun Lee and Chiehyeon Lim},
keywords = {Industry 4.0, Fourth industrial revolution, Review, Survey, Text mining},
abstract = {Industry 4.0 has attracted considerable interest from firms, governments, and individuals as the new concept of future computer, industrial, and social systems. However, the concept has yet to be fully explored in the scientific literature. Given the topic's broad scope, this work attempts to understand and clarify Industry 4.0 by analyzing 660 journal papers and 3,901 news articles through text mining with unsupervised machine learning algorithms. Based on the results, this work identifies 31 research and application issues related to Industry 4.0. These issues are categorized and described within a five-level hierarchy: 1) infrastructure development for connection, 2) artificial intelligence development for data-driven decision making, 3) system and process optimization, 4) industrial innovation, and 5) social advance. Further, a framework for convergence in Industry 4.0 is proposed, featuring six dimensions: connection, collection, communication, computation, control, and creation. The research outcomes are consistent with and complementary to existing relevant discussion and debate on Industry 4.0, which validates the utility and efficiency of the data-driven approach of this work to support experts’ insights on Industry 4.0. This work helps establish a common ground for understanding Industry 4.0 across multiple disciplinary perspectives, enabling further research and development for industrial innovation and social advance.}
}
@article{IQBAL2020119253,
title = {Big data analytics: Computational intelligence techniques and application areas},
journal = {Technological Forecasting and Social Change},
volume = {153},
pages = {119253},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517318498},
author = {Rahat Iqbal and Faiyaz Doctor and Brian More and Shahid Mahmud and Usman Yousuf},
keywords = {Smart city, Big data, Big data analytics, Computational intelligence, CI applications},
abstract = {Big Data has significant impact in developing functional smart cities and supporting modern societies. In this paper, we investigate the importance of Big Data in modern life and economy, and discuss challenges arising from Big Data utilization. Different computational intelligence techniques have been considered as tools for Big Data analytics. We also explore the powerful combination of Big Data and Computational Intelligence (CI) and identify a number of areas, where novel applications in real world smart city problems can be developed by utilizing these powerful tools and techniques. We present a case study for intelligent transportation in the context of a smart city, and a novel data modelling methodology based on a biologically inspired universal generative modelling approach called Hierarchical Spatial-Temporal State Machine (HSTSM). We further discuss various implications of policy, protection, valuation and commercialization related to Big Data, its applications and deployment.}
}
@article{JIN2022101442,
title = {Highly accurate energy consumption forecasting model based on parallel LSTM neural networks},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101442},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101442},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001944},
author = {Ning Jin and Fan Yang and Yuchang Mo and Yongkang Zeng and Xiaokang Zhou and Ke Yan and Xiang Ma},
keywords = {Long short term memory, Energy consumption, Time series data analysis, Forecasting, Singular spectrum analysis},
abstract = {The main challenges of the energy consumption forecasting problem are the concerns for reliability, stability, efficiency and accuracy of the forecasting methods. The existing forecasting models suffer from the volatility of the energy consumption data. It is desired for AI models that predict irregular sudden changes and capture long-term dependencies in the data. In this study, a novel hybrid AI empowered forecasting model that combines singular spectrum analysis (SSA) and parallel long short term memory (PLSTM) neural networks is proposed. The decomposition with the SSA enhanced the performance of the PLSTM network. According to the experimental results, the proposed model outperforms the state-of-the-art models at different time intervals in terms of both prediction accuracy and computational efficiency.}
}
@article{CESARDELIMAARAUJO2021127410,
title = {Artificial intelligence in urban forestry—A systematic review},
journal = {Urban Forestry & Urban Greening},
volume = {66},
pages = {127410},
year = {2021},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2021.127410},
url = {https://www.sciencedirect.com/science/article/pii/S1618866721004374},
author = {Henrique {César de Lima Araújo} and Fellipe {Silva Martins} and Tatiana {Tucunduva Philippi Cortese} and Giuliano Maselli Locosselli},
keywords = {Deep learning, Governance, Green infrastructure, Machine learning, Nature-based solutions, Urban planning, Urban trees},
abstract = {Environmental quality and the citizens' well-being largely depend on the urban forests. But managing this natural capital is challenging for its biological complexity and interactions with other environmental, social, and economic aspects of the cities. In line with the current digital revolution with the rise of Smart Cities, the use of Artificial Intelligence (AI) is becoming more common, including in urban forestry. In this systematic review, we evaluated 67 studies on the interplay between AI and urban forestry surveyed on Science Direct and Scopus to provide an overview of the state of the art and identify new research avenues. The sample includes studies in 23 countries and 85 cities, including 5 megacities, comprising the remote assessment of canopy cover and species distribution; ecosystem services assessment; management practices; and socioeconomic aspects of urban forestry. Most studies focused on extant urban forests, with few examples evaluating temporal trends, and only one focused on future scenarios despite the predictive potential of AI. A total of 22 AI methods were employed in these studies. Only half of them point to clear advantages of the chosen methods, such as robustness against missing data, overfitting, collinearity, non-linearity, non-normality, the combination of discrete and continuous variables, and higher accuracy. The choice of these methods depends on the various combinations of aim, timescale, data type, and data source. The application of AI in urban forestry is in full growth and will support decision making to improve livability in the cities.}
}
@article{PENG2020277,
title = {Spatial temporal incidence dynamic graph neural networks for traffic flow forecasting},
journal = {Information Sciences},
volume = {521},
pages = {277-290},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.01.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520300451},
author = {Hao Peng and Hongfei Wang and Bowen Du and Md Zakirul Alam Bhuiyan and Hongyuan Ma and Jianwei Liu and Lihong Wang and Zeyu Yang and Linfeng Du and Senzhang Wang and Philip S. Yu},
keywords = {Traffic passenger flows prediction, Graph convolutional neural network, LSTM, Importance sampling, Urban computing},
abstract = {Accurate and real-time traffic passenger flows forecasting at transportation hubs, such as subway/bus stations, is a practical application and of great significance for urban traffic planning, control, guidance, etc. Recently deep learning based methods are promised to learn the spatial-temporal features from high non-linearity and complexity of traffic flows. However, it is still very challenging to handle so much complex factors including the urban transportation network topological structures and the laws of traffic flows with spatial and temporal dependencies. Considering both the static hybrid urban transportation network structures and dynamic spatial-temporal relationships among stations from historical traffic passenger flows, a more effective and fine-grained spatial-temporal features learning framework is necessary. In this paper, we propose a novel spatial-temporal incidence dynamic graph neural networks framework for urban traffic passenger flows prediction. We first model dynamic traffic station relationships over time as spatial-temporal incidence dynamic graph structures based on historically traffic passenger flows. Then we design a novel dynamic graph recurrent convolutional neural network, namely Dynamic-GRCNN, to learn the spatial-temporal features representation for urban transportation network topological structures and transportation hubs. To fully utilize the historical passenger flows, we sample the short-term, medium-term and long-term historical traffic data in training, which can capture the periodicity and trend of the traffic passenger flows at different stations. We conduct extensive experiments on different types of traffic passenger flows datasets including subway, taxi and bus flows in Beijing. The results show that the proposed Dynamic-GRCNN effectively captures comprehensive spatial-temporal correlations significantly and outperforms both traditional and deep learning based urban traffic passenger flows prediction methods.}
}
@article{ROGIER2019643,
title = {Forecasting Photovoltaic Power Generation via an IoT Network Using Nonlinear Autoregressive Neural Network},
journal = {Procedia Computer Science},
volume = {151},
pages = {643-650},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305484},
author = {John Kevin Rogier and Nawaz Mohamudally},
keywords = {Autoregressive Neural Network, IoT Network, Supervised Learning, Photovoltaic, LoRa, Machine Learning},
abstract = {This research work is an attempt to introduce modern computing techniques as a potential decision-making tool in the field of renewable energy supply and management. We aim to demystify and take advantage of the concept of neural networks to predict the conversion of solar energy by a photovoltaic unit. In order to do so, a smart meter will be built and connected to a low power photovoltaic panel, the smart meter once in operation will capture a set of data, send them autonomously to a remote server over a LoRa IoT network which will be then processed to make predictions about the amount of power produced. Using Non Linear Autoregressive Neural Networks (NARX) with Matlab and Thingspeak IoT data capture, the results are favourable for open loop configuration although closed loop can be further improved, recommendations for the same are made at the end.}
}
@article{GORRIZ2020237,
title = {Artificial intelligence within the interplay between natural and artificial computation: Advances in data science, trends and applications},
journal = {Neurocomputing},
volume = {410},
pages = {237-270},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.05.078},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220309292},
author = {Juan M. Górriz and Javier Ramírez and Andrés Ortíz and Francisco J. Martínez-Murcia and Fermin Segovia and John Suckling and Matthew Leming and Yu-Dong Zhang and Jose Ramón Álvarez-Sánchez and Guido Bologna and Paula Bonomini and Fernando E. Casado and David Charte and Francisco Charte and Ricardo Contreras and Alfredo Cuesta-Infante and Richard J. Duro and Antonio Fernández-Caballero and Eduardo Fernández-Jover and Pedro Gómez-Vilda and Manuel Graña and Francisco Herrera and Roberto Iglesias and Anna Lekova and Javier {de Lope} and Ezequiel López-Rubio and Rafael Martínez-Tomás and Miguel A. Molina-Cabello and Antonio S. Montemayor and Paulo Novais and Daniel Palacios-Alonso and Juan J. Pantrigo and Bryson R. Payne and Félix {de la Paz López} and María Angélica Pinninghoff and Mariano Rincón and José Santos and Karl Thurnhofer-Hemsi and Athanasios Tsanas and Ramiro Varela and Jose M. Ferrández},
keywords = {Artificial intelligence (AI), Machine learning, Deep learning, Reinforcement learning, Evolutionary computation, Ontologies, Artificial neural networks (ANNs), Big data, Robotics, Neuroscience, Human–machine interaction, Virtual reality, Emotion recognition, Computational neuroethology, Autism, Dyslexia, Alzheimer, Parkinson, Glaucoma, AI for social well-being},
abstract = {Artificial intelligence and all its supporting tools, e.g. machine and deep learning in computational intelligence-based systems, are rebuilding our society (economy, education, life-style, etc.) and promising a new era for the social welfare state. In this paper we summarize recent advances in data science and artificial intelligence within the interplay between natural and artificial computation. A review of recent works published in the latter field and the state the art are summarized in a comprehensive and self-contained way to provide a baseline framework for the international community in artificial intelligence. Moreover, this paper aims to provide a complete analysis and some relevant discussions of the current trends and insights within several theoretical and application fields covered in the essay, from theoretical models in artificial intelligence and machine learning to the most prospective applications in robotics, neuroscience, brain computer interfaces, medicine and society, in general.}
}
@article{SINGH2022103364,
title = {Blockchain-enabled Secure Framework for Energy-Efficient Smart Parking in Sustainable City Environment},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103364},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103364},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721006399},
author = {Sushil Kumar Singh and Yi Pan and Jong Hyuk Park},
keywords = {Blockchain, Virtualization, Sustainable City, Deep LSTM, Energy-Efficiency},
abstract = {In the smart city environment, parking vehicle management is an essential requirement for citizens in the current situation because every city is rapidly growing as a crowded environment. Specific planning, operation, and thinking can address this problem with the Internet of things (IoT) and Information Communication Technologies (ICT). Existing research provides various solutions and methods for parking systems in the smart city. However, smart parking has many challenges, such as centralization, communication bandwidth, energy efficiency, integrity, security, and privacy. Inspired by Blockchain and AI technology, we propose a Blockchain-enabled Secure Framework for Energy-Efficient Smart Parking in Sustainable City Environment. The transport layer implements the ECC algorithm to encrypt and decrypt the parking zones data for secure communication. The RSU-based-Blockchain network offers authentication and verification of data at the security layer in a distributed manner. Virtualization technology is used for data storage and provides an energy-efficient environment using virtual machines. With Deep LSTM networks, we analyze the parking zone's data and offer the best parking space to the drivers with the best location and timing. We evaluate the proposed architecture using quantitative, qualitative analysis and provide the driver's best parking space.}
}
@article{CUI2020125,
title = {Towards predictive analysis of android vulnerability using statistical codes and machine learning for IoT applications},
journal = {Computer Communications},
volume = {155},
pages = {125-131},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.078},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420300049},
author = {Jianfeng Cui and Lixin Wang and Xin Zhao and Hongyi Zhang},
keywords = {Android vulnerability, Prediction, IoT applications, Software metrics, Machine learning},
abstract = {Recently, the Internet of Things (IoT) technology is used for several applications for exchanging information among various devices. The intelligent IoT based system utilizes an Android operating system because it is also primarily used in mobile devices. One of the main problems for different IoT applications is associated with android vulnerability is its complicated and large size. To overcome the main issue of IoT, the existing studies have proposed several effective prediction models using machine learning algorithms and software metrics. In this paper, we are focused on conducting android vulnerability prediction analysis using machine learning for intelligent IoT applications. We conducted an empirical investigation for examining security risk prediction of 1406 Android applications with varying levels of risk using a metric set of 21 static code metrics and 6 machine learning (ML) techniques. It is observed from results that ML algorithms have different performances for predicting security risks. RF algorithm performs better for Android applications of all risk levels. By analyzing the findings of the conducted empirical study, it is suggested that developers may consider object-oriented metrics and RF algorithm in the software development process for android based intelligent IoT systems.}
}
@article{WEICHENTHAL20193,
title = {A picture tells a thousand…exposures: Opportunities and challenges of deep learning image analyses in exposure science and environmental epidemiology},
journal = {Environment International},
volume = {122},
pages = {3-10},
year = {2019},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2018.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0160412018322001},
author = {Scott Weichenthal and Marianne Hatzopoulou and Michael Brauer},
abstract = {Background
Artificial intelligence (AI) is revolutionizing our world, with applications ranging from medicine to engineering.
Objectives
Here we discuss the promise, challenges, and probable data sources needed to apply AI in the fields of exposure science and environmental health. In particular, we focus on the use of deep convolutional neural networks to estimate environmental exposures using images and other complementary data sources such as cell phone mobility and social media information.
Discussion
Characterizing the health impacts of multiple spatially-correlated exposures remains a challenge in environmental epidemiology. A shift toward integrated measures that simultaneously capture multiple aspects of the urban built environment could improve efficiency and provide important insights into how our collective environments influence population health. The widespread adoption of AI in exposure science is on the frontier. This will likely result in new ways of understanding environmental impacts on health and may allow for analyses to be efficiently scaled for broad coverage. Image-based convolutional neural networks may also offer a cost-effective means of estimating local environmental exposures in low and middle-income countries where monitoring and surveillance infrastructure is limited. However, suitable databases must first be assembled to train and evaluate these models and these novel approaches should be complemented with traditional exposure metrics.
Conclusions
The promise of deep learning in environmental health is great and will complement existing measurements for data-rich settings and could enhance the resolution and accuracy of estimates in data poor scenarios. Interdisciplinary partnerships will be needed to fully realize this potential.}
}
@article{GUEVARA2020102596,
title = {On the classification of fog computing applications: A machine learning perspective},
journal = {Journal of Network and Computer Applications},
volume = {159},
pages = {102596},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102596},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520300709},
author = {Judy C. Guevara and Ricardo da S. Torres and Nelson L.S. {da Fonseca}},
keywords = {Fog computing, Edge computing, Cloud computing, Internet of things, Scheduling, Classes of service, Quality of service, Machine learning, Feature selection, Attribute noise, Classification algorithms},
abstract = {Currently, Internet applications running on mobile devices generate a massive amount of data that can be transmitted to a Cloud for processing. However, one fundamental limitation of a Cloud is the connectivity with end devices. Fog computing overcomes this limitation and supports the requirements of time-sensitive applications by distributing computation, communication, and storage services along the Cloud to Things (C2T) continuum, empowering potential new applications, such as smart cities, augmented reality (AR), and virtual reality (VR). However, the adoption of Fog-based computational resources and their integration with the Cloud introduces new challenges in resource management, which requires the implementation of new strategies to guarantee compliance with the quality of service (QoS) requirements of applications. In this context, one major question is how to map the QoS requirements of applications on Fog and Cloud resources. One possible approach is to discriminate the applications arriving at the Fog into Classes of Service (CoS). This paper thus introduces a set of CoS for Fog applications which includes, the QoS requirements that best characterize these Fog applications. Moreover, this paper proposes the implementation of a typical machine learning classification methodology to discriminate Fog computing applications as a function of their QoS requirements. Furthermore, the application of this methodology is illustrated in the assessment of classifiers in terms of efficiency, accuracy, and robustness to noise. The adoption of a methodology for machine learning-based classification constitutes a first step towards the definition of QoS provisioning mechanisms in Fog computing. Moreover, classifying Fog computing applications can facilitate the decision-making process for Fog scheduler.}
}
@article{JAINGOYAL2020462,
title = {A brief Review of Deep Learning Based Approaches for Facial Expression and Gesture Recognition Based on Visual Information},
journal = {Materials Today: Proceedings},
volume = {29},
pages = {462-469},
year = {2020},
note = {National Conference on Smart Materials: Energy and Environment for Smart Cities, NSES-2018, 28th February 2018, Gwalior, India},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.300},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320353931},
author = {Samta {Jain Goyal} and Arving {Kumar Upadhyay} and Rajesh {Singh Jadon}},
keywords = {FER, Conventional FER, Deep learning Based FER, CNN, Spatial features, Temporal features},
abstract = {Psychological researchers and Many Other Researchers have found that body language of a human can provide substantial information in detecting and interpreting emotions. It could express explicit and implicit information mutually of one’s emotional state and intentions over multi-channel modalities. These imperative channels include eye gaze, head movement, facial expression, body posture and gesture and so on. This learning focuses on detecting emotional states from the body language of the hand and face using computer vision and soft computing techniques. The facial expression and gesture recognition have widely used and challenging task in the present scenario. This paper describes brief review of all approaches which are majorly categorized into two -categories where the first category belongs to Conventional approaches and other based on Deep-learning or Non - Conventional. This paper is basically used as a survey of deep-learning approaches for hand-gesture and Facial Expression Recognitions. We describe all review through the taxonomy of deep-learning approaches proposed architecture details, fusion strategies, datasets, way to treat temporal-dimensions of data, its main features, basic knowledge & general understanding of its challenges}
}
@article{JAN2019275,
title = {Deep learning in big data Analytics: A comparative study},
journal = {Computers & Electrical Engineering},
volume = {75},
pages = {275-287},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315835},
author = {Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon},
keywords = {Big data, Deep learning, Deep belief networks, Convolutional Neural Networks},
abstract = {Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.}
}
@article{DACOSTA2019147,
title = {Internet of Things: A survey on machine learning-based intrusion detection approaches},
journal = {Computer Networks},
volume = {151},
pages = {147-157},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618308739},
author = {Kelton A.P. {da Costa} and João P. Papa and Celso O. Lisboa and Roberto Munoz and Victor Hugo C. {de Albuquerque}},
keywords = {Security networks, Machine learning, Internet-of-Things, Survey, Intelligent techniques, Machine learning},
abstract = {In the world scenario, concerns with security and privacy regarding computer networks are always increasing. Computer security has become a necessity due to the proliferation of information technologies in everyday life. The increase in the number of Internet accesses and the emergence of new technologies, such as the Internet of Things (IoT paradigm, are accompanied by new and modern attempts to invade computer systems and networks. Companies are increasingly investing in studies to optimize the detection of these attacks. Institutions are selecting intelligent techniques to test and verify by comparing the best rates of accuracy. This research, therefore, focuses on rigorous state-of-the-art literature on Machine Learning Techniques applied in Internet-of-Things and Intrusion Detection for computer network security. The work aims, therefore, recent and in-depth research of relevant works that deal with several intelligent techniques and their applied intrusion detection architectures in computer networks with emphasis on the Internet of Things and machine learning. More than 95 works on the subject were surveyed, spanning across different themes related to security issues in IoT environments.}
}
@article{MEI2020119397,
title = {A cost effective solution for pavement crack inspection using cameras and deep neural networks},
journal = {Construction and Building Materials},
volume = {256},
pages = {119397},
year = {2020},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2020.119397},
url = {https://www.sciencedirect.com/science/article/pii/S0950061820314021},
author = {Qipei Mei and Mustafa Gül},
keywords = {Camera, Pavement crack detection, Deep learning, Conditional Wasserstein generative adversarial network, Connectivity map},
abstract = {Automatic crack detection on pavement surfaces is an important research field in the scope of developing an intelligent transportation infrastructure system. In this paper, a cost effective solution for road crack inspection by mounting the commercial grade sport camera, GoPro, on the rear of the moving vehicle is introduced. Also, a novel method called ConnCrack combining conditional Wasserstein generative adversarial network and connectivity maps is proposed for road crack detection. In this method, a 121-layer densely connected neural network with deconvolution layers for multi-level feature fusion is used as generator, and a 5-layer fully convolutional network is used as discriminator. To overcome the scattered output issue related to deconvolution layers, connectivity maps are introduced to represent the crack information within the proposed ConnCrack. The proposed method is tested on a publicly available dataset as well our collected data. The results show that the proposed method achieves state-of-the-art performance compared with other existing methods in terms of precision, recall and F1 score.}
}
@article{CUI2020102620,
title = {Learning traffic as a graph: A gated graph wavelet recurrent neural network for network-scale traffic prediction},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {115},
pages = {102620},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102620},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X19306448},
author = {Zhiyong Cui and Ruimin Ke and Ziyuan Pu and Xiaolei Ma and Yinhai Wang},
keywords = {Traffic forecasting, Deep learning, Graph wavelet, Recurrent neural network, Sparsity, Interpretability},
abstract = {Network-wide traffic forecasting is a critical component of modern intelligent transportation systems for urban traffic management and control. With the rise of artificial intelligence, many recent studies attempted to use deep neural networks to extract comprehensive features from traffic networks to enhance prediction performance, given the volume and variety of traffic data has been greatly increased. Considering that traffic status on a road segment is highly influenced by the upstream/downstream segments and nearby bottlenecks in the traffic network, extracting well-localized features from these neighboring segments is essential for a traffic prediction model. Although the convolution neural network or graph convolution neural network has been adopted to learn localized features from the complex geometric or topological structure of traffic networks, the lack of flexibility in the local-feature extraction process is still a big issue. Classical wavelet transform can detect sudden changes and peaks in temporal signals. Analogously, when extending to the graph/spectral domain, graph wavelet can concentrate more on key vertices in the graph and discriminatively extract localized features. In this study, to capture the complex spatial-temporal dependencies in network-wide traffic data, we learn the traffic network as a graph and propose a graph wavelet gated recurrent (GWGR) neural network. The graph wavelet is incorporated as a key component for extracting spatial features in the proposed model. A gated recurrent structure is employed to learn temporal dependencies in the sequence data. Comparing to baseline models, the proposed model can achieve state-of-the-art prediction performance and training efficiency on two real-world datasets. In addition, experiments show that the sparsity of graph wavelet weight matrices greatly increases the interpretability of GWGR.}
}
@article{SHI20172791,
title = {A Whole System Assessment of Novel Deep Learning Approach on Short-Term Load Forecasting},
journal = {Energy Procedia},
volume = {142},
pages = {2791-2796},
year = {2017},
note = {Proceedings of the 9th International Conference on Applied Energy},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.12.423},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217361738},
author = {Heng Shi and Minghao Xu and Qiuyang Ma and Chi Zhang and Ran Li and Furong Li},
keywords = {Load Forecasting, deep learning, multi-system levels, aggregation, disaggregation, smart metering, deep recurrent neural network},
abstract = {Deep learning has been proven of great potential in various time-series forecasting applications. To exploit the potential and extendibility of deep learning in electricity load forecasting, this paper for the first time presents a comprehensive deep learning assessment on performing load forecasting at different levels through the power systems. The assessment is demonstrated via two extreme cases: 1) regional aggregated demand with an example of New England electricity load data, and 2) disaggregated household demand with examples of 100 individual households from Ireland. The state-of-the-art deep recurrent neural network is implemented for this assessment. Compared with the shallow neural network, the proposed deep model has improved the forecasting accuracy in terms of MAPE by 23% at aggregated level and RMSE by 5% at disaggregated level.}
}
@article{ASADI2020105963,
title = {A spatio-temporal decomposition based deep neural network for time series forecasting},
journal = {Applied Soft Computing},
volume = {87},
pages = {105963},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105963},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619307446},
author = {Reza Asadi and Amelia C. Regan},
keywords = {Spatio-temporal data, Deep learning, Time series forecasting, Traffic flow prediction},
abstract = {Spatio-temporal problems arise in a broad range of applications, such as climate science and transportation systems. These problems are challenging because of unique spatial, short-term and long-term patterns, as well as the curse of dimensionality. In this paper, we propose a deep learning framework for spatio-temporal forecasting problems. We explicitly design the neural network architecture for capturing various types of spatial and temporal patterns, and the model is robust to missing data. In a preprocessing step, a time series decomposition method is applied to separately feed short-term, long-term and spatial patterns into different components of the neural network. A fuzzy clustering method finds clusters of neighboring time series residuals, as these contain short-term spatial patterns. The first component of the neural network consists of multi-kernel convolutional layers which are designed to extract short-term features from clusters of time series data. Each convolutional kernel receives a single cluster of input time series. The output of convolutional layers is concatenated by trends and followed by convolutional-LSTM layers to capture long-term spatial patterns. To have a robust forecasting model when faced with missing data, a pretrained denoising autoencoder reconstructs the model’s output in a fine-tuning step. In experimental results, we evaluate the performance of the proposed model for the traffic flow prediction. The results show that the proposed model outperforms baseline and state-of-the-art neural network models.}
}
@article{WAN2021,
title = {Deep learning-based LPI radar signals analysis and identification using a Nyquist Folding Receiver architecture},
journal = {Defence Technology},
year = {2021},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2021.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S2214914721001835},
author = {Tao Wan and Kai-li Jiang and Hao Ji and Bin Tang},
keywords = {Nyquist folding receiver, Ultra-wideband, Deep learning, Time-frequency analysis, Identification, Classification},
abstract = {Nyquist Folding Receiver (NYFR) is a perceptron structure that realizes a low probability of intercept (LPI) signal analog to information. Aiming at the problem of LPI radar signal receiving, the time domain, frequency domain, and time-frequency domain problems of signals intercepted by NYFR structure are studied. Combined with the time-frequency analysis (TFA) method, a radar recognition scheme based on deep learning (DL) is introduced, which can reliably classify common LPI radar signals. First, the structure of NYFR and its characteristics in the time domain, frequency domain, and time and frequency domain are analyzed. Then, the received signal is then converted into a time-frequency image (TFI). Finally, four kinds of DL algorithms are used to classify LPI radar signals. Simulation results demonstrate the correctness of the NYFR structure, and the effectiveness of the proposed recognition method is verified by comparison experiments.}
}
@article{HAO201976,
title = {Recurrent convolutional neural network based multimodal disease risk prediction},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {76-83},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18318843},
author = {Yixue Hao and Mohd Usama and Jun Yang and M. Shamim Hossain and Ahmed Ghoneim},
keywords = {Convolution neural network, Deep learning, Healthcare, Multimodal fusion},
abstract = {With the rapid growth of biomedical and healthcare data, machine learning methods are used in more and more work to predict disease risk. However, most works use single-mode data to predict disease risk and only few works use multimodal data to predict disease risk. Thus, a new multimodal data-based recurrent convolutional neural network (MD-RCNN) for disease risk prediction is proposed. This model not only can use patient’s structured data and text data, but also can extract structured and unstructured features in fine-grained. Furthermore, in order to obtain the highly non-linear relationships between structured data and unstructured data, we use deep belief network (DBN)to fuse the features. Finally, we experiment with the medical big data of a Chinese two grade hospital during 2013–2015. Experimental results show that the accuracy of MD-RCNN algorithm can reaches 96% and outperforms several state-of-the-art methods.}
}
@article{XIANG2019488,
title = {Machine learning based optimization for vehicle-to-infrastructure communications},
journal = {Future Generation Computer Systems},
volume = {94},
pages = {488-495},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.10.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18318326},
author = {Wei Xiang and Tao Huang and Wanggen Wan},
keywords = {Vehicle-to-infrastructure, Internet of vehicles, Single-antenna system, Multi-antenna system, Machine learning, Information exchange},
abstract = {In this paper, we study wireless communications in vehicle-to-infrastructure communications. In certain situations, multiple vehicles within a local range need to exchange information via common roadside infrastructure. Example scenarios include busy intersections, and a driver with the knowledge of information from other vehicles can make safer decisions. Fast and reliable communications are essential in such use cases. We consider two different system models in this paper. In the first model, we consider the case where both the base station and vehicles are equipped with a single antenna. In the second model, we discuss the case where multiple antennas are installed on both the base station and vehicles. We show how the system can be optimized in both cases. We then discuss how machine learning can be adopted in both models to realize the optimized system performance.}
}
@article{JAISWAL2021,
title = {Real time analysis of Intelligent placing system for vehicles using IOT with Deep learning},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.443},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321040438},
author = {Sushma Jaiswal and Dilip {Kumar Sharma} and Tarun Jaiswal and Bhimraj Basumatary and Mohit Tiwari and Tripti Tiwari},
keywords = {Real Tisme Intelligent Placing, Image processing, Internet of Things, Cloud computing},
abstract = {There is an enormous in number of vehicles in last few years. So, it becomes important to make effective use of technology to enable inconvenience free parking at public and/or private places. In traditional placing systems, drivers face difficulty in finding available placing slots. These systems ignore the fact of placing the vehicles on roads, time management in peak hours, wrong parking of a vehicle in a placing slot. Moreover, the traditional systems require more human intervention in a placing zone. To deal with above said issues, there is an urgent requirement of developing Real Time Intelligent placing system. In this proposed system a Real Time Intelligent placing system based on IOT and Deep learning techniques to answer the real time management of placing and uncertainties. The proposed solution utilizes smart sensors, cloud computing and cyber physical system. Development of graphical user interface for administrator and end-user is a major challenge as it requires ensuring smooth monitoring, control and security of placing system. Moreover, it needs to establish effortless coordination with an end-user. The proposed system is successful in smartly addressing the challenges such as indicating status of placing slot well in advance to end-user, use of reserved and unreserved placing slots, wrong placing, unauthorized placing, real time analysis of free and occupied slots, detecting multiple objects in a placing slot such as bike in car slot, fault detection in one or more components and traffic management during peak hours. The system reduces the human work, saves time, money and energy.}
}
@article{QIU2021116940,
title = {Scalable coordinated management of peer-to-peer energy trading: A multi-cluster deep reinforcement learning approach},
journal = {Applied Energy},
volume = {292},
pages = {116940},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116940},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921004189},
author = {Dawei Qiu and Yujian Ye and Dimitrios Papadaskalopoulos and Goran Strbac},
keywords = {Energy management, Distributed energy resources, Multi-agent deep reinforcement learning, Peer-to-peer energy trading, Smart grid},
abstract = {The increasing penetration of small-scale distributed energy resources (DER) has the potential to support cost-efficient energy balancing in emerging electricity systems, but is also fundamentally affecting the conventional operation paradigm of the latter. In this context, innovative market mechanisms need to be devised to better coordinate and provide incentives for DER to utilize their flexibility. Peer-to-Peer (P2P) energy trading has emerged as an alternative approach to facilitate direct trading between consumers and prosumers interacting in an energy collective and fosters more efficient local demand–supply balancing. While previous research has primarily focused on the technical and economic benefits of P2P trading, little effort has been made towards the incorporation of prosumers’ heterogeneous characteristics in the P2P trading problem. Here, we address this research gap by classifying the participating prosumers into multiple clusters with regard to their portfolio of DER, and analyzing their trading decisions in a simulated P2P trading platform. The latter employs the mid-market rate (MMR) local pricing mechanism to enable energy trading among prosumers and penalizes the contribution to the system demand peak of each prosumer. We formulate the P2P trading problem as a multi-agent coordination problem and propose a novel multi-agent deep reinforcement learning (MADRL) method to address it. The proposed method is founded on the combination of the multi-agent deep deterministic policy gradient (MADDPG) algorithm and the technique of parameter sharing (PS), which not only enables accelerating the training speed by sharing experiences and learned policies between all agents in each cluster, but also sustains the policies’ diversity between multiple clusters. To address the non-stationarity and computational complexity of MADRL as well as persevering the privacy of prosumers, the P2P trading platform acts as a trusted third party which augments the market collective trading information to help training of prosumer agents. Experiments with a large-scale real-world data-set involving 300 residential households demonstrate that the proposed MADRL method exhibits a strong generalization capability in the test data-set and outperforms the state-of-the-art MADRL methods with regard to the system operation cost, demand peak as well as computational time.}
}
@article{VIVEK2022105575,
title = {Urban road network vulnerability and resilience to large-scale attacks},
journal = {Safety Science},
volume = {147},
pages = {105575},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105575},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004173},
author = {Skanda Vivek and Hannah Conner},
keywords = {Cyber-attacks, Urban road networks, Complex networks, Critical Infrastructure, Unsupervised machine learning, Smart city safety},
abstract = {The rise of connected vehicles and intelligent transportation lead to the emergence of novel complex risks. Of particular concern is the potential for large-scale attacks to disrupt road transportation, which is the lifeline of cities. This concern has only been growing with the increase in cybersecurity incidents and disinformation attacks in related infrastructures. In this study, we develop a framework to quantify, detect, and mitigate cascading consequences of attacks on road transportation networks. Application of our framework to the road network of Boston reveals that targeted attacks on a small fraction of nodes leads to disproportionately larger disruptions of routes. We develop an unsupervised machine learning algorithm based on network percolation theory and density based clustering (P-DBSCAN) to quantify risk for urban networks based on real-time traffic data. Our study illustrates a holistic approach to build resilience in existing road networks to attacks. Finally, we discuss the applicability of our framework in other smart city infrastructures.}
}
@article{AZHARI20201141,
title = {Higgs Boson Discovery using Machine Learning Methods with Pyspark},
journal = {Procedia Computer Science},
volume = {170},
pages = {1141-1146},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304907},
author = {Mourad Azhari and Abdallah Abarda and Badia Ettaki and Jamal Zerouaoui and Mohamed Dakkon},
keywords = {Boson Higgs, Spark, Pyspark, Machine Learning (ML), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Gradient Boosted Tree (GBT), AUC, Accuracy},
abstract = {Higgs Boson is an elementary particle that gives the mass to everything in the natural world. The discovery of the Higgs Boson is a major challenge for particle physics. This paper proposes to solve the Higgs Boson Classification Problem with four Machine Learning (ML) Methods, using the Pyspark environment: Logistic Regression (LR), Decision Tree (DT), Random Forest (RF) and Gradient Boosted Tree (GBT). We compare the accuracy and AUC metrics of those ML Methods. We use a large dataset as Higgs Boson, collected from public site UCI and Higgs dataset downloaded from Kaggle site, in the experimentation stage.}
}
@article{DESSI2021253,
title = {Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {253-264},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033003X},
author = {Danilo Dessì and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
abstract = {The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we (i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, (iii) show the advantage of such an hybrid system over alternative approaches, and (vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.}
}
@article{VAZ2021102319,
title = {Machine learning for analysis of wealth in cities: A spatial-empirical examination of wealth in Toronto},
journal = {Habitat International},
volume = {108},
pages = {102319},
year = {2021},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2021.102319},
url = {https://www.sciencedirect.com/science/article/pii/S0197397521000084},
author = {Eric Vaz and Fernando Bação and Bruno Damásio and Malik Haynes and Elissa Penfound},
keywords = {GIS, Regional analysis, Spatial analysis, Wealth habitats, Regional disparities, Toronto},
abstract = {Wealth in the Greater Toronto Area (GTA) continues to grow each year as Toronto's consumer market and population increase. Using a machine learning segmentation based on self-organizing maps, this paper examines the demographics, socioeconomics, and expenditure consumption patterns of the GTA's consumers. The results suggest that SOM may contribute to efficient spatial delimitation tools, enhancing the spatial patterns of clusters in the city of Toronto. The relation to urban areas displays locational neighbourhood characteristics, where the accumulation of wealth is present, pointing out a striking spatial-morphological division between census regions and geographical distribution of wealth in Toronto. In this sense, concerning regional and urban habitats, SOM position themselves as promising tools to measure wealth within highly dense urban cores with significant demographic diversity. While cities that have witnessed rapid urbanization and population growth, such as Toronto, may benefit from integrative methods that use machine learning and spatial analysis to monitor regional and urban disparities.}
}
@article{YANG2021170,
title = {Bridge health anomaly detection using deep support vector data description},
journal = {Neurocomputing},
volume = {444},
pages = {170-178},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.08.087},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221001211},
author = {JianXi Yang and Fei Yang and Likai Zhang and Ren Li and Shixin Jiang and Guiping Wang and Le Zhang and Zeng Zeng},
keywords = {Bridge health, Anomaly detection, Deep learning},
abstract = {As an extremely important part of traffic arteries, bridge structure plays an essential role in national economic construction, social development and smart city. Thus the monitoring of the bridge structure health are increasingly concerned by the bridge industry scholars and engineering people at home and aboard. In this paper, we propose a deep learning framework to evaluate the safety of the bridge structural state. More specifically, the proposed system generates a learnable transformation which attempts to map most of the data network representations into a hypersphere characterized of minimum volume. During inference, mappings of normal examples fall within the learned hypersphere, whereas mappings of anomalies fall outside the hypersphere. The whole system is end-to-end trainable and outperforms other advanced methods in real-world dataset.}
}
@article{MUNIKOTI2022211,
title = {Scalable graph neural network-based framework for identifying critical nodes and links in complex networks},
journal = {Neurocomputing},
volume = {468},
pages = {211-221},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.10.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221015113},
author = {Sai Munikoti and Laya Das and Balasubramaniam Natarajan},
keywords = {Node prediction, Link prediction, Graph neural network, Robustness, Resilience},
abstract = {Identifying critical nodes and links in graphs is a crucial task. These nodes/links typically represent critical elements/communication links that play a key role in a system’s performance. However, a majority of the methods available in the literature on the identification of critical nodes/links are based on an iterative approach that explores each node/link of a graph at a time, repeating for all nodes/links in the graph. Such methods suffer from high computational complexity and the resulting analysis is also network-specific. To overcome these challenges, this article proposes a scalable and generic graph neural network (GNN) based framework for identifying critical nodes/links in large complex networks. The proposed framework defines a GNN based model that learns the node/link criticality score on a small representative subset of nodes/links. An appropriately trained model can be employed to predict the scores of unseen nodes/links in large graphs and consequently identify the most critical ones. The scalability of the framework is demonstrated through prediction of nodes/links scores in large scale synthetic and real-world networks. The proposed approach is fairly accurate in approximating the criticality scores and offers a significant computational advantage over conventional approaches.}
}
@article{GUO2021134,
title = {Dynamic Modification Neural Network Model for Short-term Traffic Prediction},
journal = {Procedia Computer Science},
volume = {187},
pages = {134-139},
year = {2021},
note = {2020 International Conference on Identification, Information and Knowledge in the Internet of Things, IIKI2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.043},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921008231},
author = {Da Guo and Xingwen Xia and Lin Zhu and Yong Zhang},
keywords = {Network traffic prediction, Dynamic adjustment, Series re-fitting},
abstract = {Short-term precise prediction of network traffic can effectively help operators reasonably allocate network resources, improve network service quality and scheduling efficiency. Our model proposed in this paper uses dynamic parameters to adjust the predicted value and refit the predicted sequence for short-term traffic prediction which is called Dynamic Modification Neural Network (DMNN) model. The architecture of our model consists of a prediction module, an adjustment module and a series re-fitting module. The prediction module learns time closeness of the traffic data before the predicted point. The adjustment module takes dynamic period characteristics of data into account and generates adjusted value by linear discrete dynamic parameters. The series re-fitting module refits predicted series from the first two modules with a new hybrid loss function. In this paper, we take different neural networks in the prediction module, such as Long Short-Term Memory (LSTM), Bi-directional Long Short-Term Memory (BiLSTM), Convolutional Long Short-Term Memory (ConvLSTM) and Convolutional Bi-directional Long Short-Term Memory (ConvBiLSTM). We evaluate the performance of the proposed model by using two real-world datasets for short-term traffic flow prediction. Moreover, Experimental results show that the proposed model with dynamic modification has much higher accuracy than other models and decreases the prediction value error and time skew of the inflection points.}
}
@article{ABBASSPOUR2020279,
title = {A Neural Network-based Approach for Detection of Time Delay Switch Attack on Networked Control Systems},
journal = {Procedia Computer Science},
volume = {168},
pages = {279-288},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.250},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303896},
author = {Alireza Abbasspour and Arman Sargolzaei and Mauro Victorio and Navid Khoshavi},
keywords = {Time delay switch attack, network control system, neural networks, load frequency control, attack detection},
abstract = {Recent improvements in communications have led to the proliferation of networked control systems (NCSs). NCSs are used to increase the efficiency and reliability of these systems. However, these connectivities between agents, sensors, and centralized controllers expose NCSs to a range of faults, failures, delays, and cyber-attacks. Detection of intelligent and new types of attacks such as time delay switched (TDS) attacks is a crucial task in the design of NCSs. The injection of the TDS attack to NCSs has the potential to provoke inefficiency or even cause instability in these systems. The TDS or delay signal attack influences the system by introducing a random delay in the process of transmitting and receiving packets. Due to the connectivity between agents, especially in distributed power grids, the injection of a TDS attack on one agent can propagate to others, which can lead to catastrophic consequences. This paper uses a neural networked-based detection algorithm to estimate the TDS attack in real-time. The performance of the proposed TDS attack detection and estimation are evaluated through simulation for two area power systems.}
}
@article{TRIVEDI2021100280,
title = {Vision-based real-time vehicle detection and vehicle speed measurement using morphology and binary logical operation},
journal = {Journal of Industrial Information Integration},
pages = {100280},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100280},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000777},
author = {Janak D. Trivedi and Sarada Devi Mandalapu and Dhara H. Dave},
keywords = {Industrial Transportation, Smart City, Vehicle Detection, Vehicle Speed Measurement, Morphology, Blob analysis},
abstract = {In recent trends, digital information to the industrial integration for the intelligent transportation system (ITS) field is gaining importance for the researcher, academia, and industrial persons. Visual information helps to manage traffic systems in the industrial forum to build smart cities in developed countries. This paper presents vision-based real-time vehicle detection and Vehicle Speed Measurement (VSM) using morphology operation and binary logical process for an unplanned traffic scenario using image processing techniques. Vehicle detection and VSM help to reduce the number of accidents and improve road network efficiency. The bounding box size for vehicle detection is flexible according to vehicles' sizes on the road. We test this system with different colors and dimensions for a selected Region of Interest (ROI). The ROI sets using the two-line approach. Here, we compare the proposed system with the inter-frame difference method and the blob analysis method with recall, precision, and F1 performance parameters.}
}
@article{ISTEPANIAN201834,
title = {m-Health 2.0: New perspectives on mobile health, machine learning and big data analytics},
journal = {Methods},
volume = {151},
pages = {34-40},
year = {2018},
note = {Health Informatics and Translational Data Analytics},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2018.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S1046202318300860},
author = {Robert S.H. Istepanian and Turki Al-Anzi},
abstract = {Mobile health (m-Health) has been repeatedly called the biggest technological breakthrough of our modern times. Similarly, the concept of big data in the context of healthcare is considered one of the transformative drivers for intelligent healthcare delivery systems. In recent years, big data has become increasingly synonymous with mobile health, however key challenges of ‘Big Data and mobile health’, remain largely untackled. This is becoming particularly important with the continued deluge of the structured and unstructured data sets generated on daily basis from the proliferation of mobile health applications within different healthcare systems and products globally. The aim of this paper is of twofold. First we present the relevant big data issues from the mobile health (m-Health) perspective. In particular we discuss these issues from the technological areas and building blocks (communications, sensors and computing) of mobile health and the newly defined (m-Health 2.0) concept. The second objective is to present the relevant rapprochement issues of big m-Health data analytics with m-Health. Further, we also present the current and future roles of machine and deep learning within the current smart phone centric m-health model. The critical balance between these two important areas will depend on how different stakeholder from patients, clinicians, healthcare providers, medical and m-health market businesses and regulators will perceive these developments. These new perspectives are essential for better understanding the fine balance between the new insights of how intelligent and connected the future mobile health systems will look like and the inherent risks and clinical complexities associated with the big data sets and analytical tools used in these systems. These topics will be subject for extensive work and investigations in the foreseeable future for the areas of data analytics, computational and artificial intelligence methods applied for mobile health.}
}
@article{ZHENG2021265,
title = {Efficient face detection and tracking in video sequences based on deep learning},
journal = {Information Sciences},
volume = {568},
pages = {265-285},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S002002552100270X},
author = {Guangyong Zheng and Yuming Xu},
keywords = {Deep learning, Face detection, Face tracking, Regression network, Correction network},
abstract = {Video-based face detection and tracking technology has been widely used in video surveillance, safe driving, and medical diagnosis. In video sequences, most existing face detection and tracking methods face interference caused by occlusion, ambient illumination, and changes in human posture. To accurately track human faces in video sequences, we propose an efficient face detection and tracking framework based on deep learning, which includes a SENResNet face detection model and a Regression Network-based Face Tracking (RNFT) model. Firstly, the SENResNet model integrates the Squeeze and Excitation Network (SEN) with the Residual Neural Network (ResNet). To solve the problem that deep neural networks are difficult to train, we use ResNet to overcome the problem of gradient disappearance in deep network training. To fuse the features of each channel during the convolution operation, we further integrate the SEN module into the SENResNet model. SENResNet accurately detects facial information in each frame and extracts the position of the target face, thereby providing an initialization window for face tracking. Then, the RNFT model extracts facial features from adjacent frames and predict the position of the target face in the next frame. To address the problem of feature scaling, we add a correction network to the RNFT model. The improved RNFT model extracts the rectangular frame of the target face in the previous frame and strengthens the perception of feature scaling, thereby improving its accuracy. Extensive experimental results on public facial and video datasets show that the proposed SENResNet and RNFT models are superior to the state-of-the-art comparison methods in terms of accuracy and performance.}
}
@article{FDESSOARES20191201,
title = {Online travel mode detection method using automated machine learning and feature engineering},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1201-1212},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.056},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19305874},
author = {Elton {F. de S. Soares} and Carlos Alberto {V. Campos} and Sidney {C. de Lucena}},
keywords = {Travel mode detection, Feature engineering, Automated machine learning, Smart mobility, Intelligent transportation systems},
abstract = {Online travel mode detection provides context information useful for location-based services, in order to deliver a customized user experience. In the last years, many smartphone-based travel mode detection techniques have been proposed, but few explored the usage of dimensionality reduction in conjunction with hyperparameter optimization to improve accuracy with a reduced cost. In this paper, we propose a method to improve the accuracy and computational cost trade-off of travel mode detection, in which use state-of-the-art Feature Engineering and Automated Machine Learning techniques. In addition, we apply the proposed method in a real mobility dataset using different features and parameters. Our experiments showed that the combination of these techniques can greatly improve online detection performance.}
}
@article{KARAMAN2021107610,
title = {Development of smart camera systems based on artificial intelligence network for social distance detection to fight against COVID-19},
journal = {Applied Soft Computing},
volume = {110},
pages = {107610},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107610},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005317},
author = {Onur Karaman and Adi Alhudhaif and Kemal Polat},
keywords = {Corona virus (COVID-19), Deep learning, Convolutional neural network (CNN), Transfer learning},
abstract = {In this work, an artificial intelligence network-based smart camera system prototype, which tracks social distance using a bird’s-eye perspective, has been developed. “MobileNet SSD-v3”, “Faster-R-CNN Inception-v2”, “Faster-R-CNN ResNet-50” models have been utilized to identify people in video sequences. The final prototype based on the Faster R-CNN model is an integrated embedded system that detects social distance with the camera. The software developed using the “Nvidia Jetson Nano” development kit and Raspberry Pi camera module calculates all necessary actions in itself, detects social distance violations, makes audible and light warnings, and reports the results to the server. It is predicted that the developed smart camera prototype can be integrated into public spaces within the “sustainable smart cities,” the scope that the world is on the verge of a change.}
}
@article{OLSZEWSKI2021105614,
title = {Digital Agora – Knowledge acquisition from spatial databases, geoinformation society VGI and social media data},
journal = {Land Use Policy},
volume = {109},
pages = {105614},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105614},
url = {https://www.sciencedirect.com/science/article/pii/S0264837721003379},
author = {Robert Olszewski and Agnieszka Wendland},
keywords = {Spatial data mining, VGI, KDD, Social (geo)participation, Smart city, Digital Agora, urban development},
abstract = {The essence of VGI (volunteered geographic information) lies not only in obtaining spatial data from residents (the classic “citizens as sensors” approach (Goodchild, 2007), but also in stimulating them to be active as far as social (geo)participation shaping the development of urban space. The contemporary challenge of the “smart city” era is the creation of a Digital Agora, which facilitates (and analytically supports) not only social debate on spatial planning, but also enabling the acquisition of spatial knowledge. The article presents the application of geoinformation technology and spatial data mining in the analysis of VGI data and knowledge acquisition for the purpose of sustainable urban development. The aim of the article is to propose a general model of digital social debate on the participatory creation of a smart city and sustainable spatial development. This approach can be used to collect spatial data and transform them into useful spatial knowledge. The article discusses two cases of using the Digital Agora model to collect and analyze spatial data.}
}
@article{HUANG2021101487,
title = {An improving sparse coding algorithm for wireless passive target positioning},
journal = {Physical Communication},
volume = {49},
pages = {101487},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2021.101487},
url = {https://www.sciencedirect.com/science/article/pii/S187449072100224X},
author = {Huakun Huang and Chen Zhang and Huijun Wu and Zeyang Dai and Lingjun Zhao and Chunhua Su},
keywords = {Wireless passive localization, Sparse coding, Regularizer},
abstract = {With the developments of smart city, wireless passive localization (WPL) technique that detects targets without carrying any devices draws a lot of research attention. Some machine learning methods, such as sparse coding and deep learning, have been developed to formulate the WPL as a classification problem. Despite these methods have been proved to be effective in WPL, it remains hot topics to design a precise objective function that could detect the position of the targets with high accuracy and robustness. In this paper, we exploited a sparsity regularizer, named log-regularizer, in the classification objective function. By virtue of the distinguished ability in measuring sparsity, our proposed improved sparse coding algorithm (ISCA) with log-regularizer could position targets accurately with robust performance even in the challenging environments Experimental results show that our proposal achieves better results compared with five other machine learning algorithms. Even the input data is severely polluted by noise (SNR = −10 dB), the proposed method could still obtain high localization accuracy of 99.4%.}
}
@article{SAGGI2019387,
title = {Reference evapotranspiration estimation and modeling of the Punjab Northern India using deep learning},
journal = {Computers and Electronics in Agriculture},
volume = {156},
pages = {387-398},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918308779},
author = {Mandeep Kaur Saggi and Sushma Jain},
keywords = {Deep learning, Data analytics, GBM, Evapotranspiration, MissForest},
abstract = {Over the last decade, the combination of both big data and machine learning research area’s receiving considerable attention and expedite the prospect of the agricultural industry. This research aims to gain insights into a state-of-the-art big data application in smart farming. An essential issue for agriculture planning is to estimate evapotranspiration accurately because it plays a pivotal role in irrigation water scheduling for using water efficiently. This article presents H2O model framework to determine the daily ETo for Hoshiarpur and Patiala districts of Punjab. The effects of four supervised learning algorithms: Deep Learning-Multilayer Perceptrons (DL), Generalized Linear Model (GLM), Random Forest (RF), and Gradient-Boosting Machine (GBM) and also evaluate the overall ability to predict future ETo. Analysis of these four models, perform in H2O framework. This framework presents a new criterion to train, validate, test and improve the classification efficiency using machine learning algorithms. The performance of the DL model is compared with other state-of-art of models such as RF, GLM and GBM. In this respect, our analysis depicts that models presents high performance for modeling daily ETo (e.g. NSE = 0.95–0.98, r2 = 0.95–0.99, ACC = 85–95, MSE = 0.0369–0.1215, RMSE = 0.1921–0.2691).}
}
@article{MOUSTAFA2021102994,
title = {A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets},
journal = {Sustainable Cities and Society},
volume = {72},
pages = {102994},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102994},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721002808},
author = {Nour Moustafa},
keywords = {Smart cities, Network datasets, Cybersecurity applications, Machine learning, Edge, Software-Defined Network (SDN), Network Function Virtualization (NFV), Service Orchestration (SO)},
abstract = {While there has been a significant interest in understanding the cyber threat landscape of Internet of Things (IoT) networks, and the design of Artificial Intelligence (AI)-based security approaches, there is a lack of distributed architecture led to generating heterogeneous datasets that contain the actual behaviors of real-world IoT networks and complex cyber threat scenarios to evaluate the credibility of the new systems. This paper presents a novel testbed architecture of IoT network which can be used to evaluate Artificial Intelligence (AI)-based security applications. The platform NSX vCloud NFV was employed to facilitate the execution of Software-Defined Network (SDN), Network Function Virtualization (NFV) and Service Orchestration (SO) to offer dynamic testbed networks, which allow the interaction of edge, fog and cloud tiers. While deploying the architecture, real-world normal and attack scenarios are executed to collect labeled datasets. The generated datasets are named ‘TON_IoT’, as they comprise heterogeneous data sources collected from telemetry datasets of IoT services, Windows and Linux-based datasets, and datasets of network traffic. The TON_IoT network dataset is validated using four machine learning-based intrusion detection algorithms of Gradient Boosting Machine, Random Forest, Naive Bayes, and Deep Neural Networks, revealing a high performance of detection accuracy using the set of training and testing. A comparative summary of the TON_IoT network dataset and other competing network datasets demonstrates its diverse legitimate and anomalous patterns that can be used to better validate new AI-based security solutions. The architecture and datasets can be publicly accessed from TON_IOT Datasets (2020).}
}
@article{NUTKIEWICZ20181176,
title = {Data-driven Urban Energy Simulation (DUE-S): A framework for integrating engineering simulation and machine learning methods in a multi-scale urban energy modeling workflow},
journal = {Applied Energy},
volume = {225},
pages = {1176-1189},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918307165},
author = {Alex Nutkiewicz and Zheng Yang and Rishee K. Jain},
keywords = {Building energy, Data-driven, Machine learning, Multi-scale, Simulation, Urban energy modeling, Urban context},
abstract = {The world is rapidly urbanizing, and the energy intensive built environment is becoming increasingly responsible for the world’s energy consumption and associated environmental emissions. As a result, significant efforts have been put forth to develop methods that can accurately model and characterize building energy consumption in cities. These models aim to utilize physics-based building energy simulations, reduced-order calculations and statistical learning methods to assess the energy performance of buildings within a dense urban area. However, current urban building energy models are limited in their ability to account for the inter-building energy dynamics and urban microclimate factors that can have a substantial impact on building energy use. To overcome these limitations, this paper proposes a novel Data-driven Urban Energy Simulation (DUE-S) framework that integrates a network-based machine learning algorithm (ResNet) with engineering simulation to better understand how buildings consume energy on multiple temporal (hourly, daily, monthly) and spatial scales in a city (single building, block, urban). We validate the proposed DUE-S framework on a proof of concept case study of 22 densely located university buildings in California, USA. Our results indicate that the DUE-S framework is able to accurately predict urban scale energy consumption at hourly, daily and monthly intervals. Moreover, our results also demonstrate that the integration of data-driven and engineering simulation approaches can partially capture the inter-building energy dynamics and impacts of the urban context and merits future work to explore how they can be improved to predict sub-urban scale energy predictions (single building, block). In the end, successfully predicting and modeling the energy performance of urban buildings has the potential to inform the decision-making of a wide variety of urban sustainability stakeholders including architects, engineers and policymakers.}
}
@article{HAN2019101748,
title = {A review of reinforcement learning methodologies for controlling occupant comfort in buildings},
journal = {Sustainable Cities and Society},
volume = {51},
pages = {101748},
year = {2019},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101748},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719307589},
author = {Mengjie Han and Ross May and Xingxing Zhang and Xinru Wang and Song Pan and Da Yan and Yuan Jin and Liguo Xu},
keywords = {Reinforcement learning, Control, Building, Indoor comfort, Occupant},
abstract = {Classical building control systems are becoming vulnerable with increasing complexities in contemporary built environments and energy systems. Due to this, the reinforcement learning (RL) method is becoming more distinctive and applicable in control networks for buildings. This paper, therefore, conducts a comprehensive review of RL techniques applied in control systems for occupant comfort in indoor built environments. The empirical applications of RL-based control systems are presented, depending on comfort objectives (thermal comfort, indoor air quality, and lighting) along with other objectives which invariably includes energy consumption. The class of RL algorithms and implementation details regarding how the value functions have been represented and how the policies are improved are also illustrated. This paper shows there are limited works for which RL has been explored for controlling occupant comfort, especially in indoor air quality and lighting. Relatively few of the reviewed works incorporate occupancy patterns and/or occupant feedback into the control loop. Moreover, this paper identifies a gap with regard to the performance of implementing cooperative multi-agent RL (MARL). Based on our findings, current challenges and further opportunities are discussed. We expect to clarify the feasible theory and functions of RL for building control systems, which would promote their wider-spread application in built environments.}
}
@article{YUVARAJ2021107186,
title = {Automatic detection of cyberbullying using multi-feature based artificial intelligence with deep decision tree classification},
journal = {Computers & Electrical Engineering},
volume = {92},
pages = {107186},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107186},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621001877},
author = {Natarajan Yuvaraj and Victor Chang and Balasubramanian Gobinathan and Arulprakash Pinagapani and Srihari Kannan and Gaurav Dhiman and Arsath Raja Rajan},
keywords = {Smart city, Cyberbullying detection, Deep neural network, Decision trees, Artificial intelligence},
abstract = {Recent studies have shown that cyberbullying is a rising youth epidemic. In this paper, we develop a novel automated classification model that identifies the cyberbullying texts without fitting them into large dimensional space. On the other hand, a classifier .cannot provide a limited convergent solution due to its overfitting problem. Considering such limitations, we developed a text classification engine that initially pre-processes the tweets, eliminates noise and other background information, extracts the selected features and classifies without data overfitting. The study develops a novel Deep Decision Tree classifier that utilizes the hidden layers of Deep Neural Network (DNN) as its tree node to process the input elements. The validation confirms the accuracy of classification using the novel Deep classifier with its improved text classification accuracy.}
}
@article{ZIVKOVIC2021102669,
title = {COVID-19 cases prediction by using hybrid machine learning and beetle antennae search approach},
journal = {Sustainable Cities and Society},
volume = {66},
pages = {102669},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102669},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720308842},
author = {Miodrag Zivkovic and Nebojsa Bacanin and K. Venkatachalam and Anand Nayyar and Aleksandar Djordjevic and Ivana Strumberger and Fadi Al-Turjman},
keywords = {COVID-19, Beetle antennae search, ANFIS, Machine learning, Swarm intelligence, Enhanced, Prediction},
abstract = {The main objective of this paper is to further improve the current time-series prediction (forecasting) algorithms based on hybrids between machine learning and nature-inspired algorithms. After the recent COVID-19 outbreak, almost all countries were forced to impose strict measures and regulations in order to control the virus spread. Predicting the number of new cases is crucial when evaluating which measures should be implemented. The improved forecasting approach was then used to predict the number of the COVID-19 cases. The proposed prediction model represents a hybridized approach between machine learning, adaptive neuro-fuzzy inference system and enhanced beetle antennae search swarm intelligence metaheuristics. The enhanced beetle antennae search is utilized to determine the parameters of the adaptive neuro-fuzzy inference system and to improve the overall performance of the prediction model. First, an enhanced beetle antennae search algorithm has been implemented that overcomes deficiencies of its original version. The enhanced algorithm was tested and validated against a wider set of benchmark functions and proved that it substantially outperforms original implementation. Afterwards, the proposed hybrid method for COVID-19 cases prediction was then evaluated using the World Health Organization’s official data on the COVID-19 outbreak in China. The proposed method has been compared against several existing state-of-the-art approaches that were tested on the same datasets. The proposed CESBAS-ANFIS achieved R2 score of 0.9763, which is relatively high when compared to the R2 value of 0.9645, achieved by FPASSA-ANFIS. To further evaluate the robustness of the proposed method, it has also been validated against two different datasets of weekly influenza confirmed cases in China and the USA. Simulation results and the comparative analysis show that the proposed hybrid method managed to outscore other sophisticated approaches that were tested on the same datasets and proved to be a useful tool for time-series prediction.}
}
@article{BAGHEZZA202067,
title = {Activity Recognition in the City using Embedded Systems and Anonymous Sensors},
journal = {Procedia Computer Science},
volume = {170},
pages = {67-74},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.140},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305962},
author = {R. Baghezza and K. Bouchard and A. Bouzouane and C. Gouin-Vallerand},
keywords = {Activity Recognition, Smart City, Machine Learning, Anonymous Data, Healthcare, Embedded Systems},
abstract = {This paper presents an embedded system that performs activity recognition in the city. Arduino Due boards with infrared, distance and sound sensors are used to collect data in the city and the activity, profile, and group size recognition performance of different machine learning algorithms (RF, SVM, MLP) are compared. The features were extracted based on fixed-size windows around the observations. We show that it is possible to achieve a high accuracy for binary activity recognition with simple features, and we discuss the optimization of different parameters such as the sensors collection frequency, and the storage buffer size. We highlight the challenges of activity recognition using anonymous sensors in the environment, its possible applications and advantages compared to classical smartphone and wearable based approaches, as well as the improvements that will be made in future versions of this system. This work is a first step towards real-time online activity recognition in smart cities, with the long-term goal of monitoring and offering extended assistance for semi-autonomous people.}
}
@article{HUSEIEN2022100116,
title = {A review on 5G technology for smart energy management and smart buildings in Singapore},
journal = {Energy and AI},
volume = {7},
pages = {100116},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100116},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000653},
author = {Ghasan Fahim Huseien and Kwok Wei Shah},
keywords = {5G technology, Sustainability, Smart building, Facilities management, Build environment},
abstract = {Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{EMPARANZA2020100394,
title = {Land cover classification in Thailand's Eastern Economic Corridor (EEC) using convolutional neural network on satellite images},
journal = {Remote Sensing Applications: Society and Environment},
volume = {20},
pages = {100394},
year = {2020},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2020.100394},
url = {https://www.sciencedirect.com/science/article/pii/S2352938520300732},
author = {P. Ruiz Emparanza and N. Hongkarnjanakul and D. Rouquette and C. Schwob and L. Mezeix},
keywords = {Artificial intelligence, Convolutional neural network, Land cover, Image processing},
abstract = {Land cover plays an integral role in urban management as a source of information to support authorities’ decision making. Recently, computer vision methods, machine learning algorithms in particular, are increasingly used for land cover and land use mapping, which can help make these processes more efficient and more affordably. To this end, this paper focuses on leveraging artificial intelligence using convolutional neural network (CNN) to propose a new method for land cover mapping. A first CNN model is trained with a large number of 4 image classes to obtain a land cover model. We then apply the model directly on satellite images that are cropped into images of the same size as the training images. The results of the direct application of the model is not satisfactory; in particular, it confuses water and forested areas. The paper propose a different approach where land cover model is obtained by combining 2 models in series. The first model is a binary-class CNN model which contains classification of two classes (water, land) while the second model is a three-class CNN model containing classification of land, excluding non-water areas. Global accuracy obtained is 98% and 91% for the binary- and three-class CNN model respectively. This approach is used successfully on large area satellite images.}
}
@article{YU2021106592,
title = {Citywide traffic speed prediction: A geometric deep learning approach},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106592},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106592},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307218},
author = {James J.Q. Yu},
keywords = {Traffic speed prediction, Geometric deep learning, Large-scale transportation network, Intelligent transportation system, Data-driven approach},
abstract = {Accurate traffic speed prediction is critical to modern internet of things-based intelligent transportation systems. It serves as the foundation of advanced traffic management systems and travel services. Nonetheless, the large number of roads and sensors impose great computational burden to existing forecast approaches, most of which can only handle one or few roads at a time. In this paper, a novel data-driven deep learning-based approach is proposed for citywide traffic speed prediction. The proposed approach is grounded on recent developments of geometric deep learning techniques to fully utilize the topological information of road networks in the learning process. Specifically, the approach captures the geometric traffic data dependency with graph convolution and attention mechanisms, and the temporal data correlation is extracted and expanded using the encoder–decoder architecture within a generative adversarial learning framework. Comprehensive case studies are conducted with real-world urban road networks and respective data to evaluate its performance, where consistent improvements can be observed over baseline approaches. Lastly, an architectural study is carried out to discover the best-performing structure of the proposed approach, whose sensitivity to data noise and sample frequency is also assessed.}
}
@article{PUSTOKHIN2021107376,
title = {Optimal deep learning approaches and healthcare big data analytics for mobile networks toward 5G},
journal = {Computers & Electrical Engineering},
volume = {95},
pages = {107376},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107376},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621003451},
author = {Denis A. Pustokhin and Irina V. Pustokhina and Poonam Rani and Vineet Kansal and Mohamed Elhoseny and Gyanendra Prasad Joshi and K. Shankar},
keywords = {5G networking, Big data analytics, Map reduce, Disease diagnosis, Feature selection},
abstract = {Recent developments in wireless networking, big data technologies like 5G networks, healthcare big data analytics, Internet of Things (IoT) and other advanced technologies in wearables and Artificial Intelligence (AI) have enabled the development of intelligent disease diagnosis models. The current study devises a new big data analytic-based feature selection and Deep Belief Network (DBN)-based disease diagnostic model. To reduce the number of features and curse of dimensionality, a Link-based Quasi Oppositional Binary Particle Swarm Optimization Algorithm is used in feature selection to narrow down an optimal set of features. The application of quasi-oppositional mechanism in BPSO algorithm helps in increasing the convergence rate. Followed by, the DBN model is applied as a classifier to diagnose the existence of disease from feature-reduced data. A series of experiments was conducted to emphasize the performance of the presented model. The obtained experimental outcomes showcased that the presented model yielded better results in several aspects.}
}
@article{CHEN2021105335,
title = {An interpretable machine learning prognostic system for locoregionally advanced nasopharyngeal carcinoma based on tumor burden features},
journal = {Oral Oncology},
volume = {118},
pages = {105335},
year = {2021},
issn = {1368-8375},
doi = {https://doi.org/10.1016/j.oraloncology.2021.105335},
url = {https://www.sciencedirect.com/science/article/pii/S1368837521001585},
author = {Xi Chen and Yingxue Li and Xiang Li and Xun Cao and Yanqun Xiang and Weixiong Xia and Jianpeng Li and Mingyong Gao and Yuyao Sun and Kuiyuan Liu and Mengyun Qiang and Chixiong Liang and Jingjing Miao and Zhuochen Cai and Xiang Guo and Chaofeng Li and Guotong Xie and Xing Lv},
keywords = {Machine learning, Tumor burden, Prognosis, Therapeutics, Nasopharyngeal carcinoma},
abstract = {Objectives
We aimed to build a survival system by combining a highly-accurate machine learning (ML) model with explainable artificial intelligence (AI) techniques to predict distant metastasis in locoregionally advanced nasopharyngeal carcinoma (NPC) patients using magnetic resonance imaging (MRI)-based tumor burden features.
Materials and methods
1643 patients from three hospitals were enrolled according to set criteria. We employed ML to develop a survival model based on tumor burden signatures and all clinical factors. Shapley Additive exPlanations (SHAP) was utilized to explain prediction results and interpret the complex non-linear relationship among features and distant metastasis. We also constructed other models based on routinely used cancer stages, Epstein-Barr virus (EBV) DNA, or other clinical features for comparison. Concordance index (C-index), receiver operating curve (ROC) analysis and decision curve analysis (DCA) were executed to assess the effectiveness of the models.
Results
Our proposed system consistently demonstrated promising performance across independent cohorts. The concordance indexes were 0.773, 0.766 and 0.760 in the training, internal validation and external validation sets. SHAP provided personalized protective and risk factors for each NPC patient and uncovered some novel non-linear relationships between features and distant metastasis. Furthermore, high-risk patients who received induction chemotherapy (ICT) and concurrent chemoradiotherapy (CCRT) had better 5-year distant metastasis-free survival (DMFS) than those who only received CCRT, whereas ICT + CCRT and CCRT had similar DMFS in low-risk patients.
Conclusions
The interpretable machine learning system demonstrated superior performance in predicting metastasis in locoregionally advanced NPC. High-risk patients might benefit from ICT.}
}
@article{AHMAD2020102010,
title = {A review on machine learning forecasting growth trends and their real-time applications in different energy systems},
journal = {Sustainable Cities and Society},
volume = {54},
pages = {102010},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.102010},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719335516},
author = {Tanveer Ahmad and Huanxin Chen},
keywords = {Forecasting, Machine learning, Supervised models, Renewable energy forecasting, Load demand prediction, Real-time applications},
abstract = {Energy forecasting and planning play an important role in energy sector development and policy formulation. The forecasting model selection mostly based on the availability of the data, and the principal objective is the planning exercise and tool in different modern energy systems. Existing literature explicate that the supervised based machine learning algorithms are intelligent for predictions such as future load demand, wind, solar and geothermal energy forecasting, etc., across the wide range of applications. In this study, a comprehensive review is conducted on supervised based machine learning algorithms by using three well-known forecasting engines. This review aims to suggest suitable methods for forecasting analysis and several other prediciton tasks. A particular objective is to investigate and analyzed the methods used to forecast energy, real time use in multiple applications and to identify the research review with useful techniques that are approachable in the current literature. This review contains a critical comparison and study among various modeling techniques to choose a better forecasting model for performing the desired task in multiple applications. The forecasting accuracy is compared and analyzed through comprehensive literature review and with the real-time energy consumption and climate data used for modeling analysis. The Bayesian regularization backpropagation neural networks (BRBNNs) and the Levenberg Marquardt backpropagation neural networks (LMBNNs) render higher forecasting accuracy and performance with the coefficient of correlation 0.972 and 0.971 respectively. The supervised learning is useful for real-time applications because of optimal scenario of a specific model will allow for the model to precisely predict the label’s class for unseen data instances. Additionally, the supervised learning can reduce the noise and control imbalanced data in the network formulation.}
}
@article{ZHANG2020103,
title = {Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review},
journal = {Information Fusion},
volume = {59},
pages = {103-126},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519302532},
author = {Jianhua Zhang and Zhong Yin and Peng Chen and Stefano Nichele},
keywords = {Emotion recognition, Affective computing, Physiological signals, Feature dimensionality reduction, Data fusion, Machine learning, Deep learning},
abstract = {In recent years, the rapid advances in machine learning (ML) and information fusion has made it possible to endow machines/computers with the ability of emotion understanding, recognition, and analysis. Emotion recognition has attracted increasingly intense interest from researchers from diverse fields. Human emotions can be recognized from facial expressions, speech, behavior (gesture/posture) or physiological signals. However, the first three methods can be ineffective since humans may involuntarily or deliberately conceal their real emotions (so-called social masking). The use of physiological signals can lead to more objective and reliable emotion recognition. Compared with peripheral neurophysiological signals, electroencephalogram (EEG) signals respond to fluctuations of affective states more sensitively and in real time and thus can provide useful features of emotional states. Therefore, various EEG-based emotion recognition techniques have been developed recently. In this paper, the emotion recognition methods based on multi-channel EEG signals as well as multi-modal physiological signals are reviewed. According to the standard pipeline for emotion recognition, we review different feature extraction (e.g., wavelet transform and nonlinear dynamics), feature reduction, and ML classifier design methods (e.g., k-nearest neighbor (KNN), naive Bayesian (NB), support vector machine (SVM) and random forest (RF)). Furthermore, the EEG rhythms that are highly correlated with emotions are analyzed and the correlation between different brain areas and emotions is discussed. Finally, we compare different ML and deep learning algorithms for emotion recognition and suggest several open problems and future research directions in this exciting and fast-growing area of AI.}
}
@article{ZOU2021298,
title = {Detecting individual abandoned houses from google street view: A hierarchical deep learning approach},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {175},
pages = {298-310},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000915},
author = {Shengyuan Zou and Le Wang},
keywords = {Residential housing abandonment, Street view, Knowledge-guided deep learning, Patch-based classification},
abstract = {Abandoned houses (AH) are focal points in urban communities by threatening local security, destroying housing markets, and burdening government finance in the U.S. legacy cities. In particular, individual-level AH detection provides essential information for fine-resolution urban studies, government decision-makers, and private sector practitioners. However, three primary conventional data sources (field data, utility data, and remote sensing data) cannot suffice to collect such fine-resolution data in the large spatial area via a cost-effective approach. To this end, Google Street View (GSV) imagery, which emerges as the mainstream open-access data source with global coverage, provides an opportunity to address this issue. Subsequently, a follow-up challenge confronting the detection of AH arises from the fact that it lacks an effective method that can discern authentic visual features from the redundant noise in GSV images. In this study, we aim to develop an effective method to detect individual-level AH from GSV imagery. Specifically, we developed a new hierarchical deep learning method to leverage both global and local visual features of AH in the detection. The method can be further divided into three steps: (1) Scene-based classification that can extract global visual features of AH was implemented through fine-tuning a pre-trained deep convolutional neural network (CNN) model. (2) We developed a patch-based classification method that can extract specific local features of AH. In this method, patches were generated from GSV images based on auto-detected local features, followed by being labeled as three categories: building patches, vegetation patches, and others. Two deep CNN models were employed to identify deteriorated building façade patches and overgrown vegetation patches, respectively. (3) Individual-level AH were detected by integrating scene classification results and patch classification results in a decision-tree model. Experimental results showed that the F-score of AH was 0.84 in a well-prepared dataset collected from five different Rust Belt cities. The proposed hierarchical deep learning approach effectively improved the accuracy comparing with the traditional scene-based method. In addition, the proposed method was applied to generate an AH map in a new site in Detroit, MI. Our study demonstrated the feasibility of GSV imagery in AH detection and showed great potential to detect AH in a large spatial extent.}
}
@article{LUO2021102564,
title = {Boundary-Aware graph Markov neural network for semiautomated object segmentation from point clouds},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {104},
pages = {102564},
year = {2021},
issn = {0303-2434},
doi = {https://doi.org/10.1016/j.jag.2021.102564},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002713},
author = {Huan Luo and Quan Zheng and Lina Fang and Yingya Guo and Wenzhong Guo and Cheng Wang and Jonathan Li},
keywords = {Point Cloud, 3D Object Segmentation, Boundary Constraint, Graph Neural Network, Markov Random Field},
abstract = {Due to the advantages of 3D point clouds over 2D optical images, the related researches on scene understanding in 3D point clouds have been increasingly attracting wide attention from academy and industry. However, many 3D scene understanding methods largely require abundant supervised information for training a data-driven model. The acquisition of such supervised information relies on manual annotations which are laborious and arduous. Therefore, to mitigate such manual efforts for annotating training samples, this paper studies a unified neural network to segment 3D objects out of point clouds interactively. Particularly, to improve the segmentation performance on the accurate object segmentation, the boundary information of 3D objects in point clouds are encoded as a boundary energy term in the Markov Random Field (MRF) model. Moreover, the MRF model with the boundary energy term is naturally integrated with the Graphical Neural Network (GNN) to obtain a compact representation for generating the boundary-preserved 3D objects. The proposed method is evaluated on two point clouds datasets obtained from different types of laser scanning systems, i.e. terrestrial laser scanning system and mobile laser scanning system. Comparative experiments show that the proposed method is superior and effective in 3D objects segmentation in different point-cloud scenarios.}
}
@article{MUNAWAR2022151351,
title = {Disruptive technologies as a solution for disaster risk management: A review},
journal = {Science of The Total Environment},
volume = {806},
pages = {151351},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.151351},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721064299},
author = {Hafiz Suliman Munawar and Mohammad Mojtahedi and Ahmed W.A. Hammad and Abbas Kouzani and M.A. Parvez Mahmud},
keywords = {Disaster management, Smart cities, Wireless communication, Cloud computing, Internet of Things, Big data},
abstract = {Integrating disruptive technologies within smart cities improves the infrastructure needed to potentially deal with disasters. This paper provides a perspective review of disruptive technologies such as the Internet of Things (IoT), image processing, artificial intelligence (AI), big data and smartphone applications which are in use and have been proposed for future improvements in disaster management of urban regions. The key focus of this paper is exploring ways in which smart cities could be established to harness the potential of disruptive technologies and improve post-disaster management. The key questions explored are a) what are the gaps or barriers to the utilization of disruptive technologies in the area of disaster management and b) How can the existing methods of disaster management be improved through the application of disruptive technologies. To respond to these questions, a novel framework based on integrated approaches based on big data analytics and AI is proposed for developing disaster management solutions using disruptive technologies.}
}
@article{LI2022103656,
title = {Exploring the association between street built environment and street vitality using deep learning methods},
journal = {Sustainable Cities and Society},
volume = {79},
pages = {103656},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103656},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721009197},
author = {Yunqin Li and Nobuyoshi Yabuki and Tomohiro Fukuda},
keywords = {Street vitality, Built environment, Pedestrian behavior and preference, Scene classification, Semantic segmentation, Multiple object tracking},
abstract = {Street vitality has become an essential indicator for evaluating the attractiveness and potential of the sustainable development of urban blocks, and it can be reflected by the type and the frequency of people's pedestrian activities on the street. While it is recognized that street built environment features affect pedestrian behavior and street vitality, quantifying the impact of these characteristics remains inconclusive. This paper proposes an automated deep learning approach to quantitatively explore the association between the street built environment and street vitality. First, we established a deep learning model for street vitality classification for automatic evaluation of street vitality based on the volumes and activities of pedestrians in the street through multiple object tracking and scene classification. Then, we applied semantic segmentation to measure five selected vitality-related street built environment variables. Finally, a linear regression model was applied to evaluate the built environment variables’ significance and effects on street vitality. To verify our method's accuracy and applicability, we selected a commercial complex in Osaka as an illustrative example. The experimental results highlight that street width and transparency have significant positive effects on street vitality. Compared with traditional methods, our approach is feasible, reliable, transferable, and more efficient.}
}
@article{SHAKEEL2021756,
title = {Internet of things forensic data analysis using machine learning to identify roots of data scavenging},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {756-768},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329782},
author = {P. Mohamed Shakeel and S. Baskar and Hassan Fouad and Gunasekaran Manogaran and Vijayalakshmi Saravanan and Carlos Enrique Montenegro-Marin},
keywords = {Blockchain, Data scavenging, Digital forensics, Internet of things, Logical regression},
abstract = {In this paper, we proposed the blockchain-assisted shared audit framework (BSAF) to analyze digital forensic data in the IoT platform. The proposed framework was designed to detect the source/cause of data scavenging attacks in virtualized resources (VR). The proposed framework implements blockchain technology for access log and control management. Access log information is analyzed for its consistency of adversary event detection using logistic regression (LR) machine learning and cross-validation. An adversary event detected by LR is filtered using cross-validation to retain the precision of data analysis for varying user density and VRs. Experimental results prove the consistency of the proposed method by improving the data analysis, as well as reducing analysis time and the adversary event rate.}
}
@article{ERHAN202164,
title = {Smart anomaly detection in sensor systems: A multi-perspective review},
journal = {Information Fusion},
volume = {67},
pages = {64-79},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303717},
author = {L. Erhan and M. Ndubuaku and M. {Di Mauro} and W. Song and M. Chen and G. Fortino and O. Bagdasar and A. Liotta},
keywords = {Anomaly detection, Machine learning, Sensor systems, Internet of Things, Intelligent sensing},
abstract = {Anomaly detection is concerned with identifying data patterns that deviate remarkably from the expected behavior. This is an important research problem, due to its broad set of application domains, from data analysis to e-health, cybersecurity, predictive maintenance, fault prevention, and industrial automation. Herein, we review state-of-the-art methods that may be employed to detect anomalies in the specific area of sensor systems, which poses hard challenges in terms of information fusion, data volumes, data speed, and network/energy efficiency, to mention but the most pressing ones. In this context, anomaly detection is a particularly hard problem, given the need to find computing-energy-accuracy trade-offs in a constrained environment. We taxonomize methods ranging from conventional techniques (statistical methods, time-series analysis, signal processing, etc.) to data-driven techniques (supervised learning, reinforcement learning, deep learning, etc.). We also look at the impact that different architectural environments (Cloud, Fog, Edge) can have on the sensors ecosystem. The review points to the most promising intelligent-sensing methods, and pinpoints a set of interesting open issues and challenges.}
}
@article{DUAN2020108,
title = {Privacy-Preserving distributed deep learning based on secret sharing},
journal = {Information Sciences},
volume = {527},
pages = {108-127},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.074},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302553},
author = {Jia Duan and Jiantao Zhou and Yuanman Li},
keywords = {Deep neural network, Distributed deep learning, Secure multi-party computation, Privacy preserving, Secret sharing},
abstract = {Distributed deep learning (DDL) naturally provides a privacy-preserving solution to enable multiple parties to jointly learn a deep model without explicitly sharing the local datasets. However, the existing privacy-preserving DDL schemes still suffer from severe information leakage and/or lead to significant increase of the communication cost. In this work, we design a privacy-preserving DDL framework such that all the participants can keep their local datasets private with low communication and computational cost, while still maintaining the accuracy and efficiency of the learned model. By adopting an effective secret sharing strategy, we allow each participant to split the intervening parameters in the training process into shares and upload an aggregation result to the cloud server. We can theoretically show that the local dataset of a particular participant can be well protected against the honest-but-curious cloud server as well as the other participants, even under the challenging case that the cloud server colludes with some participants. Extensive experimental results are provided to validate the superiority of the proposed secret sharing based distributed deep learning (SSDDL) framework.}
}
@article{NASEERQURESHI2020486,
title = {An accurate and dynamic predictive model for a smart M-Health system using machine learning},
journal = {Information Sciences},
volume = {538},
pages = {486-502},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306113},
author = {Kashif {Naseer Qureshi} and Sadia Din and Gwanggil Jeon and Francesco Piccialli},
keywords = {Machine learning, Predictive, Models, M-Health, Classification, SVM, Decision tree, Accuracy},
abstract = {Nowadays, new highly-developed technologies are changing traditional processes related to medical and healthcare systems. Emerging Mobile Health (M-Health) systems are examples of novel technologies based on advanced data communication, deep learning, artificial intelligence, cloud computing, big data, and other machine learning methods. Data are collected from sensor nodes and forwarded to local databases through new technologies that enable cellular networks and then store the information in cloud storage systems. From cloud computing services or medical centres, the data are collected for further analysis. Furthermore, machine learning techniques are being used for accurate prediction of disease analysis and for purposes of classification. This paper presents a detailed overview of M-Health systems, their model and architecture, technologies and applications and also discusses statistical and machine learning approaches. We also propose a secure Android-based architecture to collect patient data, a reliable cloud-based model for data storage. Finally, a predictive model able to classify cardiovascular diseases according to their seriousness will be discussed. Moreover, the proposed prediction model has been compared with existing models in terms of accuracy, sensitivity, and specificity. The experimental results show encouraging results in terms of the proposed predictive model for an M-Health system. Keywords: Machine Learning, Predictive, Models, M-Health, Classification, SVM, Decision Tree, Accuracy}
}
@article{TAHSIEN2020102630,
title = {Machine learning based solutions for security of Internet of Things (IoT): A survey},
journal = {Journal of Network and Computer Applications},
volume = {161},
pages = {102630},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102630},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520301041},
author = {Syeda Manjia Tahsien and Hadis Karimipour and Petros Spachos},
keywords = {Architecture, Attack surfaces, Challenges, Internet of Things, IoT attacks, Machine learning, Security solution},
abstract = {Over the last decade, IoT platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services. Because of easy accessibility and fast-growing demand for smart devices and network, IoT is now facing more security challenges than ever before. There are existing security measures that can be applied to protect IoT. However, traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness. Thus, a strong-dynamically enhanced and up to date security system is required for next-generation IoT system. A huge technological advancement has been noticed in Machine Learning (ML) which has opened many possible research windows to address ongoing and future challenges in IoT. In order to detect attacks and identify abnormal behaviors of smart devices and networks, ML is being utilized as a powerful technology to fulfill this purpose. In this survey paper, the architecture of IoT is discussed, following a comprehensive literature review on ML approaches the importance of security of IoT in terms of different types of possible attacks. Moreover, ML-based potential solutions for IoT security has been presented and future challenges are discussed.}
}
@article{ANBARASAN2020150,
title = {Detection of flood disaster system based on IoT, big data and convolutional deep neural network},
journal = {Computer Communications},
volume = {150},
pages = {150-157},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419310357},
author = {M. Anbarasan and BalaAnand Muthu and C.B. Sivaparthipan and Revathi Sundarasekar and Seifedine Kadry and Sujatha Krishnamoorthy and Dinesh Jackson {Samuel R.} and A. Antony Dasel},
keywords = {Hadoop distributed file system (HDFS), Convolutional deep neural network (CDNN), Normalization, Rule generation, Missing value imputation},
abstract = {Natural disasters could be defined as a blend of natural risks and vulnerabilities. Each year, natural as well as human-instigated disasters, bring about infrastructural damages, distresses, revenue losses, injuries in addition to huge death roll. Researchers around the globe are trying to find a unique solution to gather, store and analyse Big Data (BD) in order to predict results related to flood based prediction system. This paper has proposed the ideas and methods for the detection of flood disaster based on IoT, BD, and convolutional deep neural network (CDNN) to overcome such difficulties. First, the input data is taken from the flood BD. Next, the repeated data are reduced by using HDFS map-reduce (). After removal of repeated data, the data are pre-processed using missing value imputation and normalization function. Then, centred on the pre-processed data, the rule is generated by using a combination of attributes method. At the last stage, the generated rules are provided as the input to the CDNN classifier which classifies them as a) chances for the occurrence of flood and b) no chances for the occurrence of a flood. The outcomes obtained from the proposed CDNN method is compared parameters like Sensitivity, Specificity, Accuracy, Precision, Recall and F-score. Moreover, when the outcomes is compared other existing algorithms like Artificial Neural Network (ANN) & Deep Learning Neural Network (DNN), the proposed system gives is very accurate result than other methods.}
}
@article{KHAN2021107023,
title = {DB-Net: A novel dilated CNN based multi-step forecasting model for power consumption in integrated local energy systems},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {133},
pages = {107023},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107023},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521002635},
author = {Noman Khan and Ijaz Ul Haq and Samee Ullah Khan and Seungmin Rho and Mi Young Lee and Sung Wook Baik},
keywords = {Artificial intelligence, Dilated CNN, Energy, Forecasting, Local energy systems, Multi-step, Power, Smart grid, Smart city, Time series, Transfer learning},
abstract = {In the era of cutting edge technology, excessive demand for electricity is rising day by day, due to the exponential growth of population, electricity reliant vehicles, and home appliances. Precise energy consumption prediction (ECP) and integrated local energy systems (ILES) are critical to boost clean energy management systems between consumers and suppliers. Various obstacles such as environmental factors and occupant behavior affects the performance of existing approaches for long- and short-term ECP. Thus, to address such concerns, we present a novel hybrid network model ‘DB-Net’ by incorporating a dilated convolutional neural network (DCNN) with bidirectional long short-term memory (BiLSTM). The proposed approach allows efficient control of power energy in ILES between consumer and supplier when employed for long- and short-term ECP. The first phase combines data acquisition and refinement procedures into a preprocessing module in which the main goal is to optimize the collected data and to handle outliers. In the next phase, the refined data is passed into DCNN layers for feature encoding followed by BiLSTM layers to learn hidden sequential patterns and decode the feature maps. In the final phase, the DB-Net model forecasts multi-step power consumption (PC), including hourly, daily, weekly, and monthly output. The proposed approach attains better predictive performance than existing methods, thereby confirming its effectiveness.}
}
@article{ANTER2022426,
title = {Real-time epileptic seizure recognition using Bayesian genetic whale optimizer and adaptive machine learning},
journal = {Future Generation Computer Systems},
volume = {127},
pages = {426-434},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003782},
author = {Ahmed M. Anter and Mohamed {Abd Elaziz} and Zhiguo Zhang},
keywords = {Epilepsy, Optimization, Electroencephalogram, Extreme learning machine, WOA},
abstract = {The electroencephalogram (EEG) has been commonly used to identify epileptic seizures, but identification of seizures from EEG remains a challenging task that requires qualified neurophysiologists. It is important to detect seizures in real time, which can be achieved in an internet of things (IoT)-based cloud platform to alert patients of impending seizures. Therefore, in this study, we propose a new model to recognize seizure states (e.g., ictal, preictal, interictal) from EEG in the IoT framework to monitor patients remotely. The proposed model uses an efficient hybrid genetic whale optimization algorithm (GWOA) based on naïve Bayes (NB-GWOA) for feature selection, and an adaptive extreme learning machine (ELM) based on a differential evolutionary (DE) algorithm (DEELM) for classification. In the NB-GWOA method, the genetic algorithm serves to enhance the exploitation of the whale optimization algorithm in the search of the optimal solutions, while the naïve Bayes method is used to determine a fitness function to assess every agent in the search space. GWOA has strong robustness and is capable of finding the best solutions in less than five iterations, so it is suitable for selecting discriminative features from a huge number of neurofeatures obtained from EEG. Further, the classification model is constructed based on ELM, which uses the DE algorithm for a fast and efficient learning solution. Results show that the proposed NB-GWOA-DEELM model can avoid over- and under-fitting and can provide better and more accurate performance in classifying seizure states from EEG than its competitors.}
}
@article{VALERIO2022103330,
title = {Dynamic hard pruning of Neural Networks at the edge of the internet},
journal = {Journal of Network and Computer Applications},
volume = {200},
pages = {103330},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103330},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521003155},
author = {Lorenzo Valerio and Franco Maria Nardini and Andrea Passarella and Raffaele Perego},
keywords = {Artificial neural networks, Pruning, Compression, Resource-constrained devices},
abstract = {Neural Networks (NN), although successfully applied to several Artificial Intelligence tasks, are often unnecessarily over-parametrized. In edge/fog computing, this might make their training prohibitive on resource-constrained devices, contrasting with the current trend of decentralizing intelligence from remote data centres to local constrained devices. Therefore, we investigate the problem of training effective NN models on constrained devices having a fixed, potentially small, memory budget. We target techniques that are both resource-efficient and performance effective while enabling significant network compression. Our Dynamic Hard Pruning (DynHP) technique incrementally prunes the network during training, identifying neurons that marginally contribute to the model accuracy. DynHP enables a tunable size reduction of the final neural network and reduces the NN memory occupancy during training. Freed memory is reused by a dynamic batch sizing approach to counterbalance the accuracy degradation caused by the hard pruning strategy, improving its convergence and effectiveness. We assess the performance of DynHP through reproducible experiments on three public datasets, comparing them against reference competitors. Results show that DynHP compresses a NN up to 10 times without significant performance drops (up to 3.5% additional error w.r.t. the competitors), reducing up to 80% the training memory occupancy.}
}
@article{JIANG2021117797,
title = {Data-driven method based on deep learning algorithm for detecting fat, oil, and grease (FOG) of sewer networks in urban commercial areas},
journal = {Water Research},
volume = {207},
pages = {117797},
year = {2021},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2021.117797},
url = {https://www.sciencedirect.com/science/article/pii/S004313542100991X},
author = {Yiqi Jiang and Chaolin Li and Yituo Zhang and Ruobin Zhao and Kefen Yan and Wenhui Wang},
keywords = {Fat, oil, and grease (FOG), Sewer networks, Deep learning algorithm, Gated recurrent unit (GRU), Multi-source data, global sensitivity analysis (GSA)},
abstract = {The content of fat, oil and grease (FOG) in the sewer network sediments is the key indicator for diagnosing sewer blockage and overflow. However, the traditional FOG detection is time-consuming and costly, and the establishment of mathematical models based on statistical methods to predict the content of FOG fail to provide satisfactory accuracy. Herein, a deep learning algorithm used a data-driven FOG content prediction model is proposed to achieve a more accurate prediction of FOG content. Meanwhile, global sensitivity analysis (GSA) is exploited to evaluate the contribution of input indicators to the output indicator (FOG) in the model, so that some input indicators that have less impact on the prediction performance can be screened out, the best combination of input indicators can be determined, and the operation cost of the model can be reduced. To evaluate the effectiveness of the proposed model, a case study was conducted in a city in southern China. The experimental results indicate that the prediction model obtains good FOG estimations and performs well from a single site to multiple sites with a mean R2 of 0.922, showing a good generalization performance. Through GSA, the key input indicators in the model were identified as pH, water temperature (T), relative humidity (RH), sewage flow (Flow), drinking water supply (DWS), velocity (V) and conductivity (σ), and the input indicators such as air pressure (AP), population (Pop.), and liquid level (LV) can be reduced without affecting the prediction accuracy of the model.}
}
@article{LALOUANI2021109,
title = {Countering radiometric signature exploitation using adversarial machine learning based protocol switching},
journal = {Computer Communications},
volume = {174},
pages = {109-121},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001432},
author = {Wassila Lalouani and Mohamed Younis and Uthman Baroudi},
keywords = {Radiometric signature, RF fingerprinting, Traffic analysis, Distributed beamforming, Adversarial machine learning},
abstract = {A Radiometric signature refers to transceiver specific features that are caused by variations in the manufacturing process even for the same circuit design. While such a radiometric signature constitutes a fingerprint that can be exploited for device authentication, it is a threat to privacy. Particularly, in the realm of wireless networks, an adversary may exploit radio frequency (RF) fingerprinting to identify devices and conduct traffic analysis in order to uncover the topology and categorize the role of various nodes. In this paper, we show how an adversary could employ RF fingerprinting to distinguish among nodes and bypass the provisioned anonymity protection in the network. We analyze the accuracy of RF fingerprinting and highlight how the accuracy affects the success of adversary attacks. To counter such a threat, we propose a novel methodology that requires no hardware changes to the radio transceiver and the associated host device. Our methodology is based on coordinated switching among preset link-layer and physical-layer communication protocols. For the latter, we particularly exploit distributed beamforming. We employ adversarial machine learning to select the protocol configuration for each transmission so that the accuracy of the RF fingerprinting diminishes. We demonstrate the effectiveness of our scheme through simulation and prototype experiments.}
}
@article{LIANG2021103628,
title = {Consumer decision-making and smart logistics planning based on FPGA and convolutional neural network},
journal = {Microprocessors and Microsystems},
volume = {80},
pages = {103628},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103628},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120307754},
author = {Tianbao Liang and Hu Wang},
keywords = {Anomaly detection, Fpga, Smart logistics, Cnn classification, Machine learning},
abstract = {In the fourth Industrial Revolution, cost-effective planning and rational management were the key to the success of the revolution. This paper mainly studies the development and application of models in machine learning technology. The abnormal activities monitored in real time are rectified so that the customer's electronic orders can be displayed through the support of big data, thus laying the foundation for the development of intelligent logistics. Under the data system, an exception model is created and classified and regressed. In this model, the security and stability of customer orders in the network can be automatically detected, and the abnormal data can be analyzed and evaluated. Unusual circumstances of this kind need to be in an intelligent logistics environment, and delivery tasks must be called intuitive for special care. Early detection of abnormal order events is expected to improve the accuracy of delivery planning. To enable new technical solutions, the logistics industry and economic decision-makers often lack the IT background and expertise needed to start developing new systems and technical solutions. Evaluate the benefits of using. Implementation and integration complexity is seen as one of the three major obstacles to the success of the IoT above. This is by hindering long-term investment in new technologies from slowing down digitization.}
}
@article{SALCEDOSANZ2020256,
title = {Machine learning information fusion in Earth observation: A comprehensive review of methods, applications and data sources},
journal = {Information Fusion},
volume = {63},
pages = {256-272},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303171},
author = {S. Salcedo-Sanz and P. Ghamisi and M. Piles and M. Werner and L. Cuadra and A. Moreno-Martínez and E. Izquierdo-Verdiguier and J. Muñoz-Marí and Amirhosein Mosavi and G. Camps-Valls},
keywords = {Earth science, Earth observation, Information fusion, Data fusion, Machine learning, Cloud computing, Gap filling, Remote sensing, Multisensor fusion, Data blending, Social networks},
abstract = {This paper reviews the most important information fusion data-driven algorithms based on Machine Learning (ML) techniques for problems in Earth observation. Nowadays we observe and model the Earth with a wealth of observations, from a plethora of different sensors, measuring states, fluxes, processes and variables, at unprecedented spatial and temporal resolutions. Earth observation is well equipped with remote sensing systems, mounted on satellites and airborne platforms, but it also involves in-situ observations, numerical models and social media data streams, among other data sources. Data-driven approaches, and ML techniques in particular, are the natural choice to extract significant information from this data deluge. This paper produces a thorough review of the latest work on information fusion for Earth observation, with a practical intention, not only focusing on describing the most relevant previous works in the field, but also the most important Earth observation applications where ML information fusion has obtained significant results. We also review some of the most currently used data sets, models and sources for Earth observation problems, describing their importance and how to obtain the data when needed. Finally, we illustrate the application of ML data fusion with a representative set of case studies, as well as we discuss and outlook the near future of the field.}
}
@article{GUIDARA2021102445,
title = {A new deep learning-based distance and position estimation model for range-based indoor localization systems},
journal = {Ad Hoc Networks},
volume = {114},
pages = {102445},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102445},
url = {https://www.sciencedirect.com/science/article/pii/S1570870521000214},
author = {Amir Guidara and Ghofrane Fersi and Maher Ben Jemaa and Faouzi Derbel},
keywords = {Indoor localization, Distance estimation, RSSI, WSN, Deep learning, Artificial Neural Networks (ANN)},
abstract = {Many fine-grained indoor localization systems rely on accurate distance estimation between anchors and a target node to determine its exact position. The Received Signal Strength Indicator (RSSI) is commonly used for distance estimation because it is available in most low cost standard wireless devices. Despite the cost efficiency, the distance estimation accuracy in the RSSI-based ranging model needs to be enhanced, especially indoors. The RSSI is sensitive to multiple indoor factors that fluctuate in time and space and lead therefore to its variation. These factors are the origin of the distance estimation error increase in RSSI-based ranging models which in turn raise the position estimation error. Previous works have presented different in-site self-calibration processes to improve the accuracy of distance estimation using RSSI-to-distance samples. It permits to settle the parameters of the RSSI-based ranging model such as the Path Loss Model (PLM) and to mitigate the changing behavior of the RSSI. However, the RSSI measurement depends not only on the distance between the transmitter and the receiver but also on the indoor ambient temperature and humidity variations. Besides, indoor obstacles such as furniture, metallic surfaces or walls have also an impact on the RSSI measurements. We present in this paper a new RSSI-based indoor ranging model using deep learning on collected in-site samples to ensure efficient and autonomic calibration process. This permits to mitigate disturbing factors such as temperature, humidity and noise in order to increase the accuracy of both distance and position estimations. The experimental results have shown that our ranging model has improved not only the precision of distance estimation but also the position estimation in the range-based indoor localization systems.}
}
@article{GUO201895,
title = {DDA: A deep neural network-based cognitive system for IoT-aided dermatosis discrimination},
journal = {Ad Hoc Networks},
volume = {80},
pages = {95-103},
year = {2018},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S1570870518304992},
author = {Kehua Guo and Ting Li and Runhe Huang and Jian Kang and Tao Chi},
keywords = {Deep neural network, Dermatosis discrimination, Cognitive system, Internet of things},
abstract = {The rapid development of the Internet of Things (IoT) and cognitive cyber-physical systems (CPS) has made people's daily lives more intelligent. Additionally, emerging technologies, such as wearable devices and machine learning, have demonstrated the potential for acquiring and processing large amounts of data from the physical world. In the medical field, effectively utilizing the collected medical data and providing more intelligent systems for doctors and patients to assist in diagnoses have also become important research topics. This paper presents a deep neural network-based cognitive system named DDA (dermatosis discrimination assistant) for classifying the dermatosis images generated by confocal laser scanning microscopes. Considering the lack of labels, we increase the labeled data automatically using an incremental model based on a small amount of labeled data and propose a disease discrimination model to distinguish and diagnose the categories of the disease images. In this system, the diagnoses of seborrheic keratosis (SK) and flat wart (FW) are used as examples, and experiments are conducted using the proposed models. Experimental results show that this system performs almost as well as individual dermatologists and can identify and diagnose other common dermatoses.}
}
@article{MALJKOVIC2020117585,
title = {Determination of influential parameters for heat consumption in district heating systems using machine learning},
journal = {Energy},
volume = {201},
pages = {117585},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.117585},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220306927},
author = {Danica Maljkovic and Bojana Dalbelo Basic},
keywords = {District heating, Consumption prediction accuracy, Forecasting, Machine learning, Variable importance, Energy efficiency},
abstract = {District heating systems are an important part of the future smart energy systems and are seen in the European Union as a vehicle for reaching energy efficiency targets. Integrating different energy systems requires high prediction accuracy for all energy sub-systems. Within this paper a data analysis was made with the goal of identifying a high accuracy prediction model and ranking the most influential parameters on heat consumption of final consumers in district heating systems. The data set consisted of the actual billing data comprising of 260 buildings and it was additionally supplemented by the behavioural data obtained from interviews and questionnaires conducted on the demonstration building in Zagreb, Croatia. The authors choose regression trees, random forest and regression support vector machines as algorithms for testing prediction accuracy and evaluating the variable importance ranking on the data set. The best performing algorithm was random forest, resulting with high prediction accuracy and the root mean squared error of prediction of specific annual heat consumption below 1 kWh/m2. Furthermore, all analysed machine learning algorithms ranked importance variables for both technical and behavioural parameters, giving the indication what parameters should be influenced in order to reach specific targets, such as energy savings.}
}
@article{HOSSAIN201969,
title = {Emotion recognition using deep learning approach from audio–visual emotional big data},
journal = {Information Fusion},
volume = {49},
pages = {69-78},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517307066},
author = {M. Shamim Hossain and Ghulam Muhammad},
abstract = {This paper proposes an emotion recognition system using a deep learning approach from emotional Big Data. The Big Data comprises of speech and video. In the proposed system, a speech signal is first processed in the frequency domain to obtain a Mel-spectrogram, which can be treated as an image. Then this Mel-spectrogram is fed to a convolutional neural network (CNN). For video signals, some representative frames from a video segment are extracted and fed to the CNN. The outputs of the two CNNs are fused using two consecutive extreme learning machines (ELMs). The output of the fusion is given to a support vector machine (SVM) for final classification of the emotions. The proposed system is evaluated using two audio–visual emotional databases, one of which is Big Data. Experimental results confirm the effectiveness of the proposed system involving the CNNs and the ELMs.}
}
@article{LI2021317,
title = {GeoTraPredict: A machine learning system of web spatio-temporal traffic flow},
journal = {Neurocomputing},
volume = {428},
pages = {317-324},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.06.121},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220312029},
author = {Jingjing Li and Jun Li and Nan Jia and Xunchun Li and Wenzhen Ma and Shanshan Shi},
keywords = {Web traffic flow prediction, Machine learning, Spatio-temporal prediction, Cloud computing, D-M-V framework},
abstract = {Traffic flow prediction is an important component for self-driving. Traffic flow is closely related to population distribution, and the traffic flow is not only related to the absolute number of human population but also to their concerns and interests. Accurate spatio-temporal web traffic flow prediction is critical in many applications, such as bandwidth allocation, anomaly detection, congestion control and admission control. Most existing traffic flow prediction methods use models based on time-series analysis and remain inadequate for many real-world applications. Web traffic flow is found to be strongly associated with the spatio-temporal distribution of the population. Increasingly, it is critical to understand and make decisions based on the relationship between population patterns and web traffic flow patterns. It has been proven that different people have different responses to web events. Due to the complexity of spatial data structures and the huge volume of web traffic flow log data, it is difficult to routinely find the relationship between web events and population distributions without an appropriate processing framework. In this paper, we propose an innovative framework named GeoTrafficPredict to support the accurate spatio-temporal prediction of web traffic flow. GeoTrafficPredict provides a machine learning platform to learn the spatio-temporal pattern of traffic flow and use the pattern to predict the trend in both spatial and temporal dimension. Also, GeoTrafficPredict provide data aggregation portal and cloud-based computation function. GeoTrafficPredict deploys a series of computational images in a cloud computing environment, and the implementation on China’s CSTNET illustrates the performance of our platform.}
}
@article{TRIPATHI2021380,
title = {AI in Fighting Covid-19: Pandemic Management},
journal = {Procedia Computer Science},
volume = {185},
pages = {380-386},
year = {2021},
note = {Big Data, IoT, and AI for a Smarter Future},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921011248},
author = {Abhishek Tripathi and Parmeet Kaur and Shwetha Suresh},
keywords = {Pandemic Management, Artificial Intelligence, Machine Learning, Coronavirus, Covid-19, Intelligent Cities, Smart Cities},
abstract = {Coronaviruses are a family of viruses found in several animal species, such as bats, cattle, cats, camels, and humans. With more than 1.6 million people dead worldwide, as of December 2020, the Covid-19 pandemic has brought about a unified need to address global health crises more aggressively. There is great urgency in decreasing the impact of a potential future outbreak, which can be done by gathering information about the disease and its effects on humans. Various artificial intelligence (AI) techniques can be utilized for the pandemic, such as COVID (CoV) management, a vast scientific field involving computers performing tasks capable of only human brains. Among the subsets of AI, there are Machine Learning (ML) techniques, which can learn from historical data examples without programming. While no prior data regarding the virus exists, the growing cases make for more data. In this research, we employ a literature review method to understand pandemic management’s current state and how it can benefit by utilizing AI capabilities.}
}